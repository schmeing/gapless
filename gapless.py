#!/usr/bin/env python3

from Bio import SeqIO
from datetime import timedelta
import getopt
import gzip
from matplotlib import use as mpl_use
mpl_use('Agg')
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.patches as patches
import matplotlib.pyplot as plt
import numpy as np
from operator import itemgetter
import os
import pandas as pd
from PIL import Image, ImageDraw, ImageFont
import re
from scipy.optimize import minimize
from scipy.special import erf
import seaborn as sns
import sys
from time import process_time

def GaplessSplit(fa_file,o_file=False,min_n=False):
    if False == o_file:
        if ".gz" == fa_file[-3:len(fa_file)]:
            o_file = fa_file.rsplit('.',2)[0]+"_split.fa"
        else:
            o_file = fa_file.rsplit('.',1)[0]+"_split.fa"
        pass
    if False == min_n:
        min_n = 1;
        pass
    
    # Remove output files, such that we do not accidentially use an old one after a crash
    if os.path.exists(o_file):
        os.remove(o_file)

    with open(o_file, 'w') as fout:
        with gzip.open(fa_file, 'rt') if 'gz' == fa_file.rsplit('.',1)[-1] else open(fa_file, 'r') as fin:
            for record in SeqIO.parse(fin, "fasta"):
                # Split scaffolds into contigs
                contigs=re.split('([nN]+)',str(record.seq))
                # Split identifier and further description
                seqid=record.description.split(' ', 1 )
                if 2 == len(seqid):
                    seqid[1] = " " + seqid[1] # Add the space here, so in case seqid[1] is empty no trailing space is in the sequence identifier
                else:
                    seqid.append("")
                
                # Combine sequences that do not reach minimum of N's at split sites, attach start and end position to seq id and print it
                start_pos = 0
                seq = ""
                num_n = 0
                for contig in contigs:
                    if -1 == num_n:
                        # Contig of only N's
                        num_n = len(contig)
                    else:
                        # Real contig
                        if num_n < min_n:
                            # Minimum number of N's not reached at this split site: Merging sequences
                            if len(seq):
                                if len(contig):
                                    seq += 'N' * num_n + contig
                            else:
                                seq = contig
                                start_pos += num_n
                             
                        else:
                            # Split valid: Print contig
                            if len(seq): # Ignore potentially empty sequences, when scaffold starts or ends with N
                                fout.write(">{0}_chunk{1}-{2}{3}\n".format(seqid[0],start_pos+1,start_pos+len(seq), seqid[1]))
                                fout.write(seq)
                                fout.write('\n')
                            # Insert new sequence
                            start_pos += len(seq) + num_n
                            seq = contig
                        num_n = -1
                
                # Print final sequence
                if len(seq):
                    if len(seq)==len(record.seq):
                        fout.write(">{0}\n".format(record.description))
                    else:
                        fout.write(">{0}_chunk{1}-{2}{3}\n".format(seqid[0],start_pos+1,start_pos+len(seq), seqid[1]))
                    fout.write(seq)
                    fout.write('\n')

def ReadContigs(assembly_file):
    # Create contig table of assembly
    names = []
    descriptions = []
    seq_lengths = []
    distances_next = []
    contig_end = -1
    last_scaffold = ""

    with gzip.open(assembly_file, 'rt') if 'gz' == assembly_file.rsplit('.',1)[-1] else open(assembly_file, 'r') as fin:
        for record in SeqIO.parse(fin, "fasta"):
            seqid = record.description.split(' ', 1 ) # Split identifier(name) and further description
            names.append(seqid[0])
            if 2 == len(seqid):
                descriptions.append(seqid[1])
            else:
                descriptions.append("") 
            seq_lengths.append( len(record.seq) )

            # Get distance to next contig from the encoding in the identifier (_chunk[start]-[end])
            chunk = seqid[0].rsplit('_chunk')
            if 2 == len(chunk):
                coords = chunk[1].split('-')
                if 2 == len(coords):
                    try:
                        if 0 < contig_end and chunk[0] == last_scaffold:
                            distances_next.append(int(coords[0]) - contig_end - 1)
                        elif 0 <= contig_end:
                            distances_next.append(-1)
    
                        contig_end = int(coords[1])
                        last_scaffold = chunk[0]
                    except ValueError:
                        print('Cannot interpret "_chunk{}-{}" as a position encoding in: '.format(coords[0], coords[1]), record.description)
                        if 0 <= contig_end:
                            distances_next.append(-1)
                        contig_end = 0
                else:
                    print("Position encoding in contig identifier wrong:", record.description)
                    if 0 <= contig_end:
                        distances_next.append(-1)
                    contig_end = 0
            else:
                if 0 <= contig_end:
                    distances_next.append(-1)
                contig_end = 0

        distances_next.append(-1) # Last contig has no next

    contigs = pd.DataFrame({ 'name' : names,
                             'description' : descriptions,
                             'length' : seq_lengths,
                             'org_dist_right' : distances_next })
    contigs['org_dist_left'] = contigs['org_dist_right'].shift(1, fill_value=-1)

    # Create dictionary for mapping names to table row
    contig_ids = {k: v for k, v in zip(names, range(len(names)))}

    return contigs, contig_ids

def calculateN50(len_values):
    # len_values must be sorted from lowest to highest
    len_total = np.sum(len_values)
    len_sum = 0
    len_N50 = 0 # Use as index first
    while len_sum < len_total/2:
        len_N50 -= 1
        len_sum += len_values[len_N50]
    len_N50 = len_values[len_N50]

    return len_N50, len_total

def GetInputInfo(result_info, contigs):
    # Calculate lengths
    con_len = contigs['length'].values

    scaf_len = contigs[['org_dist_left']].copy()
    scaf_len['length'] = con_len
    scaf_len.loc[scaf_len['org_dist_left'] >= 0, 'length'] += scaf_len.loc[scaf_len['org_dist_left'] >= 0, 'org_dist_left']
    scaf_len['scaffold'] = (scaf_len['org_dist_left'] == -1).cumsum()
    scaf_len = scaf_len.groupby('scaffold', sort=False)['length'].sum().values

    # Calculate N50
    con_len.sort()
    con_N50, con_total = calculateN50(con_len)

    scaf_len.sort()
    scaf_N50, scaf_total = calculateN50(scaf_len)

    # Store output stats
    result_info['input'] = {}
    result_info['input']['contigs'] = {}
    result_info['input']['contigs']['num'] = len(con_len)
    result_info['input']['contigs']['total'] = con_total
    result_info['input']['contigs']['min'] = con_len[0]
    result_info['input']['contigs']['max'] = con_len[-1]
    result_info['input']['contigs']['N50'] = con_N50
    
    result_info['input']['scaffolds'] = {}
    result_info['input']['scaffolds']['num'] = len(scaf_len)
    result_info['input']['scaffolds']['total'] = scaf_total
    result_info['input']['scaffolds']['min'] = scaf_len[0]
    result_info['input']['scaffolds']['max'] = scaf_len[-1]
    result_info['input']['scaffolds']['N50'] = scaf_N50

    return result_info

def ReadPaf(file_name):
    return pd.read_csv(file_name, sep='\t', header=None, usecols=range(12), names=['q_name','q_len','q_start','q_end','strand','t_name','t_len','t_start','t_end','matches','alignment_length','mapq'], dtype={'q_name':object, 'q_len':np.int32, 'q_start':np.int32, 'q_end':np.int32, 'strand':str, 't_name':object, 't_len':np.int32, 't_start':np.int32, 't_end':np.int32, 'matches':np.int32, 'alignment_length':np.int32, 'mapq':np.int16})

def stackhist(x, y, **kws):
    grouped = pd.groupby(x, y)
    data = [d for _, d in grouped]
    labels = [l for l, _ in grouped]
    plt.hist(data, histtype="barstacked", label=labels)

def PlotHist(pdf, xtitle, ytitle, values, category=[], catname='category', threshold=None, logx=False, logy=False):
    plt.close()

    if logx:      
        values = np.log10(np.extract(values>0,values))

    if len(category):
        #sns.FacetGrid(pd.DataFrame({"values":values, "category":category}), hue="category")
        ax = sns.distplot(values, bins=100, kde=False, color='w')
        
        cats = np.unique(category)
        cat_ids = {c: i for c,i in zip(cats, range(len(cats)))}
        cat_ids = np.array(itemgetter(*category)(cat_ids))

        for x in range(len(cats)):
            plt.hist(values[cat_ids>=x],bins=np.linspace(min(values),max(values),101))

        plt.legend(handles=[patches.Patch(color=col, label=lab) for col, lab in zip(sns.color_palette().as_hex()[:len(cats)],cats)], title=catname, loc='upper right')
        ax=plt.gca()
    else:
        ax = sns.distplot(values, bins=100, kde=False)
        
    plt.xlabel(xtitle)
    plt.ylabel(ytitle)

    if logx:
        plt.xlim(int(min(values)), int(max(values))+1)
        locs, labels = plt.xticks()
        ax.set(xticklabels=np.where(locs.astype(int) == locs, (10 ** locs).astype(str), ""))

    if threshold:
        if logx:
            plt.axvline(np.log10(threshold))
        else:
            plt.axvline(threshold)

    if logy:
        ax.set_yscale('log', nonpositive='clip')
    else:
        # Set at what range of exponents they are not plotted in exponential format for example (-3,4): [0.001-1000[
        ax.get_yaxis().get_major_formatter().set_powerlimits((-3,4))

    pdf.savefig()

    return

def PlotXY(pdf, xtitle, ytitle, x, y, category=[], count=[], logx=False, linex=[], liney=[]):
    plt.close()

    if logx:    
        y = np.extract(x>0, y)
        if len(liney):
            liney = np.extract(linex>0, liney)
        if len(category):
            category = np.extract(x>0, category)           
        x = np.log10(np.extract(x>0,x))
        if len(linex):
            linex = np.extract(linex>0, linex)

    if len(linex):
        sns.lineplot(x=linex, y=liney, color='red', linewidth=2.5)

    if len(count):
        if len(category):
            ax = sns.scatterplot(x=x, y=y, hue=count, style=category, linewidth=0, alpha = 0.7)
        else:
            ax = sns.scatterplot(x=x, y=y, hue=count, linewidth=0, alpha = 0.7);
    elif len(category):
        ax = sns.scatterplot(x=x, y=y, hue=category, linewidth=0, alpha = 0.7)
    else:
        ax = sns.scatterplot(x=x, y=y, linewidth=0, alpha = 0.7)
    plt.xlabel(xtitle)
    plt.ylabel(ytitle)

    if logx:
        plt.xlim(0, int(max(x))+1)
        locs, labels = plt.xticks()
        ax.set(xticklabels=10 ** locs)

    # Set at what range of exponents they are not plotted in exponential format for example (-3,4): [0.001-1000[
    ax.get_yaxis().get_major_formatter().set_powerlimits((-3,4))

    pdf.savefig()

    return

def LoadRepeats(repeat_file, contig_ids):
    repeats = ReadPaf(repeat_file)
    repeats.drop(columns=['matches','alignment_length','mapq'], inplace=True)
    repeats['q_name'] = itemgetter(*repeats['q_name'])(contig_ids)
    repeats['t_name'] = itemgetter(*repeats['t_name'])(contig_ids)
    repeats.rename(columns={'q_name':'q_id','t_name':'t_id'}, inplace=True)
#
    return repeats

def GetThresholdsFromReadLenDist(mappings, num_read_len_groups):
    # Get read length distribution
    read_len_dist = mappings[['q_id','q_len']].drop_duplicates()
    read_len_dist = read_len_dist['q_len'].sort_values().values

    return read_len_dist[[len(read_len_dist)//num_read_len_groups*i for i in range(1,num_read_len_groups)]]

def GetBinnedCoverage(mappings, length_thresholds):
    # Count coverage for bins of threshold length
    cov_counts = []
    tmp_seqs = mappings[['t_name','t_len']].drop_duplicates().sort_values('t_name').reset_index(drop=True)
    for cur_cov_thres in length_thresholds:
        # Count mappings
        cov_count = mappings.loc[mappings['t_end']-mappings['t_start'] >= cur_cov_thres, ['t_name','t_len','t_start','t_end']].copy()
        cov_count['offset'] = cov_count['t_len']%cur_cov_thres // 2
        cov_count['start_bin'] = (cov_count['t_start'] - cov_count['offset'] - 1) // cur_cov_thres + 1
        cov_count['end_bin'] = (cov_count['t_end'] - cov_count['offset']) // cur_cov_thres
        cov_count = cov_count.loc[cov_count['start_bin'] < cov_count['end_bin'], ['t_name','start_bin','end_bin']]
        cov_count = cov_count.loc[np.repeat(cov_count.index.values, (cov_count['end_bin']-cov_count['start_bin']).values)]
        cov_count.reset_index(inplace=True)
        cov_count['bin'] = cov_count.groupby(['index'], sort=False).cumcount() + cov_count['start_bin']
        cov_count = cov_count.groupby(['t_name','bin'], sort=False).size().reset_index(name='count')
        # Add zero bins
        zero_bins = tmp_seqs.loc[np.repeat(tmp_seqs.index.values, tmp_seqs['t_len']//cur_cov_thres), ['t_name']].reset_index(drop=True)
        zero_bins['bin'] = zero_bins.groupby(['t_name'], sort=False).cumcount()
        cov_count = zero_bins.merge(cov_count, on=['t_name','bin'], how='left').fillna(0)
        cov_count['count'] = cov_count['count'].astype(int)
        # Add to cov_counts
        cov_count['bin_size'] = cur_cov_thres
        cov_counts.append(cov_count)

    return pd.concat(cov_counts, ignore_index=True)

def NormCDF(x, mu, sigma):
    return (1+erf((x-mu)/sigma))/2 # We skip the sqrt(2) factor for the sigma, since sigma will be fitted anyways and we don't care about the value as long as it describes the curve

def CovChi2(par, df): #, expo
    return np.sum((NormCDF(df['x'], par[0], par[1]) - df['y'])**2)
    
def GetCoverageProbabilities(cov_counts, pdf):
    probs = cov_counts.groupby(['bin_size','count']).size().reset_index(name='nbins')
    probs['nbin_cumsum'] = probs.groupby(['bin_size'], sort=False)['nbins'].cumsum()
    cov_bins = probs.groupby(['bin_size'])['nbins'].agg(['sum','size']).reset_index()
    probs['nbin_sum'] = np.repeat( cov_bins['sum'].values, cov_bins['size'].values)

    # Fit CDF with CDF of normal distribution
    probs['lower_half'] = (np.repeat( (cov_bins['sum']*0.5).values.astype(int), cov_bins['size'].values) > probs['nbin_cumsum']).shift(1, fill_value=True) # Shift so that we also get the bin that is only partially in the lower half (Ensures that we always have at least one bin)
    probs.loc[probs['bin_size'] != probs['bin_size'].shift(1), 'lower_half'] = True # The first bin must be True, but likely has been turned to False by the shift 
    prob_len = np.unique(probs['bin_size'])
    prob_mu = []
    prob_sigma = []
    for bsize in prob_len:
        selection = (bsize == probs['bin_size']) & probs['lower_half']
        opt_par = minimize(CovChi2, [probs.loc[selection, 'count'].values[-1],1.0], args=(pd.DataFrame({'x':probs.loc[selection, 'count'], 'y':probs.loc[selection, 'nbin_cumsum']/probs.loc[selection, 'nbin_sum']})), method='Nelder-Mead').x
        prob_mu.append(opt_par[0])
        prob_sigma.append(opt_par[1])
        if pdf:
            x_values = np.arange(probs.loc[selection, 'count'].values[0], probs.loc[selection, 'count'].values[-1]+1)
            PlotXY(pdf, "Max. Coverage", "% Bins (Size: {})".format(bsize), probs.loc[selection, 'count'], probs.loc[selection, 'nbin_cumsum']/probs.loc[selection, 'nbin_sum'], linex=x_values, liney=NormCDF(x_values,opt_par[0],opt_par[1]))

    # Store results in DataFrame
    cov_probs = pd.DataFrame({'length':prob_len, 'mu':prob_mu, 'sigma':prob_sigma})

    return cov_probs

def GetBestSubreads(mappings, read_names, alignment_precision):
    # Identify groups of subread mappings (of which only one is required)
    mappings['subread'] = [ re.sub(r'^(.*?/.*?)/.+', r'\1', qname) for qname in read_names.loc[mappings['q_id'].values, 'read_name'].values ]
    mappings.sort_values(['subread','t_name','t_start','t_end'], inplace=True)
    mappings['group'] = (( (mappings['subread'] != mappings['subread'].shift(1, fill_value="")) | (mappings['t_name'] != mappings['t_name'].shift(1, fill_value="")) | ((mappings['t_start']-mappings['t_start'].shift(1, fill_value=0) > alignment_precision) & (np.abs(mappings['t_end'] - mappings['t_end'].shift(1, fill_value=0)) > alignment_precision)) )).cumsum()
    
    # Keep only the best subread (over all mappings)
    mappings.sort_values(['group','mapq','matches'], ascending=[True,False,False], inplace=True)
    mappings['pos'] = mappings.groupby(['group'], sort=False).cumcount()
    mappings.sort_values(['q_id'], inplace=True)
    tmp = mappings.groupby(['q_id'], sort=False)['pos'].agg(['size','max','mean'])
    mappings['max_pos'] = np.repeat(tmp['max'].values, tmp['size'].values)
    mappings['mean_pos'] = np.repeat(tmp['mean'].values, tmp['size'].values)
    mappings.sort_values(['group','max_pos','mean_pos'], inplace=True)
    mappings = mappings.groupby(['group'], sort=False).first().reset_index()
    
    mappings.drop(columns=['subread','group','pos','max_pos','mean_pos'], inplace=True)
    
    return mappings

def ReadMappings(mapping_file, contig_ids, min_mapq, min_mapping_length, keep_all_subreads, alignment_precision, num_read_len_groups, pdf):
    mappings = ReadPaf(mapping_file)
    if len(mappings) == 0:
        raise RuntimeError("Read mapping file empty.")

    # Assign ids instead of read names
    read_names = mappings[['q_name']].drop_duplicates()
    read_names.reset_index(drop=True, inplace=True)
    mappings['q_name'] = mappings[['q_name']].merge(read_names.reset_index(), on='q_name', how='left')['index'].values
    mappings.rename(columns={'q_name':'q_id'}, inplace=True)
    read_names.rename(columns={'q_name':'read_name'}, inplace=True)

    # Extract coverage
    length_thresholds = GetThresholdsFromReadLenDist(mappings, num_read_len_groups)
    cov_counts = GetBinnedCoverage(mappings, length_thresholds)
    cov_probs = GetCoverageProbabilities(cov_counts, pdf)
    cov_counts.rename(columns={'count':'count_all'}, inplace=True)

    # Filter low mapping qualities
    if pdf:
        PlotHist(pdf, "Mapping quality", "# Mappings", mappings['mapq'], threshold=min_mapq, logy=True)
        PlotHist(pdf, "Mapping length", "# Mappings", mappings['t_end'] - mappings['t_start'], threshold=min_mapping_length, logy=True)
    mappings = mappings[(min_mapq <= mappings['mapq']) & (min_mapping_length <= mappings['t_end'] - mappings['t_start'])].copy()

    # Remove all but the best subread
    if not keep_all_subreads:
        mappings = GetBestSubreads(mappings, read_names, alignment_precision)

    cov_counts = cov_counts.merge(GetBinnedCoverage(mappings, length_thresholds), on=['bin_size','t_name','bin'], how='outer').fillna(0)
    cov_counts['count'] = cov_counts['count'].astype(np.int64)

    mappings['t_id'] = itemgetter(*mappings['t_name'])(contig_ids)
    cov_counts['t_id'] = itemgetter(*cov_counts['t_name'])(contig_ids)

    return mappings, cov_counts, cov_probs, read_names

def RemoveUnmappedContigs(contigs, mappings, remove_zero_hit_contigs):
    # Schedule contigs for removal that don't have any high quality reads mapping to it
    contigs['remove'] = False
    if remove_zero_hit_contigs:
        mapped_reads = np.bincount(mappings['t_id'], minlength=len(contigs))
        contigs.loc[0==mapped_reads, 'remove'] = True
#
        # Get covered regions for all contigs
        covered_regions = mappings[['t_id','t_start','t_end']].rename(columns={'t_id':'con_id','t_start':'from','t_end':'to'})
        covered_regions.sort_values(['con_id','from','to'], inplace=True)
        old_len = 0
        while old_len != len(covered_regions):
            old_len  = len(covered_regions)
            covered_regions['group'] = ((covered_regions['con_id'] != covered_regions['con_id'].shift(1)) | (covered_regions['from'] > covered_regions['to'].shift(1))).cumsum()
            covered_regions = covered_regions.groupby(['group','con_id']).agg({'from':'min','to':'max'}).reset_index()
        covered_regions.drop(columns=['group'], inplace=True)
    else:
        # Set all contigs to full coverage
        covered_regions = contigs[['length']].rename(columns={'length':'to'})
        covered_regions['con_id'] = covered_regions.index.values
        covered_regions['from'] = 0
        covered_regions = covered_regions[['con_id','from','to']].copy()

    return contigs, covered_regions

def BreakReadsAtAdapters(mappings, adapter_signal_max_dist, keep_all_subreads):
    # Sort mappings by starting position and provide information on next/previous mapping (if from same read)
    mappings.sort_values(['q_id','q_start'], inplace=True)
    
    mappings['next_con'] = mappings['t_id'].shift(-1, fill_value=-1)
    mappings.loc[mappings['q_id'].shift(-1, fill_value='') != mappings['q_id'], 'next_con'] = -1
    mappings['next_strand'] = mappings['strand'].shift(-1, fill_value='')
    mappings.loc[-1 == mappings['next_con'], 'next_strand'] = ''
    
    mappings['prev_con'] = mappings['t_id'].shift(1, fill_value=-1)
    mappings.loc[mappings['q_id'].shift(1, fill_value='') != mappings['q_id'], 'prev_con'] = -1
    mappings['prev_strand'] = mappings['strand'].shift(1, fill_value='')
    mappings.loc[-1 == mappings['prev_con'], 'prev_strand'] = ''
    
    # Find potential left-over adapters
    mappings['read_start'] = 0
    mappings['read_end'] = mappings['q_len']
    
    if keep_all_subreads:
        # If we only keep the best subreads, we already removed most of the duplication and cannot separate the reads at adapters anymore, which results in spurious breaks
        adapter = (mappings['next_con'] == mappings['t_id']) & (mappings['next_strand'] != mappings['strand'])
        location_shift = np.abs(np.where('+' == mappings['strand'], mappings['t_end'] - mappings['t_end'].shift(-1, fill_value=0), mappings['t_start'] - mappings['t_start'].shift(-1, fill_value=0)))
    
        adapter = adapter & (location_shift <= adapter_signal_max_dist)
    
        # We need to be very sure that this is an adapter signal, because we do not want to miss a signal for a break point due to a missed inverted repeat
        # For this we require that they must have at least one mapping before and after the two with the adapter that are compatible with the adapter hypothesis
        adapter = adapter & (mappings['prev_con'] >= 0) & (mappings['prev_con'] == mappings['next_con'].shift(-1, fill_value=-1)) & (mappings['prev_strand'] != mappings['next_strand'].shift(-1, fill_value=''))
        adapter = adapter & (np.abs(np.where('-' == mappings['strand'], mappings['t_end'] - mappings['t_end'].shift(-1, fill_value=0), mappings['t_start'] - mappings['t_start'].shift(-1, fill_value=0))) <= adapter_signal_max_dist)
    
        # Split the read at the adapter
        mappings.loc[adapter, 'next_con'] = -1
        mappings.loc[adapter, 'next_strand'] = ''
        mappings.loc[adapter, 'read_end'] = mappings.loc[adapter, 'q_end'] # If the adapter was missed it's probably a bad part of the read, so better ignore it and don't use it for extensions
        mappings['read_end'] = mappings.loc[::-1, ['q_id','read_end']].groupby('q_id', sort=False).cummin()[::-1]
    
        adapter = adapter.shift(1, fill_value=False)
        mappings.loc[adapter, 'prev_con'] = -1
        mappings.loc[adapter, 'prev_strand'] = ''
        mappings.loc[adapter, 'read_start'] = mappings.loc[adapter, 'q_start'] # If the adapter was missed it's probably a bad part of the read, so better ignore it and don't use it for extensions
        mappings['read_start'] = mappings[['q_id','read_start']].groupby('q_id', sort=False).cummax()

    return mappings

def GetBrokenMappings(mappings, covered_regions, max_dist_contig_end, min_length_contig_break, pdf):
    # Find reads that have a break point and map on the left/right side(contig orientation) of it
    pot_breaks = mappings.copy()
    pot_breaks['next_pos'] = np.where(0 > pot_breaks['next_con'], -1, np.where(pot_breaks['next_strand'] == '+', pot_breaks['t_start'].shift(-1, fill_value=-1), pot_breaks['t_end'].shift(-1, fill_value=0)-1))
    pot_breaks['prev_pos'] = np.where(0 > pot_breaks['prev_con'], -1, np.where(pot_breaks['prev_strand'] == '-', pot_breaks['t_start'].shift(1, fill_value=-1), pot_breaks['t_end'].shift(1, fill_value=0)-1))    
    pot_breaks['next_dist'] = np.where(0 > pot_breaks['next_con'], 0, pot_breaks['q_start'].shift(-1, fill_value=0) - pot_breaks['q_end'])
    pot_breaks['prev_dist'] = np.where(0 > pot_breaks['prev_con'], 0, pot_breaks['q_start'] - pot_breaks['q_end'].shift(1, fill_value=0))

    cov_reg = covered_regions.groupby(['con_id']).agg({'from':'min','to':'max'}).reset_index().rename(columns={'con_id':'t_id'})
    left_breaks = pot_breaks[ (pot_breaks[['t_id']].merge(cov_reg, on=['t_id'], how='left')['to'].values - pot_breaks['t_end'] > max_dist_contig_end) &
                             np.where('+' == pot_breaks['strand'],
                                      (0 <= pot_breaks['next_con']) | (pot_breaks['read_end']-pot_breaks['q_end'] > min_length_contig_break),
                                      (0 <= pot_breaks['prev_con']) | (pot_breaks['q_start']-pot_breaks['read_start'] > min_length_contig_break)) ].copy()
    left_breaks['side'] = 'l'

    right_breaks = pot_breaks[ (pot_breaks['t_start'] - pot_breaks[['t_id']].merge(cov_reg, on=['t_id'], how='left')['from'].values > max_dist_contig_end) &
                              np.where('+' == pot_breaks['strand'],
                                       (0 <= pot_breaks['prev_con']) | (pot_breaks['q_start']-pot_breaks['read_start'] > min_length_contig_break),
                                       (0 <= pot_breaks['next_con']) | (pot_breaks['read_end']-pot_breaks['q_end'] > min_length_contig_break)) ].copy()
    right_breaks['side'] = 'r'

    pot_breaks = pd.concat([left_breaks, right_breaks], ignore_index=False)
    pot_breaks = pot_breaks.reset_index().rename(columns={'index':'map_index', 't_id':'contig_id'}).drop(columns=['t_name','matches','alignment_length'])
    pot_breaks['position'] = np.where('l' == pot_breaks['side'], pot_breaks['t_end'], pot_breaks['t_start'])
    pot_breaks['read_pos'] = np.where(('+' == pot_breaks['strand']) == ('l' == pot_breaks['side']), pot_breaks['q_end'], pot_breaks['q_start'])
    pot_breaks['read_side'] = np.where(('+' == pot_breaks['strand']) == ('l' == pot_breaks['side']), 'r', 'l')
    pot_breaks['map_len'] = np.where('l' == pot_breaks['side'], pot_breaks['t_end']-pot_breaks['t_start'], pot_breaks['t_end']-pot_breaks['t_start'])
    pot_breaks['con'] = np.where(('+' == pot_breaks['strand']) == ('l' == pot_breaks['side']), pot_breaks['next_con'], pot_breaks['prev_con'])
    pot_breaks['con_strand'] = np.where(('+' == pot_breaks['strand']) == ('l' == pot_breaks['side']), pot_breaks['next_strand'], pot_breaks['prev_strand'])
    pot_breaks['con_pos'] = np.where(('+' == pot_breaks['strand']) == ('l' == pot_breaks['side']), pot_breaks['next_pos'], pot_breaks['prev_pos'])
    pot_breaks['con_dist'] = np.where(('+' == pot_breaks['strand']) == ('l' == pot_breaks['side']), pot_breaks['next_dist'], pot_breaks['prev_dist'])
    pot_breaks['opos_con'] = np.where(('+' == pot_breaks['strand']) == ('l' == pot_breaks['side']), pot_breaks['prev_con'], pot_breaks['next_con']) # Connection on the opposite side of the break
    pot_breaks.drop(columns=['next_con','prev_con','next_strand','prev_strand','next_pos','prev_pos','next_dist','prev_dist'], inplace=True)

    if pdf:
        dists_contig_ends = np.where(pot_breaks['side'] == 'l', pot_breaks['t_len']-pot_breaks['t_end'], pot_breaks['t_start'])
        if len(dists_contig_ends):
            if np.sum(dists_contig_ends < 10*max_dist_contig_end):
                PlotHist(pdf, "Distance to contig end", "# Potential contig breaks", np.extract(dists_contig_ends < 10*max_dist_contig_end, dists_contig_ends), threshold=max_dist_contig_end)
            PlotHist(pdf, "Distance to contig end", "# Potential contig breaks", dists_contig_ends, threshold=max_dist_contig_end, logx=True)

    return pot_breaks

def GetNonInformativeMappings(mappings, contigs, min_extension, break_groups, pot_breaks):
    # All reads that are not extending contigs enough and do not have multiple mappings are non informative (except they overlap breaks)
    non_informative_mappings = mappings[(min_extension > mappings['q_start']) & (min_extension > mappings['q_len']-mappings['q_end']) & (mappings['q_id'].shift(1, fill_value='') != mappings['q_id']) & (mappings['q_id'].shift(-1, fill_value='') != mappings['q_id'])].index
    non_informative_mappings = np.setdiff1d(non_informative_mappings,pot_breaks['map_index'].values) # Remove breaking reads
    non_informative_mappings = mappings.loc[non_informative_mappings, ['t_id', 't_start', 't_end']]

    if len(break_groups):
        # Remove continues reads overlapping breaks from non_informative_mappings
        touching_breaks = []
        for i in range(break_groups['num'].max()+1):
            # Set breaks for every mapping, depending on their contig_id
            breaks = pd.DataFrame({'contig':np.arange(len(contigs)), 'start':sys.maxsize//2, 'end':0, 'group':-1})
            breaks.iloc[break_groups.loc[i==break_groups['num'], 'contig_id'], [breaks.columns.get_loc('start'), breaks.columns.get_loc('end')]] = break_groups.loc[i==break_groups['num'], ['start', 'end']].values
            breaks = breaks.iloc[non_informative_mappings['t_id']]
            # Find mappings that touch the previously set breaks (we could filter more, but this is the maximum set, so we don't declare informative mappings as non_informative_mappings, and the numbers we could filter more don't really matter speed wise)
            touching_breaks.append(non_informative_mappings[(non_informative_mappings['t_start'] <= breaks['end'].values) & (non_informative_mappings['t_end'] >= breaks['start'].values)].index)

        non_informative_mappings = np.setdiff1d(non_informative_mappings.index,np.concatenate(touching_breaks))
    else:
        # Without breaks no reads can overlap them
        non_informative_mappings = non_informative_mappings.index

    return non_informative_mappings

def CallAllBreaksSpurious(mappings, contigs, covered_regions, max_dist_contig_end, min_length_contig_break, min_extension, pdf):
    pot_breaks = GetBrokenMappings(mappings, covered_regions, max_dist_contig_end, min_length_contig_break, pdf)

    break_groups = []
    spurious_break_indexes = pot_breaks[['map_index','q_id','read_side','q_start','q_end']].drop_duplicates()
    non_informative_mappings = GetNonInformativeMappings(mappings, contigs, min_extension, break_groups, pot_breaks)

    return break_groups, spurious_break_indexes, non_informative_mappings

def CalculateReadExtensionAfterMappingEnd(cur_mappings):
    cur_mappings['q_left'] = cur_mappings['q_start'] - cur_mappings['read_start']
    cur_mappings['q_right'] = cur_mappings['read_end'] - cur_mappings['q_end']
    cur_mappings.loc[cur_mappings['strand'] == '-', ['q_left','q_right']] = cur_mappings.loc[cur_mappings['strand'] == '-', ['q_right','q_left']].values # Switch the values in case of negative strand, so that they represent the extension on the left/right side of the break after mapping ends
    cur_mappings['q_right'] = cur_mappings['q_right'].astype(int) # No clue why it switches from int to float in the previous command, cannot detect something that is going wrong
#
    return cur_mappings

def PreparePotBreakPoints(pot_breaks, max_break_point_distance, min_mapping_length, min_num_reads):
    break_points = pot_breaks[['contig_id','side','position','mapq','map_len','map_index','q_start','read_start','q_end','read_end','strand','q_id']].copy()
    break_points['connected'] = 0 <= pot_breaks['con']
    break_points = CalculateReadExtensionAfterMappingEnd(break_points)
    break_points['ext_len'] = np.where(break_points['side'] == 'l', break_points['q_left'], break_points['q_right']) + break_points['map_len']
    break_points.drop(columns=['q_start','q_end','strand','read_start','read_end','q_left','q_right'], inplace=True)
    break_points = break_points[ break_points['map_len'] >= min_mapping_length+max_break_point_distance ].copy() # require half the mapping length for breaks as we will later require for continuously mapping reads that veto breaks, since breaking reads only map on one side
    break_points.drop(columns=['map_len'], inplace=True)
    break_points.sort_values(['contig_id','position','mapq'], ascending=[True,True,False], inplace=True)
    if min_num_reads > 1:
        # Require at least one supporting break within +- max_break_point_distance
        break_points = break_points[ ((break_points['contig_id'] == break_points['contig_id'].shift(-1, fill_value=-1)) & (break_points['position']+max_break_point_distance >= break_points['position'].shift(-1, fill_value=-1))) | ((break_points['contig_id'] == break_points['contig_id'].shift(1, fill_value=-1)) & (break_points['position']-max_break_point_distance <= break_points['position'].shift(1, fill_value=-1))) ].copy()
    break_points.reset_index(drop=True,inplace=True)
    break_points['bindex'] = break_points.index.values
    bp_ext_len = break_points.drop(columns=['connected','mapq'])
    break_points.drop(columns=['map_index','ext_len'], inplace=True)
#
    return break_points, bp_ext_len

def CountBreakSupport(break_points, max_break_point_distance, min_num_reads):
    if len(break_points):
        # Store read_ids corresponding to each break index
        read_ids = break_points[['q_id','mapq','connected']].copy()
        # Find all breaks within +- max_break_point_distance
        break_points['ito'] = break_points['bindex']
        break_points.rename(columns={'bindex':'ifrom'}, inplace=True)
        break_points = break_points.groupby(['contig_id','position']).agg({'ifrom':'min', 'ito':'max'}).reset_index()
        break_points.sort_values(['ifrom'], inplace=True)
        break_points[['ifrom_org','ito_org']] = break_points[['ifrom','ito']].values
        for direction in [+1,-1]:
            s = direction
            while True:
                break_points['supp_pos'] = break_points['position'].shift(s, fill_value=-1)
                break_points.loc[break_points['contig_id'] != break_points['contig_id'].shift(s), 'supp_pos'] = -1
                if direction == +1:
                    break_points.loc[break_points['position'] > break_points['supp_pos']+max_break_point_distance, 'supp_pos'] = -1
                    break_points['ifrom'] = np.where(break_points['supp_pos'] == -1, break_points['ifrom'], break_points['ifrom_org'].shift(s, fill_value=-1))
                else:
                    break_points.loc[break_points['position'] < break_points['supp_pos']-max_break_point_distance, 'supp_pos'] = -1
                    break_points['ito'] = np.where(break_points['supp_pos'] == -1, break_points['ito'], break_points['ito_org'].shift(s, fill_value=-1))
                if np.sum(break_points['supp_pos'] != -1):
                    s += direction
                else:
                    break
        break_points.drop(columns=['supp_pos','ifrom_org','ito_org'], inplace=True)
        # Count support and connected support for each mapping quality
        break_supp = break_points[['ifrom','ito']].drop_duplicates()
        break_supp = break_supp.loc[np.repeat(break_supp.index.values, break_supp['ito']-break_supp['ifrom']+1)].copy()
        break_supp['bindex'] = break_supp.groupby(['ifrom','ito'], sort=False).cumcount() + break_supp['ifrom']
        break_supp[['q_id','mapq','connected']] = read_ids.loc[break_supp['bindex'].values].values
        break_supp.drop(columns='bindex', inplace=True)
        break_supp.sort_values(['ifrom','ito','q_id','mapq'], inplace=True)
        break_con_supp = break_supp[break_supp['connected']].groupby(['ifrom','ito','q_id'], sort=False)['mapq'].last().reset_index() # Take the highest mapping quality for every read (so that we count only one support for it)
        break_supp = break_supp.groupby(['ifrom','ito','q_id'], sort=False)['mapq'].last().reset_index() # Take the highest mapping quality for every read (so that we count only one support for it)
        break_con_supp = break_con_supp.groupby(['ifrom','ito','mapq']).size().reset_index(name='con_supp')
        break_supp = break_supp.groupby(['ifrom','ito','mapq']).size().reset_index(name='support')
        break_supp['con_supp'] = break_supp[['ifrom','ito','mapq']].merge(break_con_supp, on=['ifrom','ito','mapq'], how='left')['con_supp'].fillna(0).astype(int).values
        # Add support from higher mapping qualities to lower mapping qualities
        break_supp.sort_values(['ifrom','ito','mapq'], ascending=[True,True,False], inplace=True)
        break_supp['support'] = break_supp.groupby(['ifrom','ito'])['support'].cumsum()
        break_supp['con_supp'] = break_supp.groupby(['ifrom','ito'])['con_supp'].cumsum()
        # Add support to break_points
        break_points = break_points.merge(break_supp, on=['ifrom','ito'], how='left')
        break_points.sort_values(['contig_id','position','mapq'], ascending=[True,True,False], inplace=True)
        # Require a support of at least min_num_reads at some mapping quality level
        max_support = break_points.groupby(['contig_id','position'], sort=False)['support'].agg(['max','size'])
        break_points = break_points[ np.repeat(max_support['max'].values, max_support['size'].values) >= min_num_reads].copy()
#
    return break_points

def GetPrematureStopProb(ntest, ncontrol, longer):
    # Get probability of the null hypothesis that the reads are randomly drawn from the full length distribution (and not from a truncated distribution)
    # Wilks-Rosenbaum Exceedance test
    # n : ntest
    # m : ncontrol
    # c : longer
    # Pr(E1 = 0) = n/(m+n)
    # Pr(E1 = k) = Pr(E = k-1) * (m-k+1)/(m+n-k)
    # p = sum(k=c->m){ Pr(E1 = k) } = 1 - sum(k=0->c-1){ Pr(E1 = k) }
    prob = ntest/(ncontrol+ntest)
    p = 1 - prob # This has the starting value already for longer=1, but we later fix that by manually setting all entries with longer == 0 to 1.0
    for i in range(1,np.max(longer)):
        sel = longer > i
        prob[sel] *= (ncontrol[sel]-i+1)/(ncontrol[sel]+ntest[sel]-i)
        p[sel] -= prob[sel]
    return np.where(longer == 0, 1.0, p)

def NormCDFCapped(x, mu, sigma):
    return np.minimum(0.5, NormCDF(x, mu, sigma)) # Cap it at the mean so that repeats do not get a bonus

def GetConProb(cov_probs, req_length, counts):
    probs = pd.DataFrame({'length':req_length, 'counts':counts, 'mu':0.0, 'sigma':1.0})
    for plen, mu, sigma in zip(reversed(cov_probs['length']), reversed(cov_probs['mu']), reversed(cov_probs['sigma'])):
        probs.loc[plen >= probs['length'], 'mu'] = mu
        probs.loc[plen >= probs['length'], 'sigma'] = sigma

    return NormCDFCapped(probs['counts'],probs['mu'], probs['sigma'])

def CountAndApplyBreakVetos(break_points, mappings, pot_breaks, bp_ext_len, cov_probs, covered_regions, max_dist_contig_end, max_break_point_distance, min_mapping_length, min_num_reads, min_length_contig_break, prob_factor, merge_block_length, prematurity_threshold):
    if len(break_points):
        # Check how many reads veto a break (continuously map over the break region position +- (min_mapping_length+max_break_point_distance))
        break_pos = break_points[['contig_id','position']].drop_duplicates()
        break_pos['merge_block'] = break_pos['position'] // merge_block_length
        break_pos['num'] = break_pos.groupby(['contig_id','merge_block'], sort=False).cumcount()
        tmp_mappings = mappings.loc[ np.isin(mappings['t_id'], break_pos['contig_id'].values) & (mappings['t_end'] - mappings['t_start'] >= 2*min_mapping_length + 2*max_break_point_distance), ['t_id','t_start','t_end','mapq','q_start','q_end','strand','read_start','read_end'] ].copy() # Do not consider mappings that cannot fullfil veto requirements because they are too short
        tmp_mappings = CalculateReadExtensionAfterMappingEnd(tmp_mappings)
        tmp_mappings.drop(columns=['q_start','q_end','strand','read_start','read_end'], inplace=True)
        tmp_mappings['q_left'] = tmp_mappings['t_start'] - tmp_mappings['q_left']  # Set them on the contig scale, so we can later treat them like t_start and t_end
        tmp_mappings['q_right'] += tmp_mappings['t_end']
        tmp_mappings = tmp_mappings.loc[ np.repeat( tmp_mappings.index.values, tmp_mappings['t_end']//merge_block_length - tmp_mappings['t_start']//merge_block_length + 1 ) ].copy()
        tmp_mappings['merge_block'] = tmp_mappings.reset_index().groupby(['index']).cumcount().values + tmp_mappings['t_start']//merge_block_length
        tmp_mappings.rename(columns={'t_id':'contig_id'}, inplace=True)
        break_list = []
        veto_ext_len = []
        for n in range(break_pos['num'].max()+1):
            tmp_breaks = break_pos[break_pos['num'] == n]
            tmp_mappings = tmp_mappings[np.isin(tmp_mappings['contig_id'],tmp_breaks['contig_id'].values)].copy()
            # Create one entry for every mapping that potentially overlaps the break
            tmp_breaks = tmp_breaks.merge( tmp_mappings, on=['contig_id','merge_block'], how='inner')
            tmp_breaks = tmp_breaks[(tmp_breaks['t_start']+min_mapping_length+max_break_point_distance <= tmp_breaks['position']) &
                                    (tmp_breaks['t_end']-min_mapping_length-max_break_point_distance >= tmp_breaks['position'])].copy()
            break_list.append( tmp_breaks.groupby(['contig_id','position','mapq']).size().reset_index(name='vetos') )
            veto_ext_len.append( tmp_breaks.groupby(['contig_id','position']).agg({'q_left':'min', 'q_right':'max', 'mapq':'size'}).reset_index().rename(columns={'mapq':'count'}) )
        break_points = break_points.merge(pd.concat(break_list, ignore_index=True), on=['contig_id','position','mapq'], how='outer').fillna(0)
        break_points.sort_values(['contig_id','position','mapq'], ascending=[True,True,False], inplace=True)
        break_points.reset_index(inplace=True, drop=True)
        break_points[['support', 'con_supp']] = break_points.groupby(['contig_id','position'], sort=False)[['support', 'con_supp']].cummax().astype(int)
        break_points['vetos'] = break_points.groupby(['contig_id','position'], sort=False)['vetos'].cumsum().astype(int)
        break_points[['ifrom','ito']] = break_points[['contig_id','position']].merge(break_points.groupby(['contig_id','position'])[['ifrom','ito']].max().astype(int).reset_index(), on=['contig_id','position'], how='left')[['ifrom','ito']].values
#
        # Find vetos that potentially have premature stops
        veto_ext_len = pd.concat(veto_ext_len, ignore_index=True)
        cov_reg = covered_regions.groupby(['con_id']).agg({'from':'min','to':'max'}).reset_index().rename(columns={'con_id':'contig_id'})
        veto_ext_len[['ifrom','ito']] = veto_ext_len[['contig_id','position']].merge(break_points.groupby(['contig_id','position'])[['ifrom','ito']].max().reset_index(), on=['contig_id','position'], how='left')[['ifrom','ito']].values
        if len(veto_ext_len):
            # Get start and end of breaking reads
            break_ext_len = veto_ext_len[['ifrom','ito']].drop_duplicates()
            break_ext_len = break_ext_len.loc[np.repeat(break_ext_len.index.values, break_ext_len['ito'].values-break_ext_len['ifrom'].values+1)].reset_index(drop=True)
            break_ext_len['bindex'] = break_ext_len.groupby(['ifrom','ito']).cumcount() + break_ext_len['ifrom']
            break_ext_len = break_ext_len.merge(bp_ext_len, on=['bindex'], how='left')
            break_ext_len['bstart'] = break_ext_len['position'] - np.where(break_ext_len['side'] == 'l', break_ext_len['ext_len'], 0)
            break_ext_len['bend'] = break_ext_len['position'] + np.where(break_ext_len['side'] == 'r', break_ext_len['ext_len'], 0)
            break_ext_len = break_ext_len[['ifrom','ito','bstart','bend','side']].copy()
            # Only check veto positions where the break map longer than the break
            veto_ext_len[['bstart','bend']] = veto_ext_len[['ifrom','ito']].merge(break_ext_len.groupby(['ifrom','ito']).agg({'bstart':'min', 'bend':'max'}).reset_index(), on=['ifrom','ito'], how='left')[['bstart','bend']].values
            veto_ext_len = veto_ext_len[(veto_ext_len['bstart'] < veto_ext_len['q_left']) | (veto_ext_len['q_right'] < veto_ext_len['bend'])].copy()
        if len(veto_ext_len):
            # Count how many breaks are longer than the vetos and check if this is likely due to chance
            break_ext_len = veto_ext_len[['contig_id','position','q_left','q_right','ifrom','ito']].merge(break_ext_len, on=['ifrom','ito'], how='left')
            break_ext_len['longer'] = np.where(break_ext_len['side'] == 'r', break_ext_len['bend'] > break_ext_len['q_right'], break_ext_len['q_left'] > break_ext_len['bstart'])
            for s in ['l','r']:
                veto_ext_len[[f'{s}bcount',f'{s}blonger']] = veto_ext_len[['contig_id','position']].merge(break_ext_len[break_ext_len['side'] == s].groupby(['contig_id','position']).agg({'side':'size','longer':'sum'}).reset_index(), on=['contig_id','position'], how='left')[['side','longer']].fillna(0).astype(int).values
                veto_ext_len.loc[veto_ext_len[f'{s}bcount'] < min_num_reads, [f'{s}blonger']] = 0 # Filter out cases that not exceed min_num_reads
            veto_ext_len = veto_ext_len[veto_ext_len[['lblonger','rblonger']].sum(axis=1) > 0].copy()
        if len(veto_ext_len):
            for s in ['l','r']:
                veto_ext_len[f'{s}broken_veto_prob'] = GetPrematureStopProb(veto_ext_len['count'].values, veto_ext_len[f'{s}bcount'].values, veto_ext_len[f'{s}blonger'].values)
            veto_ext_len = veto_ext_len[np.minimum(veto_ext_len['lbroken_veto_prob'], veto_ext_len['rbroken_veto_prob']) <= prematurity_threshold].copy()
        if len(veto_ext_len) == 0:
            break_points[['lg_broken_veto','rg_broken_veto']] = -1
        else:
            # Assign prematurely stopping vetos into groups to use the other breaks as a filter in the count comparison to avoid a flood of break points
            for s in ['l','r']:
                veto_ext_len[f'{s}g_broken_veto'] = veto_ext_len[f'{s}broken_veto_prob'] <= prematurity_threshold
                sort_val = 'q_left' if s == 'l' else 'q_right'
                veto_ext_len.sort_values(['contig_id', f'{s}g_broken_veto', sort_val], inplace=True)
                veto_ext_len[f'{s}g_broken_veto'] = ( (veto_ext_len['contig_id'] != veto_ext_len['contig_id'].shift(1)) |
                                                      (veto_ext_len[sort_val] > veto_ext_len[sort_val].shift(1) + max_break_point_distance) |
                                                      (veto_ext_len[f'{s}g_broken_veto'] != veto_ext_len[f'{s}g_broken_veto'].shift(1)) ).cumsum()
                veto_ext_len.loc[veto_ext_len[f'{s}broken_veto_prob'] > prematurity_threshold, f'{s}g_broken_veto'] = -1
            veto_ext_len.sort_values(['contig_id', 'position'], inplace=True)
            break_points[['lg_broken_veto','rg_broken_veto']] = break_points[['contig_id', 'position']].merge(veto_ext_len[['contig_id', 'position','lg_broken_veto','rg_broken_veto']], on=['contig_id', 'position'], how='left')[['lg_broken_veto','rg_broken_veto']].fillna(-1).astype(int).values
#
        # Remove breaks where the vetos (or other breakpoints for broken vetos) are much more likely than the breaks
        break_points['break_prob'] = GetConProb(cov_probs, min_mapping_length+max_break_point_distance+min_length_contig_break, break_points['support'])
        break_points['veto_prob'] = GetConProb(cov_probs, 2*(min_mapping_length+max_break_point_distance), break_points['vetos'])
        for s in ['l','r']:
            tmp = break_points.loc[break_points[f'{s}g_broken_veto'] >= 0, [f'{s}g_broken_veto','mapq','break_prob']].drop_duplicates().sort_values([f'{s}g_broken_veto','mapq','break_prob'], ascending=[True,False,False])
            tmp['break_prob'] = tmp.groupby([f'{s}g_broken_veto','mapq'], sort=False)['break_prob'].cummax()
            tmp.drop_duplicates(inplace=True)
            break_points[f'{s}g_veto_prob'] = break_points[[f'{s}g_broken_veto','mapq']].merge(tmp, on=[f'{s}g_broken_veto','mapq'], how='left')['break_prob'].values
        break_points['veto_prob'] = np.minimum(break_points['veto_prob'], np.minimum(break_points['lg_veto_prob'].fillna(1), break_points['rg_veto_prob'].fillna(1)))
        break_points.drop(columns=['lg_veto_prob','rg_veto_prob'], inplace=True)
        break_points['vetoed'] = (break_points['vetos'] >= min_num_reads) & (break_points['veto_prob'] >= prob_factor*break_points['break_prob']) # Always make sure that vetos reach min_num_reads, before removing something based on their probability
        break_points['vetoed'] = break_points.groupby(['contig_id','position'], sort=False)['vetoed'].cumsum().astype(bool) # Propagate vetos to lower mapping qualities
        break_points = break_points[break_points['vetoed'] == False].copy()
#
        # Remove break points that will reconnected anyways
        break_points['vetoed'] = (break_points['vetos'] >= min_num_reads) & (break_points['con_supp'] < min_num_reads) # If we have enough support, but not enough con_supp the bridge finding step would anyways reseal the break with a unique bridge, so don't even break it
        break_points['vetoed'] = break_points.groupby(['contig_id','position'], sort=False)['vetoed'].cummax() # Propagate vetos to lower mapping qualities
        unconnected_break_points = [break_points[break_points['vetoed'] == True].copy()] # Store unconnected break points to later check if we can insert additional sequence to provide a connection in a second run
        break_points = break_points[break_points['vetoed'] == False].copy()
#
        # Remove break points with not enough consistent breaks
        break_pos = break_points[['ifrom','ito']].drop_duplicates().reset_index(drop=True)
        break_pos = break_pos.loc[np.repeat(break_pos.index.values, break_pos['ito']-break_pos['ifrom']+1)].reset_index()
        break_pos['bindex'] = break_pos.groupby('index', sort=False).cumcount() + break_pos['ifrom']
        break_pos.drop(columns=['index'], inplace=True)
        break_pos[['map_index','side']] = break_pos[['bindex']].merge(bp_ext_len[['bindex','map_index','side']], on=['bindex'], how='left')[['map_index','side']].values
        map_cols = ['strand','mapq','con','con_strand','con_pos','con_dist']
        break_pos[map_cols] = break_pos[['map_index','side']].merge(pot_breaks[['map_index','side']+map_cols], on=['map_index','side'], how='left')[map_cols].values
        break_pos['strand_switch'] = break_pos['strand'] != break_pos['con_strand']
        break_pos.sort_values(['ifrom','ito','side','con','strand_switch','con_pos','mapq'], inplace=True)
        break_pos['group'] = ( (break_pos['ifrom'] != break_pos['ifrom'].shift(1)) | (break_pos['ito'] != break_pos['ito'].shift(1)) | (break_pos['side'] != break_pos['side'].shift(1)) |
                               (break_pos['con'] != break_pos['con'].shift(1)) | (break_pos['strand_switch'] != break_pos['strand_switch'].shift(1)) | (break_pos['con_pos'] > break_pos['con_pos'].shift(1)+max_break_point_distance) ).cumsum()
        break_pos = break_pos.groupby(['ifrom','ito','group','mapq']).size().reset_index(name='count').merge( break_pos.groupby(['ifrom','ito','group'])['con_dist'].mean().reset_index(name='mean_dist'), on=['ifrom','ito','group'], how='inner')
        break_pos.sort_values(['group','mapq'], ascending=[True,False], inplace=True)
        break_pos['count'] = break_pos.groupby(['group'], sort=False)['count'].cumsum()
        break_pos = break_pos[ break_pos['count'] >= min_num_reads ].copy()
        break_points['vetoed'] = (break_points.merge( break_pos[['ifrom','ito','mapq']].drop_duplicates(), on=['ifrom','ito','mapq'], how='left', indicator=True )['_merge'] == "both").values # Start with setting vetoed to not vetoed ones for a following cummax
        break_points['vetoed'] = (break_points.groupby(['contig_id','position'], sort=False)['vetoed'].cummax() == False) & (break_points['vetos'] >= min_num_reads)
        break_points['vetoed'] = break_points.groupby(['contig_id','position'], sort=False)['vetoed'].cummax() # Propagate vetos to lower mapping qualities
        unconnected_break_points.append( break_points[break_points['vetoed'] == True].copy() )
        break_points = break_points[break_points['vetoed'] == False].copy()
#
        # Remove break points with not enough consistent breaks to be likely enough
        break_pos['con_prob'] = GetConProb(cov_probs, np.where(0 <= break_pos['mean_dist'], break_pos['mean_dist']+2*min_mapping_length, min_mapping_length-break_pos['mean_dist']), break_pos['count'])
        break_points['con_prob'] = break_points[['ifrom','ito','mapq']].merge(break_pos.groupby(['ifrom','ito','mapq'])['con_prob'].max().reset_index(name='con_prob'), on=['ifrom','ito','mapq'], how='left')['con_prob'].values
        break_points['con_prob'] =  break_points.fillna(0.0).groupby(['contig_id','position'], sort=False)['con_prob'].cummax()
        for s in ['l','r']:
            tmp = break_points.loc[break_points[f'{s}g_broken_veto'] >= 0, [f'{s}g_broken_veto','mapq','con_prob']].drop_duplicates().sort_values([f'{s}g_broken_veto','mapq','con_prob'], ascending=[True,False,False])
            tmp['con_prob'] = tmp.groupby([f'{s}g_broken_veto','mapq'], sort=False)['con_prob'].cummax()
            tmp.drop_duplicates(inplace=True)
            break_points[f'{s}g_veto_prob'] = break_points[[f'{s}g_broken_veto','mapq']].merge(tmp, on=[f'{s}g_broken_veto','mapq'], how='left')['con_prob'].values
        break_points['veto_prob'] = np.minimum(break_points['veto_prob'], np.minimum(break_points['lg_veto_prob'].fillna(1), break_points['rg_veto_prob'].fillna(1)))
        break_points.drop(columns=['lg_veto_prob','rg_veto_prob'], inplace=True)
        break_points['vetoed'] = (break_points['vetos'] >= min_num_reads) & (break_points['veto_prob'] >= prob_factor*break_points['con_prob']) # Always make sure that vetos reach min_num_reads, before removing something based on their probability
        break_points['vetoed'] = break_points.groupby(['contig_id','position'], sort=False)['vetoed'].cummax() # Propagate vetos to lower mapping qualities
        unconnected_break_points.append( break_points[break_points['vetoed'] == True].drop(columns=['con_prob']) )
        break_points = break_points[break_points['vetoed'] == False].drop(columns=['vetoed'])
#
        # Remove breaks that do not fullfil requirements and reduce accepted ones to a single entry per break_id
        break_points = break_points[break_points['support'] >= min_num_reads].copy()
        break_points = break_points.groupby(['contig_id','position'], sort=False).first().reset_index()
#
    return break_points, unconnected_break_points

def FindBreakPoints(mappings, contigs, covered_regions, max_dist_contig_end, min_mapping_length, min_length_contig_break, max_break_point_distance, min_num_reads, min_extension, merge_block_length, org_scaffold_trust, cov_probs, prob_factor, allow_same_contig_breaks, prematurity_threshold, pdf):
    if pdf:
        loose_reads_ends = mappings[(mappings['t_start'] > max_dist_contig_end) & np.where('+' == mappings['strand'], -1 == mappings['prev_con'], -1 == mappings['next_con'])]
        loose_reads_ends_length = np.where('+' == loose_reads_ends['strand'], loose_reads_ends['q_start']-loose_reads_ends['read_start'], loose_reads_ends['read_end']-loose_reads_ends['q_end'])
        loose_reads_ends = mappings[(mappings['t_len']-mappings['t_end'] > max_dist_contig_end) & np.where('+' == mappings['strand'], -1 == mappings['next_con'], -1 == mappings['prev_con'])]
        loose_reads_ends_length = np.concatenate([loose_reads_ends_length, np.where('+' == loose_reads_ends['strand'], loose_reads_ends['read_end']-loose_reads_ends['q_end'], loose_reads_ends['q_start']-loose_reads_ends['read_start'])])
        if len(loose_reads_ends_length):
            if np.sum(loose_reads_ends_length < 10*min_length_contig_break):
                PlotHist(pdf, "Loose end length", "# Ends", np.extract(loose_reads_ends_length < 10*min_length_contig_break, loose_reads_ends_length), threshold=min_length_contig_break, logy=True)
            PlotHist(pdf, "Loose end length", "# Ends", loose_reads_ends_length, threshold=min_length_contig_break, logx=True)

    pot_breaks = GetBrokenMappings(mappings, covered_regions, max_dist_contig_end, min_length_contig_break, pdf)
    break_points, bp_ext_len = PreparePotBreakPoints(pot_breaks, max_break_point_distance, min_mapping_length, min_num_reads)
    
    if pdf:
        break_point_dist = (break_points['position'] - break_points['position'].shift(1, fill_value=0))[break_points['contig_id'] == break_points['contig_id'].shift(1, fill_value=-1)]
        if len(break_point_dist):
            if np.sum(break_point_dist <= 10*max_break_point_distance):
                PlotHist(pdf, "Break point distance", "# Break point pairs", break_point_dist[break_point_dist <= 10*max_break_point_distance], threshold=max_break_point_distance )
            PlotHist(pdf, "Break point distance", "# Break point pairs", break_point_dist, threshold=max_break_point_distance, logx=True )

    break_points = CountBreakSupport(break_points, max_break_point_distance, min_num_reads)
    break_points, unconnected_break_points = CountAndApplyBreakVetos(break_points, mappings, pot_breaks, bp_ext_len, cov_probs, covered_regions, max_dist_contig_end, max_break_point_distance, min_mapping_length, min_num_reads, min_length_contig_break, prob_factor, merge_block_length, prematurity_threshold)

    if len(break_points):
        # Cluster break_points into groups, so that groups have a maximum length of 2*max_break_point_distance
        break_points['dist'] = np.where( break_points['contig_id'] != break_points['contig_id'].shift(1, fill_value=-1), -1, break_points['position'] - break_points['position'].shift(1, fill_value=0) )
        break_points['break_id'] = range(len(break_points))
        break_points['group'] = pd.Series(np.where( (break_points['dist'] > 2*max_break_point_distance) | (-1 == break_points['dist']), break_points['break_id'], 0 )).cummax()
        break_points.loc[break_points['group'] != break_points['group'].shift(1, fill_value=-1), 'dist'] = -1
        break_groups = break_points.groupby(['contig_id','group'], sort=False)['position'].agg(['min','max','size']).reset_index()
        while np.sum(break_groups['max']-break_groups['min'] > 2*max_break_point_distance):
            # Break at the longest distance until the break_groups are below the maximum length
            break_points['group'] = pd.Series(np.where( np.repeat(break_groups['max']-break_groups['min'] <= 2*max_break_point_distance, break_groups['size']).values |
                                                        (break_points['dist'] < np.repeat(break_points.groupby(['contig_id','group'], sort=False)['dist'].max().values, break_groups['size'].values)), break_points['group'], break_points['break_id'])).cummax()
            break_points.loc[break_points['group'] != break_points['group'].shift(1, fill_value=-1), 'dist'] = -1
            break_groups = break_points.groupby(['contig_id','group'], sort=False)['position'].agg(['min','max','size']).reset_index()
#
        break_groups.drop(columns=['group','size'], inplace=True)
        break_groups.rename(columns={'min':'start', 'max':'end'}, inplace=True)
        break_groups['end'] += 1 # Set end to one position after the last included one to be consistent with other ends (like q_end, t_end)
        break_groups['pos'] = (break_groups['end']+break_groups['start'])//2
        break_groups['num'] = break_groups.groupby(['contig_id'], sort=False).cumcount()
    else:
        break_groups = []
#
    if len(break_groups):
        # Find mappings belonging to accepted breaks and mappings that have a connection to the same part of the contig (not split by breaks)
        pot_breaks['keep'] = False
        pot_breaks['self'] = allow_same_contig_breaks & (pot_breaks['contig_id'] == pot_breaks['con']) & (pot_breaks['strand'] == pot_breaks['con_strand'])
        for i in range(break_groups['num'].max()+1):
            breaks = pd.DataFrame({'contig':np.arange(len(contigs)), 'start':sys.maxsize, 'end':-1, 'pos':-1})
            breaks.loc[break_groups.loc[i==break_groups['num'], 'contig_id'].values, ['start','end','pos']] = break_groups.loc[i==break_groups['num'], ['start','end','pos']].values
            breaks = breaks.iloc[pot_breaks['contig_id'].values]
            pot_breaks['keep'] = ( pot_breaks['keep'] | ( (pot_breaks['side'] == 'l') & (breaks['pos'].values - max_dist_contig_end <= pot_breaks['position'].values) & (pot_breaks['position'].values <= breaks['end'].values) )
                                                      | ( (pot_breaks['side'] == 'r') & (breaks['start'].values <= pot_breaks['position'].values) & (pot_breaks['position'].values <= breaks['pos'].values + max_dist_contig_end) ) )
            pot_breaks['self'] = pot_breaks['self'] & ( ((pot_breaks['position'].values < breaks['pos'].values + np.where((pot_breaks['side'] == 'l'), 1, -1)*min_mapping_length) & (pot_breaks['con_pos'].values < breaks['pos'].values + np.where((pot_breaks['side'] == 'l'), -1, 1)*min_mapping_length)) |
                                                        ((pot_breaks['position'].values > breaks['pos'].values + np.where((pot_breaks['side'] == 'l'), 1, -1)*min_mapping_length) & (pot_breaks['con_pos'].values > breaks['pos'].values + np.where((pot_breaks['side'] == 'l'), -1, 1)*min_mapping_length)) )
#
        # A read is only correct in between two valid breaks/contig ends if all breaks in that region connect to another position inside that region (or has no breaks at all if allow_same_contig_breaks==False)
        pot_breaks.sort_values(['q_id','q_start','q_end','read_pos'], inplace=True)
        pot_breaks['group_id'] = ( (pot_breaks['q_id'] != pot_breaks['q_id'].shift(1)) | (pot_breaks['read_start'] != pot_breaks['read_start'].shift(1)) |
                                   (pot_breaks['contig_id'] != pot_breaks['contig_id'].shift(1)) |
                                   (pot_breaks['keep'] & (pot_breaks['read_pos'] == pot_breaks['q_start'])) |
                                   (pot_breaks['keep'] & (pot_breaks['read_pos'] == pot_breaks['q_end'])).shift(1, fill_value=False) ).cumsum()
        pot_breaks['self'] = pot_breaks['keep'] | pot_breaks['self']
        self_con = pot_breaks.groupby(['group_id'])['self'].agg(['sum','size']).reset_index()
        self_con = np.unique(self_con.loc[self_con['sum'] == self_con['size'], ['group_id']].values)
        pot_breaks['keep'] = np.isin(pot_breaks['group_id'], self_con)
#
        # Mappings not belonging to accepted breaks or connecting to itself are spurious
        spurious_break_indexes = pot_breaks.loc[pot_breaks['keep'] == False, ['map_index','q_id','read_side','q_start','q_end']].drop_duplicates()
    else:
        # We don't have breaks, so all are spurious
        spurious_break_indexes = pot_breaks[['map_index','q_id','read_side','q_start','q_end']].drop_duplicates()
#
    non_informative_mappings = GetNonInformativeMappings(mappings, contigs, min_extension, break_groups, pot_breaks)
#
    # Filter unconnected break points that overlap a break_group (probably from vetoed low mapping qualities, where the high mapping quality was not vetoed)
    unconnected_break_points = pd.concat(unconnected_break_points, ignore_index=True)[['contig_id','position']].sort_values(['contig_id','position']).drop_duplicates()
    unconnected_break_points['remove'] = False
    for i in range(break_groups['num'].max()+1):
        breaks = pd.DataFrame({'contig':np.arange(len(contigs)), 'start':sys.maxsize, 'end':-1})
        breaks.iloc[break_groups.loc[i==break_groups['num'], 'contig_id'], [breaks.columns.get_loc('start'), breaks.columns.get_loc('end')]] = break_groups.loc[i==break_groups['num'], ['start', 'end']].values
        breaks = breaks.iloc[unconnected_break_points['contig_id']]
        unconnected_break_points['remove'] = unconnected_break_points['remove'] | ( (breaks['start'].values-max_break_point_distance <= unconnected_break_points['position'].values) & (unconnected_break_points['position'].values <= breaks['end'].values+max_break_point_distance) )
    unconnected_break_points = unconnected_break_points[unconnected_break_points['remove'] == False].drop(columns=['remove'])
#
    # Handle unconnected break points
    unconnected_break_points['dist'] = np.where( unconnected_break_points['contig_id'] != unconnected_break_points['contig_id'].shift(1, fill_value=-1), -1, unconnected_break_points['position'] - unconnected_break_points['position'].shift(1, fill_value=0) )
    unconnected_break_points['group'] = ((unconnected_break_points['dist'] > max_break_point_distance) | (-1 == unconnected_break_points['dist'])).cumsum()
    unconnected_break_points = unconnected_break_points.groupby(['contig_id','group'])['position'].agg(['min','max']).reset_index().rename(columns={'min':'bfrom','max':'bto'}).drop(columns=['group'])
    unconnected_break_points = unconnected_break_points.merge(pot_breaks, on=['contig_id'], how='left')
    unconnected_break_points = unconnected_break_points[(unconnected_break_points['position'] >= unconnected_break_points['bfrom'] - max_break_point_distance) & (unconnected_break_points['position'] <= unconnected_break_points['bto'] + max_break_point_distance)].copy()
#
    # If the unconnected break points are connected to another contig on the other side, the extension will likely take care of it and we don't need to do anything
    unconnected_break_points.sort_values(['contig_id','bfrom','bto','side','opos_con'], inplace=True)
    count_breaks = unconnected_break_points.groupby(['contig_id','bfrom','bto','side','opos_con'], sort=False).size()
    unconnected_break_points['ocon_count'] = np.where(unconnected_break_points['opos_con'] < 0, 0, np.repeat(count_breaks.values, count_breaks.values))
    count_breaks = unconnected_break_points.groupby(['contig_id','bfrom','bto','side'], sort=False)['ocon_count'].agg(['max','size'])
    unconnected_break_points['ocon_count'] = np.repeat(count_breaks['max'].values, count_breaks['size'].values)
    unconnected_break_points = unconnected_break_points[(unconnected_break_points['ocon_count'] < min_num_reads) & (np.repeat(count_breaks['size'].values, count_breaks['size'].values) >= min_num_reads)].copy()

    return break_groups, spurious_break_indexes, non_informative_mappings, unconnected_break_points

def GetContigParts(contigs, break_groups, covered_regions, remove_short_contigs, remove_zero_hit_contigs, min_mapping_length, alignment_precision, pdf):
    # Create a dataframe with the contigs being split into parts and contigs scheduled for removal not included
    if 0 == len(break_groups):
        # We do not have break points, so we don't need to split
        contigs['parts'] = 1
    else:
        contig_parts = break_groups.groupby('contig_id').size().reset_index(name='counts')
        contigs['parts'] = 0
        contigs.iloc[contig_parts['contig_id'], contigs.columns.get_loc('parts')] = contig_parts['counts'].values
        contigs['parts'] += 1

    contigs.loc[contigs['remove'], 'parts'] = 0

    contig_parts = pd.DataFrame({'contig':np.repeat(np.arange(len(contigs)), contigs['parts']), 'part':1, 'start':0})
    contig_parts['part'] = contig_parts.groupby('contig').cumsum()['part']-1
    if len(break_groups):
        for i in range(break_groups['num'].max()+1):
            contig_parts.loc[contig_parts['part']==i+1, 'start'] = break_groups.loc[break_groups['num']==i,'pos'].values
    contig_parts['end'] = contig_parts['start'].shift(-1,fill_value=0)
    contig_parts.loc[contig_parts['end']==0, 'end'] = contigs.iloc[contig_parts.loc[contig_parts['end']==0, 'contig'], contigs.columns.get_loc('length')].values
    contig_parts['name'] = contigs.iloc[contig_parts['contig'], contigs.columns.get_loc('name')].values

    # Trim unmapped part on ends of contig_parts
    if remove_zero_hit_contigs:
        cov_reg = contig_parts[['contig','part','start']].merge(covered_regions.rename(columns={'con_id':'contig'}), on='contig', how='left')
        cov_reg = cov_reg[cov_reg['to'] > cov_reg['start'] + (min_mapping_length if remove_short_contigs else 0)].copy()
        cov_reg = cov_reg.groupby(['contig','part'])['from'].min().reset_index()
        contig_parts['start'] = np.maximum(contig_parts['start'], contig_parts[['contig','part']].merge(cov_reg, on=['contig','part'], how='left')['from'].fillna(sys.maxsize*0.9).astype(int).values)
        cov_reg = contig_parts[['contig','part','end']].merge(covered_regions.rename(columns={'con_id':'contig'}), on='contig', how='left')
        cov_reg = cov_reg[cov_reg['from'] < cov_reg['end'] - (min_mapping_length if remove_short_contigs else 0)].copy()
        cov_reg = cov_reg.groupby(['contig','part'])['to'].max().reset_index()
        contig_parts['end'] = np.minimum(contig_parts['end'], contig_parts[['contig','part']].merge(cov_reg, on=['contig','part'], how='left')['to'].fillna(0).astype(int).values)

    # Remove short contig_parts < min_mapping_length + alignment_precision (adding alignment_precision gives a buffer so that a mapping to a short contig is not removed in one read and not removed in another based on a single base mapping or not)
    if remove_short_contigs:
        contig_parts = contig_parts[contig_parts['end']-contig_parts['start'] >= min_mapping_length + alignment_precision].copy()
        contig_parts['part'] = contig_parts.groupby(['contig'], sort=False).cumcount()
        contig_parts.reset_index(drop=True,inplace=True)
        contigs['parts'] = 0
        contig_sizes = contig_parts.groupby(['contig'], sort=False).size()
        contigs.loc[contig_sizes.index.values, 'parts'] = contig_sizes.values
    
    contigs['first_part'] = -1
    contigs.loc[contigs['parts']>0, 'first_part'] = contig_parts[contig_parts['part']==0].index
    contigs['last_part'] = contigs['first_part'] + contigs['parts'] - 1

    # Assign scaffold info from contigs to contig_parts
    contig_parts['org_dist_left'] = -1
    tmp_contigs = contigs[(contigs['parts']>0) & (contigs['parts'].shift(1, fill_value=-1)>0)]
    contig_parts.iloc[tmp_contigs['first_part'].values, contig_parts.columns.get_loc('org_dist_left')] = tmp_contigs['org_dist_left'].values
    contig_parts['org_dist_right'] = -1
    tmp_contigs = contigs[(contigs['parts']>0) & (contigs['parts'].shift(-1, fill_value=-1)>0)]
    contig_parts.iloc[tmp_contigs['last_part'].values, contig_parts.columns.get_loc('org_dist_right')] = tmp_contigs['org_dist_right'].values

    # Rescue scaffolds broken by removed contigs
    tmp_contigs = contigs[(contigs['parts']==0) & (contigs['org_dist_right'] != -1) & (contigs['org_dist_left'] != -1)].copy()
    tmp_contigs.reset_index(inplace=True)
    tmp_contigs['group'] = (tmp_contigs['index'] != tmp_contigs['index'].shift(1, fill_value=-1)).cumsum() # Group deleted contigs which are connected, so that we can pass the connectiong through multiple removed contigs
    tmp_contigs = tmp_contigs.groupby('group', sort=False)[['index','length','org_dist_right','org_dist_left']].agg(['first','last','sum'])
    tmp_contigs['from'] = tmp_contigs[('index','first')]-1
    tmp_contigs['to'] = tmp_contigs[('index','last')]+1
    tmp_contigs['len'] = tmp_contigs[('org_dist_left','first')] + tmp_contigs[('length','sum')] + tmp_contigs[('org_dist_right','sum')]
    tmp_contigs['from_part'] = contigs.iloc[tmp_contigs['from'], contigs.columns.get_loc('last_part')].values
    tmp_contigs['to_part'] = contigs.iloc[tmp_contigs['to'], contigs.columns.get_loc('first_part')].values

    tmp_contigs = tmp_contigs[(tmp_contigs['from_part'] >= 0) & (tmp_contigs['to_part'] >= 0)].copy()
    contig_parts.iloc[tmp_contigs['from_part'], contig_parts.columns.get_loc('org_dist_right')] = tmp_contigs['len'].values
    contig_parts.iloc[tmp_contigs['to_part'], contig_parts.columns.get_loc('org_dist_left')] = tmp_contigs['len'].values

    # Insert the break info as 0-length gaps
    contig_parts.loc[contig_parts['part'] > 0, 'org_dist_left'] = 0
    contig_parts.loc[contig_parts['part'].shift(-1, fill_value=0) > 0, 'org_dist_right'] = 0

    return contig_parts, contigs

def GetBreakAndRemovalInfo(result_info, contigs, contig_parts):
    part_count = contig_parts.groupby('contig', sort=False).size().reset_index(name='parts')
    part_count['breaks'] = part_count['parts']-1
    result_info['breaks'] = {}
    result_info['breaks']['opened'] = np.sum(part_count['breaks'])
    
    removed_length = contigs.loc[ np.isin(contigs.reset_index()['index'], part_count['contig']) == False, 'length' ].values
    result_info['removed'] = {}
    result_info['removed']['num'] = len(removed_length)
    result_info['removed']['total'] = np.sum(removed_length) if len(removed_length) else 0
    result_info['removed']['min'] = np.min(removed_length) if len(removed_length) else 0
    result_info['removed']['max'] = np.max(removed_length) if len(removed_length) else 0
    result_info['removed']['mean'] = np.mean(removed_length) if len(removed_length) else 0
    
    return result_info

def UpdateMappingsToContigParts(mappings, contig_parts, min_mapping_length, max_dist_contig_end, min_extension):
    # Duplicate mappings for every contig part (using inner, because some contigs have been removed (e.g. being too short)) 
    mappings = mappings.merge(contig_parts.reset_index()[['contig', 'index', 'part', 'start', 'end']].rename(columns={'contig':'t_id', 'index':'conpart', 'start':'part_start', 'end':'part_end'}), on=['t_id'], how='inner')
#
    # Remove mappings that do not touch a contig part or only less than min_mapping_length bases
    mappings = mappings[(mappings['t_start']+min_mapping_length < mappings['part_end']) & (mappings['t_end']-min_mapping_length > mappings['part_start'])].copy()
#
    # Update mapping information with limits from contig part
    mappings['con_from'] = np.maximum(mappings['t_start'], mappings['part_start'])  # We cannot work with 0 < mappings['num_part'], because of reads that start before mappings['con_start'] but do not cross the break region and therefore don't have a mapping before
    mappings['con_to'] = np.minimum(mappings['t_end'], mappings['part_end'])
    mappings['read_from'] = mappings['q_start']
    mappings['read_to'] = mappings['q_end']
#
    # Rough estimate of the split for the reads, better not use the new information, but not always avoidable
    # Forward strand
    multi_maps = mappings[(mappings['con_from'] > mappings['t_start']) & (mappings['strand'] == '+')]
    mappings.loc[(mappings['con_from'] > mappings['t_start']) & (mappings['strand'] == '+'), 'read_from'] = np.round(multi_maps['q_start'] + (multi_maps['q_end']-multi_maps['q_start'])/(multi_maps['t_end']-multi_maps['t_start'])*(multi_maps['con_from']-multi_maps['t_start'])).astype(int)
    multi_maps = mappings[(mappings['con_to'] < mappings['t_end']) & (mappings['strand'] == '+')]
    mappings.loc[(mappings['con_to'] < mappings['t_end']) & (mappings['strand'] == '+'), 'read_to'] = np.round(multi_maps['q_end'] - (multi_maps['q_end']-multi_maps['q_start'])/(multi_maps['t_end']-multi_maps['t_start'])*(multi_maps['t_end']-multi_maps['con_to'])).astype(int)
    # Reverse strand
    multi_maps = mappings[(mappings['con_from'] > mappings['t_start']) & (mappings['strand'] == '-')]
    mappings.loc[(mappings['con_from'] > mappings['t_start']) & (mappings['strand'] == '-'), 'read_to'] = np.round(multi_maps['q_end'] - (multi_maps['q_end']-multi_maps['q_start'])/(multi_maps['t_end']-multi_maps['t_start'])*(multi_maps['con_from']-multi_maps['t_start'])).astype(int)
    multi_maps = mappings[(mappings['con_to'] < mappings['t_end']) & (mappings['strand'] == '-')]
    mappings.loc[(mappings['con_to'] < mappings['t_end']) & (mappings['strand'] == '-'), 'read_from'] = np.round(multi_maps['q_start'] + (multi_maps['q_end']-multi_maps['q_start'])/(multi_maps['t_end']-multi_maps['t_start'])*(multi_maps['t_end']-multi_maps['con_to'])).astype(int)
#
    mappings['matches'] = np.round(mappings['matches']/(mappings['t_end']-mappings['t_start'])*(mappings['con_to']-mappings['con_from'])).astype(int) # Rough estimate use with care!
    mappings.sort_values(['q_id','read_start','read_from','read_to'], inplace=True)
    mappings.reset_index(inplace=True, drop=True)
#
    # Update connections
    mappings['next_con'] = np.where((mappings['q_id'].shift(-1, fill_value='') != mappings['q_id']) | (mappings['read_start'].shift(-1, fill_value=-1) != mappings['read_start']), -1, mappings['conpart'].shift(-1, fill_value=-1))
    mappings['next_strand'] = np.where(-1 == mappings['next_con'], '', mappings['strand'].shift(-1, fill_value=''))
    mappings['prev_con'] = np.where((mappings['q_id'].shift(1, fill_value='') != mappings['q_id']) | (mappings['read_start'].shift(1, fill_value=-1) != mappings['read_start']), -1, mappings['conpart'].shift(1, fill_value=-1))
    mappings['prev_strand'] = np.where(-1 == mappings['prev_con'], '', mappings['strand'].shift(1, fill_value=''))
#
    mappings['left_con'] = np.where('+' == mappings['strand'], mappings['prev_con'], mappings['next_con'])
    mappings['right_con'] = np.where('-' == mappings['strand'], mappings['prev_con'], mappings['next_con'])
#
    mappings['next_side'] = np.where('+' == mappings['next_strand'], 'l', 'r')
    mappings['prev_side'] = np.where('-' == mappings['prev_strand'], 'l', 'r')
    mappings['left_con_side'] = np.where('+' == mappings['strand'], mappings['prev_side'], mappings['next_side'])
    mappings.loc[-1 == mappings['left_con'], 'left_con_side'] = ''
    mappings['right_con_side'] = np.where('-' == mappings['strand'], mappings['prev_side'], mappings['next_side'])
    mappings.loc[-1 == mappings['right_con'], 'right_con_side'] = ''
#
    # Remove connections that are consistent with a contig part (not bridging it to itself) or are not starting within max_dist_contig_end
    mappings['left'] = mappings['con_from'] <= mappings['part_start']+max_dist_contig_end
    mappings['right'] = mappings['con_to'] >= mappings['part_end']-max_dist_contig_end
    mappings['remleft'] = (( (mappings['left'] == False) | ((mappings['left_con_side'] == 'r') & (False == np.where(mappings['strand'] == '+', mappings['right'].shift(1), mappings['right'].shift(-1))))
                                                         | ((mappings['left_con_side'] == 'l') & (False == np.where(mappings['strand'] == '+', mappings['left'].shift(1), mappings['left'].shift(-1)))) 
                          | ((mappings['left_con'] == mappings['conpart']) & (mappings['left_con_side'] == 'r')
                             & (mappings['con_from'] >= np.where(mappings['strand'] == '+', mappings['con_to'].shift(1), mappings['con_to'].shift(-1)))) ) & (mappings['left_con'] > 0))
    mappings['remright'] = ( ( (mappings['right'] == False) | ((mappings['right_con_side'] == 'r') & (False == np.where(mappings['strand'] == '+', mappings['right'].shift(-1), mappings['right'].shift(1))))
                                                            | ((mappings['right_con_side'] == 'l') & (False == np.where(mappings['strand'] == '+', mappings['left'].shift(-1), mappings['left'].shift(1)))) 
                          | ((mappings['right_con'] == mappings['conpart']) & (mappings['right_con_side'] == 'l')
                             & (mappings['con_to'] <= np.where(mappings['strand'] == '+', mappings['con_from'].shift(-1), mappings['con_from'].shift(1)))) ) & (mappings['right_con'] > 0))
    mappings.loc[mappings['remleft'], 'left_con'] = -1
    mappings.loc[mappings['remleft'], 'left_con_side'] = ''
    mappings.loc[mappings['remright'], 'right_con'] = -1
    mappings.loc[mappings['remright'], 'right_con_side'] = ''
#
    # Remove mappings that only connect a contig part to itself without extensions or contig part duplications
    remove = ( (mappings['remleft'] | (mappings['left'] == False) | ((mappings['left_con'] < 0) & (min_extension+mappings['con_from']-mappings['part_start'] > np.where(mappings['strand'] == '+', mappings['read_from'] - mappings['read_start'], mappings['read_end']-mappings['read_to'])))) &
               (mappings['remright'] | (mappings['right'] == False) | ((mappings['right_con'] < 0) & (min_extension+mappings['part_end']-mappings['con_to'] > np.where(mappings['strand'] == '-', mappings['read_from'] - mappings['read_start'], mappings['read_end']-mappings['read_to'])))) )
#
    # Get distance to connected contigparts
    mappings['next_dist'] = mappings['read_from'].shift(-1, fill_value=0) - mappings['read_to']
    mappings['prev_dist'] = mappings['read_from'] - mappings['read_to'].shift(1, fill_value=0)
    mappings['left_con_dist'] = np.where('+' == mappings['strand'], mappings['prev_dist'], mappings['next_dist'])
    mappings.loc[-1 == mappings['left_con'], 'left_con_dist'] = 0
    mappings['right_con_dist'] = np.where('-' == mappings['strand'], mappings['prev_dist'], mappings['next_dist'])
    mappings.loc[-1 == mappings['right_con'], 'right_con_dist'] = 0
#
    # Select the columns we still need
    mappings.rename(columns={'q_id':'read_id'}, inplace=True)
    mappings = mappings.loc[remove == False, ['read_id', 'read_start', 'read_end', 'read_from', 'read_to', 'strand', 'conpart', 'con_from', 'con_to', 'left_con', 'left_con_side', 'left_con_dist', 'right_con', 'right_con_side', 'right_con_dist', 'mapq', 'matches']]
#
    # Count how many mappings each read has
    mappings['num_mappings'] = 1
    num_mappings = mappings.groupby(['read_id','read_start'], sort=False)['num_mappings'].size().values
    mappings['num_mappings'] = np.repeat(num_mappings, num_mappings)

    return mappings



def FindDuplicatedContigPartEnds(repeats, contig_parts, max_dist_contig_end, proportion_duplicated):
    repeat_ends = repeats.copy()
    for s in ['q','t']:
        # Update repeats to contig_parts
        repeat_ends = repeat_ends.merge(contig_parts[['contig','start','end']].reset_index().rename(columns={'index':f'{s}_con','contig':f'{s}_id', 'start':f'{s}_confrom', 'end':f'{s}_conto'}), on=[f'{s}_id'], how='inner')
        repeat_ends.drop(columns=[f'{s}_id',f'{s}_len'], inplace=True)
        repeat_ends = repeat_ends[(repeat_ends[f'{s}_start'] < repeat_ends[f'{s}_conto']) & (repeat_ends[f'{s}_confrom'] < repeat_ends[f'{s}_end'])].copy()
        repeat_ends[f'{s}_start'] = np.maximum(repeat_ends[f'{s}_start'], repeat_ends[f'{s}_confrom'])
        repeat_ends[f'{s}_end'] = np.minimum(repeat_ends[f'{s}_end'], repeat_ends[f'{s}_conto'])
    repeat_ends = repeat_ends[['q_con','q_confrom','q_conto','q_start','q_end','strand','t_con','t_confrom','t_conto','t_start','t_end']].copy()
    # Find repeats at the left side of contig_parts
    cur_repeats = repeat_ends.sort_values(['q_con','t_con','q_start'])
    cur_repeats = cur_repeats[cur_repeats['q_start']-cur_repeats['q_confrom'] <= max_dist_contig_end].copy()
    cur_repeats = cur_repeats[(cur_repeats['q_end']-cur_repeats['q_start']) / (cur_repeats['q_end']-cur_repeats['q_confrom']) >= proportion_duplicated].copy()
    cur_repeats['side'] = 'l'
    # Check if the left repeats extend to a full repeat of the whole contig
    full_reps = cur_repeats[cur_repeats['q_conto']-cur_repeats['q_end'] <= max_dist_contig_end].copy()
    full_reps = full_reps[(full_reps['q_end']-full_reps['q_start']) / (full_reps['q_conto']-full_reps['q_confrom']) >= proportion_duplicated].copy()
    cur_repeats.loc[full_reps.index.values, 'side'] = 'lr'
    # Find repeats at right side of contig_parts
    right_repeats = repeat_ends.sort_values(['q_con','t_con','q_end'], ascending=[True,True,False])
    right_repeats = right_repeats[right_repeats['q_conto']-right_repeats['q_end'] <= max_dist_contig_end].copy()
    right_repeats = right_repeats[(right_repeats['q_end']-right_repeats['q_start']) / (right_repeats['q_conto']-right_repeats['q_start']) >= proportion_duplicated].copy()
    right_repeats.drop(full_reps.index.values, inplace=True)
    right_repeats['side'] = 'r'
    repeat_ends = pd.concat([cur_repeats,right_repeats], ignore_index=True)
    repeat_ends.sort_values(['q_con','t_con','side'], inplace=True)
#
    return repeat_ends

def CreateBridges(left_bridge, right_bridge, min_distance_tolerance, rel_distance_tolerance):
    bridges = pd.concat([left_bridge,right_bridge], ignore_index=True, sort=False)

    # Separate bridges if distances are far apart
    bridges.sort_values(['from','from_side','to','to_side','distance'], inplace=True)
    bridges['group'] = ( (bridges['from'] != bridges['from'].shift(1)) | (bridges['from_side'] != bridges['from_side'].shift(1)) |
                         (bridges['to'] != bridges['to'].shift(1)) | (bridges['to_side'] != bridges['to_side'].shift(1)) |
                         (bridges['distance'] > bridges['distance'].shift(1)+min_distance_tolerance+rel_distance_tolerance*np.maximum(np.abs(bridges['distance']),np.abs(bridges['distance'].shift(1)))) ).cumsum()

    # Bundle identical bridges
    bridged_dists = bridges.groupby(['from','from_side','to','to_side','group'], sort=False)['distance'].agg(['mean','min','max']).reset_index().rename(columns={'mean':'mean_dist','min':'min_dist','max':'max_dist'})
    bridged_dists['mean_dist'] = np.round(bridged_dists['mean_dist']).astype(int)
    bridges = bridges.groupby(['from','from_side','to','to_side','group','mapq']).size().reset_index(name='count')

    # Get cumulative counts (counts for this trust level and higher)
    bridges['count'] = np.where( (bridges['from'] == bridges['to']) & (bridges['from_side'] == bridges['to_side']), bridges['count']//2, bridges['count']) # Bridges that go back to the same contig end are counted twice (from each end once), so we have to correct for this
    bridges.sort_values(['from','from_side','to','to_side','group','mapq'], ascending=[True, True, True, True, True, False], inplace=True)
    bridges['cumcount'] = bridges.groupby(['from','from_side','to','to_side','group'], sort=False)['count'].cumsum().values
    bridges.drop(columns=['count'], inplace=True)

    # Add distances back in
    bridges = bridges.merge(bridged_dists, on=['from','from_side','to','to_side','group'], how='left')
    bridges.drop(columns=['group'], inplace=True) # We don't need the group anymore because it is encoded in ['from','from_side','to','to_side','mean_dist'] now

    return bridges

def PrepareBridgeExtLen(bridge_extlen, bridges):
    # Assign mean_dist from bridges to bridge_extlen
    bridge_extlen = bridge_extlen.merge(bridges[['from','from_side','to','to_side','mean_dist','min_dist','max_dist']].drop_duplicates(), on=['from','from_side','to','to_side'], how='left')
    bridge_extlen = bridge_extlen[(bridge_extlen['min_dist'] <= bridge_extlen['distance']) & (bridge_extlen['distance'] <= bridge_extlen['max_dist'])].drop(columns=['min_dist','max_dist'])    
#
    return bridge_extlen

def MarkOrgScaffoldBridges(bridges, contig_parts, requirement):
    # requirement (-1: org scaffold, 0: unbroken org scaffold)
    bridges['org_scaffold'] = False
    bridges.loc[(bridges['from']+1 == bridges['to']) & ('r' == bridges['from_side']) & ('l' == bridges['to_side']) & (requirement < contig_parts.iloc[bridges['from'].values, contig_parts.columns.get_loc('org_dist_right')].values), 'org_scaffold'] = True
    bridges.loc[(bridges['from']-1 == bridges['to']) & ('l' == bridges['from_side']) & ('r' == bridges['to_side']) & (requirement < contig_parts.iloc[bridges['from'].values, contig_parts.columns.get_loc('org_dist_left')].values), 'org_scaffold'] = True

    return bridges

def CountAlternatives(bridges):
    bridges.sort_values(['to', 'to_side','from','from_side'], inplace=True)
    alternatives = bridges.groupby(['to','to_side'], sort=False).size().values
    bridges['to_alt'] = np.repeat(alternatives, alternatives)
    bridges.sort_values(['from','from_side','to','to_side'], inplace=True)
    alternatives = bridges.groupby(['from','from_side'], sort=False).size().values
    bridges['from_alt'] = np.repeat(alternatives, alternatives)

    return bridges

def FilterBridges(bridges, bridge_extlen, borderline_removal, min_factor_alternatives, min_num_reads, org_scaffold_trust, cov_probs, prob_factor, min_mapping_length, contig_parts, prematurity_threshold, pdf=None):
    if org_scaffold_trust in ["blind", "full"]:
        bridges = MarkOrgScaffoldBridges(bridges, contig_parts, -1)
        if "blind" == org_scaffold_trust:
            # Make sure the org scaffolds are used if any read is there and do not use any other connection there even if we don't have a read
            bridges['org_scaf_conflict'] = False
            bridges.loc[('r' == bridges['from_side']) & (-1 != contig_parts.iloc[bridges['from'].values, contig_parts.columns.get_loc('org_dist_right')].values), 'org_scaf_conflict'] = True
            bridges.loc[('l' == bridges['from_side']) & (-1 != contig_parts.iloc[bridges['from'].values, contig_parts.columns.get_loc('org_dist_left')].values), 'org_scaf_conflict'] = True
            bridges.loc[('r' == bridges['to_side']) & (-1 != contig_parts.iloc[bridges['to'].values, contig_parts.columns.get_loc('org_dist_right')].values), 'org_scaf_conflict'] = True
            bridges.loc[('l' == bridges['to_side']) & (-1 != contig_parts.iloc[bridges['to'].values, contig_parts.columns.get_loc('org_dist_left')].values), 'org_scaf_conflict'] = True

            bridges = bridges[(False == bridges['org_scaf_conflict']) | bridges['org_scaffold']]

            bridges.drop(columns=['org_scaf_conflict'], inplace=True)
    elif "basic" == org_scaffold_trust:
        bridges = MarkOrgScaffoldBridges(bridges, contig_parts, 0) # Do not connect previously broken contigs through low quality reads
    else:
        bridges['org_scaffold'] = False # Do not use original scaffolds

    # Set lowq flags for all bridges that don't fulfill min_num_reads
    bridges['low_q'] = (bridges['cumcount'] < min_num_reads)
#
    # Remove lowq for the distance with the highest counts of each original scaffolds
    bridges.sort_values(['from','from_side','to','to_side','mapq','cumcount'], ascending=[True, True, True, True,False, False], inplace=True)
    bridges['max_count'] = bridges.groupby(['from','from_side','to','to_side','mapq'], sort=False)['cumcount'].cummax()
    bridges.loc[ bridges['org_scaffold'] & (bridges['max_count'] == bridges['cumcount']), 'low_q'] = False
    bridges.drop(columns=['max_count'], inplace=True)
#
    if borderline_removal:
        # Set lowq flags for all bridges that are not above min_factor_alternatives compared to other lowq bridges (so that we do not have borderline decisions where +-1 count decides that one bridge is accepted and the other not)
        old_num_lowq = np.sum(bridges['low_q'])
        bridges.sort_values(['from','from_side','mapq','cumcount'], ascending=[True, True, False, True], inplace=True)
        bridges['from_group'] = ( (bridges['from'] != bridges['from'].shift(1)) | (bridges['from_side'] != bridges['from_side'].shift(1)) | bridges['org_scaffold'] |
                                  (bridges['mapq'] != bridges['mapq'].shift(1)) | (bridges['cumcount'] > np.ceil(bridges['cumcount'].shift(1, fill_value = 0)*min_factor_alternatives)) ).astype(int).cumsum()
        bridges['low_q'] = bridges.groupby(['from_group'], sort=False)['low_q'].cumsum().astype(bool)
#
        bridges.sort_values(['to','to_side','mapq','cumcount'], ascending=[True, True, False, True], inplace=True)
        bridges['to_group'] = ( (bridges['to'] != bridges['to'].shift(1)) | (bridges['to_side'] != bridges['to_side'].shift(1)) | bridges['org_scaffold'] |
                              (bridges['mapq'] != bridges['mapq'].shift(1)) | (bridges['cumcount'] > np.ceil(bridges['cumcount'].shift(1, fill_value = 0)*min_factor_alternatives)) ).astype(int).cumsum()
        bridges['low_q'] = bridges.groupby(['to_group'], sort=False)['low_q'].cumsum().astype(bool)
#
        num_lowq = np.sum(bridges['low_q'])
        while old_num_lowq != num_lowq:
            bridges.sort_values(['from_group','cumcount'], ascending=[True, True], inplace=True)
            bridges['low_q'] = bridges.groupby(['from_group'], sort=False)['low_q'].cumsum().astype(bool)
            bridges.sort_values(['to_group','cumcount'], ascending=[True, True], inplace=True)
            bridges['low_q'] = bridges.groupby(['to_group'], sort=False)['low_q'].cumsum().astype(bool)
            old_num_lowq = num_lowq
            num_lowq = np.sum(bridges['low_q'])
#
        bridges.drop(columns=['from_group','to_group'], inplace=True)
#
    # Remove low quality bridges
    bridges = bridges[bridges['low_q'] == False].copy()
    bridges.drop(columns=['low_q'], inplace=True)
#
    # Check for prematurely ending bridges with read length after the gap (extension length)
    # Start by getting the longest extension for every bridge and summarise that for every starting conting end
    extlen_sum = bridge_extlen.groupby(['from','from_side','to','to_side','mean_dist']).agg({'from_extlen':'max','to_extlen':'max', 'distance':'size'}).reset_index().rename(columns={'distance':'counts'})
    extlen_sum = extlen_sum.merge(bridges[['from','from_side','to','to_side','mean_dist']].drop_duplicates(), on=['from','from_side','to','to_side','mean_dist'], how='inner') # Remove already filtered bridges
    max_extlen = extlen_sum.groupby(['from','from_side']).agg({'from_extlen':'max', 'to_extlen':'max'}).reset_index()
#
    # Get probability that they do not end prematurely for all bridges compared to the longest mapping/extension, conflicting bridge (for the bridge side on the same contig (from) and the other side, where contigs can be different (to))
    for e in ['from','to']:
        cur_extlen = extlen_sum[['from','from_side','to','to_side','mean_dist',f'{e}_extlen','counts']].merge(max_extlen[['from','from_side',f'{e}_extlen']].rename(columns={f'{e}_extlen':'max_extlen'}), on=['from','from_side'], how='inner') # Bridges where all alternatives reach the end do not need to be handled
        cur_extlen.rename(columns={f'{e}_extlen':'extlen'}, inplace=True)
        cur_max = cur_extlen.loc[cur_extlen['extlen'] == cur_extlen['max_extlen'], ['from','from_side','to','to_side','mean_dist', 'counts']].copy()
        cur_extlen = cur_extlen[cur_extlen['extlen'] < cur_extlen['max_extlen']].drop(columns=['max_extlen']) # If the bridge has the longest mapping it cannot end prematurely
        gaplen = bridge_extlen[['from','from_side','to','to_side','mean_dist',f'{e}_extlen',f'{e}_gaplen']].rename(columns={f'{e}_extlen':'extlen',f'{e}_gaplen':'gaplen'}).merge(cur_extlen[['from','from_side','to','to_side','mean_dist','extlen']].reset_index(), on=['from','from_side','to','to_side','mean_dist','extlen'], how='inner')
        gaplen = gaplen.groupby(['index'])['gaplen'].max().reset_index()
        cur_extlen.loc[gaplen['index'].values, 'gaplen'] = gaplen['gaplen'].values
        cur_extlen.rename(columns={col:f'test_{col}' for col in ['to','to_side','mean_dist','counts']}, inplace=True)
        cur_extlen = cur_extlen.merge(cur_max, on=['from','from_side'], how='left') # If multiple longest extensions exist test against all of them
        longer_extensions = cur_extlen[['from','from_side','to','to_side','mean_dist','extlen','gaplen']].drop_duplicates().merge(bridge_extlen[['from','from_side','to','to_side','mean_dist',f'{e}_extlen',f'{e}_gaplen']], on=['from','from_side','to','to_side','mean_dist'], how='left')
        longer_extensions = longer_extensions[longer_extensions['extlen'] + np.maximum(0, longer_extensions['gaplen']-longer_extensions[f'{e}_gaplen']) < longer_extensions[f'{e}_extlen']].groupby(['from','from_side','to','to_side','mean_dist','extlen','gaplen']).size().reset_index(name='longer')
        cur_extlen['longer'] = cur_extlen[['from','from_side','to','to_side','mean_dist','extlen','gaplen']].merge(longer_extensions, on=['from','from_side','to','to_side','mean_dist','extlen','gaplen'], how='left')['longer'].fillna(0).astype(int)
        cur_extlen.drop(columns=['extlen','gaplen','to','to_side','mean_dist'], inplace=True)
        cur_extlen['premat_prob'] = GetPrematureStopProb(cur_extlen['test_counts'], cur_extlen['counts'], cur_extlen['longer'])
        cur_extlen = cur_extlen.groupby(['from','from_side','test_to','test_to_side','test_mean_dist'])['premat_prob'].min().reset_index()  # Since we tested against all longest mappings, take the lowest probability
        cur_extlen.rename(columns={f'test_{col}':col for col in ['to','to_side','mean_dist']}, inplace=True)
        bridges[f'same_from_premat' if e == 'from' else f'diff_from_premat'] = bridges[['from','from_side','to','to_side','mean_dist']].merge(cur_extlen, on=['from','from_side','to','to_side','mean_dist'], how='left')['premat_prob'].fillna(1.0).values
        bridges[f'same_to_premat' if e == 'from' else f'diff_to_premat'] = bridges[['from','from_side','to','to_side','mean_dist']].merge(cur_extlen.rename(columns={'from':'to','to':'from','from_side':'to_side','to_side':'from_side'}), on=['from','from_side','to','to_side','mean_dist'], how='left')['premat_prob'].fillna(1.0).values
#
    # Remove prematurely ending bridges (if this would remove all bridges from both sides keep the ones with the highest probability)
    premat_cols = ['same_from_premat','same_to_premat','diff_from_premat','diff_to_premat']
    bridges['premat_prob'] = bridges[premat_cols].min(axis=1)
    bridges.drop(columns=premat_cols, inplace=True)
    bridges['not_premat'] = bridges['premat_prob'] > prematurity_threshold
    bridges[['max_premat_from','not_premat_from']] = bridges[['from','from_side']].merge(bridges.groupby(['from','from_side']).agg({'premat_prob':'max','not_premat':'sum'}).reset_index(), on=['from','from_side'], how='left')[['premat_prob','not_premat']].values
    bridges[['max_premat_to','not_premat_to']] = bridges[['to','to_side']].merge(bridges.groupby(['to','to_side']).agg({'premat_prob':'max','not_premat':'sum'}).reset_index(), on=['to','to_side'], how='left')[['premat_prob','not_premat']].values
    bridges = bridges[bridges['not_premat'] | ((bridges['not_premat_from'] == 0) & (bridges['not_premat_to'] == 0) & ((bridges['premat_prob'] == bridges['max_premat_from']) | (bridges['premat_prob'] == bridges['max_premat_to'])))].drop(columns=['premat_prob','max_premat_from','not_premat','not_premat_from','max_premat_to','not_premat_to'])
#
    # Set highq flags for the most likely bridges and the ones within prob_factor of the most likely
    bridges['probability'] = GetConProb(cov_probs, np.where(0 <= bridges['mean_dist'], bridges['mean_dist']+2*min_mapping_length, min_mapping_length+np.maximum(min_mapping_length,-bridges['mean_dist'])), bridges['cumcount'])
    bridges.sort_values(['to','to_side','mapq'], inplace=True)
    alternatives = bridges.groupby(['to','to_side','mapq'], sort=False)['probability'].agg(['max','size'])
    bridges['to_max_prob'] = np.repeat(alternatives['max'].values, alternatives['size'].values)
    bridges.sort_values(['from','from_side','mapq'], inplace=True)
    alternatives = bridges.groupby(['from','from_side','mapq'], sort=False)['probability'].agg(['max','size'])
    bridges['from_max_prob'] = np.repeat(alternatives['max'].values, alternatives['size'].values)
    bridges['highq'] = ((bridges['probability']*prob_factor >= bridges['from_max_prob']) & (bridges['probability']*prob_factor >= bridges['to_max_prob']))
#
    # Only keep high quality bridges
    bridges = bridges[bridges['highq']].copy()
    bridges.drop(columns=['highq','from_max_prob','to_max_prob'], inplace=True)
#
    # Remove bridges that compete with a bridge with a higher trust level
    bridges.sort_values(['from','from_side','mapq'], ascending=[True, True, False], inplace=True)
    bridges['max_mapq'] = bridges.groupby(['from','from_side'], sort=False)['mapq'].cummax()
    bridges.sort_values(['to','to_side','mapq'], ascending=[True, True, False], inplace=True)
    bridges['max_mapq2'] = bridges.groupby(['to','to_side'], sort=False)['mapq'].cummax()
    bridges = bridges[np.maximum(bridges['max_mapq'], bridges['max_mapq2']) <= bridges['mapq']].copy()
    bridges.drop(columns=['max_mapq','max_mapq2'], inplace=True)
#
    if "full" == org_scaffold_trust:
        # Remove ambiguous bridges that compeat with the original scaffold
        bridges.sort_values(['from','from_side','to','to_side'], inplace=True)
        org_scaffolds = bridges.groupby(['from','from_side'], sort=False)['org_scaffold'].agg(['size','sum'])
        bridges['org_from'] = np.repeat(org_scaffolds['sum'].values, org_scaffolds['size'].values)
        bridges.sort_values(['to','to_side','from','from_side'], inplace=True)
        org_scaffolds = bridges.groupby(['to','to_side'], sort=False)['org_scaffold'].agg(['size','sum'])
        bridges['org_to'] = np.repeat(org_scaffolds['sum'].values, org_scaffolds['size'].values)
        bridges = bridges[ bridges['org_scaffold'] | ((0 == bridges['org_from']) & (0 == bridges['org_to'])) ].copy()
#
    # Count alternatives
    bridges.drop(columns=['org_scaffold'], inplace=True)
    bridges = CountAlternatives(bridges)
    bridges['from'] = bridges['from'].astype(int)
    bridges['to'] = bridges['to'].astype(int)
    bridges.rename(columns={'cumcount':'bcount'}, inplace=True)
#
    return bridges

def GetBridges(mappings, borderline_removal, min_factor_alternatives, min_num_reads, org_scaffold_trust, contig_parts, cov_probs, prob_factor, min_mapping_length, min_distance_tolerance, rel_distance_tolerance, prematurity_threshold, max_dist_contig_end, pdf):
    # Get bridges
    for s in ['left','right']:
        cur_bridge = mappings.loc[mappings[f'{s}_con'] >= 0, ['conpart',f'{s}_con',f'{s}_con_side',f'{s}_con_dist','mapq','con_from','con_to','read_start','read_end','read_from','read_to','strand']].copy()
        cur_bridge.rename(columns={'conpart':'from',f'{s}_con':'to',f'{s}_con_side':'to_side','mapq':'from_mapq',f'{s}_con_dist':'distance'}, inplace=True)
        cur_bridge['from_side'] = s[0]
        cur_bridge['to_mapq'] = np.where(('+' if s == 'left' else '-') == mappings['strand'], mappings['mapq'].shift(1, fill_value = -1), mappings['mapq'].shift(-1, fill_value = -1))[mappings[f'{s}_con'] >= 0]
        cur_bridge['mapq'] = np.where(cur_bridge['to_mapq'] < cur_bridge['from_mapq'], cur_bridge['to_mapq'].astype(int)*1000+cur_bridge['from_mapq'], cur_bridge['from_mapq'].astype(int)*1000+cur_bridge['to_mapq'])
        cur_bridge.drop(columns=['from_mapq','to_mapq'], inplace=True)
        for e in ['from','to']:
            # Get maplen and gaplen
            cur_bridge['maplen'] = cur_bridge['con_to'] - cur_bridge['con_from']
            cur_bridge['con_from'] -= contig_parts.loc[cur_bridge[e].values, 'start'].values
            cur_bridge['con_to'] = contig_parts.loc[cur_bridge[e].values, 'end'].values - cur_bridge['con_to']
            cur_bridge['maplen'] += np.where(cur_bridge[f'{e}_side'] == 'l', cur_bridge['con_from'], cur_bridge['con_to']) # when it start to map later, we do not care, important is how far it maps
            cur_bridge[f'{e}_gaplen'] = np.where(cur_bridge['distance'] >= 0, cur_bridge['distance']+min_mapping_length, np.maximum(-cur_bridge['distance'], min_mapping_length)) - np.where(cur_bridge[f'{e}_side'] == 'l', cur_bridge['con_from'], cur_bridge['con_to'])
            # Get extlen
            cur_bridge[f'{e}_extlen'] = np.where((cur_bridge[f'{e}_side'] == 'l') == (cur_bridge['strand'] == '+'), cur_bridge['read_end']-cur_bridge['read_to'], cur_bridge['read_from']-cur_bridge['read_start']) + cur_bridge['maplen']
            cur_bridge.drop(columns=['maplen'], inplace=True)
            # Prepare to side
            if e == 'from':
                cur_bridge[['con_from','con_to','read_start','read_end','read_from','read_to','strand']] = np.where(('+' if s == 'left' else '-') == mappings[['strand','strand','strand','strand','strand','strand','strand']].values, mappings[['con_from','con_to','read_start','read_end','read_from','read_to','strand']].shift(1, fill_value = -1).values, mappings[['con_from','con_to','read_start','read_end','read_from','read_to','strand']].shift(-1, fill_value = -1).values)[mappings[f'{s}_con'].values >= 0]
            else:
                cur_bridge.drop(columns=['con_from','con_to','read_start','read_end','read_from','read_to','strand'], inplace=True)
        if s == 'left':
            left_bridge = cur_bridge[['from','from_side','to','to_side','distance','mapq']].copy()
            bridge_extlen = cur_bridge.drop(columns=['mapq'])
        else:
            right_bridge = cur_bridge[['from','from_side','to','to_side','distance','mapq']].copy()
            bridge_extlen = pd.concat([bridge_extlen, cur_bridge.drop(columns=['mapq'])], ignore_index=True)
#
    bridges = CreateBridges(left_bridge, right_bridge, min_distance_tolerance, rel_distance_tolerance)
    bridge_extlen = PrepareBridgeExtLen(bridge_extlen, bridges)
    bridges = FilterBridges(bridges, bridge_extlen, borderline_removal, min_factor_alternatives, min_num_reads, org_scaffold_trust, cov_probs, prob_factor, min_mapping_length, contig_parts, prematurity_threshold, pdf)

    return bridges

def InsertBridgesOnUniqueContigOverlapsWithoutConnections(bridges, repeats, min_len_contig_overlap, min_num_reads):
    if len(repeats):
        # Find overlapping ends
        overlaps = repeats.rename(columns={'side':'q_side'})
        overlaps = overlaps.merge(overlaps.rename(columns={**{col:f't{col[1:]}' for col in overlaps.columns if col[0] == 'q'}, **{col:f'q{col[1:]}' for col in overlaps.columns if col[0] == 't'}}), on=[col for col in overlaps.columns if col != 'q_side'], how='inner')
        # Filter fully duplicated contigs
        overlaps = overlaps[(overlaps['q_side'] != 'lr') & (overlaps['t_side'] != 'lr')].copy()
        # Only the duplicated part should overlap, not the rest of the contig
        overlaps = overlaps[(overlaps['q_side'] != overlaps['t_side']) == (overlaps['strand'] == '+')].drop(columns=['strand'])
    else:
        overlaps = []
    if len(overlaps):
        # Overlap needs to be unique or at least min_len_contig_overlap longer than all other options (if there is no other option it has to be at least min_len_contig_overlap long)
        for s in ['q','t']:
            overlaps[f'{s}_overlap'] = np.where(overlaps[f'{s}_side'] == 'l', overlaps[f'{s}_end'] - overlaps[f'{s}_confrom'], overlaps[f'{s}_conto'] - overlaps[f'{s}_start'])
        overlaps.drop(columns=['q_confrom','q_conto','t_confrom','t_conto'], inplace=True)
        overlaps.sort_values(['q_con','q_side','q_overlap'], ascending=[True,True,False], inplace=True)
        overlaps['q_alt_overlap'] = np.where( (overlaps['q_con'] == overlaps['q_con'].shift(-1)) & (overlaps['q_side'] == overlaps['q_side'].shift(-1)), overlaps['q_overlap'].shift(-1, fill_value=0), 0)
        overlaps = overlaps[(overlaps['q_con'] != overlaps['q_con'].shift(1)) & (overlaps['q_side'] != overlaps['q_side'].shift(1))].copy()
        overlaps = overlaps[overlaps['q_overlap'] >= overlaps['q_alt_overlap'] + min_len_contig_overlap].copy()
        overlaps.drop(columns=['q_alt_overlap'], inplace=True)
    if len(overlaps):
        overlaps = overlaps.merge(overlaps.rename(columns={**{col:f't{col[1:]}' for col in overlaps.columns if col[0] == 'q'}, **{col:f'q{col[1:]}' for col in overlaps.columns if col[0] == 't'}}), on=list(overlaps.columns), how='inner')
        # We do not handle overlaps with itself (could later implement trimming of large overlap for circular contigs)
        overlaps = overlaps[overlaps['q_con'] != overlaps['t_con']].copy()
        # Only handle overlaps if they do not have bridges/connections on that side
        for s in ['q','t']:
            if len(overlaps):
                overlaps = overlaps[ overlaps[[f'{s}_con',f'{s}_side']].rename(columns={f'{s}_con':'from',f'{s}_side':'from_side'}).merge(bridges[['from','from_side']].drop_duplicates(), on=['from','from_side'], how='left', indicator=True)['_merge'].values == "left_only" ].copy()
    # Add the valid overlaps to bridges
    overlap_bridges = overlaps.copy()
    if len(overlaps):
        overlaps.rename(columns={'q_con':'from','q_side':'from_side','t_con':'to','t_side':'to_side'}, inplace=True)
        overlaps['mapq'] = bridges['mapq'].max()
        overlaps['bcount'] = min_num_reads
        overlaps['min_dist'] = -np.maximum(overlaps['q_overlap'], overlaps['t_overlap'])
        overlaps['max_dist'] = -np.minimum(overlaps['q_overlap'], overlaps['t_overlap'])
        overlaps['mean_dist'] = (overlaps['min_dist']+overlaps['max_dist'])//2
        overlaps['probability'] = 0.5
        overlaps[['to_alt','from_alt']] = 1
        overlaps = overlaps[['from','from_side','to','to_side','mapq','bcount','mean_dist','min_dist','max_dist','probability','to_alt','from_alt']].copy()
        bridges = pd.concat([bridges, overlaps], ignore_index=True)
        bridges.sort_values(['from','from_side','to','to_side','mean_dist'], inplace=True)
    
    return bridges, overlap_bridges

def ScaffoldAlongGivenConnections(scaffolds, scaffold_parts):
    # Handle right-right scaffold connections (keep scaffold with lower id and reverse+add the other one): We can only create new r-r or r-l connections
    keep = scaffolds.loc[(scaffolds['rscaf_side'] == 'r') & (scaffolds['scaffold'] < scaffolds['rscaf']), 'scaffold'].values
    while len(keep):
        scaffolds.loc[keep, 'size'] += scaffolds.loc[scaffolds.loc[keep, 'rscaf'].values, 'size'].values
        # Update scaffold_parts
        absorbed = np.isin(scaffold_parts['scaffold'], scaffolds.loc[keep, 'rscaf'])
        scaffold_parts.loc[absorbed, 'reverse'] = scaffold_parts.loc[absorbed, 'reverse'] == False
        absorbed_values = scaffold_parts.merge(scaffolds.loc[keep, ['scaffold','rscaf','size']].rename(columns={'scaffold':'new_scaf','rscaf':'scaffold'}), on=['scaffold'], how='left').loc[absorbed, ['new_scaf','size']]
        scaffold_parts.loc[absorbed, 'pos'] = absorbed_values['size'].astype(int).values - scaffold_parts.loc[absorbed, 'pos'].values - 1
        scaffold_parts.loc[absorbed, 'scaffold'] = absorbed_values['new_scaf'].astype(int).values
        # Update scaffolds (except 'size' which was updated before)
        absorbed = scaffolds.loc[keep, 'rscaf'].values
        scaffolds.loc[keep, 'right'] = scaffolds.loc[absorbed, 'left'].values
        scaffolds.loc[keep, 'rside'] = scaffolds.loc[absorbed, 'lside'].values
        scaffolds.loc[keep, 'rextendible'] = scaffolds.loc[absorbed, 'lextendible'].values
        new_scaffold_ids = scaffolds.loc[keep, ['scaffold','rscaf']].rename(columns={'scaffold':'new_scaf','rscaf':'scaffold'}).copy() # Store this to later change the connections to the removed scaffold
        scaffolds.loc[keep, 'rscaf'] = scaffolds.loc[absorbed, 'lscaf'].values
        scaffolds.loc[keep, 'rscaf_side'] = scaffolds.loc[absorbed, 'lscaf_side'].values
        # Drop removed scaffolds and update the connections
        scaffolds.drop(absorbed, inplace=True)
        scaffolds = scaffolds.merge(new_scaffold_ids.rename(columns={'scaffold':'lscaf'}), on=['lscaf'], how='left')
        scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'lscaf_side'] = 'r'
        scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'lscaf'] = scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'new_scaf'].astype(int)
        scaffolds.drop(columns=['new_scaf'], inplace=True)
        scaffolds = scaffolds.merge(new_scaffold_ids.rename(columns={'scaffold':'rscaf'}), on=['rscaf'], how='left')
        scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'rscaf_side'] = 'r'
        scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'rscaf'] = scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'new_scaf'].astype(int)
        scaffolds.drop(columns=['new_scaf'], inplace=True)
        scaffolds.index = scaffolds['scaffold'].values # Make sure we can access scaffolds with .loc[scaffold]
        # Prepare next round
        keep = scaffolds.loc[(scaffolds['rscaf_side'] == 'r') & (scaffolds['scaffold'] < scaffolds['rscaf']), 'scaffold'].values

    # Handle left-left scaffold connections (keep scaffold with lower id and reverse+add the other one): We can only create new l-l or l-r connections
    keep = scaffolds.loc[(scaffolds['lscaf_side'] == 'l') & (scaffolds['scaffold'] < scaffolds['lscaf']), 'scaffold'].values
    while len(keep):
        # Update scaffold_parts
        absorbed = np.isin(scaffold_parts['scaffold'], scaffolds.loc[keep, 'lscaf'])
        scaffold_parts.loc[absorbed, 'reverse'] = scaffold_parts.loc[absorbed, 'reverse'] == False
        scaffold_parts.loc[absorbed, 'scaffold'] = scaffold_parts.merge(scaffolds.loc[keep, ['scaffold','lscaf']].rename(columns={'scaffold':'new_scaf','lscaf':'scaffold'}), on=['scaffold'], how='left').loc[absorbed, 'new_scaf'].astype(int).values
        scaffold_parts.loc[absorbed, 'pos'] = scaffold_parts.loc[absorbed, 'pos']*-1 - 1
        scaffold_parts.sort_values(['scaffold','pos'], inplace=True)
        scaffold_parts['pos'] = scaffold_parts.groupby(['scaffold'], sort=False).cumcount() # Shift positions so that we don't have negative numbers anymore
        # Update scaffolds
        absorbed = scaffolds.loc[keep, 'lscaf'].values
        scaffolds.loc[keep, 'left'] = scaffolds.loc[absorbed, 'right'].values
        scaffolds.loc[keep, 'lside'] = scaffolds.loc[absorbed, 'rside'].values
        scaffolds.loc[keep, 'lextendible'] = scaffolds.loc[absorbed, 'rextendible'].values
        scaffolds.loc[keep, 'size'] += scaffolds.loc[absorbed, 'size'].values
        new_scaffold_ids = scaffolds.loc[keep, ['scaffold','lscaf']].rename(columns={'scaffold':'new_scaf','lscaf':'scaffold'}).copy() # Store this to later change the connections to the removed scaffold
        scaffolds.loc[keep, 'lscaf'] = scaffolds.loc[absorbed, 'rscaf'].values
        scaffolds.loc[keep, 'lscaf_side'] = scaffolds.loc[absorbed, 'rscaf_side'].values
        # Drop removed scaffolds and update the connections
        scaffolds.drop(absorbed, inplace=True)
        scaffolds = scaffolds.merge(new_scaffold_ids.rename(columns={'scaffold':'lscaf'}), on=['lscaf'], how='left')
        scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'lscaf_side'] = 'l'
        scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'lscaf'] = scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'new_scaf'].astype(int)
        scaffolds.drop(columns=['new_scaf'], inplace=True)
        scaffolds = scaffolds.merge(new_scaffold_ids.rename(columns={'scaffold':'rscaf'}), on=['rscaf'], how='left')
        scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'rscaf_side'] = 'l'
        scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'rscaf'] = scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'new_scaf'].astype(int)
        scaffolds.drop(columns=['new_scaf'], inplace=True)
        scaffolds.index = scaffolds['scaffold'].values # Make sure we can access scaffolds with .loc[scaffold]
        # Prepare next round
        keep = scaffolds.loc[(scaffolds['lscaf_side'] == 'l') & (scaffolds['scaffold'] < scaffolds['lscaf']), 'scaffold'].values

    # Remove simple ciruclarities
    circular_scaffolds = scaffolds['scaffold'] == scaffolds['lscaf']
    scaffolds.loc[circular_scaffolds & (scaffolds['lscaf'] == scaffolds['rscaf']), 'circular'] = True # Truely circular it is only for left-right connections
    scaffolds.loc[circular_scaffolds, 'lextendible'] = False
    scaffolds.loc[circular_scaffolds, 'lscaf'] = -1
    scaffolds.loc[circular_scaffolds, 'lscaf_side'] = ''
    circular_scaffolds = scaffolds['scaffold'] == scaffolds['rscaf']
    scaffolds.loc[circular_scaffolds, 'rextendible'] = False
    scaffolds.loc[circular_scaffolds, 'rscaf'] = -1
    scaffolds.loc[circular_scaffolds, 'rscaf_side'] = ''

    # Check for two-contig circularities (right-left connections: Others don't exist anymore)
    circular_scaffolds = (scaffolds['lscaf'] == scaffolds['rscaf']) & (scaffolds['lscaf'] != -1)
    scaffolds.loc[circular_scaffolds, 'circular'] = True
    if np.sum(circular_scaffolds):
        break_left = np.unique(np.minimum(scaffolds.loc[circular_scaffolds,'lscaf'].values.astype(int), scaffolds.loc[circular_scaffolds,'scaffold'].values)) # Choose arbitrarily the lower of the scaffold ids for left side break
        break_right = np.unique(np.maximum(scaffolds.loc[circular_scaffolds,'lscaf'].values.astype(int), scaffolds.loc[circular_scaffolds,'scaffold'].values))
        scaffolds.loc[break_left, 'lextendible'] = False
        scaffolds.loc[break_left, 'lscaf'] = -1
        scaffolds.loc[break_left, 'lscaf_side'] = ''
        scaffolds.loc[break_right, 'rextendible'] = False
        scaffolds.loc[break_right, 'rscaf'] = -1
        scaffolds.loc[break_right, 'rscaf_side'] = ''

    # Handle left-right scaffold connections (Follow the right-left connections until no connection exists anymore)
    while np.sum(scaffolds['rscaf_side'] == 'l'):
        keep = scaffolds.loc[(scaffolds['rscaf_side'] == 'l') & (scaffolds['lscaf_side'] == ''), 'scaffold'].values
        if 0 == len(keep):
            # All remaining right-left connections must be circular
            # Long circularities (>=3 scaffolds) cannot be solved in a single go, because we don't know which ones are connected. Thus we simply break the left connection of the first scaffold that still has a connection and run the loop until the whole circularity is scaffolded and repeat this until no more circularities are present
            circular_scaffolds = scaffolds.loc[scaffolds['rscaf_side'] == 'l', 'scaffold'].values
            scaffolds.loc[circular_scaffolds, 'circular'] = True
            if len(circular_scaffolds):
                break_left = circular_scaffolds[0]
                break_right = scaffolds.loc[break_left, 'lscaf']
                scaffolds.loc[break_left, 'lextendible'] = False
                scaffolds.loc[break_left, 'lscaf'] = -1
                scaffolds.loc[break_left, 'lscaf_side'] = ''
                scaffolds.loc[break_right, 'rextendible'] = False
                scaffolds.loc[break_right, 'rscaf'] = -1
                scaffolds.loc[break_right, 'rscaf_side'] = ''

            # Update the scaffolds that are kept and extended with another scaffold
            keep = scaffolds.loc[(scaffolds['rscaf_side'] == 'l') & (scaffolds['lscaf_side'] == ''), 'scaffold'].values

        # Update scaffold_parts
        absorbed = np.isin(scaffold_parts['scaffold'], scaffolds.loc[keep, 'rscaf'])
        absorbed_values = scaffold_parts.merge(scaffolds.loc[keep, ['scaffold','rscaf','size']].rename(columns={'scaffold':'new_scaf','rscaf':'scaffold'}), on=['scaffold'], how='left').loc[absorbed, ['new_scaf','size']]
        scaffold_parts.loc[absorbed, 'pos'] = absorbed_values['size'].astype(int).values + scaffold_parts.loc[absorbed, 'pos'].values
        scaffold_parts.loc[absorbed, 'scaffold'] = absorbed_values['new_scaf'].astype(int).values
        # Update scaffolds
        absorbed = scaffolds.loc[keep, 'rscaf'].values
        scaffolds.loc[keep, 'right'] = scaffolds.loc[absorbed, 'right'].values
        scaffolds.loc[keep, 'rside'] = scaffolds.loc[absorbed, 'rside'].values
        scaffolds.loc[keep, 'rextendible'] = scaffolds.loc[absorbed, 'rextendible'].values
        scaffolds.loc[keep, 'size'] += scaffolds.loc[absorbed, 'size'].values
        new_scaffold_ids = scaffolds.loc[keep, ['scaffold','rscaf']].rename(columns={'scaffold':'new_scaf','rscaf':'scaffold'}).copy() # Store this to later change the connections to the absorbed scaffold
        scaffolds.loc[keep, 'rscaf'] = scaffolds.loc[absorbed, 'rscaf'].values
        scaffolds.loc[keep, 'rscaf_side'] = scaffolds.loc[absorbed, 'rscaf_side'].values
        # Drop absorbed scaffolds and update the connections
        scaffolds.drop(absorbed, inplace=True)
        scaffolds = scaffolds.merge(new_scaffold_ids.rename(columns={'scaffold':'lscaf'}), on=['lscaf'], how='left')
        scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'lscaf'] = scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'new_scaf'].astype(int)
        scaffolds.drop(columns=['new_scaf'], inplace=True)
        scaffolds = scaffolds.merge(new_scaffold_ids.rename(columns={'scaffold':'rscaf'}), on=['rscaf'], how='left')
        scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'rscaf'] = scaffolds.loc[np.isnan(scaffolds['new_scaf']) == False, 'new_scaf'].astype(int)
        scaffolds.drop(columns=['new_scaf'], inplace=True)
        scaffolds.index = scaffolds['scaffold'].values # Make sure we can access scaffolds with .loc[scaffold]

    return scaffolds, scaffold_parts

def AddDistaneInformation(scaffold_parts, unique_bridges):
    scaffold_parts.sort_values(['scaffold','pos'], inplace=True)
    scaffold_parts['from'] = scaffold_parts['conpart']
    scaffold_parts['from_side'] = np.where(scaffold_parts['reverse'], 'l', 'r')
    scaffold_parts['to'] = np.where(scaffold_parts['scaffold'].shift(-1) == scaffold_parts['scaffold'], scaffold_parts['conpart'].shift(-1, fill_value=-1), -1)
    scaffold_parts['to_side'] = np.where(scaffold_parts['reverse'].shift(-1), 'r', 'l') # This is bogus if scaffolds do not match, but 'to' is correct and prevents the merge anyways
    scaffold_parts = scaffold_parts.merge(unique_bridges[['from','from_side','to','to_side','mean_dist']].rename(columns={'mean_dist':'next_dist'}), on=['from','from_side','to','to_side'], how='left')
    scaffold_parts['next_dist'] = scaffold_parts['next_dist'].fillna(0).astype(int)
    scaffold_parts.index = scaffold_parts['conpart'].values # Set the index again, after the merge reset it
    scaffold_parts.drop(columns=['from','from_side','to','to_side'], inplace=True)
    scaffold_parts['prev_dist'] = np.where(scaffold_parts['scaffold'].shift(1) == scaffold_parts['scaffold'], scaffold_parts['next_dist'].shift(1, fill_value=0), 0)

    return scaffold_parts

def LiftBridgesFromContigsToScaffolds(bridges, scaffolds):
    # Lift from
    scaf_bridges = bridges.merge(scaffolds[['scaffold','left','lside']].rename(columns={'scaffold':'lscaf', 'left':'from', 'lside':'from_side'}), on=['from','from_side'], how='left')
    scaf_bridges = scaf_bridges.merge(scaffolds[['scaffold','right','rside']].rename(columns={'scaffold':'rscaf', 'right':'from', 'rside':'from_side'}), on=['from','from_side'], how='left')
    scaf_bridges = scaf_bridges[(np.isnan(scaf_bridges['lscaf']) == False) | (np.isnan(scaf_bridges['rscaf']) == False)].copy() # Remove bridges that are now inside scaffolds
    scaf_bridges['from'] = np.where(np.isnan(scaf_bridges['lscaf']), scaf_bridges['rscaf'], scaf_bridges['lscaf']).astype(int)
    scaf_bridges['from_side'] = np.where(np.isnan(scaf_bridges['lscaf']), 'r', 'l')
    scaf_bridges.drop(columns=['lscaf','rscaf'], inplace=True)

    # Lift to
    scaf_bridges = scaf_bridges.merge(scaffolds[['scaffold','left','lside']].rename(columns={'scaffold':'lscaf', 'left':'to', 'lside':'to_side'}), on=['to','to_side'], how='left')
    scaf_bridges = scaf_bridges.merge(scaffolds[['scaffold','right','rside']].rename(columns={'scaffold':'rscaf', 'right':'to', 'rside':'to_side'}), on=['to','to_side'], how='left')
    scaf_bridges['to'] = np.where(np.isnan(scaf_bridges['lscaf']), scaf_bridges['rscaf'], scaf_bridges['lscaf']).astype(int)
    scaf_bridges['to_side'] = np.where(np.isnan(scaf_bridges['lscaf']), 'r', 'l')
    scaf_bridges.drop(columns=['lscaf','rscaf'], inplace=True)

    return scaf_bridges

def GetOriginalScaffoldConnections(contig_parts, scaffolds):
    org_scaf_conns = scaffolds[['scaffold','left','lside','right','rside']].copy()
    org_scaf_conns['org_dist_left'] = np.where(org_scaf_conns['lside'] == 'l', contig_parts.loc[org_scaf_conns['left'].values, 'org_dist_left'].values, contig_parts.loc[org_scaf_conns['left'].values, 'org_dist_right'].values)
    org_scaf_conns['org_dist_right'] = np.where(org_scaf_conns['rside'] == 'l', contig_parts.loc[org_scaf_conns['right'].values, 'org_dist_left'].values, contig_parts.loc[org_scaf_conns['right'].values, 'org_dist_right'].values)
    org_scaf_conns = [ org_scaf_conns.loc[org_scaf_conns['org_dist_left'] >= 0, ['scaffold','left','lside','org_dist_left']].rename(columns={'left':'conpart','lside':'cside','org_dist_left':'distance'}),
                       org_scaf_conns.loc[org_scaf_conns['org_dist_right'] >= 0, ['scaffold','right','rside','org_dist_right']].rename(columns={'right':'conpart','rside':'cside','org_dist_right':'distance'}) ]
    org_scaf_conns[0]['side'] = 'l'
    org_scaf_conns[1]['side'] = 'r'
    org_scaf_conns = pd.concat(org_scaf_conns, ignore_index=True, sort=False)
    org_scaf_conns['conpart1'] = org_scaf_conns['conpart'] - np.where(org_scaf_conns['cside'] == 'l', 1, 0)
    org_scaf_conns['conpart2'] = org_scaf_conns['conpart'] + np.where(org_scaf_conns['cside'] == 'r', 1, 0)
    org_scaf_conns.drop(columns=['conpart','cside'], inplace=True)
    org_scaf_conns = org_scaf_conns.rename(columns={'scaffold':'from','side':'from_side'}).merge(org_scaf_conns.rename(columns={'scaffold':'to','side':'to_side'}), on=['conpart1','conpart2','distance'], how='left')
    org_scaf_conns = org_scaf_conns.loc[org_scaf_conns['from'] != org_scaf_conns['to'], ['from','from_side','to','to_side','distance']].copy()
#
    return org_scaf_conns

def GetSizeAndEnumeratePositionsForLongRangeConnections(long_range_connections):
    con_size = long_range_connections.groupby(['conn_id'], sort=False).size().reset_index(name='size')
    long_range_connections['size'] = np.repeat(con_size['size'].values, con_size['size'].values)
    long_range_connections['pos'] = long_range_connections.groupby(['conn_id']).cumcount()

    return long_range_connections

def GetConnectionFromTo(long_range_connections):
    long_range_connections['from'] = np.where(long_range_connections['conn_id'] != long_range_connections['conn_id'].shift(1, fill_value=-1), -1, long_range_connections['conpart'].shift(1, fill_value=-1))
    long_range_connections['from_side'] = np.where(long_range_connections['strand'].shift(1, fill_value='') == '+', 'r', 'l') # When the previous entry does not belong to same conn_id this is garbage, but 'from' is already preventing the merge happening next, so it does not matter
    long_range_connections['to'] = long_range_connections['conpart']
    long_range_connections['to_side'] = np.where(long_range_connections['strand'] == '+', 'l', 'r')

    return long_range_connections

def GetLongRangeConnections(bridges, mappings, contig_parts, max_dist_contig_end):
    # Get long_range_mappings that include a bridge that has alternatives
    long_range_mappings = mappings[mappings['num_mappings']>=3].copy()
    if len(long_range_mappings):
        alternative_connections = bridges.loc[(bridges['from_alt'] > 1) | (bridges['to_alt'] > 1), ['from','from_side']].rename(columns={'from':'conpart','from_side':'side'}).drop_duplicates()
        interesting_reads = long_range_mappings[['read_id','read_start','conpart','right_con','left_con']].merge(alternative_connections, on=['conpart'], how='inner')
        interesting_reads = interesting_reads[ ((interesting_reads['side'] == 'r') & (interesting_reads['right_con'] >= 0)) | ((interesting_reads['side'] == 'l') & (interesting_reads['left_con'] >= 0)) ].copy() # Only keep reads that have a connection in the interesting direction
        long_range_mappings = long_range_mappings.merge(interesting_reads[['read_id','read_start']].drop_duplicates(), on=['read_id','read_start'], how='inner')
#
    if len(long_range_mappings):
        # Get long_range_connections that are supported by reads
        long_range_connections = long_range_mappings[['conpart','strand','left_con_dist','right_con_dist','left_con','right_con','con_from','con_to','read_start','read_end','read_from','read_to']].copy()
        long_range_connections['conn_id'] = ((long_range_mappings['read_id'] != long_range_mappings['read_id'].shift(1, fill_value='')) | (long_range_mappings['read_start'] != long_range_mappings['read_start'].shift(1, fill_value=-1))).cumsum()
        # Get extlen
        long_range_connections['maplen'] = long_range_connections['con_to'] - long_range_connections['con_from']
        long_range_connections['con_from'] -= contig_parts.loc[long_range_connections['conpart'].values, 'start'].values
        long_range_connections['con_to'] = contig_parts.loc[long_range_connections['conpart'].values, 'end'].values - long_range_connections['con_to']
        long_range_connections['lgaplen'] = np.maximum(0, long_range_connections['left_con_dist']) - long_range_connections['con_from']
        long_range_connections['rgaplen'] = np.maximum(0, long_range_connections['right_con_dist']) - long_range_connections['con_to']
        long_range_connections['lextlen'] = np.where(long_range_connections['strand'] == '+', long_range_connections['read_end']-long_range_connections['read_to'], long_range_connections['read_from']-long_range_connections['read_start']) + (long_range_connections['maplen'] + long_range_connections['con_from'])
        long_range_connections['rextlen'] = np.where(long_range_connections['strand'] == '-', long_range_connections['read_end']-long_range_connections['read_to'], long_range_connections['read_from']-long_range_connections['read_start']) + (long_range_connections['maplen'] + long_range_connections['con_to'])
        long_range_connections.drop(columns=['con_from','con_to','read_start','read_end','read_from','read_to','maplen'], inplace=True)
        long_range_connections.loc[long_range_connections['left_con'] < 0, ['lgaplen','lextlen']] = 0
        long_range_connections.loc[long_range_connections['right_con'] < 0, ['rgaplen','rextlen']] = 0
    else:
        long_range_connections = []
#
    if len(long_range_connections):
        # If a long_range_connections goes through the same contig part multiple times without a proper repeat signature, merge those entries
        long_range_connections = GetConnectionFromTo(long_range_connections)
        long_range_connections['fake_rep'] = (long_range_connections['from'] == long_range_connections['to']) & (0 > np.where(long_range_connections['to_side'] == 'l', long_range_connections['left_con'], long_range_connections['right_con']) ) # The connection was removed due to its non-repeat signature
        remove = long_range_connections['fake_rep'] & long_range_connections['fake_rep'].shift(-1, fill_value=False) & (long_range_connections['conn_id'] == long_range_connections['conn_id'].shift(-1) )
        if np.sum(remove):
            # Remove the middle contig in case we have two fake repeat connections in a row
            long_range_connections = long_range_connections[remove == False].copy()
            GetConnectionFromTo(long_range_connections)
        selection = long_range_connections['fake_rep'] & (long_range_connections['from_side'] == 'r')
        long_range_connections.loc[selection.shift(-1, fill_value=False), ['right_con_dist','rgaplen','rextlen']] = np.where(long_range_connections.loc[selection, ['to_side','to_side','to_side']] == 'l', long_range_connections.loc[selection, ['right_con_dist','rgaplen','rextlen']].values, long_range_connections.loc[selection, ['left_con_dist','lgaplen','lextlen']]) # Fill in the other side that does not point towards the previous entry
        selection = long_range_connections['fake_rep'] & (long_range_connections['from_side'] == 'l')
        long_range_connections.loc[selection.shift(-1, fill_value=False), ['left_con_dist','lgaplen','lextlen']] = np.where(long_range_connections.loc[selection, ['to_side','to_side','to_side']] == 'l', long_range_connections.loc[selection, ['right_con_dist','rgaplen','rextlen']].values, long_range_connections.loc[selection, ['left_con_dist','lgaplen','lextlen']]) # Fill in the other side that does not point towards the previous entry
        long_range_connections = long_range_connections[long_range_connections['fake_rep'] == False].drop(columns = ['left_con','right_con','fake_rep']) # from and to will be updated in the next step, thus we do not need to remove them here
#
    if len(long_range_connections):
        # Break long_range_mappings when they go through invalid bridges and get number of prev alternatives (how many alternative to the connection of a mapping with its previous mapping exist)
        long_range_connections = GetConnectionFromTo(long_range_connections)
        long_range_connections['dist'] = np.where(long_range_connections['strand'] == '+', long_range_connections['left_con_dist'], long_range_connections['right_con_dist'])
        long_range_connections = long_range_connections.reset_index().merge(bridges[['from','from_side','to','to_side','to_alt','min_dist','max_dist','mean_dist']], on=['from','from_side','to','to_side'], how='left').fillna(0) # Keep index, so we know where we introduced duplicates with multiple possible distances for the bridges
        long_range_connections.rename(columns={'to_alt':'prev_alt', 'mean_dist':'prev_dist'}, inplace=True)
        long_range_connections['prev_alt'] = long_range_connections['prev_alt'].astype(int)
        long_range_connections['prev_dist'] = long_range_connections['prev_dist'].astype(int) # We store only the mean_dist of the distance category for later, since the exact distances are irrelevant and varying when we group connections
        long_range_connections.loc[(long_range_connections['min_dist'] > long_range_connections['dist']) | (long_range_connections['dist'] > long_range_connections['max_dist']),'prev_alt'] = 0 # Break connections that do not fit the distance
        long_range_connections = long_range_connections.sort_values(['index','prev_alt'], ascending=[True,False]).groupby(['index'], sort=False).first().reset_index() # Remove duplications from multiple possible bridge distances
        long_range_connections.drop(columns=['min_dist','max_dist'], inplace=True)
        long_range_connections['conn_id'] = (long_range_connections['prev_alt'] == 0).cumsum()
#
        # Remove connections that do not include at least 3 mappings anymore
        long_range_connections = GetSizeAndEnumeratePositionsForLongRangeConnections(long_range_connections)
        long_range_connections = long_range_connections[ long_range_connections['size'] >= 3 ].copy()
#
    if len(long_range_connections):
        # Get number of next alternatives
        long_range_connections['from'] = long_range_connections['conpart']
        long_range_connections['from_side'] = np.where(long_range_connections['strand'] == '+', 'r', 'l')
        long_range_connections['dist'] = np.where(long_range_connections['strand'] == '+', long_range_connections['right_con_dist'], long_range_connections['left_con_dist'])
        long_range_connections['to'] = np.where(long_range_connections['conn_id'] != long_range_connections['conn_id'].shift(-1, fill_value=-1), -1, long_range_connections['conpart'].shift(-1, fill_value=-1))
        long_range_connections['to_side'] = np.where(long_range_connections['strand'].shift(-1, fill_value='') == '+', 'l', 'r') # When the next entry does not belong to same conn_id this is garbage, but 'to' is already preventing the merge happening next, so it does not matter
        long_range_connections = long_range_connections.merge(bridges[['from','from_side','to','to_side','from_alt','min_dist','max_dist','mean_dist']], on=['from','from_side','to','to_side'], how='left').fillna(0)
        long_range_connections.rename(columns={'from_alt':'next_alt', 'mean_dist':'next_dist'}, inplace=True)
        long_range_connections['next_alt'] = long_range_connections['next_alt'].astype(int)
        long_range_connections['next_dist'] = long_range_connections['next_dist'].astype(int) # We store only the mean_dist of the distance category for later, since the exact distances are irrelevant and varying when we group connections
        long_range_connections.loc[(long_range_connections['min_dist'] > long_range_connections['dist']) | (long_range_connections['dist'] > long_range_connections['max_dist']),'next_alt'] = 0 # Mark wrong distances, so we can remove them and only keep the correct ones
        long_range_connections = long_range_connections.sort_values(['index','next_alt'], ascending=[True,False]).groupby(['index'], sort=False).first().reset_index(drop=True) # Remove duplications from multiple possible bridge distances
        long_range_connections.drop(columns=['from','from_side','to','to_side','dist','min_dist','max_dist','left_con_dist','right_con_dist'], inplace=True)
#
        # Trim parts at the beginning or end that do not include alternative connections
        long_range_connections['trim'] = ( ((long_range_connections['pos'] == 0) & (long_range_connections['next_alt'] == 1) & (long_range_connections['prev_alt'].shift(-1, fill_value=-1) == 1)) |
                                           ((long_range_connections['pos'] == long_range_connections['size']-1) & (long_range_connections['prev_alt'] == 1) & (long_range_connections['next_alt'].shift(1, fill_value=-1) == 1)) )
        while np.sum(long_range_connections['trim']):
            long_range_connections = long_range_connections[long_range_connections['trim'] == False].copy()
            long_range_connections = GetSizeAndEnumeratePositionsForLongRangeConnections(long_range_connections)
            long_range_connections = long_range_connections[ long_range_connections['size'] >= 3 ].copy()
            long_range_connections['trim'] = ( ((long_range_connections['pos'] == 0) & (long_range_connections['next_alt'] == 1) & (long_range_connections['prev_alt'].shift(-1, fill_value=-1) == 1)) |
                                               ((long_range_connections['pos'] == long_range_connections['size']-1) & (long_range_connections['prev_alt'] == 1) & (long_range_connections['next_alt'].shift(1, fill_value=-1) == 1)) )
        long_range_connections.drop(columns=['trim'], inplace=True)
#
    if len(long_range_connections):
        # Move extlen from left/right to prev/next
        long_range_connections.rename(columns={f'{b}{n}':f'{a}{n}' for n in ['gaplen','extlen'] for b,a in [['l','p'],['r','n']]}, inplace=True)
        long_range_connections.loc[long_range_connections['strand'] == '-', [f'{s}{n}' for n in ['gaplen','extlen'] for s in ['p','n']]] = long_range_connections.loc[long_range_connections['strand'] == '-', [f'{s}{n}' for n in ['gaplen','extlen'] for s in ['n','p']]].values
        # Make sure that reads look similar independent if they were trimmed or not
        long_range_connections.loc[long_range_connections['pos'] == 0, ['prev_alt','prev_dist','pgaplen','pextlen']] = 0
        long_range_connections.loc[long_range_connections['pos'] == long_range_connections['size']-1, ['next_alt','next_dist','ngaplen','nextlen']] = 0
#
        # Add all connections also in the reverse direction
        long_range_connections = long_range_connections.loc[np.repeat(long_range_connections.index.values, 2)].copy()
        long_range_connections['reverse'] = [False,True] * (len(long_range_connections)//2)
        long_range_connections.loc[long_range_connections['reverse'], 'strand'] = np.where(long_range_connections.loc[long_range_connections['reverse'], 'strand'] == '+', '-', '+')
        long_range_connections['conn_id'] *= 2
        long_range_connections.loc[long_range_connections['reverse'], 'conn_id'] += 1
        long_range_connections.loc[long_range_connections['reverse'], ['prev_alt','next_alt']] = long_range_connections.loc[long_range_connections['reverse'], ['next_alt','prev_alt']].values
        long_range_connections.loc[long_range_connections['reverse'], ['prev_dist','next_dist']] = long_range_connections.loc[long_range_connections['reverse'], ['next_dist','prev_dist']].values
        long_range_connections.loc[long_range_connections['reverse'], [f'{s}{n}' for n in ['gaplen','extlen'] for s in ['p','n']]] = long_range_connections.loc[long_range_connections['reverse'], [f'{s}{n}' for n in ['gaplen','extlen'] for s in ['n','p']]].values
        long_range_connections.loc[long_range_connections['reverse'], 'pos'] = long_range_connections.loc[long_range_connections['reverse'], 'size'] - long_range_connections.loc[long_range_connections['reverse'], 'pos'] - 1
        long_range_connections.sort_values(['conn_id', 'pos'], inplace=True)
#
        # Remove the duplicate created in the previous step for connections that are identical in both directions (to have accurate counts)
        long_range_connections['conn_code'] = long_range_connections['conpart'].astype(str) + long_range_connections['strand'] + np.where(long_range_connections['pos']+1 < long_range_connections['size'], '(' + long_range_connections['next_dist'].astype(str) + ')', '')
        codes = long_range_connections.groupby(['conn_id'], sort=False)['conn_code'].apply(''.join)
        long_range_connections['conn_code'] = codes.loc[long_range_connections['conn_id'].values].values
        rem = long_range_connections.loc[long_range_connections['reverse'] & (long_range_connections['pos'] == 0) & (long_range_connections['conn_code'] == long_range_connections['conn_code'].shift(1)), 'conn_id'].values
        long_range_connections = long_range_connections[np.isin(long_range_connections['conn_id'], rem) == False].drop(columns=['reverse'])
#
    return long_range_connections

def TransformContigConnectionsToScaffoldConnections(long_range_connections, scaffold_parts):
    if len(long_range_connections):
        long_range_connections[['scaffold','scaf_pos','reverse']] = scaffold_parts.loc[long_range_connections['conpart'].values,['scaffold','pos','reverse']].values
        # Reverse strand of contigs that are reversed in the scaffold to get the scaffold strand
        long_range_connections.loc[long_range_connections['reverse'], 'strand'] = np.where(long_range_connections.loc[long_range_connections['reverse'], 'strand'] == '+', '-', '+')
        # Group and combine contigs which are all part of the same scaffold (and are following it's order, so are not a repeat)
        long_range_connections['group'] = ( (long_range_connections['conn_id'] != long_range_connections['conn_id'].shift(1)) | (long_range_connections['scaffold'] != long_range_connections['scaffold'].shift(1)) | 
                                            ((long_range_connections['scaf_pos']+1 != long_range_connections['scaf_pos'].shift(1)) & (long_range_connections['scaf_pos']-1 != long_range_connections['scaf_pos'].shift(1))) ).cumsum()
        long_range_connections = long_range_connections.groupby(['group', 'conn_id', 'conn_code', 'scaffold', 'strand'], sort=False).agg({'prev_alt':'first','next_alt':'last','prev_dist':'first','next_dist':'last','pgaplen':'first','ngaplen':'last','pextlen':'first','nextlen':'last'}).reset_index().drop(columns=['group'])

        # Get size and pos again
        con_size = long_range_connections.groupby(['conn_id']).size().reset_index(name='size')
        long_range_connections['size'] = np.repeat(con_size['size'].values, con_size['size'].values)
        long_range_connections = long_range_connections[long_range_connections['size'] > 2].copy()
        long_range_connections['pos'] = long_range_connections.groupby(['conn_id'], sort=False).cumcount()

    return long_range_connections

def SummarizeLongRangeConnections(long_range_connections):
    long_range_extlen = []
    if len(long_range_connections):
        # Separate long_range_extlen
        long_range_extlen = long_range_connections[['conn_code','pos','pgaplen', 'ngaplen', 'pextlen', 'nextlen']].copy()
#
        # Summarize identical long_range_connections (since we already trimmed all contigs without alternative bridges from both sides, we do not need to update conn_id to scaffolds)
        long_range_connections = long_range_connections.groupby(['conn_code','pos','size','scaffold','strand','prev_alt','next_alt','prev_dist','next_dist']).size().reset_index(name='count')
        long_range_connections['conn_id'] = (long_range_connections['conn_code'] != long_range_connections['conn_code'].shift(1)).cumsum()
        long_range_extlen['conn_id'] = long_range_extlen[['conn_code']].merge(long_range_connections[['conn_code','conn_id']].drop_duplicates(), on='conn_code', how='left')['conn_id'].values
        long_range_extlen.drop(columns=['conn_code'], inplace=True)
        long_range_connections.drop(columns=['conn_code'], inplace=True)
#
    return long_range_connections, long_range_extlen

def GetScaffoldLength(scaffold_parts, contig_parts):
    scaf_len = scaffold_parts[['scaffold','conpart','prev_dist']].rename(columns={'prev_dist':'length'})
    scaf_len['length'] += contig_parts.loc[scaf_len['conpart'].values, 'end'] - contig_parts.loc[scaf_len['conpart'].values, 'start']
    scaf_len = scaf_len.groupby(['scaffold'])['length'].sum().reset_index()
    scaf_len.index = scaf_len['scaffold'].values
#
    return scaf_len

def FilterLongRangeConnections(long_range_connections, long_range_extlen, scaffold_parts, contig_parts, cov_probs, prob_factor, min_mapping_length, prematurity_threshold):
    scaf_len = GetScaffoldLength(scaffold_parts, contig_parts)
    if len(long_range_connections):
        for l in range(3,long_range_connections['size'].max()+1):
            for cfilter in ['prematurity','counts']:
                # Get all connections of length l (this includes the connections within longer connections)
                long_range_connections.reset_index(drop=True, inplace=True)
                cons = long_range_connections.loc[long_range_connections['pos'] + l <= long_range_connections['size'], ['scaffold','strand','count']].reset_index()
                cons.rename(columns={'index':'cindex', 'scaffold':'scaf0', 'strand':'strand0'}, inplace=True)
                for i in range(1,l):
                    cons[[f'scaf{i}',f'strand{i}',f'dist{i}']] = long_range_connections.loc[cons['cindex']+i, ['scaffold','strand','prev_dist']].values
                mcols = ['scaf0','strand0']+[f'{n}{s}' for s in range(1,l) for n in ['scaf','strand','dist']]
                if cfilter == 'prematurity':
                    cons[['conn_id', 'pos']] = long_range_connections.loc[cons['cindex'].values, ['conn_id', 'pos']].values
                    for s in ['n','p']:
                        complen = cons.drop(columns=['count']).merge(long_range_extlen[['conn_id','pos',f'{s}gaplen',f'{s}extlen']], on=['conn_id','pos'], how='left')
                        complen.rename(columns={f'{s}{n}':n for n in ['gaplen','extlen']}, inplace=True)
                        complen.drop(columns=['conn_id','pos'], inplace=True)
                        if s == 'n':
                            gcols = ['scaf1','strand1']+[f'{n}{s}' for s in range(2,l) for n in ['scaf','strand','dist']]
                        else:
                            gcols = ['scaf0','strand0']+[f'{n}{s}' for s in range(1,l-1) for n in ['scaf','strand','dist']]
                        clen = complen.groupby(mcols).agg({'extlen':'max','gaplen':'size'}).rename(columns={'gaplen':'counts'}).reset_index()
                        clen.sort_values(gcols, inplace=True)
                        maxlen = clen.groupby(gcols, sort=False)['extlen'].agg(['size','max'])
                        clen['maxlen'] = np.repeat(maxlen['max'].values, maxlen['size'].values)
                        longest = clen[clen['extlen'] == clen['maxlen']].drop(columns=['extlen'])
                        longest.rename(columns={'counts':'ncontrol'}, inplace=True)
                        clen = clen[clen['maxlen'] > clen['extlen']].copy()
                        clen.reset_index(drop=True, inplace=True)
                        clen['gaplen'] = clen[mcols+['extlen']].merge(complen.groupby(mcols+['extlen'])['gaplen'].max().reset_index(), on=mcols+['extlen'], how='left')['gaplen'].values
                        longest = longest.merge(clen[gcols+['extlen','counts','gaplen']].reset_index().rename(columns={'index':'cindex','extlen':'tlen','counts':'ntest','gaplen':'tgaplen'}), on=gcols, how='inner')
                        longer = longest[mcols+['tlen','tgaplen']].drop_duplicates().merge(complen[mcols+['extlen','gaplen']], on=mcols, how='left')
                        longer = longer[longer['extlen'] > longer['tlen'] + np.maximum(0, longer['tgaplen']-longer['gaplen'])].groupby(mcols+['tlen','tgaplen']).size().reset_index(name='longer')
                        longest['longer'] = longest[mcols+['tlen','tgaplen']].merge(longer, on=mcols+['tlen','tgaplen'], how='left')['longer'].fillna(0).astype(int).values
                        if np.sum(longest['longer'] > 0):
                            longest['premat_prob'] = GetPrematureStopProb(longest['ntest'], longest['ncontrol'], longest['longer'])
                        else:
                            longest['premat_prob'] = 1.0
                        tmp = longest.groupby(['cindex'])['premat_prob'].min().reset_index()
                        clen.loc[tmp['cindex'].values, 'premat_prob'] = tmp['premat_prob'].values
                        cons[f'{s}premat_prob'] = cons[mcols].merge(clen[mcols+['premat_prob']], on=mcols, how='left')['premat_prob'].fillna(1.0).values
                        if s == 'n':
                            cons['pos'] += l-1
                    cons['ndel'] = cons['npremat_prob'] <= prematurity_threshold
                    cons['pdel'] = cons['ppremat_prob'] <= prematurity_threshold
                    one_sided_dels = cons.loc[cons['ndel'] != cons['pdel'], mcols+['ndel','pdel']].drop_duplicates()
                    cons = cons[cons['ndel'] & cons['pdel']].drop(columns=['conn_id', 'pos','npremat_prob','ppremat_prob','ndel','pdel'])
                else:
                    # Sum counts for identical cons
                    counts = cons.groupby(mcols)['count'].sum().reset_index()
                    # Get minimum length of a read to span that connection
                    counts['min_length'] = 2*min_mapping_length + np.maximum(0, counts['dist1']) + np.maximum(0, counts[f'dist{l-1}'])
                    for i in range(1,l-1):
                        counts['min_length'] += scaf_len.loc[counts[f'scaf{i}'].values, 'length'].values
                        if i > 1:
                            counts['min_length'] += counts[f'dist{i}']
                    # Get probability of observing this number of counts or lower
                    counts['probability'] = GetConProb(cov_probs, counts['min_length'], counts['count'])
                    # Compare probability between long_range_connections that are identical except for the first scaffold
                    counts.sort_values(['scaf1','strand1']+[f'{n}{s}' for s in range(2,l) for n in ['scaf','strand','dist']], inplace=True)
                    max_prob = counts.groupby(['scaf1','strand1']+[f'{n}{s}' for s in range(2,l) for n in ['scaf','strand','dist']], sort=False)['probability'].agg(['size','max'])
                    counts['nmax_prob'] = np.repeat(max_prob['max'].values, max_prob['size'].values)
                    # Compare probability between long_range_connections that are identical except for the last scaffold
                    counts.sort_values(['scaf0','strand0']+[f'{n}{s}' for s in range(1,l-1) for n in ['scaf','strand','dist']], inplace=True)
                    max_prob = counts.groupby(['scaf0','strand0']+[f'{n}{s}' for s in range(1,l-1) for n in ['scaf','strand','dist']], sort=False)['probability'].agg(['size','max'])
                    counts['pmax_prob'] = np.repeat(max_prob['max'].values, max_prob['size'].values)
                    # Delete connections that are unprobable in both directions (since we keep the shorter connections, removing only one direction would keep the problematic split, but throw away information how to proceed after the split, while in the case of a two-sided removal on both sides are better alternatives and we can savely remove it)
                    counts['ndel'] = (counts['probability']*prob_factor < counts['nmax_prob']) | counts[mcols].merge(one_sided_dels, on=mcols, how='left')['ndel'].fillna(False).values
                    counts['pdel'] = (counts['probability']*prob_factor < counts['pmax_prob']) | counts[mcols].merge(one_sided_dels, on=mcols, how='left')['pdel'].fillna(False).values
                    counts = counts[counts['ndel'] & counts['pdel']].drop(columns=['count','min_length','probability','nmax_prob','pmax_prob','ndel','pdel'])
                    cons = cons.merge(counts, on=mcols, how='inner')
                split_index = np.concatenate([cons['cindex'].values, cons['cindex'].values+l-2])
                long_range_connections['split'] = long_range_connections['conn_id'] != long_range_connections['conn_id'].shift(-1)
                long_range_connections.loc[split_index, 'split'] = True
                long_range_connections['split'] = long_range_connections['split'].shift(1, fill_value=True)
                long_range_connections['conn_id'] = long_range_connections['split'].cumsum()
                long_range_connections['pos'] = long_range_connections.groupby(['conn_id']).cumcount()
                con_size = long_range_connections.groupby(['conn_id'], sort=False).size().values
                long_range_connections['size'] = np.repeat(con_size, con_size)
                long_range_connections = long_range_connections[long_range_connections['size'] > 2].drop(columns=['split'])
                long_range_connections.loc[long_range_connections['pos'] == 0, ['prev_alt','prev_dist']] = 0
                long_range_connections.loc[long_range_connections['pos'] == long_range_connections['size']-1, ['next_alt','next_dist']] = 0
#
    return long_range_connections

def RemoveRedundantEntriesInScaffoldGraph(scaffold_graph):
    # Now remove all the paths that overlap a longer one (for equally long ones just take one of them)
    scaffold_graph.sort_values(['from','from_side']+[col for sublist in [['scaf'+str(s),'strand'+str(s),'dist'+str(s)] for s in range(1,scaffold_graph['length'].max())] for col in sublist], inplace=True)
    scaffold_graph['redundant'] = (scaffold_graph['from'] == scaffold_graph['from'].shift(1, fill_value=-1)) & (scaffold_graph['from_side'] == scaffold_graph['from_side'].shift(1, fill_value=''))
    for s in range(1,scaffold_graph['length'].max()):
        scaffold_graph['redundant'] = scaffold_graph['redundant'] & ( np.isnan(scaffold_graph['scaf'+str(s)]) | ( (scaffold_graph['scaf'+str(s)] == scaffold_graph['scaf'+str(s)].shift(1, fill_value=-1)) & (scaffold_graph['strand'+str(s)] == scaffold_graph['strand'+str(s)].shift(1, fill_value='')) & (scaffold_graph['dist'+str(s)] == scaffold_graph['dist'+str(s)].shift(1, fill_value='')))) 
    scaffold_graph = scaffold_graph[ scaffold_graph['redundant'] == False ].copy()
    scaffold_graph.drop(columns=['redundant'],inplace=True)

    # Remove overlapping paths with repeated starting scaffold
#    scaffold_graph.reset_index(inplace=True, drop=True)
#    scaffold_graph['rep_len'] = -1
#    reps = []
#    for s in range(1,scaffold_graph['length'].max()):
#        scaffold_graph.loc[(scaffold_graph['scaf'+str(s)] == scaffold_graph['from']) & (scaffold_graph['strand'+str(s)] == np.where(scaffold_graph['from_side'] == 'r', '+', '-')), 'rep_len'] = s
#        reps.append( scaffold_graph[(scaffold_graph['rep_len'] == s) & (scaffold_graph['rep_len']+1 < scaffold_graph['length'])].copy() )
#    reps = pd.concat(reps, ignore_index=True)
#    if len(reps):
#        reps = reps.merge(scaffold_graph.reset_index()[['from','from_side','index']].rename(columns={'index':'red_ind'}), on=['from','from_side'], how='inner') # Add all possibly redundant indexes in scaffold_graph
#        reps = reps[scaffold_graph.loc[reps['red_ind'], 'length'].values == reps['length'] - reps['rep_len']].copy() # Filter out all non-redundant entries
#        for s in range(1, (reps['length']-reps['rep_len']).max()):
#            for rl in range(1, reps['rep_len'].max()):
#                if s+rl < reps['length'].max():
#                    reps = reps[(reps['rep_len'] != rl) | (reps['length']-rl-1 < s) | ((scaffold_graph.loc[reps['red_ind'], 'scaf'+str(s)].values == reps['scaf'+str(s+rl)]) & (scaffold_graph.loc[reps['red_ind'], 'strand'+str(s)].values == reps['strand'+str(s+rl)]) & (scaffold_graph.loc[reps['red_ind'], 'dist'+str(s)].values == reps['dist'+str(s+rl)]))].copy()
#        scaffold_graph.drop(index=np.unique(reps['red_ind'].values), inplace=True)
#        scaffold_graph.drop(columns=['rep_len'], inplace=True)

    return scaffold_graph

def BuildScaffoldGraph(long_range_connections, scaf_bridges):
    # First start from every contig and extend in both directions on valid reads
    if len(long_range_connections):
        scaffold_graph = long_range_connections[['scaffold','strand','conn_id','pos','size']].copy()
        scaffold_graph = scaffold_graph[scaffold_graph['pos']+1 < scaffold_graph['size']].copy() # If we are already at the last position we cannot extend
        scaffold_graph['org_pos'] = scaffold_graph['pos']
        scaffold_graph.rename(columns={'scaffold':'from', 'strand':'from_side'}, inplace=True)
        scaffold_graph['from_side'] = np.where(scaffold_graph['from_side'] == '+','r','l')
        for s in range(1,scaffold_graph['size'].max()):
            scaffold_graph['pos'] += 1
            scaffold_graph = scaffold_graph.merge(long_range_connections[['conn_id','pos','scaffold','strand','prev_dist']].rename(columns={'scaffold':'scaf'+str(s), 'strand':'strand'+str(s), 'prev_dist':'dist'+str(s)}), on=['conn_id','pos'], how='left')
        scaffold_graph.drop(columns=['pos','conn_id'],inplace=True)
        scaffold_graph['length'] = scaffold_graph['size'] - scaffold_graph['org_pos']
        scaffold_graph.drop(columns=['size','org_pos'],inplace=True)
    else:
        scaffold_graph = []
#
    # Then add scaf_bridges with alternatives
    if len(scaf_bridges):
        short_bridges = scaf_bridges[['from','from_side','to','to_side','mean_dist']].copy()
        short_bridges.rename(columns={'to':'scaf1','to_side':'strand1','mean_dist':'dist1'}, inplace=True)
        short_bridges['strand1'] = np.where(short_bridges['strand1'] == 'l', '+', '-')
        short_bridges['length'] = 2
        short_bridges = short_bridges[['from','from_side','length','scaf1','strand1','dist1']].copy()
        if len(scaffold_graph):
            scaffold_graph = pd.concat([scaffold_graph, short_bridges], ignore_index=True, sort=False)
        else:
            scaffold_graph = short_bridges
#
        # Remove redundant entries
        scaffold_graph = RemoveRedundantEntriesInScaffoldGraph(scaffold_graph)

    return scaffold_graph

def FindFullyDuplicatedScaffolds(scaffold_parts, repeats):
    # Find fully repeated contigs on scaffolds
    scaf_reps = scaffold_parts.merge(repeats.loc[repeats['side'] == 'lr', ['q_con','strand','t_con','t_start','t_end']].rename(columns={'q_con':'conpart'}), on='conpart', how='inner')
    # Filter scaffolds where not all positions are repeated by the same contig (if we have a full duplication of a multi-contig scaffolds inside another multi-contig scaffold those two scaffolds must be distinct enough and we can ignore it, or they would not have been scaffolded along unique bridges)
    scaf_reps.sort_values(['scaffold','t_con','pos'], inplace=True)
    rep_len = scaf_reps.groupby(['scaffold','t_con'], sort=False)['pos'].agg(['min','max','size']).reset_index()
    scaf_len = scaffold_parts.groupby(['scaffold']).size().reset_index(name='ncons')
    rep_len = rep_len.merge(scaf_len, on='scaffold', how='left')
    scaf_reps = scaf_reps.merge(rep_len.loc[(rep_len['min'] == 0) & (rep_len['max'] == rep_len['ncons']-1) & (rep_len['size'] == rep_len['ncons']), ['scaffold','t_con','ncons']], on=['scaffold','t_con'], how='inner')
    # Filter scaffolds where t_con has been scaffolded
    scaf_reps = scaf_reps.merge(scaf_len.loc[scaf_len['ncons'] == 1, ['scaffold']].rename(columns={'scaffold':'t_con'}), on='t_con', how='inner')
    # If multiple options exist for a given position create all possible groupings of alternative options
    scaf_reps.loc[scaf_reps['reverse'], 'strand'] = np.where(scaf_reps.loc[scaf_reps['reverse'], 'strand'] == '+', '-', '+')
    scaf_reps.sort_values(['scaffold','t_con','strand','pos'], inplace=True)
    scaf_reps['group'] = (scaf_reps.groupby(['scaffold','t_con','strand'], sort=False).cumcount() == 0).cumsum()
    multi_opts = scaf_reps.groupby(['scaffold','t_con','strand','pos'], sort=False).size()
    multi_opts = multi_opts[multi_opts > 1].reset_index(name='nopts')
    if len(multi_opts):
        multi_opts['order'] = multi_opts.groupby(['scaffold','t_con','strand'], sort=False).cumcount()
        for o in range(multi_opts['order'].max()+1):
            cur = multi_opts[multi_opts['order'] == o].copy()
            cur = cur.merge(scaf_reps[['scaffold','t_con','strand','pos','t_start','t_end','group']], on=['scaffold','t_con','strand','pos'], how='left')
            cur.sort_values(['group'], inplace=True)
            cur['opt'] = cur.groupby(['group'], sort=False).cumcount()
            cur = cur[['group','opt','pos','t_start','t_end']].rename(columns={'pos':'opos','t_start':'ot_start','t_end':'ot_end'})
            scaf_reps = scaf_reps.merge(cur, on='group', how='left')
            scaf_reps = scaf_reps[np.isnan(scaf_reps['opos']) | (scaf_reps['opos'] != scaf_reps['pos']) | ((scaf_reps['t_start'] == scaf_reps['ot_start']) & (scaf_reps['t_end'] == scaf_reps['ot_end']))].drop(columns=['opos','ot_start','ot_end'])
            scaf_reps['opt'] = scaf_reps['opt'].fillna(0).astype(int)
            scaf_reps.sort_values(['group','opt','pos'], inplace=True)
            scaf_reps['group'] = (scaf_reps.groupby(['group','opt'], sort=False).cumcount() == 0).cumsum()
            scaf_reps.drop(columns=['opt'], inplace=True)
    # Filter repeats, where the order does not match
    inconsistent = scaf_reps.loc[(scaf_reps['pos']+1 != scaf_reps['ncons']) & ( (scaf_reps['group'] != scaf_reps['group'].shift(-1)) |
                                                                                ((scaf_reps['strand'] == '+') & ((scaf_reps['t_start'] > scaf_reps['t_start'].shift(-1)) | (scaf_reps['t_end'] > scaf_reps['t_end'].shift(-1)))) |
                                                                                ((scaf_reps['strand'] == '-') & ((scaf_reps['t_start'] < scaf_reps['t_start'].shift(-1)) | (scaf_reps['t_end'] < scaf_reps['t_end'].shift(-1)))) |
                                                                                ( (scaf_reps['t_start'] == scaf_reps['t_start'].shift(-1)) & (scaf_reps['t_end'] == scaf_reps['t_end'].shift(-1)) & (scaf_reps['conpart']+np.where(scaf_reps['reverse'], -1, 1) != scaf_reps['conpart'].shift(-1)) ) ), 'group'].values # This last line filters split contigs that are repeated in another contig if the order does not match
    scaf_reps = scaf_reps[np.isin(scaf_reps['group'], inconsistent) == False].drop(columns=['conpart','pos','reverse','next_dist','prev_dist','ncons'])
    scaf_reps = scaf_reps.groupby(['group','scaffold','strand','t_con']).agg({'t_start':'min','t_end':'max'}).reset_index().drop(columns=['group']).drop_duplicates()
#
    return scaf_reps

def FindSingleContigScaffoldsWithOneDuplicatedUnconnectedEnd(scaffold_parts, repeats, scaffold_graph):
    # Find unscaffolded one sides duplications (full duplicates are two separate one sided duplicates)
    end_reps = scaffold_parts.groupby(['scaffold']).size().reset_index(name='len')
    end_reps = end_reps[end_reps['len'] == 1].drop(columns='len').merge(repeats[['q_con','side','q_start','q_end','strand','t_con']].rename(columns={'q_con':'scaffold'}), on='scaffold', how='inner')
    end_reps = end_reps.loc[np.repeat(end_reps.index.values, np.where(end_reps['side'] == 'lr', 2, 1))].reset_index()
    end_reps.loc[end_reps['index'] == end_reps['index'].shift(1), 'side'] = 'r'
    end_reps.loc[end_reps['index'] == end_reps['index'].shift(-1), 'side'] = 'l'
    end_reps.drop(columns='index', inplace=True)
    # Require that they do not have a bridge on that side
    bridges = scaffold_graph[['from','from_side']].drop_duplicates().rename(columns={'from':'scaffold','from_side':'side'})
    end_reps = end_reps[ end_reps.merge(bridges, on=['scaffold','side'], how='left', indicator=True)['_merge'].values == "left_only" ].copy()
    # Remove contigs that do not have a connection on the other side (where we cannot check that they are continuing on the same path)
    bridges['side'] = np.where(bridges['side'] == 'r', 'l', 'r') # Invert the bridge side here, so that we check if there is a connection of the opposite side (of the duplication)
    end_reps = end_reps.merge(bridges, on=['scaffold','side'], how='inner')
    # Require that t_con is at an scaffold end
    scaffold_ends = scaffold_parts.loc[scaffold_parts['scaffold'] != scaffold_parts['scaffold'].shift(1), ['conpart','scaffold','reverse']].copy()
    scaffold_ends['end'] = 'l'
    scaffold_ends = pd.concat([scaffold_ends, scaffold_parts.loc[scaffold_parts['scaffold'] != scaffold_parts['scaffold'].shift(-1), ['conpart','scaffold','reverse']].copy()], ignore_index=True)
    scaffold_ends['end'] = scaffold_ends['end'].fillna('r')
    end_reps = end_reps.merge(scaffold_ends.rename(columns={'conpart':'t_con','scaffold':'t_scaf'}), on='t_con', how='inner')
    # The scaffold is only allowed to continue on the duplicated side
    end_reps['t_side'] = np.where((end_reps['strand'] == '+') != end_reps['reverse'], end_reps['side'], np.where(end_reps['side'] == 'r', 'l', 'r'))
    end_reps = end_reps[end_reps['t_side'] != end_reps['end']].drop(columns=['reverse','end'])
    # Require that t_scaf also has connections on the non-duplicated side
    end_reps = end_reps.merge(bridges.rename(columns={'scaffold':'t_scaf','side':'t_side'}), on=['t_scaf','t_side'], how='inner')
    # Make output consistent with FindFullyDuplicatedScaffolds (where only single contig scaffolds can be target (t_con))
    end_reps['strand'] = np.where(end_reps['side'] == end_reps['t_side'], '+', '-')
    end_reps.drop(columns=['t_con','t_side'], inplace=True)
    end_reps.rename(columns={'t_scaf':'t_con'}, inplace=True)
#
    return end_reps

def DropRemovedScaffoldsFromRepeats(onesided_reps, remindex):
    if len(remindex):
        tmpindex = np.unique(remindex['sindex'].values)
        onesided_reps = onesided_reps[np.isin(onesided_reps['sindex'], tmpindex) == False].copy()
        onesided_reps = onesided_reps[np.isin(onesided_reps['tindex'], tmpindex) == False].copy()
#
    return onesided_reps

def DeleteIndexInScaffoldGraph(scaffold_graph, remindex, fullrem):
    trim_graph = scaffold_graph.loc[remindex].copy()
    scaffold_graph.drop(remindex, inplace=True)
    trim_graph.rename(columns={'from':'scaf0','from_side':'strand0'}, inplace=True)
    trim_graph['strand0'] = np.where(trim_graph['strand0'] == 'r', '+', '-')
    if fullrem:
        trim_graph = pd.concat([trim_graph, ReverseVerticalPaths(trim_graph).drop(columns=['lindex'])], ignore_index=True)
    for s in range(trim_graph['length'].max()-1, 1, -1):
        shorten = trim_graph[trim_graph['length'] == s+1].copy()
        shorten[[f'scaf{s}',f'strand{s}',f'dist{s}']] = np.nan
        shorten['length'] -= 1
        trim_graph = pd.concat([trim_graph, shorten], ignore_index=True)
        trim_graph.drop_duplicates(inplace=True)
    trim_graph = ReverseVerticalPaths(trim_graph)
    trim_graph.drop(columns=['lindex'], inplace=True)
    trim_graph.rename(columns={'scaf0':'from','strand0':'from_side'}, inplace=True)
    trim_graph['from_side'] = np.where(trim_graph['from_side'] == '+', 'r', 'l')
    scaffold_graph['trim'] = scaffold_graph[trim_graph.columns].merge(trim_graph, on=list(trim_graph.columns), how='left', indicator=True)['_merge'].values == "both"
    if fullrem:
        scaffold_graph = scaffold_graph[scaffold_graph['trim'] == False].drop(columns=['trim'])
    else:
        scaffold_graph = scaffold_graph[(scaffold_graph['trim'] == False) | (scaffold_graph['length'] > 2)].copy()
        for l in np.unique(scaffold_graph.loc[scaffold_graph['trim'], 'length'].values): # The increasing order provided by np.unique is important here
            scaffold_graph.loc[scaffold_graph['trim'] & (scaffold_graph['length'] == l), [f'scaf{l-1}',f'strand{l-1}',f'dist{l-1}']] = np.nan
            scaffold_graph.loc[scaffold_graph['trim'] & (scaffold_graph['length'] == l), 'length'] -= 1
        scaffold_graph.drop(columns=['trim'], inplace=True)
        scaffold_graph = RemoveRedundantEntriesInScaffoldGraph(scaffold_graph)
#
    return scaffold_graph

def DisconnectRepeatedContigsWithConnectionsOnOneSide(scaffold_graph, scaf_reps, end_reps, scaf_bridges):
    # Merge scaf_reps and end_reps
    all_reps = scaf_reps.drop(columns=['t_start','t_end']).drop_duplicates()
    all_reps['side'] = 'lr'
    all_reps = pd.concat([all_reps, end_reps[ end_reps[['scaffold','strand','t_con']].merge(all_reps[['scaffold','strand','t_con']], on=['scaffold','strand','t_con'], how='left', indicator=True)['_merge'].values == "left_only" ].drop(columns=['q_start','q_end'])], ignore_index=True)
    # Find scaf_reps that have connections only on one side
    onesided_reps = scaffold_graph[['from','from_side']].drop_duplicates().rename(columns={'from':'scaffold','from_side':'side'})
    onesided_reps = onesided_reps[(onesided_reps['scaffold'] != onesided_reps['scaffold'].shift(1)) & (onesided_reps['scaffold'] != onesided_reps['scaffold'].shift(-1))].copy()
    onesided_reps = all_reps.rename(columns={'strand':'repstrand','side':'rep_side'}).merge(onesided_reps, on='scaffold', how='inner')
    # Remove repeats with itself
    onesided_reps = onesided_reps[onesided_reps['scaffold'] != onesided_reps['t_con']].copy()
    # Check if that connection is identical along the graph
    if len(onesided_reps):
        onesided_reps = onesided_reps.merge(scaffold_graph[['from','from_side']].reset_index().rename(columns={'index':'sindex','from':'scaffold','from_side':'side'}), on=['scaffold','side'], how='left')
        onesided_reps['tside'] = np.where(onesided_reps['repstrand'] == '+', onesided_reps['side'], np.where(onesided_reps['side'] == 'r', 'l', 'r'))
        onesided_reps.drop(columns=['repstrand'], inplace=True)
        onesided_reps = onesided_reps.merge(scaffold_graph[['from','from_side']].reset_index().rename(columns={'index':'tindex','from':'t_con','from_side':'tside'}), on=['t_con','tside'], how='inner')
    if len(onesided_reps):
        onesided_reps = onesided_reps[(scaffold_graph.loc[onesided_reps['sindex'].values, ['scaf1','strand1']].values == scaffold_graph.loc[onesided_reps['tindex'].values, ['scaf1','strand1']].values).all(axis=1)].copy()
    if len(onesided_reps):
        onesided_reps['length'] = np.minimum(scaffold_graph.loc[onesided_reps['sindex'].values, 'length'].values, scaffold_graph.loc[onesided_reps['tindex'].values, 'length'].values)
        for s in range(2,onesided_reps['length'].max()):
            if len(onesided_reps) == 0:
                break
            else:
                onesided_reps = onesided_reps[(onesided_reps['length'] <= s) | (scaffold_graph.loc[onesided_reps['sindex'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values == scaffold_graph.loc[onesided_reps['tindex'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values).all(axis=1)].copy()
    if len(onesided_reps):
        onesided_reps['length'] = scaffold_graph.loc[onesided_reps['sindex'].values, 'length'].values # Restore original length for later comparison, not minimum of the duplicated paths
    remindex = []
    if len(onesided_reps):
        # Remove indexes in scaffold_graph, where the target(t_con,tindex) is not in onesided_reps
        remindex.append( onesided_reps.loc[onesided_reps[['t_con','tindex']].rename(columns={'t_con':'scaffold','tindex':'sindex'}).merge(onesided_reps[['scaffold','sindex']].drop_duplicates(), on=['scaffold','sindex'], how='left', indicator=True)['_merge'].values == "left_only", ['scaffold','sindex','rep_side']].drop_duplicates() )
        onesided_reps = DropRemovedScaffoldsFromRepeats(onesided_reps, remindex[-1])
        # Remove indexes that are fully repeated, but the target is not
        if len(onesided_reps):
            remindex.append( onesided_reps.loc[(onesided_reps['rep_side'] == 'lr') & (onesided_reps[['t_con','tindex']].rename(columns={'t_con':'scaffold','tindex':'sindex'}).merge(onesided_reps.loc[onesided_reps['rep_side'] == 'lr', ['scaffold','sindex']].drop_duplicates(), on=['scaffold','sindex'], how='left', indicator=True)['_merge'].values == "left_only"), ['scaffold','sindex','rep_side']].drop_duplicates() )
            onesided_reps = DropRemovedScaffoldsFromRepeats(onesided_reps, remindex[-1])
        # Remove indexes that have the shorter scaffold_graph entries
        if len(onesided_reps):
            remindex.append( onesided_reps.loc[onesided_reps['length'] < onesided_reps[['t_con','tindex']].rename(columns={'t_con':'scaffold','tindex':'sindex'}).merge(onesided_reps[['scaffold','sindex','length']].drop_duplicates(), on=['scaffold','sindex'], how='left')['length'].values, ['scaffold','sindex','rep_side']].drop_duplicates() )
            onesided_reps.drop(columns='length', inplace=True)
            onesided_reps = DropRemovedScaffoldsFromRepeats(onesided_reps, remindex[-1])
        # If the target is also in scaffold_graph remove the one with fewer bridging reads
        if len(onesided_reps):
            for q in ['s','t']:
                onesided_reps[[f'{q}to',f'{q}to_side',f'{q}dist']] = scaffold_graph.loc[onesided_reps[f'{q}index'].values, ['scaf1','strand1','dist1']].values
                onesided_reps[f'{q}to_side'] = np.where(onesided_reps[f'{q}to_side'] == '+', 'l', 'r')
            onesided_reps['scount'] = onesided_reps[['scaffold','side','sto','sto_side','sdist']].rename(columns={'scaffold':'from','side':'from_side','sto':'to','sto_side':'to_side','sdist':'mean_dist'}).merge(scaf_bridges[['from','from_side','to','to_side','mean_dist','bcount']], on=['from','from_side','to','to_side','mean_dist'], how='left')['bcount'].values
            onesided_reps['tcount'] = onesided_reps[['t_con','tside','tto','tto_side','tdist']].rename(columns={'t_con':'from','tside':'from_side','tto':'to','tto_side':'to_side','tdist':'mean_dist'}).merge(scaf_bridges[['from','from_side','to','to_side','mean_dist','bcount']], on=['from','from_side','to','to_side','mean_dist'], how='left')['bcount'].values
            remindex.append( onesided_reps.loc[onesided_reps['tcount'] > onesided_reps['scount'], ['scaffold','sindex','rep_side']].drop_duplicates() )
            onesided_reps.drop(columns=['side','tside','sto','sto_side','sdist','scount','tto','tto_side','tdist','tcount'], inplace=True)
            onesided_reps = DropRemovedScaffoldsFromRepeats(onesided_reps, remindex[-1])
        # In case of a tie remove the scaffold with the higher index
        if len(onesided_reps):
            remindex.append( onesided_reps.loc[onesided_reps['scaffold'] > onesided_reps['t_con'], ['scaffold','sindex','rep_side']].drop_duplicates() )
        remindex = pd.concat(remindex, ignore_index=True).drop_duplicates()
        # Delete remindex in scaffold_graph and trim reverse entries (multiple reverse entries, because we have one starting at every position)
        scaffold_graph = DeleteIndexInScaffoldGraph(scaffold_graph, np.unique(remindex['sindex'].values), False)
    trim_repeats = remindex
    if len(trim_repeats):
        trim_repeats.drop(columns='sindex', inplace=True)
        trim_repeats.drop_duplicates(inplace=True)
        # Remove all scaffolds which are fully duplicated (including the repeats on it that are not)
        trim_repeats = trim_repeats[np.isin(trim_repeats['scaffold'], trim_repeats.loc[trim_repeats['rep_side'] == 'lr', 'scaffold'].values) == False].copy()
        trim_repeats.rename(columns={'scaffold':'q_con'}, inplace=True)
#
    return scaffold_graph, trim_repeats

def RemoveEmptyColumns(df):
    # Clean up columns with all NaN
    cols = df.count()
    df.drop(columns=cols[cols == 0].index.values,inplace=True)
    return df

def ApplyDistancesToDuplicationAlternative(cur, lpot, alt_graph, scaf_bridges, s):
    cur[f'side{s}'] = np.where(cur[f'strand{s}'] == '+', 'r', 'l')
    cur[f'side{s+1}'] = np.where(cur[f'strand{s+1}'] == '+', 'l', 'r')
    cur = cur.merge(scaf_bridges[['from','from_side','to','to_side','mean_dist']].rename(columns={'from':f'scaf{s}','from_side':f'side{s}','to':f'scaf{s+1}','to_side':f'side{s+1}'}), on=[f'scaf{s}',f'side{s}',f'scaf{s+1}',f'side{s+1}'], how='inner')
    cur[f'dist{s+1}'] = cur['mean_dist']
    cur.drop(columns=[f'side{s}',f'side{s+1}','mean_dist'], inplace=True)
    lpot.append( cur.copy() ) # Store the change to apply potential extensions of the replaced scaffold to the left later
    cur[f'side{s-1}'] = np.where(cur[f'strand{s-1}'] == '+', 'r', 'l')
    cur[f'side{s}'] = np.where(cur[f'strand{s}'] == '+', 'l', 'r')
    cur = cur.merge(scaf_bridges[['from','from_side','to','to_side','mean_dist']].rename(columns={'from':f'scaf{s-1}','from_side':f'side{s-1}','to':f'scaf{s}','to_side':f'side{s}'}), on=[f'scaf{s-1}',f'side{s-1}',f'scaf{s}',f'side{s}'], how='inner')
    cur[f'dist{s}'] = cur['mean_dist']
    alt_graph.append( cur.drop(columns=['dscaf','t_con','t_start','t_end',f'side{s-1}',f'side{s}','mean_dist']) )
#
    return cur, lpot, alt_graph

def AddConnectedIndexes(remindex, dup_combs):
    old_len = 0
    while old_len != len(remindex):
        old_len = len(remindex)
        remindex = np.unique(np.concatenate([remindex, dup_combs.loc[np.isin(dup_combs['sindex1'], remindex), 'sindex2'].values]))
#
    return remindex

def DisconnectRepeatedContigsWithConnectionsOnBothSides(scaffold_graph, scaf_reps, scaf_bridges):
    # Find graph entries that contain a duplicated scaffold (not as first or last entry)
    dup_scafs = np.unique(scaf_reps['scaffold'].values)
    dup_graph = []
    for s in range(1, scaffold_graph['length'].max()-1):
        dup_graph.append(scaffold_graph[np.isin(scaffold_graph[f'scaf{s}'], dup_scafs) & (scaffold_graph['length'] > s+1)].reset_index().rename(columns={'index':'sindex'}))
        dup_graph[-1]['dup'] = s
    if len(dup_graph):
        dup_graph = RemoveEmptyColumns(pd.concat(dup_graph, ignore_index=True))
    if len(dup_graph):
        dup_graph.sort_values(['sindex','dup'], inplace=True)
        # Only keep the longest entries (on both sides) for each dup
        shortened_graph = dup_graph[dup_graph['dup'] > 1].drop(columns=['sindex','from','from_side','dist1','dup'])
        if len(shortened_graph):
            shortened_graph.drop_duplicates(inplace=True)
            shortened_graph.rename(columns={f'{n}{s}':f'{n}{s-1}' for s in range(1,dup_graph['length'].max()) for n in ['scaf','strand','dist']}, inplace=True) # Use dup_graph['length'].max() instead of shortened_graph here to get all columns in case we removed the longest entries
            shortened_graph['length'] -= 1
            shortened_graph.rename(columns={'scaf0':'from','strand0':'from_side'}, inplace=True)
            shortened_graph['from_side'] = np.where(shortened_graph['from_side'] == '+', 'r', 'l')
            dup_graph = dup_graph[dup_graph[shortened_graph.columns].merge(shortened_graph, on=list(shortened_graph.columns), how='left', indicator=True)['_merge'].values == "left_only"].copy()
        # Create possible alternatives to dup_graph by replacing the duplicated scaffold with the alternative
        for s in np.unique(dup_graph['dup']):
            dup_graph.loc[dup_graph['dup'] == s, 'dscaf'] = dup_graph.loc[dup_graph['dup'] == s, f'scaf{s}']
        dup_graph['dscaf'] = dup_graph['dscaf'].astype(int)
        dup_graph = dup_graph.merge(scaf_reps[(scaf_reps['scaffold'] != scaf_reps['t_con']) | (scaf_reps['strand'] == '-')].rename(columns={'scaffold':'dscaf','strand':'repstrand'}), on=['dscaf'], how='inner')
        alt_graph = []
        for s in np.unique(dup_graph['dup']):
            # Get alternatives that replace a single scaffold
            cur = dup_graph[dup_graph['dup'] == s].rename(columns={'from':'scaf0','from_side':'strand0'})
            cur['strand0'] = np.where(cur['strand0'] == 'r', '+', '-')
            cur[f'scaf{s}'] = cur['t_con']
            cur.loc[cur['repstrand'] == '-', f'strand{s}'] = np.where(cur.loc[cur['repstrand'] == '-', f'strand{s}'] == '+', '-', '+')
            cur.drop(columns=['repstrand'], inplace=True)
            rpot = cur[cur['length'] > s+2].copy() # Store the change to apply potential extensions of the replaced scaffold to the right later
            lpot = []
            cur, lpot, alt_graph = ApplyDistancesToDuplicationAlternative(cur, lpot, alt_graph, scaf_bridges, s)
            # Extend the replacement to the right
            rpot[['dscaf2','t_start2','t_end2']] = rpot[['dscaf','t_start','t_end']].values
            while len(rpot):
                # Check if the scaffold to the right is also a duplicate of t_con
                rpot['dscaf3'] = rpot[f'scaf{s+1}']
                rpot = rpot.merge(scaf_reps.drop(columns=['strand']).rename(columns={'scaffold':'dscaf3','t_start':'t_start3','t_end':'t_end3'}), on=['dscaf3','t_con'], how='inner')
                # Check that the order of duplications matches the one in the graph
                if len(rpot):
                    rpot = rpot[ np.where(rpot[f'strand{s}'] == '+', (rpot['t_start2'] < rpot['t_start3']) & (rpot['t_end2'] < rpot['t_end3']), (rpot['t_start2'] > rpot['t_start3']) & (rpot['t_end2'] > rpot['t_end3']) ) |
                                ( (rpot['t_start2'] == rpot['t_start3']) & (rpot['t_end2'] == rpot['t_end3']) & (rpot['dscaf2'] + np.where(rpot[f'strand{s}'] == '+', 1, -1) == rpot['dscaf3']) ) ].drop(columns=['dscaf2','t_start2','t_end2'])
                    rpot.rename(columns={'dscaf3':'dscaf2','t_start3':'t_start2','t_end3':'t_end2'}, inplace=True)
                if len(rpot):
                    # Extend the replacement by removing the additional scaffold
                    rpot.drop(columns=[f'scaf{s+1}',f'strand{s+1}',f'dist{s+1}'], inplace=True)
                    rpot = RemoveEmptyColumns(rpot)
                    rpot.rename(columns={f'{n}{s2}':f'{n}{s2-1}' for s2 in range(s+2, rpot['length'].max()) for n in ['scaf','strand','dist']}, inplace=True)
                    rpot['length'] -= 1
                    # Add the distances, which is specific for this extension and not relevant for further extensions
                    cur = rpot.drop(columns=['dscaf2','t_start2','t_end2'])
                    cur, lpot, alt_graph = ApplyDistancesToDuplicationAlternative(cur, lpot, alt_graph, scaf_bridges, s)
                    # Prepare next iteration
                    rpot = rpot[rpot['length'] > s+2].copy()
            # Extend the replacement to the left
            lpot = pd.concat(lpot, ignore_index=True)
            for e in range(s-1,0,-1):
                # Check if the scaffold to the left is also a duplicate of t_con
                lpot['dscaf2'] = lpot[f'scaf{e}']
                lpot = lpot.merge(scaf_reps.drop(columns=['strand']).rename(columns={'scaffold':'dscaf2','t_start':'t_start2','t_end':'t_end2'}), on=['dscaf2','t_con'], how='inner')
                # Check that the order of duplications matches the one in the graph
                if len(lpot):
                    lpot = lpot[ np.where(lpot[f'strand{e+1}'] == '+', (lpot['t_start'] > lpot['t_start2']) & (lpot['t_end'] > lpot['t_end2']), (lpot['t_start'] < lpot['t_start2']) & (lpot['t_end'] < lpot['t_end2']) ) |
                                ( (lpot['t_start'] == lpot['t_start2']) & (lpot['t_end'] == lpot['t_end2']) & (lpot['dscaf'] == lpot['dscaf2'] + np.where(lpot[f'strand{e+1}'] == '+', 1, -1)) ) ].drop(columns=['dscaf','t_start','t_end'])
                    lpot.rename(columns={'dscaf2':'dscaf','t_start2':'t_start','t_end2':'t_end'}, inplace=True)
                if len(lpot):
                    # Extend the replacement by removing the additional scaffold
                    lpot.drop(columns=[f'scaf{e}',f'strand{e}',f'dist{e}'], inplace=True)
                    lpot = RemoveEmptyColumns(lpot)
                    lpot.rename(columns={f'{n}{s2}':f'{n}{s2-1}' for s2 in range(e+1, lpot['length'].max()) for n in ['scaf','strand','dist']}, inplace=True)
                    lpot['length'] -= 1
                    # Add the distances, which is specific for this extension and not relevant for further extensions
                    cur = lpot.drop(columns=['t_con','dscaf','t_start','t_end'])
                    cur[f'side{e-1}'] = np.where(cur[f'strand{e-1}'] == '+', 'r', 'l')
                    cur[f'side{e}'] = np.where(cur[f'strand{e}'] == '+', 'l', 'r')
                    cur = cur.merge(scaf_bridges[['from','from_side','to','to_side','mean_dist']].rename(columns={'from':f'scaf{e-1}','from_side':f'side{e-1}','to':f'scaf{e}','to_side':f'side{e}'}), on=[f'scaf{e-1}',f'side{e-1}',f'scaf{e}',f'side{e}'], how='inner')
                    cur[f'dist{e}'] = cur['mean_dist']
                    alt_graph.append( cur.drop(columns=[f'side{e-1}',f'side{e}','mean_dist']) )
                else:
                    break
        alt_graph = pd.concat(alt_graph, ignore_index=True)
        alt_graph.drop_duplicates(inplace=True)
        alt_graph.rename(columns={'scaf0':'from','strand0':'from_side'}, inplace=True)
        alt_graph['from_side'] = np.where(alt_graph['from_side'] == '+', 'r', 'l')
    # Get the entries in alt_graph that exist in scaffold_graph
    found_dup = []
    if len(dup_graph):
        for l in np.unique(alt_graph['length'].values):
            mcols = ['from','from_side']+[f'{n}{s}' for s in range(1,l) for n in ['scaf','strand','dist']]
            found_dup.append( alt_graph.loc[alt_graph['length'] == l, ['sindex']+mcols].merge(scaffold_graph[mcols].reset_index().rename(columns={'index':'tindex'}), on=mcols, how='inner').drop(columns=mcols) )
    if len(found_dup):
        found_dup = pd.concat(found_dup, ignore_index=True)
        found_dup.drop_duplicates(inplace=True)
        # Find groups of scaffold_graph entries that must be removed together to keep scaffold_graph consistent (Start by making both directions identical by going into the perspective from dscaf)
        dup_groups = dup_graph.drop(columns=['t_start','t_end'])
        dup_groups.rename(columns={'from':'scaf0','from_side':'strand0'}, inplace=True)
        dup_groups['strand0'] = np.where(dup_groups['strand0'] == 'r', '+', '-')
        dup_groups['llen'] = dup_groups['dup']+1
        dup_groups['rlen'] = dup_groups['length'] - dup_groups['dup']
        for d in np.unique(dup_groups['dup'].values):
            cur = dup_groups['dup'] == d
            dup_groups.loc[cur & (dup_groups[f'strand{d}'] == '-'), ['llen','rlen']] = dup_groups.loc[cur & (dup_groups[f'strand{d}'] == '-'), ['rlen','llen']].values
            # Go from d to lower s
            for s in range(d-1,-1,-1):
                # Positive strand
                cur2 = cur & (dup_groups[f'strand{d}'] == '+')
                if np.sum(cur2):
                    dup_groups.loc[cur2, [f'lscaf{d-s}',f'lstrand{d-s}',f'ldist{d-s}']] = dup_groups.loc[cur2, [f'scaf{s}',f'strand{s}',f'dist{s+1}']].values
                    dup_groups.loc[cur2, f'lstrand{d-s}'] = np.where(dup_groups.loc[cur2, f'lstrand{d-s}'] == '+', '-', '+')
                # Negative strand
                cur2 = cur & (dup_groups[f'strand{d}'] == '-')
                if np.sum(cur2):
                    dup_groups.loc[cur2, [f'rscaf{d-s}',f'rstrand{d-s}',f'rdist{d-s}']] = dup_groups.loc[cur2, [f'scaf{s}',f'strand{s}',f'dist{s+1}']].values
                    dup_groups.loc[cur2, f'rstrand{d-s}'] = np.where(dup_groups.loc[cur2, f'rstrand{d-s}'] == '+', '-', '+')
            # Go from d to higher s
            for s in range(d+1,dup_groups.loc[cur, 'length'].max()):
                cur2 = cur & (dup_groups['length'] > s)
                # Positive strand
                cur3 = cur2 & (dup_groups[f'strand{d}'] == '+')
                if np.sum(cur3):
                    dup_groups.loc[cur3, [f'rscaf{s-d}',f'rstrand{s-d}',f'rdist{s-d}']] = dup_groups.loc[cur3, [f'scaf{s}',f'strand{s}',f'dist{s}']].values
                # Negative strand
                cur3 = cur2 & (dup_groups[f'strand{d}'] == '-')
                if np.sum(cur3):
                    dup_groups.loc[cur3, [f'lscaf{s-d}',f'lstrand{s-d}',f'ldist{s-d}']] = dup_groups.loc[cur3, [f'scaf{s}',f'strand{s}',f'dist{s}']].values
        dup_groups = dup_groups[['sindex','dscaf','repstrand','t_con','llen','rlen']+[col for col in [f'{o}{n}{s}' for s in range(1,dup_groups['length'].max()) for o in ['l','r'] for n in ['scaf','strand','dist']] if col in dup_groups.columns]].copy()
        dup_groups.sort_values([col for col in dup_groups.columns if col not in ['sindex','llen','rlen']], inplace=True)
        dup_groups[['llen','rlen']] = dup_groups[['llen','rlen']].astype(int).values
        # Now group together the overlapping entries
        dup_combs = []
        for o in ['l','r']:
            for l in np.unique(dup_groups[f'{o}len']):
                mcols = ['dscaf','repstrand','t_con']+[f'{o}{n}{s}' for s in range(1,l) for n in ['scaf','strand','dist']]
                dcomb = dup_groups.loc[dup_groups[f'{o}len'] == l, mcols+['sindex']].rename(columns={'sindex':'sindex1'}).merge(dup_groups[mcols+['sindex']].rename(columns={'sindex':'sindex2'}), on=mcols, how='inner').drop(columns=mcols)
                dup_combs.append(dcomb.copy())
                dup_combs.append(dcomb.rename(columns={'sindex1':'sindex2','sindex2':'sindex1'}))
        dup_combs = pd.concat(dup_combs, ignore_index=True).drop_duplicates()
        dup_combs = dup_combs[dup_combs['sindex1'] != dup_combs['sindex2']].copy()
        # Filter found_dup, where not all required dup_combs are found
        old_len = 0
        while old_len != len(found_dup):
            old_len = len(found_dup)
            dup_combs['del'] = np.isin(dup_combs['sindex2'], found_dup['sindex'].values)
            accepted = dup_combs.groupby(['sindex1'])['del'].min().reset_index()
            accepted = accepted[accepted['del']].copy()
            found_dup = found_dup[np.isin(found_dup['sindex'], accepted['sindex1'].values)].copy()
        if len(found_dup):
            dup_combs.drop(columns=['del'], inplace=True)
    if len(found_dup):
        # Remove indexes where the target index is definitely not removed and indexes grouping with them
        remindex = np.unique(found_dup.loc[np.isin(found_dup['tindex'], found_dup['sindex'].values) == False, 'sindex'].values)
        remindex = AddConnectedIndexes(remindex, dup_combs)
        found_dup = found_dup[(np.isin(found_dup['sindex'], remindex) == False) & (np.isin(found_dup['tindex'], remindex) == False)].copy()
    if len(found_dup):
        # Remove indexes with the lower counts at the diverging side
        found_dup['len'] = np.minimum(scaffold_graph.loc[found_dup['sindex'].values, 'length'].values, scaffold_graph.loc[found_dup['tindex'].values, 'length'].values)-1
        scaffold_graph.rename(columns={'from':'scaf0','from_side':'strand0'}, inplace=True)
        scaffold_graph['strand0'] = np.where(scaffold_graph['strand0'] == 'r', '+', '-')
        for s in range(1,found_dup['len'].max()):
            cur = (found_dup['len'] > s) & (scaffold_graph.loc[found_dup['sindex'].values, [f'scaf{s}',f'strand{s}']].values != scaffold_graph.loc[found_dup['tindex'].values, [f'scaf{s}',f'strand{s}']].values).any(axis=1)
            for q in ['s','t']:
                for o in ['l','r']:
                    if o == 'l':
                        bridge = scaffold_graph.loc[found_dup.loc[cur, f'{q}index'].values, [f'scaf{s-1}',f'strand{s-1}',f'scaf{s}',f'strand{s}',f'dist{s}']].rename(columns={f'scaf{s-1}':'from',f'strand{s-1}':'from_side',f'scaf{s}':'to',f'strand{s}':'to_side',f'dist{s}':'mean_dist'})
                    else:
                        bridge = scaffold_graph.loc[found_dup.loc[cur, f'{q}index'].values, [f'scaf{s}',f'strand{s}',f'scaf{s+1}',f'strand{s+1}',f'dist{s+1}']].rename(columns={f'scaf{s}':'from',f'strand{s}':'from_side',f'scaf{s+1}':'to',f'strand{s+1}':'to_side',f'dist{s+1}':'mean_dist'})
                    bridge['from_side'] = np.where(bridge['from_side'] == '+', 'r', 'l')
                    bridge['to_side'] = np.where(bridge['to_side'] == '+', 'l', 'r')
                    bridge['bcount'] = bridge.merge(scaf_bridges[['from','from_side','to','to_side','mean_dist','bcount']], on=['from','from_side','to','to_side','mean_dist'], how='left')['bcount'].values
                    found_dup.loc[cur, f'{o}{q}count'] = bridge['bcount'].values
        scaffold_graph.rename(columns={'scaf0':'from','strand0':'from_side'}, inplace=True)
        scaffold_graph['from_side'] = np.where(scaffold_graph['from_side'] == '+', 'r', 'l')
        for q in ['s','t']:
            found_dup[f'min{q}count'] = np.minimum(found_dup[f'l{q}count'], found_dup[f'r{q}count'])
            found_dup[f'max{q}count'] = np.maximum(found_dup[f'l{q}count'], found_dup[f'r{q}count'])
        remindex2 = np.unique(found_dup.loc[(found_dup['minscount'] < found_dup['mintcount']) | ((found_dup['minscount'] == found_dup['mintcount']) & (found_dup['maxscount'] < found_dup['maxtcount'])), 'sindex'])
        found_dup.drop(columns=['len','lscount','rscount','ltcount','rtcount','minscount','maxscount','mintcount','maxtcount'], inplace=True)
        remindex2 = AddConnectedIndexes(remindex2, dup_combs)
        remdups = found_dup[np.isin(found_dup['sindex'], remindex2)].copy()
        remdups = remdups[np.isin(remdups['tindex'], remindex2) == False].copy()
        remindex2 = np.unique(remdups['sindex'].values)
        remindex = np.concatenate([remindex, remindex2])
        found_dup = found_dup[(np.isin(found_dup['sindex'], remindex2) == False) & (np.isin(found_dup['tindex'], remindex2) == False)].copy()
        # In case of a tie remove arbitrarily the group, where the lowest index is the highest
        dcomb = dup_combs[np.isin(dup_combs['sindex1'], found_dup['sindex'].values) & np.isin(dup_combs['sindex2'], found_dup['sindex'].values)].copy()
        dcomb['group'] = np.minimum(dcomb['sindex1'], dcomb['sindex2'])
        while True:
            min_group = pd.concat([dcomb[['sindex1','group']].rename(columns={'sindex1':'sindex'}), dcomb[['sindex2','group']].rename(columns={'sindex2':'sindex'})], ignore_index=True).groupby(['sindex'])['group'].min().reset_index()
            dcomb['mingroup'] = np.minimum(dcomb[['sindex1']].rename(columns={'sindex1':'sindex'}).merge(min_group, on='sindex', how='left')['group'].values, dcomb[['sindex2']].rename(columns={'sindex2':'sindex'}).merge(min_group, on='sindex', how='left')['group'].values)
            if np.sum(dcomb['group'] != dcomb['mingroup']):
                dcomb['group'] = dcomb['mingroup']
            else:
                break
        found_dup['sgroup'] = found_dup[['sindex']].merge(min_group, on=['sindex'], how='left')['group'].values
        found_dup['tgroup'] = found_dup[['tindex']].rename(columns={'tindex':'sindex'}).merge(min_group, on=['sindex'], how='left')['group'].values
        remindex2 = np.unique(found_dup.loc[found_dup['sgroup'] > found_dup['tgroup'], 'sindex'])
        remindex = np.concatenate([remindex, remindex2])
        # Apply the removal and reconstruct entries, where we removed too much
        scaffold_graph = DeleteIndexInScaffoldGraph(scaffold_graph, remindex, True)
        for l in range(scaffold_graph['length'].max(), 2, -1):
            cur = scaffold_graph.loc[scaffold_graph['length'] == l].drop(columns=['from','from_side','dist1'])
            cur = RemoveEmptyColumns(cur)
            cur.rename(columns={f'{n}{s}':f'{n}{s-1}' for s in range(1,l) for n in ['scaf','strand','dist']}, inplace=True)
            cur.rename(columns={'scaf0':'from','strand0':'from_side'}, inplace=True)
            cur['from_side'] = np.where(cur['from_side'] == '+', 'r', 'l')
            cur[['scaf1','dist1']] = cur[['scaf1','dist1']].astype(int)
            cur['length'] -= 1
            scaffold_graph = pd.concat([scaffold_graph, cur], ignore_index=True)
            scaffold_graph = RemoveRedundantEntriesInScaffoldGraph(scaffold_graph)
#
    return scaffold_graph

def DeduplicateScaffoldsInGraph(scaffold_graph, repeats, scaffold_parts, scaf_bridges):
    if len(scaffold_graph):
        scaf_reps = FindFullyDuplicatedScaffolds(scaffold_parts, repeats)
        end_reps = FindSingleContigScaffoldsWithOneDuplicatedUnconnectedEnd(scaffold_parts, repeats, scaffold_graph)
        scaffold_graph, trim_repeats = DisconnectRepeatedContigsWithConnectionsOnOneSide(scaffold_graph, scaf_reps, end_reps, scaf_bridges)
        scaffold_graph = DisconnectRepeatedContigsWithConnectionsOnBothSides(scaffold_graph, scaf_reps, scaf_bridges)
    else:
        trim_repeats = []
#
    return scaffold_graph, trim_repeats

def UpdateScafBridgesAccordingToScaffoldGraph(scaf_bridges, scaffold_graph):
    if len(scaffold_graph):
        valid_bridges = scaffold_graph[['from','from_side','scaf1','strand1','dist1']].drop_duplicates()
        valid_bridges.rename(columns={'scaf1':'to','strand1':'to_side','dist1':'mean_dist'}, inplace=True)
        valid_bridges['to_side'] = np.where(valid_bridges['to_side'] == '+', 'l', 'r')
        scaf_bridges = scaf_bridges.merge(valid_bridges, on=['from','from_side','to','to_side','mean_dist'], how='inner')
        scaf_bridges = CountAlternatives(scaf_bridges)
#
    return scaf_bridges

def FindValidExtensionsInScaffoldGraph(scaffold_graph):
    if len(scaffold_graph) == 0:
        origins = []
        extensions = []
        pairs = []
        ocont = []
    else:
        # Get all the possible continuations from a given scaffold
        extensions = scaffold_graph.rename(columns={'from':'scaf0','from_side':'strand0'})
        extensions['strand0'] = np.where(extensions['strand0'] == 'r', '+', '-')
        extensions.reset_index(drop=True, inplace=True)
        # All possible origins are just the inverse
        origins = extensions.rename(columns={col:f'o{col}' for col in extensions.columns if col not in ['scaf0','strand0']})
        origins.rename(columns={**{f'odist{s}':f'odist{s-1}' for s in range(1,origins['olength'].max())}}, inplace=True)
        origins['strand0'] = np.where(origins['strand0'] == '+', '-', '+')
        for s in range(1,origins['olength'].max()):
            origins.loc[origins[f'ostrand{s}'].isnull() == False, f'ostrand{s}'] = np.where(origins.loc[origins[f'ostrand{s}'].isnull() == False, f'ostrand{s}'] == '+', '-', '+')
        # Get all the branch points for the origins
        branches = origins[['scaf0','strand0']].reset_index().rename(columns={'index':'oindex1'}).merge(origins[['scaf0','strand0']].reset_index().rename(columns={'index':'oindex2'}), on=['scaf0','strand0'], how='inner').drop(columns=['scaf0','strand0'])
        nbranches = branches.groupby(['oindex1'], sort=False).size().reset_index(name='nbranches')
        branches = branches[np.isin(branches['oindex1'], nbranches.loc[nbranches['nbranches'] == 1, 'oindex1'].values) == False].copy() # With only one matching branch the branches do not have a branch point
        nbranches = nbranches[nbranches['nbranches'] > 1].copy()
        s = 1
        branch_points = []
        while len(branches):
            # Remove pairs where branch 2 stops matching branch 1 at scaffold s 
            branches = branches[(origins.loc[branches['oindex1'].values, [f'oscaf{s}',f'ostrand{s}',f'odist{s-1}']].values == origins.loc[branches['oindex2'].values, [f'oscaf{s}',f'ostrand{s}',f'odist{s-1}']].values).all(axis=1)].copy()
            # Add a branch point for the branches 1 that now have less pairs
            nbranches['nbranches_new'] = nbranches[['oindex1']].merge(branches.groupby(['oindex1'], sort=False).size().reset_index(name='nbranches'), on=['oindex1'], how='left')['nbranches'].fillna(0).astype(int).values
            branch_points.append( nbranches.loc[nbranches['nbranches_new'] < nbranches['nbranches'], ['oindex1']].rename(columns={'oindex1':'oindex'}) )
            branch_points[-1]['pos'] = s
            # Remove the branches that do not have further branch points
            nbranches['nbranches'] = nbranches['nbranches_new']
            nbranches.drop(columns=['nbranches_new'], inplace=True)
            branches = branches[np.isin(branches['oindex1'], nbranches.loc[nbranches['nbranches'] == 1, 'oindex1'].values) == False].copy() # With only one matching branch the branches do not have a branch point
            nbranches = nbranches[nbranches['nbranches'] > 1].copy()
            # Prepare next round
            s += 1
        branch_points = pd.concat(branch_points, ignore_index=True)
        branch_points.sort_values(['oindex','pos'], ascending=[True,False], inplace=True)
        # Get all the valid extensions for a given origin possible origin-extension-pairs and count how long they match
        pairs = origins[['scaf0','strand0']].reset_index().rename(columns={'index':'oindex'}).merge(extensions[['scaf0','strand0']].reset_index().rename(columns={'index':'eindex'}), on=['scaf0','strand0'], how='inner').drop(columns=['scaf0','strand0'])
        pairs[['scaf1','strand1','dist1']] = extensions.loc[pairs['eindex'].values, ['scaf1','strand1','dist1']].values
        while len(branch_points):
            # Get the furthest branch_point and find extensions starting from there
            highest_bp = branch_points.groupby(['oindex']).first().reset_index()
            cur_ext = []
            for bp in np.unique(highest_bp['pos']):
                cur = origins.loc[highest_bp.loc[highest_bp['pos'] == bp, 'oindex'].values].rename(columns={**{'scaf0':f'scaf{bp}','strand0':f'strand{bp}','odist0':f'dist{bp}'}, **{f'o{n}{s}':f'{n}{bp-s}' for s in range(1,bp+1) for n in ['scaf','strand','dist']}})
                cur.drop(columns=['olength']+[col for col in cur.columns if col[:5] == "oscaf" or col[:7] == "ostrand" or col[:5] == "odist"], inplace=True)
                cur.drop(columns=['dist0'], inplace=True, errors='ignore') # dist0 does not exist for a branch point at the highest position in scaffold_graph
                cur.reset_index(inplace=True)
                cur.rename(columns={'index':'oindex','scaf0':'from','strand0':'from_side'}, inplace=True)
                cur['from_side'] = np.where(cur['from_side'] == '+', 'r', 'l')
                cur = cur.merge(scaffold_graph[scaffold_graph['length'] > bp+1], on=[col for col in cur if col != 'oindex'], how='inner')
                if len(cur):
                    # Trim cur to only contain the extensions
                    cur.drop(columns=['from','from_side']+[f'{n}{s}' for s in range(1,bp+1) for n in ['scaf','strand','dist']], inplace=True)
                    cur = RemoveEmptyColumns(cur)
                    cur.rename(columns={f'{n}{s}':f'{n}{s-bp}' for s in range(bp+1,cur['length'].max()) for n in ['scaf','strand','dist']}, inplace=True)
                    cur['length'] -= bp
                    cur_ext.append(cur)
            if len(cur_ext):
                cur = pd.concat(cur_ext, ignore_index=True)
                del cur_ext
                if len(cur):
                    # Check whether they map until the end
                    cur_pairs = cur[['oindex','length','scaf1','strand1','dist1']].reset_index().rename(columns={'index':'cindex'}).merge(pairs[['oindex','eindex','scaf1','strand1','dist1']], on=['oindex','scaf1','strand1','dist1'], how='inner').drop(columns=['scaf1','strand1','dist1'])
                    cur_pairs['matches'] = 1
                    for s in range(2, cur['length'].max()):
                        comp = (cur_pairs['matches'] == s-1) & (cur_pairs['length'] > s)
                        cur_pairs.loc[comp,'matches'] += (cur.loc[cur_pairs.loc[comp, 'cindex'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values == extensions.loc[cur_pairs.loc[comp, 'eindex'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values).all(axis=1).astype(int)
                    cur_pairs['end'] = cur_pairs['matches']+1 == cur_pairs['length']
                    # Check if the extension matches at least one of the possible extensions of origin (An origin can have multiple scaffold_graph entries that extend it and extensions are compared to each of them)
                    cur_pairs = cur_pairs.groupby(['oindex','eindex'])[['end']].max().reset_index()
                    # Add a end=False for every extension/origin pair that should have been checked, but was not, because already scaf1 did not match and they never went into cur_pairs
                    cur_pairs = pairs[['oindex','eindex']].merge(cur[['oindex']].drop_duplicates(), on=['oindex'], how='inner').merge(cur_pairs, on=['oindex','eindex'], how='left')
                    cur_pairs['end'] = cur_pairs['end'].fillna(False)
                    # Remove pairs that do not match until the end
                    pairs.drop(pairs[pairs[['oindex','eindex']].merge(cur_pairs, on=['oindex','eindex'], how='left')['end'].values == False].index.values, inplace=True)
            branch_points = branch_points[branch_points.merge(highest_bp, on=['oindex','pos'], how='left', indicator=True)['_merge'].values == "left_only"].copy()
        pairs.drop(columns=['scaf1','strand1','dist1'], inplace=True)
        # Add everything also in the other direction
        pairs = pairs.merge(pairs.rename(columns={'oindex':'eindex','eindex':'oindex'}), on=['oindex','eindex'], how='outer')
        pairs.sort_values(['oindex','eindex'], inplace=True)
        # Get continuations of origins and extensions to make neighbours consistent
        ocont = []
        for l in np.unique(origins.loc[origins['olength'] > 1, 'olength'].values):
                cur = origins[origins['olength'] == l].reset_index()
                if len(cur):
                    cur.rename(columns={**{'index':'oindex2','scaf0':'nscaf','strand0':'nstrand','odist0':'ndist'}, **{f'{n}{s}':f'{n}{s-1}' for s in range(1,l) for n in ['oscaf','ostrand','odist']}}, inplace=True)
                    cur.rename(columns={'oscaf0':'scaf0','ostrand0':'strand0'}, inplace=True)
                    cur.drop(columns=['olength'] + [f'{n}{s}' for s in range(l,l) for n in ['oscaf','ostrand']] + [f'odist{s}' for s in range(l,l-1)], inplace=True)
                    cur = RemoveEmptyColumns(cur)
                    mcols = ['scaf0','strand0'] + [f'{n}{s}' for s in range(1,l-1) for n in ['oscaf','ostrand']] + [f'odist{s}' for s in range(l-2)]
                    cur = cur.merge(origins[mcols].reset_index().rename(columns={'index':'oindex1'}), on=mcols, how='inner').drop(columns=mcols)
                    ocont.append(cur)
        perfect_ocont = pd.concat(ocont, ignore_index=True).drop(columns=['nscaf','nstrand','ndist'])
        econt = []
        for l in np.unique(extensions['length'].values):
            cur = extensions[extensions['length'] == l].reset_index()
            cur.drop(columns=['scaf0','strand0','dist1','length'], inplace=True)
            cur = RemoveEmptyColumns(cur)
            cur.rename(columns={**{'index':'eindex1'}, **{f'{n}{s}':f'{n}{s-1}' for s in range(1,l) for n in ['scaf','strand','dist']}}, inplace=True)
            mcols = ['scaf0','strand0'] + [f'{n}{s}' for s in range(1,l-1) for n in ['scaf','strand','dist']]
            cur = cur.merge(extensions[mcols].reset_index().rename(columns={'index':'eindex2'}), on=mcols, how='inner').drop(columns=mcols)
            econt.append(cur)
        if len(econt):
            econt = pd.concat(econt, ignore_index=True)
            econt.sort_values(['eindex1','eindex2'], inplace=True)
        else:
            econt = pd.DataFrame({'eindex1':[],'eindex2':[]})
        # Check that neighbours have consistent pairs
        while True:
            # Extend the extension to the next scaffold and then propagate back the origins to get the extension/origin pairs that should be there based on the neighbours
            new_pairs = econt.copy()
            new_pairs[['oscaf1','ostrand1','odist0']] = extensions.loc[new_pairs['eindex1'].values, ['scaf0','strand0','dist1']].values
            pairs[['oscaf1','ostrand1','odist0']] = origins.loc[pairs['oindex'].values, ['oscaf1','ostrand1','odist0']].values
            new_pairs = new_pairs.merge(pairs.rename(columns={'eindex':'eindex2','oindex':'oindex2'}), on=['eindex2','oscaf1','ostrand1','odist0'], how='inner').drop(columns=['oscaf1','ostrand1','odist0'])
            pairs.drop(columns=['oscaf1','ostrand1','odist0'], inplace=True)
            new_pairs = new_pairs.merge(perfect_ocont, on=['oindex2'], how='inner')
            new_pairs.drop(columns=['eindex2','oindex2'], inplace=True)
            new_pairs.drop_duplicates(inplace=True)
            #Add them to pairs if they are not already present
            new_pairs.rename(columns={'eindex1':'eindex','oindex1':'oindex'}, inplace=True)
            new_pairs = new_pairs.merge(new_pairs.rename(columns={'oindex':'eindex','eindex':'oindex'}), on=['oindex','eindex'], how='outer')
            old_len = len(pairs)
            pairs = pairs.merge(new_pairs, on=['oindex','eindex'], how='outer')
            pairs.sort_values(['oindex','eindex'], inplace=True)
            if old_len == len(pairs):
                break
        # Get continuations of origins to reduce computations later
        ocont = []
        missing_cont = pairs[['oindex']].rename(columns={'oindex':'oindex1'}) # All origins that have an extension must have a following origin in the direction of extension
        missing_cont[['nscaf','nstrand','ndist']] = extensions.loc[pairs['eindex'].values, ['scaf1','strand1','dist1']].values
        missing_cont[['nscaf','ndist']] = missing_cont[['nscaf','ndist']].astype(int)
        missing_cont.drop_duplicates(inplace=True)
        if len(missing_cont):
            for cut in range(origins['olength'].max()-1): # If we do not find a following origin for a given origin look for the longest match by cutting away scaffolds
                for l in np.unique(origins.loc[origins['olength'] > cut+1, 'olength'].values):
                    cur = origins[origins['olength'] == l].reset_index()
                    cur.rename(columns={**{'index':'oindex2','scaf0':'nscaf','strand0':'nstrand','odist0':'ndist'}, **{f'{n}{s}':f'{n}{s-1}' for s in range(1,l-cut) for n in ['oscaf','ostrand','odist']}}, inplace=True)
                    cur.rename(columns={'oscaf0':'scaf0','ostrand0':'strand0'}, inplace=True)
                    cur.drop(columns=['olength'] + [f'{n}{s}' for s in range(l-cut,l) for n in ['oscaf','ostrand']] + [f'odist{s}' for s in range(l-cut,l-1)], inplace=True)
                    cur = RemoveEmptyColumns(cur)
                    mcols = ['scaf0','strand0'] + [f'{n}{s}' for s in range(1,l-cut-1) for n in ['oscaf','ostrand']] + [f'odist{s}' for s in range(l-cut-2)]
                    cur = cur.merge(origins.loc[np.isin(origins.index.values, np.unique(missing_cont['oindex1'])), mcols].reset_index().rename(columns={'index':'oindex1'}), on=mcols, how='inner').drop(columns=mcols)
                    if cut > 0:
                        cur = cur.merge(missing_cont, on=['oindex1','nscaf','nstrand','ndist'], how='inner') # Do not use this for perfect matches, because sometimes an extension gets lost due to the simplification from all reads to the scaffold_graph
                    cur['matches'] = l-cut
                    ocont.append(cur)
                if cut == 0:
                    # no cut means we have perfect matches, so we cannot improve on them anymore, so remove them from missing cont (For other cut values, we might cut away more from a longer origin and still achieve more matches, thus we cannnot only use it for perfect matches)
                    ocont = pd.concat(ocont, ignore_index=True)
                    mcols = ['oindex1','nscaf','nstrand','ndist']
                    missing_cont = missing_cont[ missing_cont.merge(ocont[mcols].drop_duplicates(), on=mcols, how='left', indicator=True)['_merge'].values == "left_only" ].copy()
                    perfect_ocont = ocont.drop(columns=['matches','nscaf','nstrand','ndist'])
                    ocont = [ ocont ]
        if len(ocont):
            ocont = pd.concat(ocont, ignore_index=True)[['oindex1','oindex2','nscaf','nstrand','ndist','matches']]
            mcols = ['oindex1','nscaf','nstrand','ndist']
            ocont = ocont[ocont['matches'] == ocont[mcols].merge(ocont.groupby(mcols)['matches'].max().reset_index(), on=mcols, how='left')['matches'].values].copy()
            ocont.drop(columns=['matches'], inplace=True)
            ocont.sort_values(['oindex1','oindex2'], inplace=True)
        else:
            ocont = pd.DataFrame({'oindex1':[],'oindex2':[],'nscaf':[],'nstrand':[],'ndist':[]})
    # Combine it into one dictionary for easier passing as parameter
    graph_ext = {}
    graph_ext['org'] = origins
    graph_ext['ext'] = extensions
    graph_ext['pairs'] = pairs
    graph_ext['ocont'] = ocont
#
    return graph_ext

def UnravelKnots(scaffold_graph):
    knots = []
    for oside in ['l','r']:
        # First take one side in scaffold_graph as origin
        pot_knots = scaffold_graph[scaffold_graph['from_side'] == oside].drop(columns=['from_side'])
        pot_knots['oindex'] = pot_knots.index
        pot_knots.rename(columns={'from':'oscaf0'}, inplace=True)
        pot_knots['ostrand0'] = '+' if oside == 'l' else '-'
        pot_knots['elen'] = 0 # Extension length
        pot_knots.rename(columns={'length':'olen'}, inplace=True)
        end_olen = pot_knots['olen'].max()
        pot_knots['olen'] -= 1
        pot_knots.rename(columns={f'scaf{s}':f'oscaf{s}' for s in range(1,end_olen)}, inplace=True)
        pot_knots.rename(columns={f'strand{s}':f'ostrand{s}' for s in range(1,end_olen)}, inplace=True)
        for s in range(1,end_olen):
            # In scaffold_graph strand + means the left side points to the center('from'), but now we want it to mean left side points to beginning of path / for second direction we invert the extension/origin, which does the same
            pot_knots.loc[pot_knots[f'ostrand{s}'].isnull() == False, f'ostrand{s}'] = np.where(pot_knots.loc[pot_knots[f'ostrand{s}'].isnull() == False, f'ostrand{s}'] == '+', '-', '+')
        pot_knots.rename(columns={f'dist{s}':f'odist{s-1}' for s in range(1,end_olen)}, inplace=True) # Distance is now also the one towards the beginning and not the center anymore (thus highest oscaf does not have a distance anymore)
        max_olen = end_olen-1
        pot_knots = pot_knots[['oindex','olen','elen']+[f'o{n}{s}' for s in range(max_olen) for n in ['scaf','strand','dist']]+[f'oscaf{max_olen}',f'ostrand{max_olen}']].copy()
        # Find the first position, where pot_knots origins are unique (upos)
        pot_knots.sort_values([f'o{n}{s}' for s in range(max_olen) for n in ['scaf','strand','dist']]+[f'oscaf{max_olen}',f'ostrand{max_olen}'], inplace=True)
        pot_knots['group'] = ((pot_knots['oscaf0'] != pot_knots['oscaf0'].shift(1)) | (pot_knots['ostrand0'] != pot_knots['ostrand0'].shift(1))).cumsum()
        pot_knots['upos'] = 0
        s = 1
        while np.sum(pot_knots['group'] == pot_knots['group'].shift(1)):
            pot_knots.loc[(pot_knots['group'] == pot_knots['group'].shift(1)) | (pot_knots['group'] == pot_knots['group'].shift(-1)), 'upos'] += 1
            pot_knots['group'] = ((pot_knots['group'] != pot_knots['group'].shift(1)) | (pot_knots[f'odist{s-1}'] != pot_knots[f'odist{s-1}'].shift(1)) |
                                  (pot_knots[f'oscaf{s}'] != pot_knots[f'oscaf{s}'].shift(1)) | (pot_knots[f'ostrand{s}'] != pot_knots[f'ostrand{s}'].shift(1))).cumsum()
            s += 1
        pot_knots.drop(columns=['group'], inplace=True)
        # Try to find a unique extension past oscaf0 starting from upos (we only consider one direction, if they are unique in the other direction we will check by merging the unique ones from both directions)
        extensions = []
        for s in range(0,pot_knots['upos'].max()+1):
            # Find extensions
            cur_ext = pot_knots.loc[s == pot_knots['upos'], ['oindex',f'oscaf{s}',f'ostrand{s}']+[f'o{n}{s1}' for s1 in range(s) for n in ['scaf','strand','dist']]].rename(columns={**{f'oscaf{s}':'from',f'ostrand{s}':'from_side'}, **{f'o{n}{s1}':f'{n}{s-s1}' for s1 in range(s) for n in ['scaf','strand','dist']}})
            cur_ext['from_side'] = np.where(cur_ext['from_side'] == '+', 'r','l')
            cur_ext = cur_ext.merge(scaffold_graph, on=['from','from_side']+[f'{n}{s1}' for s1 in range(1,s+1) for n in ['scaf','strand','dist']], how='inner')
            cur_ext.drop(columns=['from','from_side']+[f'{n}{s1}' for s1 in range(1,s+1) for n in ['scaf','strand','dist']], inplace=True)
            # Keep only extensions that are unique up to here
            cur_ext = cur_ext[(cur_ext['length'] > s+1)].copy() # Not an extensions
            cur_ext = cur_ext[1 == cur_ext[['oindex']].merge( cur_ext.groupby(['oindex']).size().reset_index(name='alts'), on=['oindex'], how='left' )['alts'].values].copy()
            # Store the extensions to check later if they match to a unique entry in scaffold_graph
            if len(cur_ext):
                end_elen = cur_ext['length'].max()
                cur_ext.rename(columns={f'{n}{s1}':f'e{n}{s1-s}' for s1 in range(s+1,end_elen) for n in ['scaf','strand','dist']}, inplace=True)
                cur_ext.drop(columns=[col for col in cur_ext.columns if (col[:4] == "scaf") | (col[:6] == "strand") | (col[:4] == "dist")], inplace=True)
                cur_ext['length'] -= s+1
                extensions.append(cur_ext)
        pot_knots.drop(columns=['upos'], inplace=True)
        pot_knots = pot_knots.merge(pd.concat(extensions, ignore_index=True, sort=False), on=['oindex'], how='inner')
        pot_knots['elen'] = pot_knots['length']
        pot_knots.drop(columns=['length'], inplace=True)
        # Find the entries in scaffold_graph for the other side that match the extensions
        other_side = scaffold_graph[scaffold_graph['from_side'] == ('r' if oside == 'l' else 'l')].drop(columns=['from_side'])
        other_side.reset_index(inplace=True)
        other_side.rename(columns={**{'from':'oscaf0'}, **{col:f'e{col}' for col in other_side.columns if col not in ['from','length']}}, inplace=True)
        extensions = []
        for s in range(1,pot_knots['elen'].max()+1):
            mcols = ['oscaf0']+[f'{n}{s1}' for s1 in range(1,s+1) for n in ['escaf','estrand','edist']]
            extensions.append( pot_knots.loc[pot_knots['elen'] == s, ['oindex']+mcols].merge(other_side[['eindex']+mcols], on=mcols, how='left').drop(columns=[col for col in mcols if col not in ['escaf1','estrand1','edist1']]) )
        extensions = pd.concat(extensions, ignore_index=True)
        # Filter the ambiguous matches to scaffold_graph (again considering only on direction)
        extensions = extensions[1 == extensions[['oindex']].merge( extensions.groupby(['oindex']).size().reset_index(name='alts'), on=['oindex'], how='left' )['alts'].values].copy()
        extensions[['escaf1','edist1']] = extensions[['escaf1','edist1']].astype(int)
        extensions.rename(columns={'escaf1':'escaf','estrand1':'estrand','edist1':'edist'}, inplace=True)
        # Store the remaining knots for this side
        pot_knots.drop(columns=['elen']+[col for col in pot_knots.columns if (col[:5] == "escaf") | (col[:7] == "estrand") | (col[:5] == "edist")], inplace=True)
        pot_knots = pot_knots.merge(extensions, on=['oindex'], how='inner')
        knots.append(pot_knots)
       
    # Finally keep only the traversals through the knot that are unique in both directions
    knots[0] = knots[0].merge(knots[1][['oindex','eindex']].rename(columns={'oindex':'eindex','eindex':'oindex'}), on=['oindex','eindex'], how='inner')
    knots[0].reset_index(inplace=True)
    knots[0].rename(columns={'index':'knot'}, inplace=True)
    knots[1] = knots[1].merge(knots[0][['knot','oindex','eindex']].rename(columns={'oindex':'eindex','eindex':'oindex'}), on=['oindex','eindex'], how='inner')
    knots = pd.concat(knots, ignore_index=True, sort=False)
    knots = RemoveEmptyColumns(knots)
    
    return knots

def FollowUniquePathsThroughGraphOld(scaffold_graph):
    # Get connected knots, which have to be handled in sequence instead of parallel
    knots = UnravelKnots(scaffold_graph)
    ravels = []
    for s in range(0,knots['olen'].max()+1):
        ravels.append(knots.loc[knots['olen'] >= s, ['knot',f'oscaf{s}']].rename(columns={f'oscaf{s}':'scaf'}))
    ravels = pd.concat(ravels, ignore_index=True)
    ravels.drop_duplicates(inplace=True)
    ravels['scaf'] = ravels['scaf'].astype(int)
    ravels['ravel'] = ravels['knot']
    while True:
        ravels['min_ravel'] = ravels[['scaf']].merge(ravels.groupby(['scaf'])['ravel'].min().reset_index(), on=['scaf'], how='left')['ravel'].values
        ravels['new_ravel'] = ravels[['ravel']].merge(ravels.groupby(['ravel'])['min_ravel'].min().reset_index(), on=['ravel'], how='left')['min_ravel'].values
        if np.sum(ravels['ravel'] != ravels['new_ravel']) == 0:
            break
        else:
            ravels['ravel'] = ravels['new_ravel']
    ravels = ravels[['ravel','knot']].drop_duplicates()
    ravels.sort_values(['ravel','knot'], inplace=True)
#
    # Follow the unique paths through knots
    scaffold_paths = []
    pid = 0
    while len(ravels):
        for cdir in ['+','-']:
            # Prepare extension
            extensions = ravels.groupby(['ravel']).first().reset_index(drop=True).merge(knots[knots['ostrand0'] == cdir], on=['knot'], how='left')
            extensions.drop(columns=['oindex','eindex'], inplace=True)
            extensions['pid'] = np.arange(len(extensions))+pid
            if cdir == '-': # Only on the second round change pid to get the same pid for both directions
                pid += len(extensions)
                extensions['pos'] = -1
            else: # Only on the first round we add the center scaffold and create used knots
                used_knots = extensions['knot'].values
                extensions['pos'] = 0
                scaffold_paths.append(extensions[['pid','pos','oscaf0','ostrand0','odist0']].rename(columns={'oscaf0':'scaf','ostrand0':'strand','odist0':'dist'}))
                extensions['pos'] = 1
            scaffold_paths.append(extensions[['pid','pos','escaf','estrand','edist']].rename(columns={'escaf':'scaf','estrand':'strand','edist':'dist'})) # In the negative direction unfortunately strands are the opposite and we do not have the correct distance yet, so we store the distance to the next instead of previous entry and later shift
#
            # Extend as long as possible
            while len(extensions):
                # Shift to next position
                extensions.drop(columns=['knot'], inplace=True)
                extensions = RemoveEmptyColumns(extensions)
                max_olen = extensions['olen'].max()
                extensions.rename(columns={**{'escaf':'oscaf0','estrand':'ostrand0','edist':'odist0'}, **{f'{n}{s}':f'{n}{s+1}' for s in range(max_olen) for n in ['oscaf','ostrand','odist']}, **{f'oscaf{max_olen}':f'oscaf{max_olen+1}',f'ostrand{max_olen}':f'ostrand{max_olen+1}'}}, inplace=True)
                extensions['pos'] += 1 if cdir == '+' else -1
                # Check if we have a valid extensions
                pos_ext = knots[np.isin(knots['oscaf0'], extensions['oscaf0'].values) & (knots['olen'] <= extensions['olen'].max()+1)].drop(columns=['oindex','eindex']) # The olen can be one higher, because we shifted the extensions by one, thus knots['olen'] == extensions['olen']+1 means we reach back to exactly the same scaffold
                if len(pos_ext) == 0:
                    extensions = []
                else:
                    new_ext = []
                    for s in range(1,pos_ext['olen'].max()+1):
                        mcols = [f'{n}{s1}' for s1 in range(s) for n in ['oscaf','ostrand','odist']]+[f'oscaf{s}',f'ostrand{s}']
                        new_ext.append( extensions[['pid','pos']+mcols].merge(pos_ext.loc[pos_ext['olen'] == s, ['knot','olen','escaf','estrand','edist']+mcols], on=mcols, how='inner') )
                    extensions = pd.concat(new_ext, ignore_index=True)
                    # Check if this knot was not already included (otherwise circular genomes would continue endlessly)
                    extensions = extensions[ np.isin(extensions['knot'], used_knots) == False ].copy()
                    used_knots = np.concatenate([used_knots, extensions['knot'].values])
                    # Add new scaffold to path
                    scaffold_paths.append(extensions[['pid','pos','escaf','estrand','edist']].rename(columns={'escaf':'scaf','estrand':'strand','edist':'dist'}))
        # Drop already used knots, so that we do not start from them anymore
        ravels = ravels[np.isin(ravels['knot'], used_knots) == False].copy()
#
    # Invert the scaffold paths in the negative direction to point in positive direction
    scaffold_paths = pd.concat(scaffold_paths, ignore_index=True)
    scaffold_paths.sort_values(['pid','pos'], inplace=True)
    invert = scaffold_paths['pos'] < 0
    scaffold_paths.loc[invert, 'strand'] = np.where(scaffold_paths.loc[invert, 'strand'] == '+', '-', '+')
    scaffold_paths['dist'] = np.where(invert, np.where(scaffold_paths['pid'] == scaffold_paths['pid'].shift(1), scaffold_paths['dist'].shift(1, fill_value=0), 0), scaffold_paths['dist'])
#
    # Finalize paths
    scaffold_paths['pos'] = scaffold_paths.groupby(['pid'], sort=False).cumcount()
    scaffold_paths.rename(columns={'scaf':'scaf0','strand':'strand0','dist':'dist0'}, inplace=True)
    handled_graph_entries = knots['oindex'].values
#
    return scaffold_paths, handled_graph_entries

def FollowUniquePathsThroughGraph(graph_ext):
    # Prepare origin-extension pairs
    origins = graph_ext['org']
    extensions = graph_ext['ext']
    pairs = graph_ext['pairs'].copy()
    if len(pairs) == 0:
        path_ext = []
    else:
        pairs[['oscaf1','ostrand1','scaf0','strand0','odist0']] = origins.loc[pairs['oindex'], ['oscaf1','ostrand1','scaf0','strand0','odist0']].values
        pairs[['scaf1','strand1','dist1']] = extensions.loc[pairs['eindex'], ['scaf1','strand1','dist1']].values
        pairs[['oscaf1','scaf0','odist0','scaf1','dist1']] = pairs[['oscaf1','scaf0','odist0','scaf1','dist1']].astype(int)
        # Group pairs that share origins/extensions
        pairs['group'] = np.minimum(pairs['oindex'],pairs['eindex'])
        while True:
            pairs['min_group'] = np.minimum( pairs[['oindex']].merge(pairs.groupby(['oindex'])['group'].min().reset_index(), on=['oindex'], how='left')['group'].values,
                                             pairs[['eindex']].merge(pairs.groupby(['eindex'])['group'].min().reset_index(), on=['eindex'], how='left')['group'].values )
            pairs['new_group'] = pairs[['group']].merge(pairs.groupby(['group'])['min_group'].min().reset_index(), on=['group'], how='left')['min_group'].values
            if np.sum(pairs['group'] != pairs['new_group']) == 0:
                break
            else:
                pairs['group'] = pairs['new_group']
        pairs.drop(columns=['min_group','new_group'], inplace=True)
        # For path extension we can only use the "unique" origin-extension pairs in that sense that all grouped pairs connect the same first scaffold in the origin with the same first scaffold in the extension
        alternatives = pairs[['group','strand0','oscaf1','ostrand1','odist0','scaf1','strand1','dist1']].drop_duplicates().groupby(['group','strand0']).size().reset_index(name='alternatives')
        alternatives = alternatives.groupby(['group'])['alternatives'].max().reset_index()
        path_ext = pairs[['oindex','scaf1','strand1','dist1','group','strand0']].drop_duplicates().merge(alternatives.loc[alternatives['alternatives'] == 1, ['group']], on=['group'], how='inner')
    if len(path_ext):
        # Check if a group can be extended even further
        ocont = graph_ext['ocont']
        if len(ocont):
            path_ext.rename(columns={'strand0':'strand','scaf1':'nscaf','strand1':'nstrand','dist1':'ndist'}, inplace=True)
            path_ext = path_ext.merge(ocont[np.isin(ocont['oindex2'], path_ext['oindex'].values)].rename(columns={'oindex1':'oindex'}), on=['oindex','nscaf','nstrand','ndist'], how='left')
        else:
            path_ext['oindex2'] = np.nan
        # Make sure extensions in groups are consistent (if one cannot extend, all others are also not allowed to extend)
        inconsistency = path_ext.groupby(['group','strand'])['oindex2'].agg(['size','count'])
        inconsistency = inconsistency[(inconsistency['count'] != 0) & (inconsistency['count'] != inconsistency['size'])].reset_index()[['group','strand']]
        other_side = path_ext[['group','strand','oindex2']].merge(inconsistency, on=['group','strand'], how='inner')[['oindex2']].rename(columns={'oindex2':'oindex'}).merge(path_ext[['group','strand','oindex']])[['group','strand']].drop_duplicates()
        other_side['strand'] = np.where(other_side['strand'] == '+', '-', '+')
        path_ext.loc[ path_ext[['group','strand']].merge(pd.concat([inconsistency, other_side], ignore_index=True).drop_duplicates(), on=['group','strand'], how='left', indicator=True)['_merge'].values == "both" , 'oindex2'] = np.nan
    if len(path_ext):
        # Prevent separate path from merging into one
        inconsistency = path_ext.loc[np.isnan(path_ext['oindex2']) == False, ['group','strand','oindex2']].rename(columns={'oindex2':'oindex'}).merge(path_ext[['oindex','group','strand']].rename(columns={'group':'ngroup','strand':'nstrand'}), on='oindex', how='left')
        inconsistency.drop(columns='oindex', inplace=True)
        inconsistency.drop_duplicates(inplace=True)
        inconsistency['nalt'] = inconsistency[['ngroup','nstrand']].merge( inconsistency.groupby(['ngroup','nstrand']).size().reset_index(name='nalt'), on=['ngroup','nstrand'], how='left')['nalt'].values
        inconsistency = inconsistency.loc[inconsistency['nalt'] > 1].drop(columns='nalt')
        inconsistency['nstrand'] = np.where(inconsistency['nstrand'] == '+', '-', '+') # For the next group we need to block extension in the other direction, because their origins are duplicated not their extensions
        inconsistency = pd.concat([ inconsistency[['group','strand']], inconsistency[['ngroup','nstrand']].rename(columns={'ngroup':'group', 'nstrand':'strand'}) ], ignore_index=True).drop_duplicates()
        path_ext.loc[ path_ext[['group','strand']].merge(inconsistency, on=['group','strand'], how='left', indicator=True)['_merge'].values == "both" , 'oindex2'] = np.nan
    if len(path_ext) == 0:
        path_ends = []
    else:
        # Find the path ends to start from there (starting on both sides will duplicate all paths, but is less effort then linearly going through all the paths to prevent this)
        path_ends = path_ext.groupby(['group','strand'])['oindex2'].agg(['count']).reset_index()
        path_ends = path_ends[path_ends['count'] == 0].drop(columns=['count'])
        path_ends['strand'] = np.where(path_ends['strand'] == '+', '-', '+') # Reverse the direction of the ends to point in the direction that is not ending
        path_ends = pairs.drop(columns=['oindex','eindex']).drop_duplicates().merge(path_ends.rename(columns={'strand':'strand0'}), on=['group','strand0'], how='inner')
    # Start all possible paths
    scaffold_paths = []
    if len(path_ends):
        path_ends['pid'] = np.arange(len(path_ends))
        scaffold_paths.append( path_ends[['pid','oscaf1','ostrand1']].rename(columns={'oscaf1':'scaf','ostrand1':'strand'}) )
        scaffold_paths[-1].insert(1, 'pos', 0)
        scaffold_paths[-1]['dist'] = 0
        scaffold_paths.append( path_ends[['pid','scaf0','strand0','odist0']].rename(columns={'scaf0':'scaf','strand0':'strand','odist0':'dist'}) )
        scaffold_paths[-1].insert(1, 'pos', 1)
        scaffold_paths.append( path_ends[['pid','scaf1','strand1','dist1']].rename(columns={'scaf1':'scaf','strand1':'strand','dist1':'dist'}) )
        scaffold_paths[-1].insert(1, 'pos', 2)
        next_org = path_ext.merge( path_ends[['group','strand0','pid']].drop_duplicates().rename(columns={'strand0':'strand'}), on=['group','strand'], how='inner' )[['pid','oindex2']].drop_duplicates()
        # Continue along all paths as long as supported
        p = 3
        while len(next_org):
            # Remove the groups that do not have enough support to continue
            test = next_org.groupby(['pid'])['oindex2'].agg(['size','count']).reset_index()
            next_org = next_org[np.isin(next_org['pid'], test.loc[test['size'] == test['count'], 'pid'].values)].copy()
            if len(next_org):
                # Add the next scaffold to scaffold_paths
                cur_ext = path_ext.merge(next_org.rename(columns={'oindex2':'oindex'}), on='oindex', how='inner')
                scaffold_paths.append( cur_ext[['pid','nscaf','nstrand','ndist']].drop_duplicates().rename(columns={'nscaf':'scaf','nstrand':'strand','ndist':'dist'}) )
                scaffold_paths[-1].insert(1, 'pos', p)
                p += 1
                # Find next origins to continue
                next_org = path_ext[['group','strand','oindex2']].merge( cur_ext[['group','strand','pid']].drop_duplicates(), on=['group','strand'], how='inner' )[['pid','oindex2']].drop_duplicates()
        # Finalize paths
        scaffold_paths = pd.concat(scaffold_paths, ignore_index=True)
        scaffold_paths.rename(columns={'scaf':'scaf0','strand':'strand0','dist':'dist0'}, inplace=True)
        scaffold_paths.sort_values(['pid','pos'], inplace=True)
        handled_origins = np.sort(path_ext['oindex'].values)
    else:
        handled_origins = []
#
    return scaffold_paths, handled_origins

def FindRepeatedScaffolds(scaffold_graph):
    repeated_scaffolds = []
    for s in range(1, scaffold_graph['length'].max()):
        repeated_scaffolds.append(np.unique(scaffold_graph.loc[scaffold_graph['from'] == scaffold_graph[f'scaf{s}'], 'from']))
    repeated_scaffolds = np.unique(np.concatenate(repeated_scaffolds))
    repeat_graph = scaffold_graph[np.isin(scaffold_graph['from'], repeated_scaffolds)].copy()
#
    return repeated_scaffolds, repeat_graph

def ConnectLoopsThatShareScaffolds(loop_scafs):
    while True:
        loop_scafs['min_loop'] = loop_scafs[['scaf']].merge(loop_scafs.loc[loop_scafs['inside'], ['scaf','loop']].groupby(['scaf']).min().reset_index(), on=['scaf'], how='left')['loop'].values
        loop_scafs['min_loop'] = loop_scafs[['loop']].merge(loop_scafs.loc[loop_scafs['inside'], ['loop','min_loop']].groupby(['loop']).min().reset_index(), on=['loop'], how='left')['min_loop'].values
        if np.sum(loop_scafs['min_loop'] != loop_scafs['loop']) == 0:
            break
        else:
            loop_scafs['loop'] = loop_scafs['min_loop'].astype(int)
    loop_scafs.drop(columns=['min_loop'], inplace=True)
    loop_scafs = loop_scafs.groupby(['loop','scaf']).agg({'exit':['min'], 'inside':['max']}).droplevel(1, axis=1).reset_index()
    loop_scafs.sort_values(['loop','scaf','exit','inside'], ascending=[True,True,False,False], inplace=True)
#
    return loop_scafs

def AddConnectedScaffolds(loop_scafs, scaffold_graph):
    loop_graph = scaffold_graph.merge(loop_scafs.loc[loop_scafs['inside'], ['scaf','loop']].rename(columns={'scaf':'from'}), on=['from'], how='inner')
    new_scafs = []
    for s in range(1,loop_graph['length'].max()):
        new_scafs.append( loop_graph.loc[loop_graph['length'] > s, ['loop',f'scaf{s}']].drop_duplicates().astype(int).rename(columns={f'scaf{s}':'scaf'}) )
    new_scafs = pd.concat(new_scafs)
    new_scafs.drop_duplicates(inplace=True)
    loop_scafs = loop_scafs.merge(new_scafs, on=['loop','scaf'], how='outer')
    loop_scafs[['inside','exit']] = loop_scafs[['inside','exit']].fillna(False)
#
    return loop_scafs

def FindScaffoldsConnectedToLoops(scaffold_graph):
    # Get the loop units and find scaffolds in them
    repeated_scaffolds, repeat_graph = FindRepeatedScaffolds(scaffold_graph)
    loops = []
    if len(repeated_scaffolds):
        for s in range(1, repeat_graph['length'].max()):
            loops.append(repeat_graph.loc[(repeat_graph['from'] == repeat_graph[f'scaf{s}']) & (repeat_graph['from_side'] == 'r') & (repeat_graph[f'strand{s}'] == '+'), ['from','length']+[f'scaf{s1}' for s1 in range(1,s+1) for n in ['scaf']]])
            loops[-1]['length'] = s+1
        loops = pd.concat(loops, ignore_index=True, sort=False)
        loops['scaf0'] = loops['from']
    loop_scafs = []
    if len(loops):
        for s in range(loops['length'].max()-1): # The last base is just the repeated scaffold again
            loop_scafs.append( loops.loc[loops['length'] > s+1, ['from',f'scaf{s}']].drop_duplicates().astype(int).rename(columns={'from':'loop',f'scaf{s}':'scaf'}) )
        loop_scafs = pd.concat(loop_scafs)
        loop_scafs.drop_duplicates(inplace=True)
        # Find all scaffolds connected to the loop
        loop_scafs['inside'] = True
        loop_scafs['exit'] = False
        loop_scafs = ConnectLoopsThatShareScaffolds(loop_scafs)
        loop_scafs = AddConnectedScaffolds(loop_scafs, scaffold_graph)
        while np.sum(loop_scafs['inside'] == loop_scafs['exit']): # New entries, where we do not know yet if they are inside the loop or an exit
            loop_scafs = FindLoopExits(loop_scafs, scaffold_graph)
            loop_size = loop_scafs.groupby(['loop']).size().reset_index(name='nbefore')
            loop_scafs = ConnectLoopsThatShareScaffolds(loop_scafs)
            loop_size['nafter'] = loop_size[['loop']].merge(loop_scafs.groupby(['loop']).size().reset_index(name='nafter'), on=['loop'], how='left')['nafter'].values
            loop_scafs.loc[np.isin(loop_scafs['loop'], loop_size.loc[loop_size['nbefore'] < loop_size['nafter'], 'loop'].values), 'exit'] = False # We cannot guarantee that the exits are still exits in merged loops
            loop_scafs = AddConnectedScaffolds(loop_scafs, scaffold_graph)
#
    return loop_scafs

def FindLoopExits(loop_scafs, scaffold_graph):
    loop_scafs.loc[np.isin(loop_scafs['loop'], loop_scafs.loc[loop_scafs['inside'] == loop_scafs['exit'], 'loop'].drop_duplicates().values), 'exit'] = False # If we added new scaffolds to the loop we cannot guarantee anymore that the exits are still exits, so check again
    exit_graph = scaffold_graph[['from','from_side','length']+[f'scaf{s}' for s in range(1,scaffold_graph['length'].max())]].merge(loop_scafs.loc[loop_scafs['inside'] == loop_scafs['exit'], ['scaf','loop']].rename(columns={'scaf':'from'}), on=['from'], how='inner')
    exit_graph = RemoveEmptyColumns(exit_graph)
    while len(exit_graph):
        num_undecided = np.sum(loop_scafs['inside'] == loop_scafs['exit'])
        exit_graph['exit'] = True
        exit_graph['inside'] = False
        for s in range(1,exit_graph['length'].max()):
            # Everything that is connected to something that is not an exit scaffolds is not a guaranteed exit paths
            exit_graph.loc[exit_graph[['loop',f'scaf{s}']].rename(columns={f'scaf{s}':'scaf'}).merge(loop_scafs.loc[loop_scafs['exit'] == False, ['loop','scaf']], on=['loop','scaf'], how='left', indicator=True)['_merge'].values == "both", 'exit'] = False
            # Everything that is connected to at least one scaffold inside the loop is an inside paths
            exit_graph.loc[exit_graph[['loop',f'scaf{s}']].rename(columns={f'scaf{s}':'scaf'}).merge(loop_scafs.loc[loop_scafs['inside'], ['loop','scaf']], on=['loop','scaf'], how='left', indicator=True)['_merge'].values == "both", 'inside'] = True
        # If all paths on one side are an exit paths the scaffold is an exit and if at least one paths on both sides is an inside paths the scaffold is inside the loop
        summary = exit_graph.groupby(['loop','from','from_side']).agg({'exit':['min'], 'inside':['max']}).droplevel(1, axis=1).reset_index()
        summary.sort_values(['loop','from','from_side'], inplace=True)
        summary = summary.loc[np.repeat(summary.index.values, 1+( ((summary['from_side'] == 'l') & ((summary['loop'] != summary['loop'].shift(-1)) | (summary['from'] != summary['from'].shift(-1)))) |
                                                                  ((summary['from_side'] == 'r') & ((summary['loop'] != summary['loop'].shift(1)) | (summary['from'] != summary['from'].shift(1)))) ))].reset_index(drop=True)
        summary.loc[(summary['from_side'] == 'l') & (summary['loop'] == summary['loop'].shift(1)) & (summary['from'] == summary['from'].shift(1)), ['from_side','exit','inside']] = ['r',True,False] # Add the exits where nothing connects on one side
        summary.loc[(summary['from_side'] == 'r') & (summary['loop'] == summary['loop'].shift(-1)) & (summary['from'] == summary['from'].shift(-1)), ['from_side','exit','inside']] = ['l',True,False]
        summary = summary.groupby(['loop','from']).agg({'exit':['max'], 'inside':['min']}).droplevel(1, axis=1).reset_index()
        summary.rename(columns={'from':'scaf'}, inplace=True)
        loop_scafs.loc[loop_scafs[['loop','scaf']].merge(summary.loc[summary['exit'], ['loop','scaf']], on=['loop','scaf'], how='left', indicator=True)['_merge'] == "both", 'exit'] = True
        loop_scafs.loc[loop_scafs[['loop','scaf']].merge(summary.loc[summary['inside'], ['loop','scaf']], on=['loop','scaf'], how='left', indicator=True)['_merge'] == "both", 'inside'] = True
        # If we cannot reduce the number of undecided scaffolds anymore they are looped within itself, but that means they are exits (into another loop)
        if np.sum(loop_scafs['inside'] == loop_scafs['exit']) == num_undecided:
            loop_scafs.loc[loop_scafs['inside'] == loop_scafs['exit'], 'exit'] = True
        # Delete all decided scaffolds from exit_graph
        exit_graph = exit_graph.merge(loop_scafs.loc[loop_scafs['inside'] == loop_scafs['exit'], ['scaf','loop']].rename(columns={'scaf':'from'}), on=['from','loop'], how='inner')
#
    return loop_scafs

def ReverseVerticalPaths(loops):
    reverse_loops = []
    for l in np.unique(loops['length']):
        reverse_loops.append( loops.loc[loops['length'] == l, ['length','scaf0','strand0']+[f'{n}{s}' for s in range(1,l) for n in ['scaf','strand','dist']]].rename(columns={**{f'{n}{s}':f'{n}{l-s-1}' for s in range(l) for n in ['scaf','strand']}, **{f'dist{s+1}':f'dist{l-s-1}' for s in range(l)}}).reset_index() )
    reverse_loops = pd.concat(reverse_loops, ignore_index=True, sort=False)
    reverse_loops = reverse_loops[['index','length','scaf0','strand0']+[f'{n}{s}' for s in range(1,reverse_loops['length'].max()) for n in ['scaf','strand','dist']]].copy()
    for s in range(reverse_loops['length'].max()):
        reverse_loops.loc[reverse_loops['length'] > s, f'strand{s}'] = np.where(reverse_loops.loc[reverse_loops['length'] > s, f'strand{s}'] == '+', '-', '+')
    reverse_loops.rename(columns={'index':'lindex'}, inplace=True)
#
    return reverse_loops

def GetLoopUnits(loop_scafs, scaffold_graph, max_loop_units):
    # Get loops by extending the inside scaffolds until we find an exit or they have multiple options after finding the starting scaffold again
    loops = []
    pot_loops = loop_scafs.loc[loop_scafs['inside'], ['loop','scaf']].rename(columns={'scaf':'scaf0'})
    pot_loops['strand0'] = '+'
    pot_loops['len'] = 1
    loop_graph = scaffold_graph[np.isin(scaffold_graph['from'], pot_loops['scaf0'].values)].copy()
    loop_graph.rename(columns={'from':'scaf0','from_side':'strand0'}, inplace=True)
    loop_graph['strand0'] = np.where(loop_graph['strand0'] == 'r', '+', '-')
    sfix = 0
    while len(pot_loops):
        # Merge with loop_graph to extend
        pot_loops['index'] = pot_loops.index.values
        new_loops = []
        for l in range(pot_loops['len'].min(),pot_loops['len'].max()+1):
            mcols = ['scaf0','strand0']+[f'{n}{s}' for s in range(1,l) for n in ['scaf','strand','dist']]
            new_loops.append( pot_loops.loc[pot_loops['len'] == l, ['loop']+[col for col in pot_loops.columns if col[:3] == "fix"]+['index','len']+(['dist0'] if 'dist0' in pot_loops.columns else [])+mcols].merge(loop_graph, on=mcols, how='inner') )
        pot_loops = pd.concat(new_loops, ignore_index=True, sort=False)
        pot_loops = RemoveEmptyColumns(pot_loops)
        # Prepare columns for next round
        pot_loops.rename(columns={f'{n}0':f'fix{n}{sfix}' for n in ['scaf','strand','dist']}, inplace=True)
        sfix += 1
        pot_loops.rename(columns={f'{n}{s}':f'{n}{s-1}' for s in range(1,pot_loops['length'].max())for n in ['scaf','strand','dist']}, inplace=True)
        pot_loops['len'] = pot_loops['length']-1
        pot_loops.drop(columns=['length'], inplace=True)
        # Check if we have an exit, close the loop or have a repeated scaffold
        repeats = []
        for s in range(1,sfix): # We explicitly compare for fixscaf0 later, so we do not need it here
            repeats.append( pot_loops.loc[np.isnan(pot_loops[f'fixscaf{s}']) == False, [f'fixscaf{s}',f'fixstrand{s}']].rename(columns={f'fixscaf{s}':'scaf',f'fixstrand{s}':'strand'}).reset_index() )
        pot_loops['exit'] = False
        pot_loops['closed'] = False
        for s in range(pot_loops['len'].max()):
            pot_loops.loc[(pot_loops['fixscaf0'] == pot_loops[f'scaf{s}']) & (pot_loops['fixstrand0'] == pot_loops[f'strand{s}']), 'closed'] = True
            pot_loops.loc[ pot_loops[['loop',f'scaf{s}']].rename(columns={f'scaf{s}':'scaf'}).merge(loop_scafs.loc[loop_scafs['exit'], ['loop','scaf']], on=['loop','scaf'], how='left', indicator=True)['_merge'] == "both", 'exit'] = True
            repeats.append( pot_loops.loc[np.isnan(pot_loops[f'scaf{s}']) == False, [f'scaf{s}',f'strand{s}']].rename(columns={f'scaf{s}':'scaf',f'strand{s}':'strand'}).reset_index() )
        repeats = pd.concat(repeats, ignore_index=True, sort=False).groupby(['index','scaf','strand']).size().reset_index(name='repeats')
        pot_loops['repeat'] = False
        pot_loops.loc[ np.unique(repeats.loc[repeats['repeats'] > 1, 'index'].values), 'repeat' ] = True
        pot_loops['max_units'] = pot_loops[['loop','fixscaf0']].merge(pot_loops.groupby(['loop','fixscaf0']).size().reset_index(name='count'), on=['loop','fixscaf0'], how='left')['count'] > max_loop_units
        # Add closed loops to loops and remove the dead ends or closed loops that do not involve the starting scaffold (and would lead to endless loops)
        loops.append( RemoveEmptyColumns(pot_loops[pot_loops['closed']].drop(columns=['index','exit','closed','repeat','max_units']).rename(columns={**{f'{n}{s}':f'{n}{s+sfix}' for s in range(pot_loops['len'].max()) for n in ['scaf','strand','dist']}, **{f'fix{n}{s}':f'{n}{s}' for s in range(sfix) for n in ['scaf','strand','dist']}})) )
        if len(loops[-1]):
            loops[-1]['len'] += sfix
        else:
            del loops[-1]
        pot_loops = pot_loops[ (pot_loops['closed'] == False) & (pot_loops['repeat'] == False) & (pot_loops['exit'] == False) & (pot_loops['max_units'] == False) ].copy()
        pot_loops.drop(columns=['exit','closed','repeat','max_units'], inplace=True)
    loops = pd.concat(loops, ignore_index=True, sort=False)
    loops.rename(columns={'len':'length'}, inplace=True)
    # Truncate the ends that reach into the next loop unit
    loops['truncate'] = True
    for s in range(loops['length'].max()-1, 1, -1):
        loops.loc[(loops['scaf0'] == loops[f'scaf{s}']) & (loops['strand0'] == loops[f'strand{s}']), 'truncate'] = False
        truncate = loops['truncate'] & (loops['length'] > s)
        loops.loc[truncate, [f'scaf{s}',f'strand{s}',f'dist{s}']] = np.nan
        loops.loc[truncate, 'length'] -= 1
    loops.drop(columns=['truncate'], inplace=True)
    loops = RemoveEmptyColumns(loops)
    loops.drop_duplicates(inplace=True)
    # Verify that the loop units are also valid in reverse direction
    loops.reset_index(drop=True, inplace=True)
    reverse_loops = ReverseVerticalPaths(loops)
    valid_indexes = []
    mcols = ['from','from_side','scaf1','strand1','dist1']
    while len(reverse_loops):
        # Remove invalid loop units
        reverse_loops.rename(columns={'scaf0':'from','strand0':'from_side'}, inplace=True)
        reverse_loops['from_side'] = np.where(reverse_loops['from_side'] == '+', 'r', 'l')
        check = reverse_loops[mcols].reset_index().rename(columns={'index':'rindex'}).merge(scaffold_graph[mcols].reset_index().rename(columns={'index':'sindex'}), on=mcols, how='inner').drop(columns=mcols)
        check['length'] = np.minimum(reverse_loops.loc[check['rindex'].values, 'length'].values, scaffold_graph.loc[check['sindex'].values, 'length'].values)
        reverse_loops['valid'] = False
        s = 2
        while len(check):
            reverse_loops.loc[np.unique(check.loc[check['length'] == s, 'rindex'].values), 'valid'] = True
            check = check[check['length'] > s].copy()
            if len(check):
                check = check[(reverse_loops.loc[check['rindex'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values == scaffold_graph.loc[check['sindex'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values).all(axis=1)].copy()
                s += 1
        reverse_loops = reverse_loops[reverse_loops['valid']].copy()
        # Store valid indexces
        valid_indexes.append( np.unique(reverse_loops.loc[reverse_loops['length'] == 2, 'lindex'].values) )
        reverse_loops = reverse_loops[reverse_loops['length'] > 2].copy()
        # Prepare next round
        if len(reverse_loops):
            reverse_loops.drop(columns=['from','from_side','dist1'], inplace=True)
            reverse_loops['length'] -= 1
            reverse_loops.rename(columns={f'{n}{s+1}':f'{n}{s}' for s in range(reverse_loops['length'].max()) for n in ['scaf','strand','dist']}, inplace=True)
    valid_indexes = np.unique(np.concatenate(valid_indexes))
    loops = loops.loc[valid_indexes].copy()
    # Finish
    loops.sort_values(['scaf0','length'], inplace=True)
    loops.reset_index(drop=True, inplace=True)
#
    return loops

def CheckConsistencyOfVerticalPaths(vpaths):
    if 'from' in vpaths.columns:
        inconsistent = vpaths[np.isnan(vpaths['from']) | ((vpaths['from_side'] != 'r') & (vpaths['from_side'] != 'l'))].copy()
    else:
        inconsistent = vpaths[np.isnan(vpaths['scaf0']) | ((vpaths['strand0'] != '+') & (vpaths['strand0'] != '-'))].copy()
    if len(inconsistent):
        print("Warning: Position 0 is inconsistent in vertical paths.")
        print(inconsistent)
#
    for s in range(1, vpaths['length'].max()):
        inconsistent = vpaths[ ((vpaths['length'] > s) & (np.isnan(vpaths[f'scaf{s}']) | ((vpaths[f'strand{s}'] != '+') & (vpaths[f'strand{s}'] != '-')) | np.isnan(vpaths[f'dist{s}']))) |
                              ((vpaths['length'] <= s) & ((np.isnan(vpaths[f'scaf{s}']) == False) | (vpaths[f'strand{s}'].isnull() == False) | (np.isnan(vpaths[f'dist{s}']) == False))) ].copy()
        if len(inconsistent):
            print(f"Warning: Position {s} is inconsistent in vertical paths.")
            print(inconsistent)

def GetLoopUnitsInBothDirections(loops):
    bidi_loops = ReverseVerticalPaths(loops)
    bidi_loops['loop'] = loops.loc[bidi_loops['lindex'].values, 'loop'].values
    bidi_loops = pd.concat([loops.reset_index().rename(columns={'index':'lindex'}), bidi_loops], ignore_index=True, sort=False)
    bidi_loops.sort_values(['lindex','strand0'], inplace=True)
    bidi_loops.reset_index(drop=True,inplace=True)
#
    return bidi_loops

def FindLoopUnitExtensions(bidi_loops, scaffold_graph):
    # Get the positions where the scaffold_graph from the end of the loop units splits to later check only there to reduce the advantage of long reads just happen to be on one connection
    first_diff = scaffold_graph.loc[np.isin(scaffold_graph['from'], np.unique(bidi_loops['scaf0'].astype(int).values)), ['from','from_side']].reset_index().rename(columns={'index':'index1'})
    first_diff = first_diff.merge(first_diff.rename(columns={'index1':'index2'}), on=['from','from_side'], how='inner').drop(columns=['from','from_side'])
    first_diff = first_diff[first_diff['index1'] != first_diff['index2']].copy()
    first_diff['diff'] = -1
    s = 1
    diffs = []
    while len(first_diff['diff']):
        first_diff.loc[(scaffold_graph.loc[first_diff['index1'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values != scaffold_graph.loc[first_diff['index2'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values).any(axis=1), 'diff'] = s
        diffs.append(first_diff.loc[first_diff['diff'] >= 0, ['index1','diff']].drop_duplicates())
        first_diff = first_diff[first_diff['diff'] < 0].copy()
        s += 1
    diffs = pd.concat(diffs, ignore_index=True, sort=False)
    # Find associated loop units
    units = diffs[['index1']].drop_duplicates()
    units[['scaf0','strand0']] = scaffold_graph.loc[ units['index1'].values, ['from','from_side']].values
    units['strand0'] = np.where(units['strand0'] == 'l', '+', '-') # For the first_diffs we went into the oppositee direction to get the diffs from which we can extend over the ends
    
    units = units.merge(bidi_loops[['scaf0','strand0']].reset_index().rename(columns={'index':'bindex'}), on=['scaf0','strand0'], how='left').drop(columns=['scaf0','strand0'])
    units['ls'] = bidi_loops.loc[units['bindex'].values, 'length'].values - 2
    units['len'] = np.minimum(scaffold_graph.loc[units['index1'].values, 'length'].values, units['ls']+2)
    valid_units = []
    s = 1
    units[['scaf','strand','dist']] = np.nan
    while len(units):
        for ls in np.unique(units['ls']):
            units.loc[units['ls'] == ls, ['scaf','strand','dist']] = bidi_loops.loc[ units.loc[units['ls'] == ls, 'bindex'].values, [f'scaf{ls}',f'strand{ls}',f'dist{ls+1}']].values
        units['strand'] = np.where(units['strand'] == '+', '-', '+') # We compare graph entries in different orientations
        units = units[(units[['scaf','strand','dist']].values == scaffold_graph.loc[units['index1'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values).all(axis=1)].copy()
        units['ls'] -= 1
        s += 1
        valid_units.append( units.loc[units['len'] == s, ['index1','bindex']].copy() )
        units = units[units['len'] > s].copy()
    valid_units = pd.concat(valid_units, ignore_index=True, sort=False)
    diffs = diffs.merge(valid_units, on=['index1'], how='inner')
    # Check if the scaffold_graph extends from the position of difference into the next loop unit (otherwise it does not hold any information)
    info = diffs[['index1','diff']].drop_duplicates()
    info[['from','from_side']+[f'{n}{s}' for s in range(1,info['diff'].max()+1) for n in ['scaf','strand','dist']]] = np.nan
    for d in np.unique(info['diff']):
        info.loc[info['diff'] == d, ['from','from_side']+[f'{n}{s}' for s in range(1,d+1) for n in ['scaf','strand']]+[f'dist{s}' for s in range(1,d+1)]] = scaffold_graph.loc[info.loc[info['diff'] == d,'index1'].values, [f'{n}{s}' for s in range(d,0,-1) for n in ['scaf','strand']]+['from','from_side']+[f'dist{s}' for s in range(d,0,-1)]].values
        info.loc[info['diff'] == d, f'strand{d}'] = np.where(info.loc[info['diff'] == d, f'strand{d}'] == 'l', '+', '-')
    info['from_side'] = np.where(info['from_side'] == '+', 'l', 'r')
    for s in range(1,info['diff'].max()):
        info.loc[info['diff'] > s, [f'strand{s}']] = np.where(info.loc[info['diff'] > s, [f'strand{s}']] == '+', '-', '+')
    extensions = []
    for d in np.unique(info['diff']):
        mcols = ['from','from_side'] + [f'{n}{s}' for s in range(1,d+1) for n in ['scaf','strand','dist']]
        extensions.append( info.loc[info['diff'] == d, ['index1','diff']+mcols].merge(scaffold_graph, on=mcols, how='inner').drop(columns=[col for col in mcols if col not in [f'scaf{d}',f'strand{d}']]).rename(columns={f'{n}{s}':f'{n}{s-d}' for s in range(d,scaffold_graph['length'].max()) for n in ['scaf','strand','dist']}) )
    extensions = pd.concat(extensions, ignore_index=True, sort=False)
    extensions = extensions[extensions['diff']+1 < extensions['length']].copy()
    extensions['length'] -= extensions['diff']
    # Merge the extensions from different positions (only keep extensions with highest diff/longest mapping in the loop unit, but use all consistent extending scafs)
    if len(extensions):
        extensions.sort_values(['index1','scaf0','strand0']+[f'{n}{s}' for s in range(1,extensions['length'].max()) for n in ['scaf','strand','dist']]+['diff'], inplace=True)
        first_iter = True
        while True:
            extensions['consistent'] = (extensions['index1'] == extensions['index1'].shift(-1)) & (extensions['diff'] < extensions['diff'].shift(-1))
            for s in range(1, extensions['length'].max()):
                cur = extensions['consistent'] & (extensions['length'].shift(-1) > s)
                extensions.loc[cur, 'consistent'] = (extensions.loc[cur, [f'scaf{s}',f'strand{s}',f'dist{s}']].values == extensions.loc[cur.shift(1, fill_value=False).values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values).all(axis=1)
            if first_iter:
                # Extensions that have a lower diff, but the same length and are fully consistent can be removed, because they do not contain additional information
                extensions = extensions[(extensions['consistent'] == False) | (extensions['length'] > extensions['length'].shift(-1))].copy()
                first_iter = False
            else:
                extensions['del'] = extensions['consistent'].shift(1) & (extensions['consistent'] == False)
                extensions.loc[extensions['del'].shift(-1, fill_value=False).values, 'diff'] = extensions.loc[extensions['del'], 'diff'].values
                extensions = extensions[extensions['del'] == False].drop(columns=['del'])
            if np.sum(extensions['consistent']) == 0:
                break
        extensions.drop(columns=['consistent'], inplace=True)
        max_diff = extensions.groupby(['index1'])['diff'].agg(['max','size'])
        extensions = extensions[extensions['diff'] == np.repeat(max_diff['max'].values, max_diff['size'].values)].drop(columns=['diff'])
        extensions = RemoveEmptyColumns(extensions)
        extendible = diffs.loc[np.isin(diffs['index1'], max_diff.index.values), ['index1','bindex']].drop_duplicates()
    else:
        extendible = []
#
    return extensions, extendible

def FindConnectionsBetweenLoopUnits(loops, scaffold_graph, full_info):
    bidi_loops = GetLoopUnitsInBothDirections(loops)
    extensions, extendible = FindLoopUnitExtensions(bidi_loops, scaffold_graph)
    # Get the loop units matching the extensions
    loop_conns = []
    if len(extensions):
        ext_units = extensions.reset_index().rename(columns={'index':'extindex'}).merge(bidi_loops[['scaf0','strand0','scaf1','strand1','dist1']].reset_index().rename(columns={'index':'bindex'}), on=['scaf0','strand0','scaf1','strand1','dist1'], how='inner')
        if len(ext_units):
            ext_units['length'] = np.minimum(ext_units['length'].values, bidi_loops.loc[ext_units['bindex'].values, 'length'].values)
            for s in range(2,ext_units['length'].max()):
                ext_units = ext_units[(ext_units['length'] <= s) | (ext_units[[f'scaf{s}',f'strand{s}',f'dist{s}']].values == bidi_loops.loc[ext_units['bindex'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values).all(axis=1)].copy()
        if len(ext_units):
            # Get the connected loop units
            loop_conns = extendible.rename(columns={'bindex':'bindex1'}).merge(ext_units[['index1','bindex']].rename(columns={'bindex':'bindex2'}), on=['index1'], how='inner').drop(columns=['index1'])
            loop_conns['wildcard'] = False
            # Only keep extensions that extend into another loop unit, because we later use them to check if we have evidence for an additional copy of a loop unit
            extensions = RemoveEmptyColumns( extensions.loc[ext_units['extindex'].values].copy() )
        else:
            extensions = []
    # Non extendible loop units can connect to all other loop units, because we do not know what comes after them (we only take into account other loop units here, because extensions into exits are not an issue, if we do not have a true loop, but just a repeated scaffold, we will only create a duplicated paths that we later remove)
    if len(loop_conns):
        non_extendible = np.setdiff1d(bidi_loops.index.values, np.unique(loop_conns['bindex1'].values))
    else:
        non_extendible = bidi_loops.index.values
    non_extendible = bidi_loops.loc[non_extendible, ['scaf0','strand0']].reset_index().rename(columns={'index':'bindex1'})
    non_extendible = non_extendible.merge(bidi_loops[['scaf0','strand0']].reset_index().rename(columns={'index':'bindex2'}), on=['scaf0','strand0'], how='left').drop(columns=['scaf0','strand0'])
    non_extendible['wildcard'] = True
    if len(non_extendible):
        if len(loop_conns):
            loop_conns = pd.concat([loop_conns, non_extendible], ignore_index=True, sort=False)
        else:
            loop_conns = non_extendible
    # Get the loop indixes from the bidirectional loop indexes and verify that both directions are supported
    if len(loop_conns):
        loop_conns.drop_duplicates(inplace=True)
        for i in [1,2]:
            loop_conns[f'lindex{i}'] = bidi_loops.loc[loop_conns[f'bindex{i}'].values, 'lindex'].values
            loop_conns[f'dir{i}'] = bidi_loops.loc[loop_conns[f'bindex{i}'].values, 'strand0'].values
            loop_conns[f'rdir{i}'] = np.where(loop_conns[f'dir{i}'] == '+', '-', '+')
        loop_conns = loop_conns[['lindex1','dir1','lindex2','dir2','wildcard']].merge(loop_conns[['lindex1','rdir1','lindex2','rdir2']].rename(columns={'lindex1':'lindex2','rdir1':'dir2','lindex2':'lindex1','rdir2':'dir1'}), on=['lindex1','dir1','lindex2','dir2'], how='inner')
#
    if len(loop_conns):
        if full_info == False:
            loop_conns = loop_conns[(loop_conns[['dir1','dir2']] == '+').all(axis=1)].drop(columns=['dir1','dir2'])
            loop_conns.sort_values(['lindex1','lindex2'], inplace=True)
        else:
            # Get loop units that overlap by more than the starting scaffold
            overlapping_units = []
            for sfrom in range(1,bidi_loops['length'].max()-1):
                for l in np.unique(bidi_loops.loc[bidi_loops['length'] > sfrom+1, 'length'].values):
                    overlapping_units.append( bidi_loops[bidi_loops['length'] == l].reset_index().rename(columns={'index':'bindex1'}).merge(bidi_loops[bidi_loops['length'] > l-sfrom+1].reset_index().rename(columns={'index':'bindex2'}), left_on=['loop',f'scaf{sfrom}',f'strand{sfrom}']+[f'{n}{s}' for s in range(sfrom+1, l) for n in ['scaf','strand','dist']], right_on=['loop','scaf0','strand0']+[f'{n}{s}' for s in range(1, l-sfrom) for n in ['scaf','strand','dist']], how='inner')[['bindex1','bindex2']].copy() )
                    overlapping_units[-1]['sfrom'] = sfrom
            if len(overlapping_units):
                overlapping_units = pd.concat(overlapping_units, ignore_index=True)
            # Filter out the ones that do not match an extension
            if len(overlapping_units):
                if len(extendible):
                    overlapping_units = overlapping_units.merge(extendible.rename(columns={'bindex':'bindex1'}), on=['bindex1'], how='left')
                    overlapping_units['index1'] = overlapping_units['index1'].fillna(-1).astype(int)
                else:
                    overlapping_units['index1'] = -1
                if len(extensions):
                    overlapping_units = overlapping_units.merge(extensions, on=['index1'], how='left')
                    overlapping_units['length'] = overlapping_units['length'].fillna(0).astype(int)
                else:
                    overlapping_units['length'] = 0
                overlapping_units['s2'] = bidi_loops.loc[overlapping_units['bindex1'].values, 'length'].values - overlapping_units['sfrom']
                for s in range(1, overlapping_units['length'].max()):
                    overlapping_units['valid'] = overlapping_units['length'] <= s
                    overlapping_units.loc[overlapping_units['valid'], 's2'] = -1 # So we do not compare them in the next step
                    for s2 in np.unique(overlapping_units.loc[overlapping_units['valid'] == False, 's2'].values):
                        overlapping_units.loc[overlapping_units['s2'] == s2, 'valid'] = (overlapping_units.loc[overlapping_units['s2'] == s2, [f'scaf{s}',f'strand{s}',f'dist{s}']].values == bidi_loops.loc[overlapping_units.loc[overlapping_units['s2'] == s2, 'bindex2'].values, [f'scaf{s2}',f'strand{s2}',f'dist{s2}']].values).all(axis=1)
                    overlapping_units = overlapping_units[overlapping_units['valid']].copy()
                    overlapping_units['s2'] += 1
                overlapping_units = overlapping_units[['bindex1','bindex2','sfrom']].drop_duplicates()
            # Get the loop indixes from the biderectional loop indexes and verify that both directions are supported
            if len(overlapping_units):
                for i in [1,2]:
                    overlapping_units[[f'lindex{i}',f'dir{i}']] = bidi_loops.loc[overlapping_units[f'bindex{i}'].values, ['lindex','strand0']].values
                    overlapping_units[f'rdir{i}'] = np.where(overlapping_units[f'dir{i}'] == '+', '-', '+')
                overlapping_units['overlap'] = bidi_loops.loc[overlapping_units['bindex1'].values, 'length'].values - overlapping_units['sfrom']
                overlapping_units = overlapping_units[['lindex1','dir1','lindex2','dir2','sfrom','overlap']].merge(overlapping_units[['lindex1','rdir1','lindex2','rdir2','overlap']].rename(columns={'lindex1':'lindex2','rdir1':'dir2','lindex2':'lindex1','rdir2':'dir1'}), on=['lindex1','dir1','lindex2','dir2','overlap'], how='inner').drop(columns=['overlap'])
                overlapping_units['wildcard'] = False
            loop_conns['sfrom'] = loops.loc[loop_conns['lindex1'].values, 'length'].values - 1
            if len(overlapping_units):
                loop_conns = pd.concat([loop_conns, overlapping_units], ignore_index=True)
            # If multiple starting positions of unit 2 exist in unit 1 take the first, such that we have the minimum length of the combined unit
            loop_conns = loop_conns.groupby(['lindex1','dir1','lindex2','dir2'])[['wildcard','sfrom']].min().reset_index()
            # Prepare possible extensions of the loop units
            if len(extensions):
                extensions = extendible.merge(extensions, on=['index1'], how='inner').drop(columns=['index1'])
                extensions = bidi_loops[['lindex']].reset_index().rename(columns={'index':'bindex'}).merge(extensions, on=['bindex'], how='inner').drop(columns=['bindex'])
#
    if full_info:
        return loop_conns, extensions
    else:
        return loop_conns

def TryReducingAlternativesToPloidy(alternatives, scaf_bridges, ploidy):
    # If we have less than ploidy alternatives  they are directly ok
    alternatives.sort_values(['group'], inplace=True)
    nalts = alternatives.groupby(['group'], sort=False).size().values
    nalts = np.repeat(nalts, nalts)
    valid_alts = alternatives[nalts <= ploidy].copy()
    alternatives['nalts'] = nalts
    alternatives = alternatives[nalts > ploidy].copy()
    if len(alternatives) == 0:
        alternatives = valid_alts
    else:
        # Find the alternatives that differ only by distance or additionally by a missing scaffold
        alternatives.reset_index(drop=True, inplace=True)
        pairs = alternatives[['group']].reset_index().rename(columns={'index':'index1'}).merge( alternatives[['group']].reset_index().rename(columns={'index':'index2'}), on=['group'], how='left' )
        pairs = pairs[pairs['index1'] != pairs['index2']].drop(columns=['group'])
        pairs['scaf_miss'] = False
        for i in [1,2]:
            pairs[f'len{i}'] = alternatives.loc[pairs[f'index{i}'].values, 'length'].values
        pairs['s2'] = 1
        reducible = []
        pairs[['scaf','strand']] = [-1,'']
        for s1 in range(1,alternatives['length'].max()):
            pairs['match'] = False
            while np.sum(pairs['match'] == False):
                for s2 in np.unique(pairs.loc[pairs['match'] == False, 's2'].values):
                    cur = (pairs['match'] == False) & (pairs['s2'] == s2)
                    pairs.loc[cur, ['scaf','strand']] = alternatives.loc[pairs.loc[cur, 'index2'].values, [f'scaf{s2}',f'strand{s2}']].values
                pairs.loc[pairs['match'] == False, 'match'] = (pairs.loc[pairs['match'] == False, ['scaf','strand']].values == alternatives.loc[pairs.loc[pairs['match'] == False, 'index1'].values, [f'scaf{s1}',f'strand{s1}']].values).all(axis=1)
                pairs.loc[pairs['match'] == False, 's2'] += 1
                pairs.loc[pairs['match'] == False, 'scaf_miss'] = True
                pairs = pairs[ pairs['s2'] < pairs['len2'] ].copy()
            reducible.append( pairs.loc[ s1+1 == pairs['len1'], ['index1','index2','scaf_miss'] ].copy() )
            pairs = pairs[ s1+1 < pairs['len1'] ].copy()
            pairs['s2'] += 1
        reducible = pd.concat(reducible, ignore_index=True)
        # Get bridge support for reducible alternatives
        bsupp_indexes = reducible[['index1']].copy()
        bsupp = alternatives.loc[bsupp_indexes['index1'].values, ['from','from_side','scaf1','strand1','dist1']].reset_index().rename(columns={'index':'index1','scaf1':'to','strand1':'to_side','dist1':'mean_dist'})
        bsupp['to_side'] = np.where(bsupp['to_side'] == '+', 'l', 'r')
        bsupp = [bsupp]
        bsupp_indexes['len1'] = alternatives.loc[bsupp_indexes['index1'].values, 'length'].values
        for s1 in range(1,alternatives['length'].max()-1):
            bsupp.append( alternatives.loc[bsupp_indexes.loc[bsupp_indexes['len1'] > s1+1, 'index1'].values, [f'scaf{s1}',f'strand{s1}',f'scaf{s1+1}',f'strand{s1+1}',f'dist{s1+1}']].reset_index().rename(columns={'index':'index1',f'scaf{s1}':'from',f'strand{s1}':'from_side',f'scaf{s1+1}':'to',f'strand{s1+1}':'to_side',f'dist{s1+1}':'mean_dist'}) )
            bsupp[-1]['from_side'] = np.where(bsupp[-1]['from_side'] == '+', 'r', 'l')
            bsupp[-1]['to_side'] = np.where(bsupp[-1]['to_side'] == '+', 'l', 'r')
        bsupp = pd.concat(bsupp, ignore_index=True)
        bsupp = bsupp.merge(scaf_bridges[['from','from_side','to','to_side','mean_dist','bcount']], on=['from','from_side','to','to_side','mean_dist'], how='left')
        bsupp = bsupp.groupby(['index1'])['bcount'].agg(['min','median','max']).reset_index()
        reducible = reducible.merge(bsupp, on=['index1'], how='left')
        # Distance only differences are only allowed if the reversed pair does not exist or the bridge support is lower (otherwise we might wrongly remove both of the alternatives)
        for col in ['min','median','max']:
            reducible = reducible[ reducible[col] <= reducible[['index1','index2']].rename(columns={'index1':'index2','index2':'index1'}).merge(reducible, on=['index1','index2'], how='left')[col].fillna(sys.maxsize).values ].copy()
        # In case of a tie remove the one with the higher index
        reducible = reducible[ (reducible['index1'] > reducible['index2']) | (reducible[['index1','index2']].rename(columns={'index1':'index2','index2':'index1'}).merge(reducible[['index1','index2']], on=['index1','index2'], how='left', indicator=True)['_merge'].values == "left_only") ].copy()
        reducible = reducible.groupby(['index1'])['scaf_miss'].min().reset_index()
        reducible[['group','loop','from','from_side','nalts']] = alternatives.loc[reducible['index1'].values, ['group','loop','from','from_side','nalts']].values
        # Remove the reducible ones with the lowest bridge support until we arrive at ploidy alternatives (distance only alternatives are always removed before missing scaffold alternatives)
        alternatives.drop(columns=['nalts'], inplace=True)
        reducible = reducible.merge(bsupp, on=['index1'], how='left')
        reducible.sort_values(['group','scaf_miss','min','median','max'], inplace=True)
        alternatives.drop(reducible.loc[reducible['nalts'] - reducible.groupby(['group'], sort=False).cumcount() > ploidy, 'index1'].values, inplace=True)
        # Add the already valid ones back in
        alternatives = pd.concat([valid_alts, alternatives], ignore_index=True).sort_values(['group']).reset_index(drop=True)
#
    return alternatives

def GetOtherSideOfExitConnection(conns, exit_conns):
    other_side = conns[['ito']].merge(exit_conns[['ifrom','length','from','from_side']+[f'{n}{s}' for s in range(1,exit_conns['length'].max()) for n in ['scaf','strand','dist']]].drop_duplicates().rename(columns={'ifrom':'ito'}), on=['ito'], how='left')
    other_side.rename(columns={'from':'scaf0','from_side':'strand0'}, inplace=True)
    other_side['strand0'] = np.where(other_side['strand0'] == 'r', '+', '-')
    other_side = ReverseVerticalPaths(other_side)
    other_side.sort_values(['lindex'], inplace=True)
    other_side.drop(columns=['lindex'], inplace=True)
    other_side.rename(columns={f'{col}':f'r{col}' for col in other_side.columns}, inplace=True)
    conns = pd.concat([conns.reset_index(drop=True), other_side.reset_index(drop=True)], axis=1)
#
    return conns

def AppendScaffoldsToExitConnection(conns):
    conns[[f'{n}{s}' for s in range(conns['length'].max(), (conns['sfrom']+conns['rlength']).max()) for n in ['scaf','strand','dist']]] = np.nan
    for sfrom in np.unique(conns.loc[conns['rlength'] > 0, 'sfrom']):
        inconsistent = conns[ (conns['rlength'] > 0) & (conns['sfrom'] == sfrom) & (conns[[f'{n}{sfrom}' for n in ['scaf','strand']]].values != conns[[f'r{n}0' for n in ['scaf','strand']]].values).any(axis=1) ].copy()
        if len(inconsistent):
            print("Warning: Appending vertical paths without proper match.")
            print(inconsistent)
        max_len = conns.loc[conns['sfrom'] == sfrom, 'rlength'].max()
        conns.loc[conns['sfrom'] == sfrom, [f'{n}{s}' for s in range(sfrom+1, sfrom+max_len) for n in ['scaf','strand','dist']]] = conns.loc[conns['sfrom'] == sfrom, [f'r{n}{s}' for s in range(1, max_len) for n in ['scaf','strand','dist']]].values
    conns['length'] = conns['sfrom']+conns['rlength']
    conns.drop(columns=['sfrom','rlength','rscaf0','rstrand0']+[f'r{n}{s}' for s in range(1, conns['rlength'].max()) for n in ['scaf','strand','dist']], inplace=True)
    conns = RemoveEmptyColumns(conns)
#
    return conns

def GetHandledScaffoldConnectionsFromVerticalPath(vpaths):
    handled_scaf_conns = []
    for s in range(1,vpaths['length'].max()):
        handled_scaf_conns.append(vpaths.loc[vpaths['length'] > s, [f'scaf{s-1}',f'strand{s-1}',f'scaf{s}',f'strand{s}',f'dist{s}']].rename(columns={f'scaf{s-1}':'scaf0',f'strand{s-1}':'strand0',f'scaf{s}':'scaf1',f'strand{s}':'strand1',f'dist{s}':'dist1'}))
    handled_scaf_conns = pd.concat(handled_scaf_conns, ignore_index=True)
    handled_scaf_conns[['scaf0','scaf1','dist1']] = handled_scaf_conns[['scaf0','scaf1','dist1']].astype(int)
    handled_scaf_conns.drop_duplicates(inplace=True)
    # Also get reverse direction
    reversed_scaf_conns = handled_scaf_conns.rename(columns={'scaf0':'scaf1','scaf1':'scaf0','strand0':'strand1','strand1':'strand0'})
    for s in [0,1]:
        reversed_scaf_conns[f'strand{s}'] = np.where(reversed_scaf_conns[f'strand{s}'] == '+', '-', '+')
    handled_scaf_conns = pd.concat([handled_scaf_conns, reversed_scaf_conns], ignore_index=True)
    handled_scaf_conns.drop_duplicates(inplace=True)
#
    return handled_scaf_conns

def TurnHorizontalPathIntoVertical(vpaths, min_path_id):
    vpaths['dist0'] = 0
    vpaths['pid'] = np.arange(min_path_id, min_path_id+len(vpaths))
#
    hpaths = []
    for s in range(vpaths['length'].max()):
        hpaths.append( vpaths.loc[vpaths['length'] > s, ['pid',f'scaf{s}',f'strand{s}',f'dist{s}']].rename(columns={f'scaf{s}':'scaf0',f'strand{s}':'strand0',f'dist{s}':'dist0'}) )
        hpaths[-1]['pos'] = s
    hpaths = pd.concat(hpaths, ignore_index=True)
    hpaths[['scaf0','dist0']] = hpaths[['scaf0','dist0']].astype(int)
    hpaths = hpaths[['pid','pos','scaf0','strand0','dist0']].copy()
    hpaths.sort_values(['pid','pos'], inplace=True)
#
    vpaths.drop(columns=['dist0','pid'], inplace=True)
#
    return hpaths

def AddPathThroughLoops(scaffold_paths, scaffold_graph, graph_ext, scaf_bridges, org_scaf_conns, ploidy, max_loop_units):
    loop_scafs = FindScaffoldsConnectedToLoops(scaffold_graph)
#
    if len(loop_scafs):
        # Get loop units by exploring possible paths in scaffold_graph
        loops = GetLoopUnits(loop_scafs, scaffold_graph, max_loop_units)
        if len(loops):
            CheckConsistencyOfVerticalPaths(loops)
    else:
        loops = []
    if len(loops):
        # Remove loop units that are already part of a multi-repeat unit
        multi_repeat = []
        max_len = loops['length'].max()
        for s in range(1, max_len-1):
            found_repeat = (loops['scaf0'] == loops[f'scaf{s}']) & (loops[f'strand{s}'] == '+') & (loops['length'] > s+1) # Do not take the last position into account, otherwise we would end up with the full loop unit
            multi_repeat.append( loops[found_repeat].drop(columns=[f'scaf{s1}' for s1 in range(0,s)]+[f'strand{s1}' for s1 in range(0,s)]+[f'dist{s1}' for s1 in range(1,s+1)]).rename(columns={f'{n}{s1}':f'{n}{s1-s}' for s1 in range(s, max_len) for n in ['scaf','strand','dist']}) )
            multi_repeat[-1]['length'] -= s
            multi_repeat.append( loops[found_repeat].drop(columns=[f'{n}{s1}' for s1 in range(s+1, max_len) for n in ['scaf','strand','dist']]) )
            multi_repeat[-1]['length'] = s+1
        if len(multi_repeat):
            multi_repeat = pd.concat(multi_repeat, ignore_index=True, sort=False)
        if len(multi_repeat):
            multi_repeat.drop_duplicates(inplace=True)
            multi_repeat = RemoveEmptyColumns(multi_repeat)
            loops = loops[ loops.merge(multi_repeat, on=list(multi_repeat.columns.values), how='left', indicator=True)['_merge'].values == "left_only" ].copy()
        # Find connections between loop units
        loop_conns = FindConnectionsBetweenLoopUnits(loops, scaffold_graph, False)
        loop_conns['loop'] = loops.loc[loop_conns['lindex1'].values, 'loop'].values
        loops = loops[np.isin(loops['loop'], np.unique(loop_conns['loop']))].copy()
    else:
        loop_conns = []
    if len(loop_conns):
        # Check if we have evidence for an order for the repeat copies (only one valid connection)
        conn_count = loop_conns.groupby(['lindex1']).size().values
        loop_conns['unique'] = (loop_conns['lindex1'] != loop_conns['lindex2']) & np.repeat(conn_count == 1, conn_count) & loop_conns[['lindex2']].merge( (loop_conns.groupby(['lindex2']).size() == 1).reset_index(name='unique'), on=['lindex2'], how='left')['unique'].values
        while np.sum(loop_conns['unique']):
            loop_conns['append'] = loop_conns['unique'] & (np.isin(loop_conns['lindex2'].values, loop_conns.loc[loop_conns['unique'], 'lindex1'].values) == False) # Make sure we are not appending something that itselfs appends something in this round
            if np.sum(loop_conns['append']) == 0:
                loop_conns.reset_index(drop=True, inplace=True)
                loop_conns.loc[np.max(loop_conns[loop_conns['unique']].index.values), 'unique'] = False
            else:
                for i in [1,2]:
                    loop_conns[f'len{i}'] = loops.loc[loop_conns[f'lindex{i}'].values, 'length'].values
                for l in np.unique(loop_conns.loc[loop_conns['append'], 'len1']):
                    cur = loop_conns[loop_conns['append'] & (loop_conns['len1'] == l)]
                    loops.loc[cur['lindex1'].values, [f'{n}{s}' for s in range(l,l+cur['len2'].max()) for n in ['scaf','strand','dist']]] = loops.loc[cur['lindex2'].values, [f'{n}{s}' for s in range(1,1+cur['len2'].max()) for n in ['scaf','strand','dist']]].values
                cur = loop_conns[loop_conns['append']].copy()
                loops.loc[cur['lindex1'].values, 'length'] += cur['len2'].values - 1
                loop_conns.drop(columns=['len1','len2'], inplace=True) # Lengths change, so do not keep outdated information
                loops.drop(cur['lindex2'].values, inplace=True)
                loop_conns = loop_conns[loop_conns['append'] == False].copy()
                loop_conns['new_index'] = loop_conns[['lindex1']].merge(cur[['lindex1','lindex2']].rename(columns={'lindex1':'new_index','lindex2':'lindex1'}), on=['lindex1'], how='left')['new_index'].values
                loop_conns.loc[np.isnan(loop_conns['new_index']) == False, 'lindex1'] = loop_conns.loc[np.isnan(loop_conns['new_index']) == False, 'new_index'].astype(int)
        CheckConsistencyOfVerticalPaths(loops)
#
    # Get exits for the repeats
    if len(loop_scafs):
        exits = loop_scafs.loc[loop_scafs['exit'],['loop','scaf']].rename(columns={'scaf':'from'})
        if len(exits):
            exits = exits.merge(scaffold_graph, on=['from'], how='left')
            exits = exits[ exits[['loop','scaf1']].rename(columns={'scaf1':'scaf'}).merge(loop_scafs.loc[loop_scafs['exit'],['loop','scaf']], on=['loop','scaf'], how='left', indicator=True)['_merge'].values == "left_only" ].copy() # Only keep the first scaffold of the exit
            exits['loopside'] = False
            for s in range(1,exits['length'].max()):
                exits.loc[exits[['loop',f'scaf{s}']].rename(columns={f'scaf{s}':'scaf'}).merge(loop_scafs.loc[loop_scafs['inside'],['loop','scaf']], on=['loop','scaf'], how='left', indicator=True)['_merge'].values == "both", 'loopside'] = True
            exits = exits[exits['loopside']].drop(columns=['loopside'])
    else:
        exits = []
#
    # Handle bridged repeats
    bridged_repeats = []
    if len(exits):
        all_loops_with_exits = np.unique(exits['loop'].values)
        for s in range(2,exits['length'].max()):
            exits['bridged'] = exits[['loop',f'scaf{s}']].rename(columns={f'scaf{s}':'scaf'}).merge(loop_scafs.loc[loop_scafs['exit'],['loop','scaf']], on=['loop','scaf'], how='left', indicator=True)['_merge'].values == "both"
            bridged_repeats.append( exits.loc[exits['bridged'] & (exits['from'] < exits[f'scaf{s}']), ['loop','from','from_side']+[f'{n}{s}' for s in range(1,s+1) for n in ['scaf','strand','dist']]].drop_duplicates() )
            bridged_repeats[-1]['length'] = s+1
            exits = exits[exits['bridged'] == False].copy()
        exits.drop(columns=['bridged'], inplace=True)
        bridged_repeats = pd.concat(bridged_repeats, ignore_index=True, sort=False)
        if len(bridged_repeats):
            bridged_repeats.rename(columns={'from':'scaf0','from_side':'strand0'}, inplace=True)
            bridged_repeats['strand0'] = np.where(bridged_repeats['strand0'] == 'r', '+', '-')
    else:
        all_loops_with_exits = []
#
    # Only keep exits of unbridged repeats, where we know from where to where they go (either because we only have two exits or because we have scaffolding information that tells us), for the rest we will later add the loop units as path
    if len(exits):
        exit_scafs = exits[['loop','from','from_side']].drop_duplicates()
        exit_conns = exit_scafs.merge(org_scaf_conns[org_scaf_conns['distance'] > 0], on=['from','from_side'], how='inner')
        if len(exit_conns):
            exit_conns = exit_conns.merge( exit_conns.rename(columns={'from':'to','from_side':'to_side','to':'from','to_side':'from_side'}), on=['loop','from','from_side','to','to_side','distance'], how='inner')
        if len(exit_conns):
            exit_scafs = exit_scafs[ exit_scafs[['loop','from','from_side']].merge(exit_conns[['loop','from','from_side']], on=['loop','from','from_side'], how='left', indicator=True)['_merge'].values == "left_only" ].copy()
        if len(exit_scafs):
            exit_scafs.sort_values(['loop','from','from_side'], inplace=True)
            exit_count = exit_scafs.groupby(['loop'], sort=False).size().values
            exit_scafs = exit_scafs[np.repeat(exit_count == 2, exit_count)].copy()
        if len(exit_scafs):
            exit_scafs['to'] = np.where(exit_scafs['loop'] == exit_scafs['loop'].shift(-1), exit_scafs['from'].shift(-1, fill_value=-1), exit_scafs['from'].shift(1, fill_value=-1))
            exit_scafs['to_side'] = np.where(exit_scafs['loop'] == exit_scafs['loop'].shift(-1), exit_scafs['from_side'].shift(-1, fill_value=''), exit_scafs['from_side'].shift(1, fill_value=''))
            exit_scafs['distance'] = 0
            exit_conns = pd.concat([exit_conns, exit_scafs], ignore_index=True)
    else:
        exit_conns = []
    # Unbridged repeats are not allowed to have more alternatives than ploidy
    if len(exit_conns):
        exit_conns = exit_conns.merge(exits, on=['loop','from','from_side'], how='left')
        exit_conns.sort_values(['loop','from','from_side'], inplace=True)
        exit_conns['group'] = (exit_conns.groupby(['loop','from','from_side'], sort=False).cumcount() == 0).cumsum()
        exit_conns = TryReducingAlternativesToPloidy(exit_conns, scaf_bridges, ploidy)
        exit_conns['from_alts'] = exit_conns.merge(exit_conns.groupby(['group']).size().reset_index(name='count'), on=['group'], how='left')['count'].values
        exit_conns.drop(columns=['group'], inplace=True)
        exit_conns = exit_conns[exit_conns['from_alts'] <= ploidy].copy()
        exit_conns = exit_conns.merge( exit_conns[['loop','from','from_side','to','to_side','distance']].drop_duplicates().rename(columns={'from':'to','from_side':'to_side','to':'from','to_side':'from_side','from_alts':'to_alts'}), on=['loop','from','from_side','to','to_side','distance'], how='inner')
        exit_conns = RemoveEmptyColumns(exit_conns)
    # Reverse loop units to have both directions
    if len(loops):
        bidi_loops = GetLoopUnitsInBothDirections(loops)
        CheckConsistencyOfVerticalPaths(bidi_loops)
    # Get the loop units to fill the connections
    start_units = []
    if len(exit_conns) and len(loops):
        # Store all loop units consistent with the sequence from the exit in start_units
        for sfrom in range(1,exit_conns['length'].max()):
            pairs = exit_conns.loc[exit_conns['length'] > sfrom, ['loop',f'scaf{sfrom}',f'strand{sfrom}']].reset_index().rename(columns={'index':'eindex',f'scaf{sfrom}':'scaf0',f'strand{sfrom}':'strand0'}).merge(bidi_loops[['loop','scaf0','strand0']].reset_index().rename(columns={'index':'bindex'}), on=['loop','scaf0','strand0'], how='inner').drop(columns=['loop','scaf0','strand0'])
            pairs['length'] = np.minimum(exit_conns.loc[pairs['eindex'].values, 'length'].values - sfrom, bidi_loops.loc[pairs['bindex'].values, 'length'].values)
            s = 1
            while len(pairs):
                start_units.append(pairs.loc[pairs['length'] == s, ['eindex','bindex']].copy())
                start_units[-1]['sfrom'] = sfrom
                pairs = pairs[pairs['length'] > s].copy()
                if len(pairs):
                    pairs = pairs[(exit_conns.loc[pairs['eindex'].values, [f'scaf{sfrom+s}',f'strand{sfrom+s}',f'dist{sfrom+s}']].values == bidi_loops.loc[pairs['bindex'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values).all(axis=1)].copy()
                    s += 1
        # Update exit_conns with start_units
        if len(start_units):
            # Filter out start_units that are not consistent with the extensions of the loop unit
            extensions, extendible = FindLoopUnitExtensions(bidi_loops, scaffold_graph)
            exit_conns['scaf0'] = exit_conns['from']
            exit_conns['strand0'] = np.where(exit_conns['from_side'] == 'r', '+', '-')
            start_units = pd.concat(start_units, ignore_index=True)
            start_units['lindex'] = bidi_loops.loc[start_units['bindex'].values, 'lindex'].values
            start_units['rstrand'] = np.where( bidi_loops.loc[start_units['bindex'].values, 'strand0'].values == '+', '-', '+' )
            start_units['rindex'] = start_units[['lindex','rstrand']].rename(columns={'rstrand':'strand0'}).merge(bidi_loops[['lindex','strand0']].reset_index().rename(columns={'index':'rindex'}), on=['lindex','strand0'], how='left')[['rindex']].values
            start_units.drop(columns=['lindex','rstrand'], inplace=True)
            start_units = start_units.merge(extendible.rename(columns={'bindex':'rindex'}), on='rindex', how='left')
            start_units.drop(columns=['rindex'], inplace=True)
            start_units['index1'] = start_units['index1'].fillna(-1).astype(int)
            start_units = start_units.merge(extensions[['index1','length']].reset_index().rename(columns={'index':'extindex'}), on='index1', how='left')
            start_units.drop(columns=['index1'], inplace=True)
            start_units['extindex'] = start_units['extindex'].fillna(-1).astype(int)
            start_units['length'] = np.minimum(start_units['sfrom']+1, start_units['length'].fillna(0).astype(int))
            for s in range(1,start_units['length'].max()):
                cur = start_units['length'] > s
                if np.sum(cur):
                    for f in np.unique(start_units.loc[cur, 'sfrom'].values):
                        cur2 = cur & (start_units['sfrom'] == f)
                        start_units.loc[cur2, ['cscaf','cstrand','cdist']] = exit_conns.loc[start_units.loc[cur2, 'eindex'].values, [f'scaf{f-s}',f'strand{f-s}',f'dist{f-s+1}']].values
                    start_units['cstrand'] = np.where(start_units['cstrand'] == '+', '-', '+')
                    start_units['keep'] = cur == False
                    start_units.loc[cur, 'keep'] = (start_units.loc[cur, ['cscaf','cstrand','cdist']].values == extensions.loc[start_units.loc[cur, 'extindex'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values).all(axis=1)
                    start_units = start_units[ start_units['keep'] ].copy()
            start_units.drop(columns=['extindex','length','cscaf','cstrand','cdist','keep'], inplace=True)
            start_units.drop_duplicates(inplace=True)
            exit_conns.drop(columns=['scaf0','strand0'], inplace=True)
        exit_conns['from_units'] = 0
        if len(start_units):
            # Get the earliest valid start points (first_match) in start_units
            start_units.sort_values(['eindex','sfrom','bindex'], inplace=True)
            first_match = start_units.groupby(['eindex'])['sfrom'].agg(['min','size'])
            start_units = start_units[start_units['sfrom'] == np.repeat(first_match['min'].values, first_match['size'].values)].copy()
            # Remove loop units that do not exceed the exit path
            start_units = start_units[exit_conns.loc[start_units['eindex'].values, 'length'].values < start_units['sfrom'] + bidi_loops.loc[start_units['bindex'].values, 'length'].values].copy()
            # Add loop unit information and try to reduce to ploidy alternatives
            start_units[['loop','from','from_side','length']] = bidi_loops.loc[ start_units['bindex'].values, ['loop','scaf0','strand0','length']].values
            start_units['from_side'] = np.where(start_units['from_side'] == '+', 'r', 'l')
            cols = [f'{n}{s}' for s in range(1,start_units['length'].max()) for n in ['scaf','strand','dist']]
            start_units[cols] = bidi_loops.loc[ start_units['bindex'].values, cols].values
            start_units['group'] = start_units['eindex']
            start_units = TryReducingAlternativesToPloidy(start_units, scaf_bridges, ploidy)
            start_units.drop(columns=['group'], inplace=True)
            # Remove exit_conns with more than ploidy units
            nalts = start_units.groupby(['eindex']).size().reset_index(name='nalts')
            exit_conns.loc[nalts['eindex'].values, 'from_units'] = nalts['nalts'].values
            exit_conns = exit_conns[exit_conns['from_units'] <= ploidy].copy()
        exit_conns['ifrom'] = exit_conns.index.values
        exit_conns = exit_conns.merge( exit_conns[['loop','from','from_side','to','to_side','distance','from_alts','from_units','ifrom']].rename(columns={'from':'to','from_side':'to_side','to':'from','to_side':'from_side','from_alts':'to_alts','from_units':'to_units','ifrom':'ito'}), on=['loop','from','from_side','to','to_side','distance'], how='inner')
        exit_conns.drop_duplicates(inplace=True)
#
    # Handle direct connections, where no loop unit fits in by chosing one side and then extending it with the other side
    if len(exit_conns) and len(loops):
        direct_conns = exit_conns.loc[(exit_conns['from_units'] == 0) & ((exit_conns['to_units'] != 0) | (exit_conns['ifrom'] < exit_conns['ito'])), ['loop','ifrom','ito','length','from','from_side']+[f'{n}{s}' for s in range(1,exit_conns['length'].max()) for n in ['scaf','strand','dist']]].copy()
        if len(direct_conns):
            direct_conns = GetOtherSideOfExitConnection(direct_conns, exit_conns)
            direct_conns['sfrom'] = 1
    else:
        direct_conns = []
    if len(direct_conns):
        for sfrom in range(1,direct_conns['length'].max()):
            direct_conns['match'] = True
            cur = direct_conns['sfrom'] == sfrom
            direct_conns.loc[cur, 'match'] = ( (direct_conns.loc[cur, [f'scaf{sfrom}',f'strand{sfrom}']].values == direct_conns.loc[cur, ['rscaf0','rstrand0']].values).all(axis=1) &
                                               (direct_conns.loc[cur, 'length'] < direct_conns.loc[cur, 'sfrom'] + direct_conns.loc[cur, 'rlength']) ) # Otherwise we would have bridged the loop
            if np.sum(direct_conns['match']):
                for s in range(sfrom+1, direct_conns.loc[direct_conns['match'], 'length'].max()):
                    cur = direct_conns['match'] & (direct_conns['sfrom'] == sfrom) & (direct_conns['length'] > s)
                    direct_conns.loc[cur, 'match'] = (direct_conns.loc[cur, [f'scaf{s}',f'strand{s}',f'dist{s}']].values == direct_conns.loc[cur, [f'rscaf{s-sfrom}',f'rstrand{s-sfrom}',f'rdist{s-sfrom}']].values).all(axis=1)
            direct_conns.loc[direct_conns['match'] == False, 'sfrom'] += 1
        direct_conns = direct_conns[direct_conns['match']].drop(columns=['match'])
    if len(direct_conns):
        direct_conns = AppendScaffoldsToExitConnection(direct_conns)
        CheckConsistencyOfVerticalPaths(direct_conns)
    # Remove direct conns, where the extensions do not match
    if len(direct_conns):
        # Check where the extensions overlap
        direct_conns['lena'] = direct_conns[['ifrom','ito']].merge(exit_conns[['ifrom','ito','length']], on=['ifrom','ito'], how='left')['length'].values
        direct_conns['lenb'] = direct_conns[['ifrom','ito']].merge(exit_conns[['ifrom','ito','length']].rename(columns={'ifrom':'ito','ito':'ifrom'}), on=['ifrom','ito'], how='left')['length'].values
        direct_conns['startb'] = direct_conns['length'] - direct_conns['lenb']
        # Prepare ends
        ends = direct_conns[['ifrom','ito','lena','startb']].copy()
        ends['amin'] = ends['startb']
        ends['amax'] = ends['lena'] - 1
        ends['bmin'] = 0
        ends['bmax'] = ends['lena'] - ends['startb'] - 1
        ends.drop(columns={'lena','startb'}, inplace=True)
        ends['aside'] = 'r'
        ends['bside'] = 'l'
        ends[['ahap','bhap']] = 0
        ends.rename(columns={'ifrom':'apid','ito':'bpid'}, inplace=True)
        ends = pd.concat([ends, ends.rename(columns={'apid':'bpid','bpid':'apid','aside':'bside','bside':'aside','amin':'bmin','bmin':'amin','amax':'bmax','bmax':'amax'})], ignore_index=True)
        # Prepare paths
        test_paths = direct_conns[['ifrom','from','from_side']].rename(columns={'ifrom':'pid','from':'scaf0','from_side':'strand0'})
        test_paths['strand0'] = np.where(test_paths['strand0'] == 'r', '+', '-')
        test_paths['pos'] = 0
        test_paths['dist0'] = 0
        test_paths = [test_paths]
        for s in range(1, direct_conns['lena'].max()):
            test_paths.append( direct_conns.loc[direct_conns['lena'] > s, ['ifrom',f'scaf{s}',f'strand{s}',f'dist{s}']].rename(columns={'ifrom':'pid',f'scaf{s}':'scaf0',f'strand{s}':'strand0',f'dist{s}':'dist0'}) )
            test_paths[-1]['pos'] = s
        for s in range(direct_conns['startb'].min(), direct_conns['length'].max()):
            test_paths.append( direct_conns.loc[(direct_conns['startb'] <= s) & (direct_conns['length'] > s), ['ito',f'scaf{s}',f'strand{s}',f'dist{s}']].rename(columns={'ito':'pid',f'scaf{s}':'scaf0',f'strand{s}':'strand0',f'dist{s}':'dist0'}) )
            test_paths[-1]['pos'] = s
        test_paths = pd.concat(test_paths, ignore_index=True)
        test_paths.sort_values(['pid','pos'], inplace=True)
        test_paths.drop_duplicates(inplace=True) # We might add the same paths from multiple combinations of ifrom with ito or vice versa
        test_paths['pos'] = test_paths.groupby(['pid'], sort=False).cumcount()
        test_paths['dist0'] = np.where(test_paths['pos'] == 0, 0, test_paths['dist0'].astype(int))
        test_paths['scaf0'] = test_paths['scaf0'].astype(int)
        test_paths['phase0'] = test_paths['pid'] + 1
        # Check validity
        ends = FilterInvalidConnections(ends, test_paths, graph_ext, 1)
        if len(ends):
            ends[ends['valid_path'] == 'ab'].drop(columns='valid_path', inplace=True)
        if len(ends):
            direct_conns.drop(columns={'lena','lenb','startb'}, inplace=True)
            direct_conns = direct_conns.merge(ends[['apid','bpid']].rename(columns={'apid':'ifrom','bpid':'ito'}), on=['ifrom','ito'], how='inner')
        else:
            direct_conns = []
    # If we do not have loop units and cannot build a direct connection, we cannot use the exit_conns
    if len(loops) == 0:
        exit_conns = []
    # Handle indirect connections by first finding a valid path through the loop units to connect from and to
    if len(exit_conns):
        indirect_conns = exit_conns.loc[(exit_conns['from_units'] > 0) & (exit_conns['to_units'] > 0) & (exit_conns['ifrom'] < exit_conns['ito']), ['loop','ifrom','ito','length','from','from_side']+[f'{n}{s}' for s in range(1,exit_conns['length'].max()) for n in ['scaf','strand','dist']]].copy()
    else:
        indirect_conns = []
    if len(loops) and len(indirect_conns):
        loop_conns, extensions = FindConnectionsBetweenLoopUnits(loops[np.isin(loops['loop'], np.unique(indirect_conns['loop'].values))], scaffold_graph, True)
    else:
        loop_conns = []
    if len(loop_conns):
        for i in [1,2]:
            loop_conns = loop_conns.merge(bidi_loops[['lindex','strand0']].reset_index().rename(columns={'index':f'bindex{i}','lindex':f'lindex{i}','strand0':f'dir{i}'}), on=[f'lindex{i}',f'dir{i}'], how='left')
    if len(indirect_conns) and len(start_units):
        indirect_paths = indirect_conns[['ifrom','ito']].reset_index().rename(columns={'index':'cindex'})
        indirect_paths = indirect_paths.merge(start_units[['eindex','bindex']].rename(columns={'eindex':'ifrom','bindex':'bifrom'}), on=['ifrom'], how='left')
        indirect_paths = indirect_paths.merge(start_units[['eindex','bindex']].rename(columns={'eindex':'ito','bindex':'bito'}), on=['ito'], how='left')
        indirect_paths[['lindex','strand0']] = bidi_loops.loc[indirect_paths['bito'].values, ['lindex','strand0']].values # We need to get the bidi_loop that goes in the other direction, because we start from 'from' not 'to'
        indirect_paths.drop(columns=['ifrom','ito','bito'], inplace=True)
        indirect_paths['strand0'] = np.where(indirect_paths['strand0'] == '+', '-', '+')
        indirect_paths = indirect_paths.merge(bidi_loops[['lindex','strand0']].reset_index().rename(columns={'index':'bito'}), on=['lindex','strand0'], how='left').drop(columns=['lindex','strand0'])
        indirect_paths['len'] = 0
        finished_paths = [ indirect_paths[indirect_paths['bifrom'] == indirect_paths['bito']].copy() ]
        indirect_paths = indirect_paths[indirect_paths['bifrom'] != indirect_paths['bito']].copy()
        indirect_paths['bicur'] = indirect_paths['bifrom']
        indirect_paths['nwildcards'] = 0
        s = 0
        while True:
            # Extend path on valid connections with loop units
            new_path = indirect_paths[indirect_paths['len'] == s].rename(columns={'bicur':'bindex1'}).merge(loop_conns.loc[loop_conns['wildcard'] == False, ['bindex1','bindex2']], on=['bindex1'], how='inner')
            if len(new_path) == 0:
                # Add wildcard connections (loop units that do not have an extension and can connect to all other loop units) only if we have no other choice
                new_path = indirect_paths[indirect_paths['len'] == s].rename(columns={'bicur':'bindex1'}).merge(loop_conns.loc[loop_conns['wildcard'], ['bindex1','bindex2']], on=['bindex1'], how='inner')
                new_path['nwildcards'] += 1
                if len(new_path) == 0:
                    break
            new_path.rename(columns={'bindex2':'bicur','bindex1':f'bi{s}'}, inplace=True)
            new_path['len'] += 1
            # Only keep the shortest path of the ones with the lowest number of wildcard connections to each possible loop unit
            indirect_paths[f'bi{s}'] = -1 # We cannot use NaN here because the following first command ignores NaNs and would mix rows
            indirect_paths = pd.concat([indirect_paths, new_path], ignore_index=True)
            indirect_paths.sort_values(['cindex','bifrom','bito','bicur','nwildcards','len'], inplace=True)
            indirect_paths = indirect_paths.groupby(['cindex','bifrom','bito','bicur'], sort=False).first().reset_index()
            # If we reached 'bito' store the path and remove all other path for this search
            finished_paths.append( indirect_paths[indirect_paths['bicur'] == indirect_paths['bito']].drop(columns=['bicur','nwildcards']) )
            indirect_paths = indirect_paths[ indirect_paths.merge(finished_paths[-1][['cindex','bifrom','bito']], on=['cindex','bifrom','bito'], how='left', indicator=True)['_merge'].values == "left_only" ].copy()
            s += 1
        if len(finished_paths):
            finished_paths = pd.concat(finished_paths, ignore_index=True)
            finished_paths['bi'+str(finished_paths['len'].max())] = np.nan # Make room for bito
            for l in np.unique(finished_paths['len']):
                finished_paths.loc[finished_paths['len'] == l, f'bi{l}'] = finished_paths.loc[finished_paths['len'] == l, 'bito']
            finished_paths.drop(columns=['bifrom'], inplace=True) # bi0 is identical to bifrom
            finished_paths['len'] += 1
    else:
        finished_paths = []
    # Now extend along the valid path
    if len(finished_paths) == 0:
        indirect_conns = []
    else:
        indirect_conns['cindex'] = indirect_conns.index.values
        indirect_conns = indirect_conns.merge(finished_paths, on=['cindex'], how='inner')
        indirect_conns.drop(columns=['cindex'], inplace=True)
        for b in range(indirect_conns['len'].max()):
            if b == 0:
                indirect_conns['sfrom'] = indirect_conns[['ifrom','bi0']].rename(columns={'ifrom':'eindex','bi0':'bindex'}).merge(start_units[['eindex','bindex','sfrom']], on=['eindex','bindex'], how='left')['sfrom'].values
            else:
                indirect_conns['sfrom'] = indirect_conns['length']
                indirect_conns.loc[indirect_conns['len'] > b, 'sfrom'] = indirect_conns.loc[indirect_conns['len'] > b, [f'bi{b-1}',f'bi{b}']].rename(columns={f'bi{b-1}':'bindex1',f'bi{b}':'bindex2'}).merge(loop_conns[['bindex1','bindex2','sfrom']], on=['bindex1','bindex2'], how='left')['sfrom'].values + indirect_conns.loc[indirect_conns['len'] > b, 'old_sfrom']
            indirect_conns['old_sfrom'] = indirect_conns['sfrom']
            ext = RemoveEmptyColumns(bidi_loops.loc[indirect_conns.loc[indirect_conns['len'] > b, f'bi{b}']].drop(columns=['lindex','loop']).rename(columns={col:f'r{col}' for col in bidi_loops.columns}))
            ext.index = indirect_conns[indirect_conns['len'] > b].index.values
            indirect_conns = pd.concat([indirect_conns, ext], axis=1)
            indirect_conns['rlength'] = indirect_conns['rlength'].fillna(0).astype(int)
            indirect_conns = AppendScaffoldsToExitConnection(indirect_conns)
        indirect_conns.drop(columns=['old_sfrom'], inplace=True)
        # Check if we have evidence for another loop unit at the end (no extension of the last loop unit is already present in the indirect connection and the loop unit has a valid connection to itself)
        if len(extensions):
            extensions['lindex'] = extensions[['lindex','strand0']].merge(bidi_loops[['lindex','strand0']].reset_index(), on=['lindex','strand0'], how='left')['index'].values
            extensions.rename(columns={'lindex':'bindex'}, inplace=True)
            indirect_conns['extra_unit'] = np.isin(indirect_conns['bito'], np.unique(extensions['bindex'].values)) & np.isin(indirect_conns['bito'], loop_conns.loc[loop_conns['bindex1'] == loop_conns['bindex2'], 'bindex1'].values)
            extensions.rename(columns={f'{n}{s}':f'e{n}{s+1}' for s in range(extensions['length'].max()) for n in ['scaf','strand','dist']}, inplace=True) # We add the second to last position in the loop unit to the extension, such that it is likely that it came from a loop unit if something matches the whole extension
            extensions['length'] += 1
            extensions['blen'] = bidi_loops.loc[extensions['bindex'].values, 'length'].values
            extensions[['escaf0','estrand0','edist1']] = [-1,'',0]
            for l in np.unique(extensions['blen']):
                extensions.loc[extensions['blen'] == l, ['escaf0','estrand0','edist1']] = bidi_loops.loc[extensions.loc[extensions['blen'] == l, 'bindex'].values, [f'scaf{l-2}',f'strand{l-2}',f'dist{l-1}']].values
            extensions.drop(columns=['blen'], inplace=True)
            extensions.rename(columns={'bindex':'bito','length':'elength'}, inplace=True)
            extensions = indirect_conns[indirect_conns['extra_unit']].reset_index().rename(columns={'index':'iindex'}).merge(extensions, on=['bito'], how='left')
            sfrom = 1
            extensions = extensions[extensions['length'] >= sfrom + extensions['elength']].copy()
            while len(extensions):
                extensions['valid'] = False
                for l in np.unique(extensions['elength']):
                    extensions.loc[extensions['elength'] == l, 'valid'] = (extensions.loc[extensions['elength'] == l, [f'scaf{sfrom}',f'strand{sfrom}']+[f'{n}{s}' for s in range(sfrom+1, sfrom+l) for n in ['scaf','strand','dist']]].values == extensions.loc[extensions['elength'] == l, ['escaf0','estrand0']+[f'e{n}{s}' for s in range(1, l) for n in ['scaf','strand','dist']]].values).all(axis=1)
                found_index = np.unique(extensions.loc[extensions['valid'], 'iindex'].values)
                indirect_conns.loc[found_index, 'extra_unit'] = False
                sfrom += 1
                extensions = extensions[(np.isin(extensions['iindex'].values, found_index) == False) & (extensions['length'] >= sfrom + extensions['elength'])].copy()
            # Add the extra loop unit, where we found evidence
            if np.sum(indirect_conns['extra_unit']):
                indirect_conns['sfrom'] = indirect_conns['length'] - np.where(indirect_conns['extra_unit'], 1, 0)
                ext = RemoveEmptyColumns(bidi_loops.loc[indirect_conns.loc[indirect_conns['extra_unit'], 'bito']].drop(columns=['lindex','loop']).rename(columns={col:f'r{col}' for col in bidi_loops}))
                ext.index = indirect_conns[indirect_conns['extra_unit']].index.values
                indirect_conns = pd.concat([indirect_conns, ext], axis=1)
                indirect_conns['rlength'] = indirect_conns['rlength'].fillna(0).astype(int)
                indirect_conns = AppendScaffoldsToExitConnection(indirect_conns)
            indirect_conns.drop(columns=['extra_unit'], inplace=True)
        # Add the exit on the other side
        indirect_conns[['lindex','lstrand0']] = bidi_loops.loc[indirect_conns['bito'].values, ['lindex','strand0']].values # The bindex is the reverse of what is stored in start_units, so reverse it
        indirect_conns['lstrand0'] = np.where(indirect_conns['lstrand0'] == '+', '-', '+')
        indirect_conns['bito'] = indirect_conns[['lindex','lstrand0']].merge(bidi_loops[['lindex','strand0']].reset_index().rename(columns={'index':'bito','strand0':'lstrand0'}), on=['lindex','lstrand0'], how='left')['bito'].values
        indirect_conns.drop(columns=['lindex','lstrand0'], inplace=True)
        indirect_conns['sfrom'] = indirect_conns[['ito','bito']].rename(columns={'ito':'eindex','bito':'bindex'}).merge(start_units[['eindex','bindex','sfrom']], on=['eindex','bindex'], how='left')['sfrom'].values
        indirect_conns = GetOtherSideOfExitConnection(indirect_conns, exit_conns)
        indirect_conns['sfrom'] = indirect_conns['length'] - (indirect_conns['rlength'] - indirect_conns['sfrom'])
        indirect_conns = AppendScaffoldsToExitConnection(indirect_conns)
        indirect_conns.drop(columns=['bito'], inplace=True)
        CheckConsistencyOfVerticalPaths(indirect_conns)
#
    # Reduce number of alternatives in direct and indirect connections together to ploidy
    if len(direct_conns):
        direct_conns['lunits'] = 0
    if len(indirect_conns):
        used_units = []
        for u in range(indirect_conns['len'].max()):
            used_units.append( indirect_conns.loc[indirect_conns['len'] > u, [f'bi{u}']].reset_index().rename(columns={'index':'cindex',f'bi{u}':'bindex'}) )
        used_units = pd.concat(used_units, ignore_index=True)
        used_units['lindex'] = bidi_loops.loc[used_units['bindex'].values, 'lindex'].values
        used_units.drop(columns=['bindex'], inplace=True)
        used_units.sort_values(['cindex','lindex'], inplace=True)
        used_units.drop_duplicates(inplace=True)
        used_units['pos'] = used_units.groupby(['cindex'], sort=False).cumcount()
        indirect_conns.drop(columns=['len']+[f'bi{u}' for u in range(indirect_conns['len'].max())], inplace=True)
        indirect_conns['lunits'] = 0
        lunits = used_units.groupby(['cindex']).size().reset_index(name='lunits')
        indirect_conns.loc[lunits['cindex'].values, 'lunits'] = lunits['lunits'].values
        for u in range(lunits['lunits'].max()):
            indirect_conns[f'li{u}'] = np.nan
            indirect_conns.loc[used_units.loc[used_units['pos'] == u, 'cindex'].values, f'li{u}'] = used_units.loc[used_units['pos'] == u, 'lindex'].values
    if len(direct_conns):
        if len(indirect_conns):
            combined_conns = pd.concat([direct_conns, indirect_conns], ignore_index=True)
        else:
            combined_conns = direct_conns
    else:
        if len(indirect_conns):
            combined_conns = indirect_conns
        else:
            combined_conns = []
    if len(combined_conns):
        combined_conns[['to','to_side']] = [-1,'']
        for l in np.unique(combined_conns['length']):
            combined_conns.loc[combined_conns['length'] == l, ['to','to_side']] = combined_conns.loc[combined_conns['length'] == l, [f'scaf{l-1}',f'strand{l-1}']].values
        combined_conns['to'] = combined_conns['to'].astype(int)
        combined_conns['to_side'] = np.where(combined_conns['to_side'] == '+', 'l', 'r')
        combined_conns.sort_values(['from','from_side','to','to_side'], inplace=True)
        nalts = combined_conns.groupby(['from','from_side','to','to_side'], sort=False).size().values
        nalts = np.repeat(nalts <= ploidy, nalts)
        drop_cols = ['ifrom','ito','lunits']+[f'li{u}' for u in range(lunits['lunits'].max())]
        unbridged_repeats = combined_conns[nalts].drop(columns=drop_cols)
        combined_conns = combined_conns[nalts==False].copy()
    else:
        unbridged_repeats = []
    if len(combined_conns):
        # Get all possible combinations of conns to keep
        combinations = combined_conns[['from','from_side','to','to_side']].copy()
        combinations['c0'] = combinations.index.values
        for h in range(1,ploidy):
            combinations = combinations.merge(combined_conns[['from','from_side','to','to_side']].reset_index().rename(columns={'index':f'c{h}'}), on=['from','from_side','to','to_side'], how='left')
            combinations = combinations[combinations[f'c{h-1}'] < combinations[f'c{h}']].copy()
        combinations['group'] = (combinations.groupby(['from','from_side','to','to_side'], sort=False).cumcount() == 0).cumsum()
        combinations.drop(columns=['from','from_side','to','to_side'], inplace=True)
        # Pick the combinations that include the most exit paths
        comb_inc = []
        for h in range(ploidy):
            cur = combinations[[f'c{h}']].reset_index().rename(columns={f'c{h}':'conn'})
            cur[['e1','e2']] = combined_conns.loc[cur['conn'].values, ['ifrom','ito']].values
            for i in [1,2]:
                comb_inc.append(cur[['index',f'e{i}']].rename(columns={f'e{i}':'exit'}))
        comb_inc = pd.concat(comb_inc, ignore_index=True).drop_duplicates().groupby(['index']).size().reset_index(name='count')
        combinations['ninc'] = 0
        combinations.loc[comb_inc['index'].values, 'ninc'] = comb_inc['count'].values
        ninc = combinations.groupby(['group'], sort=False)['ninc'].agg(['size','max'])
        combinations = combinations[combinations['ninc'] == np.repeat(ninc['max'].values, ninc['size'].values)].copy()
        # Pick the combinations that include the most kinds of loop units
        comb_inc = []
        for h in range(ploidy):
            cur = combinations[[f'c{h}']].reset_index().rename(columns={f'c{h}':'conn'})
            cols = ['lunits']+[f'li{u}' for u in range(combined_conns['lunits'].max())]
            cur[cols] = combined_conns.loc[cur['conn'].values, cols].values
            usize = cur['lunits'].max()
            if np.isnan(usize) == False:
                for u in range(int(usize)):
                    comb_inc.append(cur.loc[cur['lunits'] > u, ['index',f'li{u}']].rename(columns={f'li{u}':'lunit'}))
        if len(comb_inc):
            comb_inc = pd.concat(comb_inc, ignore_index=True).drop_duplicates().groupby(['index']).size().reset_index(name='count')
        combinations['ninc'] = 0
        if len(comb_inc):
            combinations.loc[comb_inc['index'].values, 'ninc'] = comb_inc['count'].values
        ninc = combinations.groupby(['group'], sort=False)['ninc'].agg(['size','max'])
        combinations = combinations[combinations['ninc'] == np.repeat(ninc['max'].values, ninc['size'].values)].drop(columns=['ninc'])
        # Pick the ones with the shortest maximum length and if we have multiple where all length are identical, n arbitrary (the first) one (sorting length with simple bubble sort)
        for h in range(ploidy):
            combinations[f'len{h}'] = combined_conns.loc[combinations[f'c{h}'].values, 'length'].values
        n = ploidy-1
        while(n > 0):
            swapped = 0
            for i in range(n):
                swap = combinations[f'len{i}'] < combinations[f'len{i+1}']
                if np.sum(swap):
                    swapped = i
                    combinations.loc[swap, [f'len{i}',f'len{i+1}']] = combinations.loc[swap, [f'len{i+1}',f'len{i}']].values
            n = swapped
        combinations.sort_values(['group']+[f'len{h}' for h in range(ploidy)], inplace=True)
        combinations = combinations.groupby(['group']).first().reset_index()
        # Pick the chosen indexes and add those connections to unbridged repeats
        chosen_indexes = []
        for h in range(ploidy):
            chosen_indexes.append(combinations[f'c{h}'].values)
        combined_conns = combined_conns.loc[np.concatenate(chosen_indexes)].drop(columns=drop_cols)
        if len(combined_conns):
            if len(unbridged_repeats):
                unbridged_repeats = pd.concat( [unbridged_repeats, combined_conns], ignore_index=True )
            else:
                unbridged_repeats = combined_conns
    if len(unbridged_repeats):
        unbridged_repeats = RemoveEmptyColumns(unbridged_repeats)
#
    # # Only keep loop units, where we have unconnected exits or never had exits at all
    # if len(all_loops_with_exits) and len(loops):
        # if len(exits):
            # unconnected_exits = exits[['loop','from','from_side']].drop_duplicates()
            # if len(unbridged_repeats):
                # unconnected_exits = unconnected_exits[unconnected_exits.merge(unbridged_repeats[['loop','from','from_side']].drop_duplicates(), on=['loop','from','from_side'], how='left', indicator=True)['_merge'].values == "left_only"].copy()
                # unconnected_exits = unconnected_exits[unconnected_exits.merge(unbridged_repeats[['loop','to','to_side']].drop_duplicates().rename(columns={'to':'from','to_side':'from_side'}), on=['loop','from','from_side'], how='left', indicator=True)['_merge'].values == "left_only"].copy()
            # loops = loops[np.isin(loops['loop'], np.setdiff1d(all_loops_with_exits, np.unique(unconnected_exits['loop'].values))) == False].copy()
        # else:
            # loops = loops[np.isin(loops['loop'], all_loops_with_exits) == False].copy()
    if len(unbridged_repeats):
        unbridged_repeats.drop(columns=['to','to_side'], inplace=True)
        unbridged_repeats.rename(columns={'from':'scaf0','from_side':'strand0'}, inplace=True)
        unbridged_repeats['strand0'] = np.where(unbridged_repeats['strand0'] == 'r', '+', '-')
    # if len(loops):
        # # Only keep the start position with the most loop units per loop
        # loops.sort_values(['loop','scaf0'], inplace=True)
        # loop_count = loops.groupby(['loop','scaf0'], sort=False).size().reset_index(name='count')
        # loop_count.sort_values(['loop','count'], ascending=[True,False], inplace=True)
        # loop_count = loop_count.groupby(['loop'], sort=False).first().reset_index()
        # loops = loops.merge(loop_count[['loop','scaf0']], on=['loop','scaf0'], how='inner')
    # # Remove loop units already in bridged or unbridged repeats
    if len(unbridged_repeats):
        if len(bridged_repeats):
            loop_paths = pd.concat([unbridged_repeats, bridged_repeats], ignore_index=True)
        else:
            loop_paths = unbridged_repeats
    else:
        if len(bridged_repeats):
            loop_paths = bridged_repeats
        else:
            loop_paths = []
    # if len(loop_paths) and len(loops):
        # max_len = loop_paths['length'].max()
        # search_space = np.unique(loops['scaf0'].values)
        # inner_repeats = []
        # for s1 in range(1, max_len-1):
            # for s2 in range(s1+1, max_len-1):
                # found_repeats = loop_paths.loc[np.isin(loop_paths[f'scaf{s1}'], search_space) & (loop_paths[f'scaf{s1}'] == loop_paths[f'scaf{s2}']) & (loop_paths[f'strand{s1}'] == loop_paths[f'strand{s2}']), ['length']+[f'scaf{s1}',f'strand{s1}']+[f'{n}{s}' for s in range(s1+1, s2+1) for n in ['scaf','strand','dist']]].rename(columns={f'{n}{s}':f'{n}{s-s1}' for s in range(s1, s2+1) for n in ['scaf','strand','dist']})
                # if len(found_repeats):
                    # found_repeats['length'] = s2-s1+1
                    # inner_repeats.append(found_repeats[found_repeats['strand0'] == '+'].copy())
                    # found_repeats = found_repeats[found_repeats['strand0'] == '-'].copy()
                    # if len(found_repeats):
                        # found_repeats = ReverseVerticalPaths(found_repeats)
                        # inner_repeats.append(found_repeats.drop(columns=['lindex']))
        # if len(inner_repeats):
            # inner_repeats = pd.concat(inner_repeats, ignore_index=True, sort=False)
        # if len(inner_repeats):
            # inner_repeats.drop_duplicates(inplace=True)
            # loops = loops[ loops.merge(inner_repeats, on=list(inner_repeats.columns.values), how='left', indicator=True)['_merge'].values == "left_only" ].copy()
#
    # # Add remaining loop units to loop_paths
    # if len(loops):
        # if len(loop_paths):
            # loop_paths = pd.concat([loop_paths, loops], ignore_index=True).drop(columns=['loop'])
        # else:
            # loop_paths = loops.drop(columns=['loop'])
    # Get all scaffold connections in the path and make it a vertical path
    if len(loop_paths):
        handled_scaf_conns = GetHandledScaffoldConnectionsFromVerticalPath(loop_paths)
        if len(scaffold_paths):
            loop_paths = TurnHorizontalPathIntoVertical(loop_paths, scaffold_paths['pid'].max()+1)
            scaffold_paths = pd.concat([scaffold_paths, loop_paths], ignore_index=True)
        else:
            scaffold_paths = TurnHorizontalPathIntoVertical(loop_paths, 0)
    else:
        handled_scaf_conns = []
# 
    return scaffold_paths, handled_scaf_conns

def AddAlternativesToInversionsToPaths(inv_paths, inversions, scaffold_graph):
    # Add all paths to inv_path between the two outer scaffolds that go through the inverted scaffold
    pot_paths = scaffold_graph.merge(inversions, on=['from','from_side','scaf1'], how='inner')
    for l in range(3,scaffold_graph['length'].max()):
        pot_paths = pot_paths[pot_paths['length'] >= l].copy()
        cur = pot_paths.loc[(pot_paths[f'scaf{l-2}'] == pot_paths['scaf1']) & (pot_paths[f'scaf{l-1}'] == pot_paths['escaf']) & (pot_paths[f'strand{l-1}'] == pot_paths['estrand']), ['length','from','from_side']+[f'{n}{s}' for s in range(1,l) for n in ['scaf','strand','dist']]].rename(columns={'from':'scaf0','from_side':'strand0'})
        if len(cur):
            cur['strand0'] = np.where(cur['strand0'] == 'r', '+', '-')
            cur['length'] = l
            cur.drop_duplicates(inplace=True)
            inv_paths.append(cur)
#
    return inv_paths

def RemoveInvertedRepeatsThatMightBeTwoHaplotypesOnOneSide(cur, scaffold_graph, max_length):
    # Find the connected scaffold through the repeat also on the not repeat side
    cur['other_side'] = np.where(cur['to_side'] == 'r','l','r')
    remove = cur[['to','other_side','from']].reset_index().rename(columns={'from':'veto','to':'from','other_side':'from_side'}).merge(scaffold_graph, on=['from','from_side'], how='inner')
    if len(remove):
        remove['veto_pos'] = -1
        for s in range(remove['length'].max()-1,0,-1):
            remove.loc[remove[f'scaf{s}'] == remove['veto'], 'veto_pos'] = s
        remove = remove.loc[remove['veto_pos'] > 0].copy()
    # If the reversed path goes to somewhere else and not the inverted repeat it was a false alarm
    if len(remove):
        # Get the found path up to the veto
        pot_vetos = []
        for l1 in np.unique(remove['veto_pos'].values):
            pot_vetos.append( remove.loc[remove['veto_pos'] == l1, ['index','veto_pos','from','from_side']+[f'{n}{s}' for s in range(1,l1+1) for n in ['scaf','strand','dist']]].drop_duplicates() )
        pot_vetos = pd.concat(pot_vetos, ignore_index=True)
        pot_vetos.rename(columns={'index':'dindex','from':'scaf0','from_side':'strand0','veto_pos':'length'}, inplace=True)
        pot_vetos['strand0'] = np.where(pot_vetos['strand0'] == 'r', '+', '-')
        # Append the inverted repeat to the paths
        pot_vetos.rename(columns={f'{n}{s}':f'{n}{s+max_length-1}' for s in range(pot_vetos['length'].max()+1) for n in ['scaf','strand','dist']}, inplace=True)
        pot_vetos[[f'{n}{s}' for s in range(max_length-1) for n in ['scaf','strand','dist']]+[f'dist{max_length-1}']] = cur.loc[pot_vetos['dindex'].values, [f'{n}{s}' for s in range(1,max_length) for n in ['scaf','strand','dist']]+[f'dist{max_length}']].values
        pot_vetos.drop(columns=['dist0'], inplace=True)
        pot_vetos['length'] += max_length
        # Reverse it
        remove = ReverseVerticalPaths(pot_vetos)
        remove['lindex'] = pot_vetos.loc[remove['lindex'].values, 'dindex'].values
        remove.rename(columns={'lindex':'dindex'}, inplace=True)
        # Check if we find a consistent paths (which means we have evidence for two haplotypes on one side instead of both sides of an inverted repeat)
        remove.rename(columns={'scaf0':'from','strand0':'from_side'}, inplace=True)
        remove['from_side'] = np.where(remove['from_side'] == '+', 'r', 'l')
        pairs = remove[['from','from_side','scaf1','strand1','dist1']].reset_index().rename(columns={'index':'rindex'}).merge(scaffold_graph[['from','from_side','scaf1','strand1','dist1']].reset_index().rename(columns={'index':'sindex'}), on=['from','from_side','scaf1','strand1','dist1'], how='inner').drop(columns=['from','from_side','scaf1','strand1','dist1'])
        pairs['length'] = np.minimum(remove.loc[pairs['rindex'].values, 'length'].values, scaffold_graph.loc[pairs['sindex'].values, 'length'].values)
        s = 2
        valid_removal = [ np.unique(remove.loc[pairs.loc[pairs['length'] <= s, 'rindex'].values, 'dindex'].values) ]
        pairs = pairs[np.isin(remove.loc[pairs['rindex'].values, 'dindex'].values, valid_removal[-1]) == False].copy()
        while len(pairs):
            pairs = pairs[(remove.loc[pairs['rindex'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values == scaffold_graph.loc[pairs['sindex'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values).all(axis=1)].copy()
            s += 1
            valid_removal.append( np.unique(remove.loc[pairs.loc[pairs['length'] <= s, 'rindex'].values, 'dindex'].values) )
            pairs = pairs[np.isin(remove.loc[pairs['rindex'].values, 'dindex'].values, valid_removal[-1]) == False].copy()
        valid_removal = np.concatenate(valid_removal)
        # Drop the valid_removal (and their partners)
        cur.drop(columns=['other_side'], inplace=True)
        cur.drop(valid_removal, inplace=True)
        mcols = ['from','from_side','to','to_side']+[f'{n}{s}' for s in range(1,max_length) for n in ['scaf','strand','dist']]+[f'dist{max_length}']
        cur = cur.merge(cur[mcols].rename(columns={**{'from':'to','to':'from','from_side':'to_side','to_side':'from_side'}, **{f'dist{s}':f'dist{max_length+1-s}' for s in range(1,max_length+1)}}), on=mcols, how='inner')
#
    return cur

def AddPathThroughInvertedRepeats(scaffold_paths, handled_scaf_conns, scaffold_graph, scaf_bridges, ploidy):
    inv_paths = []
    # Check for bridged inverted repeats without a scaffold in between
    if scaffold_graph['length'].max() > 3:
        bridged_inversions = []
        l = 4
        while True:
            if scaffold_graph['length'].max() < l:
                cur = []
            else:
                cur = scaffold_graph.loc[(scaffold_graph['length'] >= l) & (scaffold_graph['scaf1'] == scaffold_graph[f'scaf{l-2}']) & (scaffold_graph['strand1'] != scaffold_graph[f'strand{l-2}']), ['from','from_side']+[f'{n}{s}' for s in range(1,l) for n in ['scaf','strand','dist']]].copy()
                for s in range(2,l//2):
                    cur = cur[(cur[f'scaf{s}'] == cur[f'scaf{l-s-1}']) & (cur[f'strand{s}'] != cur[f'strand{l-s-1}'])].copy()
            if len(cur):
                cur.drop_duplicates(inplace=True)
                # Filter the ones that will show up in the next iteration again (and if they are too short to do so, they are not truely bridged)
                cur = cur[(cur['from'] != cur[f'scaf{l-1}']) | ((cur['from_side'] == 'r') == (cur[f'strand{l-1}'] == '+'))].copy()
                # Only keep one direction for each inverted repeat
                if len(cur):
                    cur = cur[(cur['from'] < cur[f'scaf{l-1}']) | ((cur['from'] == cur[f'scaf{l-1}']) & (cur['from_side'] == 'r'))].copy()
                    # Only keep the relevant columns to later find all connections that match this
                    cur = cur[['from','from_side','scaf1']+[f'scaf{l-1}',f'strand{l-1}']].drop_duplicates().rename(columns={f'scaf{l-1}':'escaf',f'strand{l-1}':'estrand'})
                    bridged_inversions.append(cur)
                l += 2
            else:
                break
        if len(bridged_inversions):
            bridged_inversions = pd.concat(bridged_inversions, ignore_index=True)
            # Add all paths to inv_path between the two outer scaffolds that go through the inverted scaffold
            inv_paths = AddAlternativesToInversionsToPaths(inv_paths, bridged_inversions, scaffold_graph)
#
    # Check for scaffolds that are uniquely connected by an inverted (but not bridged) repeat
    if scaffold_graph['length'].max() > 2:
        unique_inversions = []
        l = 3
        while True:
            if scaffold_graph['length'].max() < l:
                cur = []
            else:
                max_length = 2*l - 3
                cur = scaffold_graph.loc[(scaffold_graph['length'] >= l) & (scaffold_graph['length'] <= max_length) & (scaffold_graph[f'scaf{l-2}'] == scaffold_graph[f'scaf{l-1}']) & (scaffold_graph[f'strand{l-2}'] != scaffold_graph[f'strand{l-1}']), ['length','from','from_side']+[f'{n}{s}' for s in range(1,np.minimum(max_length, scaffold_graph['length'].max())) for n in ['scaf','strand','dist']]].copy()
                for s in range(1,l-2):
                    if cur['length'].max() > max_length-s:
                        cur = cur[(cur['length'] <= max_length-s) | ((cur[f'scaf{s}'] == cur[f'scaf{max_length-s}']) & (cur[f'strand{s}'] != cur[f'strand{max_length-s}']))].copy()
            if len(cur):
                cur.drop_duplicates(inplace=True)
                cur.reset_index(drop=True, inplace=True)
                cur['ifrom'] = cur.index.values
                # Guarantee we have all possible dist columns
                cur[[f'dist{s}' for s in range(cur['length'].max(), max_length)]] = np.nan
                # Combine the paths with the same inverted repeats
                cols = ['ifrom','from','from_side']+[f'{n}{s}' for s in range(1,l) for n in ['scaf','strand']]+[f'dist{s}' for s in range(1,max_length)]
                cur = cur[cols].merge(cur[cols].rename(columns={**{'from':'to','from_side':'to_side','ifrom':'ito'}, **{f'dist{s}':f'tdist{max_length-s+1}' for s in range(1,max_length) if s != l-1}}), on=[f'{n}{s}' for s in range(1,l) for n in ['scaf','strand']]+[f'dist{l-1}'])
                cur = cur[(cur['ifrom'] != cur['ito'])].drop(columns=['ito','ifrom'])
                check_dist = [s for s in list(range(2,l-1)) if (np.isnan(cur[f'tdist{s}']) == False).any()] + [s for s in list(range(l,max_length)) if (np.isnan(cur[f'dist{s}']) == False).any()]
                if len(check_dist):
                    cur = cur[((cur[[f'dist{s}' for s in check_dist]].values == cur[[f'tdist{s}' for s in check_dist]].values) | np.isnan(cur[[f'dist{s}' for s in check_dist]].values) | np.isnan(cur[[f'tdist{s}' for s in check_dist]].values)).all(axis=1)].copy()
                cur.drop(columns=[f'dist{s}' for s in range(l,max_length)]+[f'tdist{s}' for s in range(2,l-1)], inplace=True)
                cur.rename(columns={f'tdist{s}':f'dist{s}' for s in range(l,max_length+1)}, inplace=True)
                if len(cur):
                    # Only keep the paths through the inverted repeats that do not have alternatives
                    cur.drop_duplicates(inplace=True)
                    cols = ['from','from_side']+[f'{n}{s}' for s in range(1,l-1) for n in ['scaf','strand','dist']]+[f'dist{l-1}']
                    cur.sort_values(cols, inplace=True)
                    nalts = cur.groupby(cols, sort=False).size().values
                    cur = cur[np.repeat(nalts==1, nalts)].copy()
                    # Filter the ones that will show up in the next iteration again (and if they are too short to do so, they are not guaranteed to be unique)
                    cur = cur[(cur['from'] != cur['to']) | (cur['from_side'] != cur['to_side'])].copy()
                if len(cur):
                    if l < max_length:
                        cur[[f'{n}{s}' for s in range(l,max_length) for n in ['scaf','strand']]] = cur[[f'{n}{s}' for s in range(l-3,0,-1) for n in ['scaf','strand']]].values
                        cur[[f'strand{s}' for s in range(l,max_length)]] = np.where(cur[[f'strand{s}' for s in range(l,max_length)]].values == '+', '-', '+')
                    # Remove the ones, where we have evidence that we might have two haplotypes on one side instead of both sides of the inverted repeat
                    if ploidy > 1:
                        cur = RemoveInvertedRepeatsThatMightBeTwoHaplotypesOnOneSide(cur, scaffold_graph, max_length)
                if len(cur):
                    # Only keep one direction
                    cur = cur[(cur['from'] < cur['to']) | ((cur['from'] == cur['to']) & (cur['from_side'] == 'r'))].copy()
                    # Add the valid paths to unique_inversions to later get all (bridged) alternatives and to inv_paths directly, because we will not retrieve them, when we check for the alternatives
                    cur['to_side'] = np.where(cur['to_side'] == 'l', '+', '-')
                    unique_inversions.append( cur[['from','from_side','scaf1','to','to_side']].rename(columns={'to':'escaf','to_side':'estrand'}) )
                    cur.rename(columns={'from':'scaf0','from_side':'strand0','to':f'scaf{max_length}','to_side':f'strand{max_length}'}, inplace=True)
                    cur['strand0'] = np.where(cur['strand0'] == 'r', '+', '-')
                    cur['length'] = max_length+1
                    inv_paths.append(cur[['length','scaf0','strand0']+[f'{n}{s}' for s in range(1,max_length+1) for n in ['scaf','strand','dist']]])
                l += 1
            else:
                break
        # Add all paths to inv_path between the two outer scaffolds that go through the inverted scaffold
        if len(unique_inversions):
            unique_inversions = pd.concat(unique_inversions, ignore_index=True)
            inv_paths = AddAlternativesToInversionsToPaths(inv_paths, unique_inversions, scaffold_graph)
#
    # Check for inverted repeat, which only have two connected scaffolds, so the paths is clear
    inv_scafs = scaf_bridges.loc[(scaf_bridges['from'] == scaf_bridges['to']) & (scaf_bridges['from_side'] == scaf_bridges['to_side']) & (scaf_bridges['from_alt'] == 1), ['from','from_side','mean_dist']].rename(columns={'mean_dist':'idist0'})
    inv_scafs['from_side'] = np.where(inv_scafs['from_side'] == 'r', 'l', 'r') # Flip 'from_side' so it is pointing in the direction of the continuation not the inversion
    s = 0
    two_connections = []
    inv_scafs[f'iscaf{s}'] = inv_scafs['from']
    inv_scafs[f'istrand{s}'] = np.where(inv_scafs['from_side'] == 'l', '+', '-') # Choice is arbitrary here, just needs to be consistent
    while len(inv_scafs):
        inv_scafs['length'] = s+1
        inv_scafs['nalt'] = inv_scafs[['from','from_side']].merge(scaf_bridges[['from','from_side','from_alt']].drop_duplicates(), on=['from','from_side'], how='left')['from_alt'].fillna(0).astype(int).values
        two_connections.append(inv_scafs[inv_scafs['nalt'] == 2].drop(columns=['nalt']))
        # If we have a single connection, we can continue and see if we find two connections in the next scaffold
        inv_scafs = inv_scafs[inv_scafs['nalt'] == 1].merge(scaf_bridges.loc[scaf_bridges['to_alt'] == 1, ['from','from_side','to','to_side','mean_dist']], on=['from','from_side'], how='inner')
        if len(inv_scafs):
            s += 1
            inv_scafs.rename(columns={'mean_dist':f'idist{s}'}, inplace=True)
            inv_scafs['from'] = inv_scafs['to']
            inv_scafs['from_side'] = np.where(inv_scafs['to_side'] == 'r', 'l', 'r')
            inv_scafs.drop(columns=['to','to_side'], inplace=True)
            inv_scafs[f'iscaf{s}'] = inv_scafs['from']
            inv_scafs[f'istrand{s}'] = np.where(inv_scafs['from_side'] == 'l', '+', '-') # Choice is arbitrary here, just needs to be consistent
    if len(two_connections):
        two_connections = pd.concat(two_connections, ignore_index=True)
    if len(two_connections):
        # Create the paths through the inverted repeat
        for l in np.unique(two_connections['length']):
            for s in range(l):
                two_connections.loc[two_connections['length'] == l, [f'scaf{l-s}',f'strand{l-s}',f'strand{l-s+1}']] = two_connections.loc[two_connections['length'] == l, [f'iscaf{s}',f'istrand{s}',f'idist{s}']].values
                two_connections.loc[two_connections['length'] == l, [f'scaf{l+1+s}',f'strand{l+1+s}',f'dist{l+1+s}']] = two_connections.loc[two_connections['length'] == l, [f'iscaf{s}',f'istrand{s}',f'idist{s}']].values
                two_connections.loc[two_connections['length'] == l, f'strand{l+1+s}'] = np.where(two_connections.loc[two_connections['length'] == l, f'strand{l+1+s}'] == '+', '-', '+')
        two_connections['length'] = two_connections['length']*2+2
        exits = two_connections[['from','from_side']].reset_index().merge(scaf_bridges[['from','from_side','to','to_side','mean_dist']], on=['from','from_side'], how='left')
        exits.sort_values(['index','to','to_side'], inplace=True)
        start = exits.groupby(['index']).first().reset_index()
        two_connections.loc[start['index'].values, 'scaf0'] = start['to'].values
        two_connections.loc[start['index'].values, 'strand0'] = np.where(start['to_side'].values == 'r', '+', '-') # Needs to be consistent with previous choice
        two_connections.loc[start['index'].values, 'dist1'] = start['mean_dist'].values
        ends = exits.groupby(['index']).last().reset_index()
        ends['len'] = two_connections.loc[ends['index'].values, 'length'].values
        for l in np.unique(two_connections['length']):
            two_connections.loc[ends.loc[ends['len'] == l, 'index'].values, f'scaf{l-1}'] = ends.loc[ends['len'] == l, 'to'].values
            two_connections.loc[ends.loc[ends['len'] == l, 'index'].values, f'strand{l-1}'] = np.where(ends.loc[ends['len'] == l, 'to_side'].values == 'l', '+', '-') # Needs to be consistent with previous choice
            two_connections.loc[ends.loc[ends['len'] == l, 'index'].values, f'dist{l-1}'] = ends.loc[ends['len'] == l, 'mean_dist'].values
        # Remove the ones, where we have evidence that we might have two haplotypes on one side instead of both sides of the inverted repeat
        if ploidy > 1:
            valid_connections = []
            for l in np.unique(two_connections['length']):
                cur = two_connections.loc[two_connections['length'] == l, ['length','scaf0','strand0']+[f'{n}{s}' for s in range(1,l) for n in ['scaf','strand','dist']]].rename(columns={'scaf0':'from','strand0':'from_side',f'scaf{l-1}':'to',f'strand{l-1}':'to_side'})
                cur['from_side'] = np.where(cur['from_side'] == '+', 'r', 'l')
                cur['to_side'] = np.where(cur['to_side'] == '+', 'l', 'r')
                cur = pd.concat([cur, cur.rename(columns={**{'from':'to','to':'from','from_side':'to_side','to_side':'from_side'}, **{f'dist{s}':f'dist{l-s}' for s in range(1,l)}})], ignore_index=True)
                cur = RemoveInvertedRepeatsThatMightBeTwoHaplotypesOnOneSide(cur, scaffold_graph, l-1)
                cur.rename(columns={'from':'scaf0','from_side':'strand0','to':f'scaf{l-1}','to_side':f'strand{l-1}'}, inplace=True)
                cur['strand0'] = np.where(cur['strand0'] == 'r', '+', '-')
                cur[f'strand{l-1}'] = np.where(cur[f'strand{l-1}'] == 'l', '+', '-')
                valid_connections.append(cur)
            two_connections = pd.concat(valid_connections)
    if len(two_connections):
        inv_paths.append( two_connections[['length','scaf0','strand0']+[f'{n}{s}' for s in range(1,two_connections['length'].max()) for n in ['scaf','strand','dist']]].copy() )
#
    # Filter the inv_paths that are fully included in another inv_paths
    if len(inv_paths):
        inv_paths = pd.concat(inv_paths, ignore_index=True).drop_duplicates()
        inv_paths['included'] = False
        for sfrom in range(inv_paths['length'].max()-2):
            for l in range(3,inv_paths['length'].max()+1-sfrom):
                check = inv_paths.loc[inv_paths['length'] >= sfrom+l, ['length',f'scaf{sfrom}',f'strand{sfrom}']+[f'{n}{s}' for s in range(sfrom+1,sfrom+l) for n in ['scaf','strand','dist']]].rename(columns={f'{n}{s}':f'{n}{s-sfrom}' for s in range(sfrom,sfrom+l) for n in ['scaf','strand','dist']})
                if sfrom == 0:
                    check = check[check['length'] > l].copy() # To avoid self-matching of inv_paths
                check['length'] = l
                inv_paths.loc[inv_paths[check.columns].merge(check, on=list(check.columns.values), how='left', indicator=True)['_merge'].values == "both", 'included'] = True
        inv_paths = inv_paths[inv_paths['included'] == False].copy()
#
    # Add information to scaffold_paths and handled_scaf_conns
    if len(inv_paths):
        if len(handled_scaf_conns):
            handled_scaf_conns = pd.concat([ handled_scaf_conns, GetHandledScaffoldConnectionsFromVerticalPath(inv_paths)], ignore_index=True).drop_duplicates()
        else:
            handled_scaf_conns = GetHandledScaffoldConnectionsFromVerticalPath(inv_paths)
        if len(scaffold_paths):
            inv_paths = TurnHorizontalPathIntoVertical(inv_paths, scaffold_paths['pid'].max()+1)
            scaffold_paths = pd.concat([scaffold_paths, inv_paths], ignore_index=True)
        else:
            scaffold_paths = TurnHorizontalPathIntoVertical(inv_paths, 0)
#
    return scaffold_paths, handled_scaf_conns

def AddUntraversedConnectedPaths(scaffold_paths, graph_ext, handled_origins, handled_scaf_conns):
    # Get untraversed connections to neighbouring scaffolds in graph
    conn_path = graph_ext['org'][['oscaf1','ostrand1','scaf0','strand0','odist0']].drop(handled_origins)
    conn_path.drop_duplicates(inplace=True)
    conn_path.rename(columns={'oscaf1':'scaf0','ostrand1':'strand0','scaf0':'scaf1','strand0':'strand1','odist0':'dist1'}, inplace=True)
    # Require that it is untraversed in both directions, otherwise we would duplicate the end of a phased path
    conn_path['side0'] = np.where(conn_path['strand0'] == '+', 'r', 'l')
    conn_path['side1'] = np.where(conn_path['strand1'] == '+', 'l', 'r')
    conn_path = conn_path.merge(conn_path[['scaf0','side0','scaf1','side1','dist1']].rename(columns={'scaf0':'scaf1','side0':'side1','scaf1':'scaf0','side1':'side0'}), on=['scaf0','side0','scaf1','side1','dist1'], how='inner')
    # Only use one of the two directions for the path
    conn_path = conn_path[(conn_path['scaf0'] < conn_path['scaf1']) | ((conn_path['scaf0'] == conn_path['scaf1']) & ((conn_path['side0'] == conn_path['side1']) | (conn_path['side0'] == 'r')))].drop(columns=['side0','side1'])
    # Require that is has not been handled before
    if len(handled_scaf_conns):
        conn_path = conn_path[ conn_path.merge(handled_scaf_conns, on=['scaf0','strand0','scaf1','strand1','dist1'], how='left', indicator=True)['_merge'].values == "left_only" ].copy()
    # Turn into proper format for scaffold_paths
    if len(conn_path):
        conn_path['pid'] = np.arange(len(conn_path)) + (1 + scaffold_paths['pid'].max() if len(scaffold_paths) else 0)
        conn_path['dist0'] = 0
        conn_path['pos'] = 0
        conn_path['pos1'] = 1
        scaffold_paths = pd.concat( ([scaffold_paths] if len(scaffold_paths) else []) + [conn_path[['pid','pos','scaf0','strand0','dist0']], conn_path[['pid','pos1','scaf1','strand1','dist1']].rename(columns={'pos1':'pos','scaf1':'scaf0','strand1':'strand0','dist1':'dist0'})], ignore_index=True)
    if len(scaffold_paths):
        scaffold_paths.sort_values(['pid','pos'], inplace=True)

    return scaffold_paths

def AddUnconnectedPaths(scaffold_paths, scaffolds, scaffold_graph):
    # All the scaffolds that are not in scaffold_graph, because they do not have connections are inserted as their own knot
    unconn_path = scaffolds[['scaffold']].rename(columns={'scaffold':'scaf0'})
    if len(scaffold_graph):
        unconn_path = unconn_path[ unconn_path.merge(scaffold_graph[['from']].drop_duplicates().rename(columns={'from':'scaf0'}), on=['scaf0'], how='left', indicator=True)['_merge'].values == "left_only" ].copy()
    unconn_path['strand0'] = '+'
    unconn_path['dist0'] = 0
    unconn_path['pid'] = np.arange(len(unconn_path)) + 1 + (scaffold_paths['pid'].max() if len(scaffold_paths) else 0)
    unconn_path['pos'] = 0
    unconn_path = unconn_path[['pid','pos','scaf0','strand0','dist0']].copy()

    if len(scaffold_paths):
        scaffold_paths = pd.concat([scaffold_paths, unconn_path], ignore_index=True)
    else:
        scaffold_paths = unconn_path

    return scaffold_paths

def CheckScaffoldPathsConsistency(scaffold_paths):
    # Check that the first positions have zero distance
    check = scaffold_paths[(scaffold_paths['pos'] == 0) & (scaffold_paths[[col for col in scaffold_paths.columns if col[:4] == "dist"]] != 0).any(axis=1)].copy()
    if len(check):
        print(check)
        raise RuntimeError("First distances are not zero.")
#
    # Check that positions are consistent in scaffold_paths
    inconsistent = scaffold_paths[(scaffold_paths['pos'] < 0) |
                                  ((scaffold_paths['pos'] == 0) & (scaffold_paths['pid'] == scaffold_paths['pid'].shift(1))) |
                                  ((scaffold_paths['pos'] > 0) & ((scaffold_paths['pid'] != scaffold_paths['pid'].shift(1)) | (scaffold_paths['pos'] != scaffold_paths['pos'].shift(1)+1)))].copy()
    if len(inconsistent):
        print(inconsistent)
        raise RuntimeError("Scaffold paths got inconsistent.")
        
    # Check that we do not have a phase 0, because it cannot be positive/negative as required for a phase
    inconsistent = scaffold_paths[(scaffold_paths[[col for col in scaffold_paths.columns if col[:5] == "phase"]] == 0).any(axis=1)].copy()
    if len(inconsistent):
        print(inconsistent)
        raise RuntimeError("Scaffold paths has a zero phase.")

def CheckIfScaffoldPathsFollowsValidBridges(scaffold_paths, scaf_bridges, ploidy):
    test_paths = scaffold_paths.copy()
    if 'phase0' not in test_paths.columns:
        test_paths['phase0'] = 1
    for h in range(1,ploidy):
        test_paths.loc[test_paths[f'phase{h}'] < 0, [f'scaf{h}',f'strand{h}',f'dist{h}']] = test_paths.loc[test_paths[f'phase{h}'] < 0, ['scaf0','strand0','dist0']].values
    test_bridges = []
    for h in range(ploidy):
        cur = test_paths.loc[(test_paths[f'scaf{h}'] >= 0), ['pid','pos',f'phase{h}',f'scaf{h}',f'strand{h}',f'dist{h}']].copy()
        cur['from'] = cur[f'scaf{h}'].shift(1, fill_value=-1)
        cur['from_side'] = np.where(cur[f'strand{h}'].shift(1, fill_value='') == '+', 'r', 'l')
        cur = cur[(cur['pid'] == cur['pid'].shift(1)) & ((cur[f'phase{h}'] > 0) | (cur[f'phase{h}'].shift(1) > 0))].drop(columns=[f'phase{h}'])
        cur.rename(columns={f'scaf{h}':'to',f'strand{h}':'to_side',f'dist{h}':'mean_dist'}, inplace=True)
        cur['to_side'] = np.where(cur['to_side'] == '+', 'l', 'r')
        cur['hap'] = h
        test_bridges.append(cur)
    test_bridges = pd.concat(test_bridges, ignore_index=True)
    test_bridges = test_bridges.groupby(['pid','pos','from','from_side','to','to_side','mean_dist'])['hap'].min().reset_index()
    test_bridges = test_bridges[ test_bridges.merge(scaf_bridges[['from','from_side','to','to_side','mean_dist']], on=['from','from_side','to','to_side','mean_dist'], how='left', indicator=True)['_merge'].values == 'left_only' ].copy()
    if len(test_bridges):
        print(test_bridges[['pid','pos','hap','from','from_side','to','to_side','mean_dist']])
        raise RuntimeError("Scaffold path contains invalid bridges.")
        

def ReverseScaffolds(scaffold_paths, reverse, ploidy):
    # Reverse Strand
    for h in range(ploidy):
        sreverse = reverse & (scaffold_paths[f'strand{h}'] != '')
        scaffold_paths.loc[sreverse, f'strand{h}'] = np.where(scaffold_paths.loc[sreverse, f'strand{h}'] == '+', '-', '+')
    # Reverse distance and phase
    for h in range(ploidy-1,-1,-1):
        # Shift distances in alternatives
        missing_information = reverse & (scaffold_paths['pid'] == scaffold_paths['pid'].shift(-1)) & (scaffold_paths[f'phase{h}'] < 0) & (scaffold_paths[f'phase{h}'].shift(-1) >= 0)
        if np.sum(missing_information):
            scaffold_paths.loc[missing_information, [f'scaf{h}', f'strand{h}', f'dist{h}']] = scaffold_paths.loc[missing_information, ['scaf0','strand0','dist0']].values
            scaffold_paths.loc[missing_information, [f'phase{h}']] = -scaffold_paths.loc[missing_information, [f'phase{h}']]
        scaffold_paths.loc[reverse & (scaffold_paths[f'phase{h}'] < 0), f'dist{h}'] = scaffold_paths.loc[reverse & (scaffold_paths[f'phase{h}'] < 0), 'dist0']
        scaffold_paths[f'dist{h}'] = np.where(reverse, scaffold_paths[f'dist{h}'].shift(-1, fill_value=0), scaffold_paths[f'dist{h}'])
        scaffold_paths.loc[reverse & (scaffold_paths['pid'] != scaffold_paths['pid'].shift(-1)) | (scaffold_paths[f'phase{h}'] < 0), f'dist{h}'] = 0
        while True:
            scaffold_paths['jumped'] = (scaffold_paths[f'phase{h}'] >= 0) & (scaffold_paths[f'scaf{h}'] < 0) & (scaffold_paths[f'dist{h}'] != 0) # Jump deletions
            if 0 == np.sum(scaffold_paths['jumped']):
                break
            else:
                scaffold_paths.loc[scaffold_paths['jumped'].shift(-1, fill_value=False), f'dist{h}'] = scaffold_paths.loc[scaffold_paths['jumped'], f'dist{h}'].values
                scaffold_paths.loc[scaffold_paths['jumped'], f'dist{h}'] = 0
        # We also need to shift the phase, because the distance variation at the end of the bubble is on the other side now
        scaffold_paths[f'phase{h}'] = np.where(reverse & (scaffold_paths['pid'] == scaffold_paths['pid'].shift(-1)), np.sign(scaffold_paths[f'phase{h}'])*np.abs(scaffold_paths[f'phase{h}'].shift(-1, fill_value=0)), scaffold_paths[f'phase{h}'])
    scaffold_paths.drop(columns=['jumped'], inplace=True)
    scaffold_paths = TrimAlternativesConsistentWithMain(scaffold_paths, ploidy)
    # Reverse ordering
    scaffold_paths.loc[reverse, 'pos'] *= -1
    scaffold_paths.sort_values(['pid','pos'], inplace=True)
    scaffold_paths['pos'] = scaffold_paths.groupby(['pid'], sort=False).cumcount()
#
    return scaffold_paths

def GetDuplications(scaffold_paths, ploidy, groups=[]):
    # Collect all the haplotypes for comparison
    if 1 == ploidy:
        duplications = scaffold_paths[['scaf0','pid','pos']].rename(columns={'scaf0':'scaf'})
        duplications['hap'] = 0
        pass
    else:
        duplications = []
        for h in range(ploidy):
            duplications.append(scaffold_paths.loc[scaffold_paths[f'phase{h}'] >= 0,[f'scaf{h}','pid','pos']].rename(columns={f'scaf{h}':'scaf'}))
            duplications[-1]['hap'] = h
        duplications = pd.concat(duplications, ignore_index=True)
    duplications = duplications[duplications['scaf'] >= 0].copy() # Ignore deletions
    # Get duplications
    if len(groups):
        duplications = duplications.merge(groups, on=['pid'], how='inner')
        mcols = ['scaf','group']
    else:
        mcols = ['scaf']
    duplications = duplications.rename(columns={col:f'a{col}' for col in duplications.columns if col not in mcols}).merge(duplications.rename(columns={col:f'b{col}' for col in duplications.columns if col not in mcols}), on=mcols, how='left') # 'left' keeps the order and we always have at least the self mapping, thus 'inner' does not reduce the size
    duplications = duplications[(duplications['apid'] != duplications['bpid'])].drop(columns=['scaf']) # Remove self mappings
#
    return duplications

def AddStrandToDuplications(duplications, scaffold_paths, ploidy):
    for p in ['a','b']:
        dup_strans = duplications[[f'{p}pid',f'{p}pos']].rename(columns={f'{p}pid':'pid',f'{p}pos':'pos'}).merge(scaffold_paths[['pid','pos']+[f'strand{h}' for h in range(ploidy)]], on=['pid','pos'], how='left')
        duplications[f'{p}strand'] = dup_strans['strand0'].values
        for h in range(1, ploidy):
            hap = duplications[f'{p}hap'].values == h
            duplications.loc[hap, f'{p}strand'] = dup_strans.loc[hap, f'strand{h}'].values
#
    return duplications

def SeparateDuplicationsByHaplotype(duplications, scaffold_paths, ploidy):
    if ploidy == 1 or len(duplications) == 0:
        duplications.reset_index(drop=True, inplace=True)
    else:
        #for h in range(1,ploidy):
        #        duplications[f'add{h}'] = False # Already create the variables to prevent an error creating multiple columns at once
        for p in ['a','b']:
            # All the duplications of the main. where another haplotype is identical to main we potentially need to clone
            duplications[[f'add{h}' for h in range(1,ploidy)]] = duplications[[f'{p}pid',f'{p}pos']].rename(columns={f'{p}pid':'pid',f'{p}pos':'pos'}).merge(scaffold_paths[['pid','pos']+[f'phase{h}' for h in range(1,ploidy)]], on=['pid','pos'], how='left')[[f'phase{h}' for h in range(1,ploidy)]].values < 0
            # Only clone the main haplotype
            duplications.loc[duplications[f'{p}hap'] != 0, [f'add{h}' for h in range(1,ploidy)]] = False
            # Only create duplicates for a haplotype that is different to main
            existing = (scaffold_paths.groupby(['pid'])[[f'phase{h}' for h in range(1,ploidy)]].max() >= 0).reset_index()
            existing.rename(columns={'pid':f'{p}pid'}, inplace=True)
            for h in range(1,ploidy):
                duplications.loc[duplications[[f'{p}pid']].merge(existing.loc[existing[f'phase{h}'], [f'{p}pid']], on=[f'{p}pid'], how='left', indicator=True)['_merge'].values == "left_only", f'add{h}'] = False
            # Clone duplicates and assign new haplotypes
            duplications['copies'] = 1 + duplications[[f'add{h}' for h in range(1,ploidy)]].sum(axis=1)
            duplications[[f'add{h}' for h in range(1,ploidy)]] = duplications[[f'add{h}' for h in range(1,ploidy)]].cumsum(axis=1)
            for h in range(2,ploidy):
                duplications.loc[duplications[f'add{h}'] == duplications[f'add{h-1}'], f'add{h}'] = -1
            if ploidy > 1:
                duplications.loc[duplications['add1'] == 0, 'add1'] = -1
            duplications.reset_index(drop=True, inplace=True)
            duplications = duplications.loc[np.repeat(duplications.index.values, duplications['copies'].values)].copy()
            duplications['index'] = duplications.index.values
            duplications.reset_index(drop=True, inplace=True)
            duplications['ipos'] = duplications.groupby(['index']).cumcount()
            for h in range(1,ploidy):
                duplications.loc[duplications['ipos'] == duplications[f'add{h}'], f'{p}hap'] = h
        duplications.drop(columns=[f'add{h}' for h in range(1,ploidy)]+['copies','index','ipos'], inplace=True)
#
    return duplications

def RequireContinuousDirectionForDuplications(duplications):
    # Sort them in the direction the positions should change
    duplications.loc[duplications['samedir'] == False, 'bpos'] *= -1
    duplications.sort_values(['apid','ahap','bpid','bhap','apos','bpos'], inplace=True)
    duplications.loc[duplications['samedir'] == False, 'bpos'] *= -1
    duplications['did'] = ((duplications['apid'] != duplications['apid'].shift(1)) | (duplications['ahap'] != duplications['ahap'].shift(1)) | (duplications['bpid'] != duplications['bpid'].shift(1)) | (duplications['bhap'] != duplications['bhap'].shift(1))).cumsum()
    # Find conflicts
    duplications.reset_index(drop=True, inplace=True) # Make sure the index is continuous with no missing numbers
    s = 1
    conflicts = []
    while True:
        duplications['dist'] = duplications['bpos'].shift(-s) - duplications['bpos']
        conflict_found = ((duplications['did'] == duplications['did'].shift(-s)) & ((duplications['apos'] == duplications['apos'].shift(-s)) | (duplications['dist'] == 0) | ((duplications['dist'] < 0) == duplications['samedir'])))
        conflicts.append(pd.DataFrame({'index1':duplications[conflict_found].index.values, 'index2':duplications[conflict_found].index.values + s}))
        s += 1
        if len(conflicts[-1]) == 0:
            break
    duplications.drop(columns=['dist'], inplace=True)
    conflicts = pd.concat(conflicts, ignore_index=True)
    conflicts['did'] = duplications.loc[conflicts['index1'].values, 'did'].values
    conflicts.sort_values(['did','index1','index2'], inplace=True)
    # Assign conflict id to group conflicts that handle the same indexes
    conflicts['cid'] = conflicts['index1'].values
    while True:
        new_cid = pd.concat([conflicts[['index1','cid']].rename(columns={'index1':'index'}), conflicts[['index2','cid']].rename(columns={'index2':'index'})], ignore_index=True).groupby(['index'])['cid'].min().reset_index()
        conflicts['new_cid'] = np.minimum(conflicts[['index1']].rename(columns={'index1':'index'}).merge(new_cid, on=['index'], how='left')['cid'].values, conflicts[['index2']].rename(columns={'index2':'index'}).merge(new_cid, on=['index'], how='left')['cid'].values)
        if np.sum(conflicts['new_cid'] != conflicts['cid']) == 0:
            break
        else:
            conflicts['cid'] = conflicts['new_cid']
    conflicts.drop(columns=['new_cid'], inplace=True)
    # Assign the cid to duplications and automatically keep the ones without, since they are not involved in any conflict
    duplications['cid'] = -1
    cids = pd.concat([conflicts[['index1','cid']].rename(columns={'index1':'index'}), conflicts[['index2','cid']].rename(columns={'index2':'index'})], ignore_index=True).drop_duplicates()
    duplications.loc[cids['index'].values, 'cid'] = cids['cid'].values
    valid_dups = duplications[duplications['cid'] == -1].drop(columns=['cid'])
    # Merge adjacent cids (not separated by a duplication without conflict) to get proper lengths measures later and prevent double counting of space in between
    while True:
        duplications['new_cid'] = np.where(duplications['did'] != duplications['did'].shift(1), -1, duplications['cid'].shift(1, fill_value=-1))
        new_cids = duplications.loc[(duplications['new_cid'] != duplications['cid']) & (duplications['new_cid'] >= 0) & (duplications['cid'] >= 0), ['cid','new_cid']].copy()
        if len(new_cids) == 0:
            break
        else:
            duplications['new_cid'] = duplications[['cid']].merge(new_cids, on=['cid'], how='left')['new_cid'].values
            duplications.loc[np.isnan(duplications['new_cid']) == False, 'cid'] = duplications.loc[np.isnan(duplications['new_cid']) == False, 'new_cid'].astype(int).values
            conflicts['new_cid'] = conflicts[['cid']].merge(new_cids, on=['cid'], how='left')['new_cid'].values
            conflicts.loc[np.isnan(conflicts['new_cid']) == False, 'cid'] = conflicts.loc[np.isnan(conflicts['new_cid']) == False, 'new_cid'].astype(int).values
            conflicts.drop(columns=['new_cid'], inplace=True)
    duplications.drop(columns=['new_cid'], inplace=True)
    # Check for later how much unreducible buffer we have on each side due to an adjacent non-conflicting duplication
    for p in ['a','b']:
        # Flip min/max to get the default (which is no buffer)
        duplications[[f'{p}max',f'{p}min']] = duplications[['cid']].merge(duplications.groupby(['cid'])[f'{p}pos'].agg(['min','max']).reset_index(), on=['cid'], how='left')[['min','max']].values
    duplications['left'] = (duplications['did'] == duplications['did'].shift(1)) & (duplications['cid'] != duplications['cid'].shift(1))
    duplications['amin'] = np.where(duplications['left'], duplications['apos'].shift(1, fill_value=-1), duplications['amin'])
    duplications['bmin'] = np.where(duplications['left'] & duplications['samedir'], duplications['bpos'].shift(1, fill_value=-1), duplications['bmin'])
    duplications['bmax'] = np.where(duplications['left'] & (duplications['samedir'] == False), duplications['bpos'].shift(1, fill_value=-1), duplications['bmax'])
    duplications['right'] =  (duplications['did'] == duplications['did'].shift(-1)) & (duplications['cid'] != duplications['cid'].shift(-1))
    duplications['amax'] = np.where(duplications['right'], duplications['apos'].shift(-1, fill_value=-1), duplications['amax'])
    duplications['bmax'] = np.where(duplications['right'] & duplications['samedir'], duplications['bpos'].shift(-1, fill_value=-1), duplications['bmax'])
    duplications['bmin'] = np.where(duplications['right'] & (duplications['samedir'] == False), duplications['bpos'].shift(-1, fill_value=-1), duplications['bmin'])
    duplications = duplications[duplications['cid'] >= 0].copy()
    duplications[['amin','bmin']] = duplications[['cid']].merge(duplications.groupby(['cid'])[['amin','bmin']].min().reset_index(), on=['cid'], how='left')[['amin','bmin']].values
    duplications[['amax','bmax','left','right']] = duplications[['cid']].merge(duplications.groupby(['cid'])[['amax','bmax','left','right']].max().reset_index(), on=['cid'], how='left')[['amax','bmax','left','right']].values
#
    # Get longest(most matches) valid index combinations for every conflict pool (cid)
    ext = duplications[['cid']].reset_index()
    alternatives = []
    if len(ext):
        cur_alts = ext.rename(columns={'index':'i0'})
        ext = ext.rename(columns={'index':'index1'}).merge(ext.rename(columns={'index':'index2'}), on=['cid'], how='left')
        ext = ext[ext['index1'] < ext['index2']].copy() # Only allow increasing index to avoid duplications, because the order is arbitrary
        ext = ext[ ext[['cid','index1','index2']].merge(conflicts[['cid','index1','index2']], on=['cid','index1','index2'], how='left', indicator=True)['_merge'].values == "left_only" ].copy()
        s=1
        while len(cur_alts):
            new_alts = cur_alts.merge(ext.rename(columns={'index1':f'i{s-1}','index2':f'i{s}'}), on=['cid',f'i{s-1}'], how='inner')
            # Remove conflicts
            for s2 in range(s-1):
                new_alts = new_alts[ new_alts[['cid',f'i{s2}',f'i{s}']].rename(columns={f'i{s2}':'index1',f'i{s}':'index2'}).merge(conflicts[['cid','index1','index2']], on=['cid','index1','index2'], how='left', indicator=True)['_merge'].values == "left_only" ].copy()
            alternatives.append( cur_alts[np.isin(cur_alts['cid'].values, np.unique(new_alts['cid'].values)) == False].copy() )
            alternatives[-1]['len'] = s
            cur_alts = new_alts
            s += 1
        alternatives = pd.concat(alternatives, ignore_index=True)
    if len(alternatives):
        # Get shortest merged path length (from first duplication to last duplication)
        alternatives['mlen'] = np.maximum( np.maximum(0, duplications.loc[alternatives['i0'].values, 'apos'].values - duplications.loc[alternatives['i0'].values, 'amin'].values),
                                           np.maximum(0, np.where(duplications.loc[alternatives['i0'].values, 'samedir'].values, duplications.loc[alternatives['i0'].values, 'bpos'].values - duplications.loc[alternatives['i0'].values, 'bmin'].values, duplications.loc[alternatives['i0'].values, 'bmax'].values - duplications.loc[alternatives['i0'].values, 'bpos'].values) ) )
        alternatives['li'] = alternatives['i0'].values
        for s in range(1,alternatives['len'].max()):
            cur = alternatives['len'] > s
            alternatives.loc[cur, 'mlen'] += np.maximum( np.abs(duplications.loc[alternatives.loc[cur, f'i{s}'].values, 'apos'].values - duplications.loc[alternatives.loc[cur, f'i{s-1}'].values, 'apos'].values),
                                                         np.abs(duplications.loc[alternatives.loc[cur, f'i{s}'].values, 'bpos'].values - duplications.loc[alternatives.loc[cur, f'i{s-1}'].values, 'bpos'].values) )
            alternatives.loc[alternatives['len'] == s+1, 'li'] = alternatives.loc[alternatives['len'] == s+1, f'i{s}'].astype(int).values
        alternatives['mlen'] += np.maximum( np.maximum(0, duplications.loc[alternatives['li'].values, 'amax'].values - duplications.loc[alternatives['li'].values, 'apos'].values),
                                            np.maximum(0, np.where(duplications.loc[alternatives['li'].values, 'samedir'].values, duplications.loc[alternatives['li'].values, 'bmax'].values - duplications.loc[alternatives['li'].values, 'bpos'].values, duplications.loc[alternatives['li'].values, 'bpos'].values - duplications.loc[alternatives['li'].values, 'bmin'].values) ) )
        alternatives = alternatives[alternatives['mlen'].values == alternatives[['cid']].merge(alternatives.groupby(['cid'])['mlen'].min().reset_index(name='minlen'), on=['cid'], how='left')['minlen'].values].drop(columns=['mlen'])
        duplications.drop(columns=['amin','amax','bmin','bmax'], inplace=True)
        # Take the alternatives that cut the least amount from the ends (truly shortest path, because we reduce the overhang at the ends to a minimum)
        alternatives[['amin','bmin']] = duplications.loc[alternatives['i0'].values, ['apos','bpos']].values
        alternatives[['amax','bmax']] = duplications.loc[alternatives['li'].values, ['apos','bpos']].values
        alternatives['samedir'] = duplications.loc[alternatives['i0'].values, 'samedir'].values
        if np.sum(alternatives['samedir'] == False):
            alternatives.loc[alternatives['samedir'] == False, ['bmin','bmax']] = alternatives.loc[alternatives['samedir'] == False, ['bmax','bmin']].values
        alternatives[['left','right']] = duplications.loc[alternatives['i0'].values, ['left','right']].values == False # Here we are interested if we do not have a connected duplication (and not the other way round as before)
        alternatives['cut'] = np.where(alternatives['left'], alternatives['amin'].values - alternatives[['cid']].merge(alternatives.groupby(['cid'])[['amin']].min().reset_index(), on=['cid'], how='left')['amin'].values, 0)
        alternatives['cut'] += np.where(alternatives['right'], alternatives[['cid']].merge(alternatives.groupby(['cid'])[['amax']].max().reset_index(), on=['cid'], how='left')['amax'].values - alternatives['amax'].values, 0)
        if np.sum(alternatives['samedir'] == False):
            alternatives.loc[alternatives['samedir'] == False, ['left','right']] = alternatives.loc[alternatives['samedir'] == False, ['right','left']].values
        alternatives['cut'] += np.where(alternatives['left'], alternatives['bmin'].values - alternatives[['cid']].merge(alternatives.groupby(['cid'])[['bmin']].min().reset_index(), on=['cid'], how='left')['bmin'].values, 0)
        alternatives['cut'] += np.where(alternatives['right'], alternatives[['cid']].merge(alternatives.groupby(['cid'])[['bmax']].max().reset_index(), on=['cid'], how='left')['bmax'].values - alternatives['bmax'].values, 0)
        alternatives.drop(columns=['li','amin','bmin','amax','bmax','left','right'], inplace=True)
        alternatives = alternatives[alternatives['cut'].values == alternatives[['cid']].merge(alternatives.groupby(['cid'])['cut'].min().reset_index(), on=['cid'], how='left')['cut'].values].drop(columns=['cut'])
        duplications.drop(columns=['left','right'], inplace=True)
        # If we have an alternative with the same direction prefer it over alternatives with different directions
        alternatives = alternatives[alternatives['samedir'].values == alternatives[['cid']].merge(alternatives.groupby(['cid'])['samedir'].max().reset_index(name='samedir'), on=['cid'], how='left')['samedir'].values].copy()
        # Take the lowest indexes first for the lower pid and then for the higher pid (such that it is consistent no matter which one is a and b)
        alternatives['blower'] = (duplications.loc[alternatives['i0'].values, 'bpid'].values < duplications.loc[alternatives['i0'].values, 'apid'].values) | ((duplications.loc[alternatives['i0'].values, 'bpid'].values == duplications.loc[alternatives['i0'].values, 'apid'].values) & (duplications.loc[alternatives['i0'].values, 'bhap'].values < duplications.loc[alternatives['i0'].values, 'ahap'].values))
        for s in range(alternatives['len'].max()):
            cur = alternatives['len'] > s
            alternatives.loc[cur, [f'll{s}',f'lh{s}']] = np.where(alternatives.loc[cur, ['blower','blower']].values, duplications.loc[alternatives.loc[cur, f'i{s}'].values, ['bpos','apos']].values, duplications.loc[alternatives.loc[cur, f'i{s}'].values, ['apos','bpos']].values)
        for l in np.unique(alternatives['len']):
            cur = (alternatives['len'] == l) & (alternatives['samedir'] == False)
            for s in range(l//2):
                alternatives.loc[cur & alternatives['blower'], [f'll{s}',f'll{l-1-s}']] = alternatives.loc[cur, [f'll{l-1-s}',f'll{s}']].values
                alternatives.loc[cur & (alternatives['blower'] == False), [f'lh{s}',f'lh{l-1-s}']] = alternatives.loc[cur, [f'lh{l-1-s}',f'lh{s}']].values
        for s in range(alternatives['len'].max()-1,-1,-1): # Start comparing at the back, where we have the highest indexes
            for o in ['l','h']:
                cur = alternatives['len'] > s
                comp = alternatives.loc[cur, ['cid',f'l{o}{s}']].reset_index().merge(alternatives.loc[cur, ['cid',f'l{o}{s}']].groupby(['cid']).min().reset_index().rename(columns={f'l{o}{s}':'min'}), on=['cid'], how='left')
                alternatives.drop(comp.loc[comp[f'l{o}{s}'] > comp['min'], 'index'].values, inplace=True)
        # Take the duplications chosen by the remaining alternatives
        dup_ind = []
        for s in range(alternatives['len'].max()):
            dup_ind.append( alternatives.loc[alternatives['len'] > s, f'i{s}'].astype(int).values )
        dup_ind = np.concatenate(dup_ind)
        duplications = duplications.loc[dup_ind].drop(columns=['cid'])
        duplications = pd.concat([valid_dups, duplications], ignore_index=True)
    else:
        duplications = valid_dups
#
    # We need to sort again, since we destroyed the order by concatenating (but this time we have only one bpos per apos, which makes it easier)
    duplications.sort_values(['did','apos'], inplace=True)
    duplications.reset_index(drop=True, inplace=True)
#
    return duplications

def GetPositionFromPaths(df, scaffold_paths, ploidy, new_col, get_col, pid_col, pos_col, hap_col):
    #for col in [new_col]+[f'{n}{h}' for h in range(1,ploidy) for n in ['phase',get_col]]:
    #    if col not in df.columns:
    #        df[col] = np.nan # Already create the variables to prevent an error creating multiple columns at once
    df[[new_col]+[f'{n}{h}' for h in range(1,ploidy) for n in ['phase',get_col]]] = df[[pid_col,pos_col]].rename(columns={pid_col:'pid',pos_col:'pos'}).merge(scaffold_paths[['pid','pos',f'{get_col}0']+[f'{n}{h}' for h in range(1,ploidy) for n in ['phase',get_col]]], on=['pid','pos'], how='left')[[f'{get_col}0']+[f'{n}{h}' for h in range(1,ploidy) for n in ['phase',get_col]]].values
    for h in range(1,ploidy):
        alt_hap = (df[hap_col] == h) & (df[f'phase{h}'] >= 0)
        df.loc[alt_hap, new_col] = df.loc[alt_hap, f'{get_col}{h}']
        df.drop(columns=[f'phase{h}',f'{get_col}{h}'], inplace=True)
#
    return df

def GetPositionsBeforeDuplication(duplications, scaffold_paths, ploidy, invert):
    for p in ['a','b']:
        # Get previous position and skip duplications
        duplications[f'{p}prev_pos'] = duplications[f'{p}pos']
        update = np.repeat(True, len(duplications))
        while np.sum(update):
            duplications.loc[update, f'{p}prev_pos'] -= (1 if p == 'a' else np.where(duplications.loc[update, 'samedir'], 1, -1)) * (-1 if invert else 1)
            duplications = GetPositionFromPaths(duplications, scaffold_paths, ploidy, f'{p}prev_scaf', 'scaf', f'{p}pid', f'{p}prev_pos', f'{p}hap')
            update = duplications[f'{p}prev_scaf'] < 0
        # Get distance between current and previous position
        duplications.drop(columns=[f'{p}prev_scaf'], inplace=True)
        duplications['dist_pos'] = np.where((True if p == 'a' else duplications['samedir']) != invert, duplications[f'{p}pos'], duplications[f'{p}prev_pos'])
        duplications = GetPositionFromPaths(duplications, scaffold_paths, ploidy, f'{p}dist', 'dist', f'{p}pid', 'dist_pos', f'{p}hap')
        duplications.drop(columns=['dist_pos'], inplace=True)
#
    return duplications

def GetDuplicationDifferences(duplications):
    return duplications.groupby(['group','did','apid','ahap','bpid','bhap'])[['scaf_diff','dist_diff']].sum().reset_index()

def RemoveHaplotype(scaffold_paths, rem_pid, h):
    scaffold_paths.loc[rem_pid, f'phase{h}'] = -scaffold_paths.loc[rem_pid, 'phase0'].values
    scaffold_paths.loc[rem_pid, [f'scaf{h}', f'strand{h}', f'dist{h}']] = [-1,'',0]
#
    return scaffold_paths

def TrimAlternativesConsistentWithMain(scaffold_paths, ploidy):
    for h in range(1, ploidy):
        consistent = (scaffold_paths['scaf0'] == scaffold_paths[f'scaf{h}']) & (scaffold_paths['strand0'] == scaffold_paths[f'strand{h}']) & (scaffold_paths['dist0'] == scaffold_paths[f'dist{h}'])
        if np.sum(consistent):
            scaffold_paths.loc[consistent, [f'scaf{h}', f'dist{h}', f'strand{h}']] = [-1,0,'']
            scaffold_paths.loc[consistent, f'phase{h}'] = -scaffold_paths.loc[consistent, f'phase{h}']
#
    return scaffold_paths

def RemoveMainPath(scaffold_paths, rem_pid_org, ploidy):
    rem_pid = rem_pid_org.copy() # Do not change the input
    for h in range(1, ploidy):
        # Check if we can replace the main with another path
        cur_pid = (scaffold_paths.loc[rem_pid, ['pid',f'phase{h}']].groupby(['pid']).max() >= 0).reset_index().rename(columns={f'phase{h}':'replace'})
        cur_pid = scaffold_paths[['pid']].merge(cur_pid.loc[cur_pid['replace'], ['pid']], on=['pid'], how='left', indicator=True)['_merge'].values == "both"
        if np.sum(cur_pid):
            # For the cases, where we can replace we need to fill all later existing haplotypes (the previous ones are empty) with the main path to not lose the information
            for h1 in range(h+1, ploidy):
                fill = (scaffold_paths.loc[cur_pid, ['pid',f'phase{h1}']].groupby(['pid']).max() >= 0).reset_index().rename(columns={f'phase{h1}':'fill'})
                fill = (scaffold_paths[f'phase{h1}'] < 0) & (scaffold_paths[['pid']].merge(fill.loc[fill['fill'], ['pid']], on=['pid'], how='left', indicator=True)['_merge'].values == "both")
                scaffold_paths.loc[fill, f'phase{h1}'] = -scaffold_paths.loc[fill, f'phase{h1}'].values
                scaffold_paths.loc[fill, [f'scaf{h1}', f'strand{h1}', f'dist{h1}']] = scaffold_paths.loc[fill, ['scaf0', 'strand0', 'dist0']].values
            # Set alternative as main
            scaffold_paths.loc[cur_pid, 'phase0'] = np.abs(scaffold_paths.loc[cur_pid, f'phase{h}'].values)
            scaffold_paths.loc[cur_pid & (scaffold_paths[f'phase{h}'] >= 0), ['scaf0', 'strand0', 'dist0']] = scaffold_paths.loc[cur_pid & (scaffold_paths[f'phase{h}'] >= 0), [f'scaf{h}', f'strand{h}', f'dist{h}']].values
            scaffold_paths = RemoveHaplotype(scaffold_paths, cur_pid, h)
            # Remove already handled cases from rem_pid
            rem_pid = rem_pid & (cur_pid == False)
    scaffold_paths = TrimAlternativesConsistentWithMain(scaffold_paths, ploidy)
    # In the case where only one haplotype exists remove the whole path
    scaffold_paths = scaffold_paths[rem_pid == False].copy()
#
    return scaffold_paths

def AssignLowestHaplotypeToMain(duplications):
    # This becomes necessary when the main paths was removed
    min_hap = duplications[['apid','ahap']].drop_duplicates().groupby(['apid']).min().reset_index()
    min_hap = min_hap[min_hap['ahap'] > 0].copy()
    duplications.loc[duplications[['apid','ahap']].merge(min_hap, on=['apid','ahap'], how='left', indicator=True)['_merge'].values == "both", 'ahap'] = 0
    min_hap.rename(columns={'apid':'bpid','ahap':'bhap'}, inplace=True)
    duplications.loc[duplications[['bpid','bhap']].merge(min_hap, on=['bpid','bhap'], how='left', indicator=True)['_merge'].values == "both", 'bhap'] = 0
#
    return duplications

def GetHaplotypes(haps, scaffold_paths, ploidy):
    # Get full haplotype without deletions
    haps = haps.merge(scaffold_paths, on=['pid'], how='left')
    haps.rename(columns={'phase0':'phase','scaf0':'scaf','strand0':'strand','dist0':'dist'}, inplace=True)
    for h in range(1, ploidy):
        fill = (haps['hap'] == h) & (haps[f'phase{h}'] >= 0)
        haps.loc[fill, 'phase'] = np.abs(haps.loc[fill, f'phase{h}'])
        haps.loc[fill, ['scaf','strand','dist']] = haps.loc[fill, [f'scaf{h}',f'strand{h}',f'dist{h}']].values
        haps.drop(columns=[f'phase{h}',f'scaf{h}',f'strand{h}',f'dist{h}'], inplace=True)
    haps[['phase','scaf','dist']] = haps[['phase','scaf','dist']].astype(int)
    haps = haps[haps['scaf'] >= 0].copy()
#
    return haps

def GetBridgeSupport(bsupp, scaffold_paths, scaf_bridges, ploidy):
    bsupp = GetHaplotypes(bsupp, scaffold_paths, ploidy)
    bsupp.drop(columns=['phase'], inplace=True)
    bsupp.sort_values(['group','pid','hap','pos'], inplace=True)
    bsupp.rename(columns={'scaf':'to','dist':'mean_dist'}, inplace=True)
    bsupp['to_side'] = np.where(bsupp['strand'] == '+', 'l', 'r')
    bsupp['from'] = bsupp['to'].shift(1, fill_value=-1)
    bsupp.loc[(bsupp['pid'] != bsupp['pid'].shift(1)) | (bsupp['hap'] != bsupp['hap'].shift(1)), 'from'] = -1
    bsupp['from_side'] = np.where(bsupp['strand'].shift(1, fill_value='') == '+', 'r', 'l')
    bsupp = bsupp[bsupp['from'] >= 0].merge(scaf_bridges[['from','from_side','to','to_side','mean_dist','bcount']], on=['from','from_side','to','to_side','mean_dist'], how='left')
    # Sort by lowest distinct bridge support and assign a place
    if len(bsupp):
        bsupp.sort_values(['group','pid','hap','bcount'], inplace=True)
        bsupp['pos'] = bsupp.groupby(['group','pid','hap']).cumcount()
        vertical = bsupp[['group','pid','hap']].drop_duplicates()
        for p in range(bsupp['pos'].max()+1):
            vertical[f'bcount{p}'] = vertical[['group','pid','hap']].merge(bsupp.loc[bsupp['pos'] == p, ['group','pid','hap','bcount']], on=['group','pid','hap'], how='left')['bcount'].fillna(-1).astype(int).values
        vertical.sort_values([f'bcount{p}' for p in range(bsupp['pos'].max()+1)], inplace=True)
        bsupp = vertical[['group','pid','hap']].copy()
        bsupp['bplace'] = np.arange(len(bsupp),0,-1)
#
    return bsupp

def RemoveDuplicatedHaplotypesWithLowestSupport(scaffold_paths, duplications, rem_haps, bsupp, ploidy):
    if len(bsupp):
        # Find haplotype with lowest support in each group
        rem_haps = rem_haps.merge(bsupp, on=['group','pid','hap'], how='left')
        rem_haps.sort_values(['group','bplace'], ascending=[True,False], inplace=True)
        rem_haps = rem_haps.groupby(['group']).first().reset_index()
        # Remove those haplotypes
        for h in range(ploidy-1, -1, -1): # We need to go through in the inverse order, because we still need the main for the alternatives
            rem_pid = np.unique(rem_haps.loc[rem_haps['hap'] == h, 'pid'].values)
            duplications = duplications[((np.isin(duplications['apid'], rem_pid) == False) | (duplications['ahap'] != h)) & ((np.isin(duplications['bpid'], rem_pid) == False) | (duplications['bhap'] != h))].copy()
            rem_pid = np.isin(scaffold_paths['pid'], rem_pid)
            if h==0:
                scaffold_paths = RemoveMainPath(scaffold_paths, rem_pid, ploidy)
                duplications = AssignLowestHaplotypeToMain(duplications)
            else:
                scaffold_paths = RemoveHaplotype(scaffold_paths, rem_pid, h)
    differences = GetDuplicationDifferences(duplications)
#
    return scaffold_paths, duplications, differences

def ShiftHaplotypesToLowestPossible(scaffold_paths, ploidy):
    if ploidy > 1:
        existing_haps = []
        for h in range(1, ploidy):
            existing_haps.append(scaffold_paths.loc[scaffold_paths[f'phase{h}'] >= 0, ['pid']].drop_duplicates())
            existing_haps[-1]['hap'] = h
        existing_haps = pd.concat(existing_haps, ignore_index=True).sort_values(['pid','hap'])
        existing_haps['new_hap'] = existing_haps.groupby(['pid']).cumcount()
        existing_haps = existing_haps[existing_haps['new_hap'] != existing_haps['hap']].copy()
        if len(existing_haps):
            for hnew in range(1, ploidy):
                for hold in range(hnew+1, ploidy):
                    shift = np.isin(scaffold_paths['pid'], existing_haps.loc[(existing_haps['new_hap'] == hnew) & (existing_haps['new_hap'] == hold), 'pid'].values)
                    scaffold_paths.loc[shift, [f'phase{hnew}',f'scaf{hnew}', f'strand{hnew}', f'dist{hnew}']] = scaffold_paths.loc[shift, [f'phase{hold}',f'scaf{hold}', f'strand{hold}', f'dist{hold}']].values
                    scaffold_paths = RemoveHaplotype(scaffold_paths, shift, hold)
#
    return scaffold_paths

def CompressPaths(scaffold_paths, ploidy):
    # Remove positions with only deletions
    scaffold_paths = scaffold_paths[(scaffold_paths[[f'scaf{h}' for h in range(ploidy)]] >= 0).any(axis=1)].copy()
    scaffold_paths['pos'] = scaffold_paths.groupby(['pid'], sort=False).cumcount()
    scaffold_paths.reset_index(drop=True, inplace=True)
    # Compress paths where we have alternating deletions
    while True:
        shifts = scaffold_paths.loc[ ((np.where(scaffold_paths[[f'phase{h}' for h in range(ploidy)]].values < 0, scaffold_paths[['scaf0' for h in range(ploidy)]].values, scaffold_paths[[f'scaf{h}' for h in range(ploidy)]].values) < 0) |
                                      (np.where(scaffold_paths[[f'phase{h}' for h in range(ploidy)]].shift(1).values < 0, scaffold_paths[['scaf0' for h in range(ploidy)]].shift(1).values, scaffold_paths[[f'scaf{h}' for h in range(ploidy)]].shift(1).values) < 0)).all(axis=1) &
                                     (scaffold_paths['pos'] > 0), ['pid'] ]
        if len(shifts) == 0:
            break
        else:
            shifts['index'] = shifts.index.values
            shifts = shifts.groupby(['pid'], sort=False).first() # We can only take the first in each path, because otherwise we might block the optimal solution
            shifts['new_index'] = shifts['index'] - np.where(scaffold_paths.loc[shifts['index'].values, 'pos'].values > 1, 2, 1) # Make sure we do not go into the previous path
            while True:
                further = ( ((np.where(scaffold_paths.loc[shifts['index'].values, [f'phase{h}' for h in range(ploidy)]].values < 0, scaffold_paths.loc[shifts['index'].values, ['scaf0' for h in range(ploidy)]].values, scaffold_paths.loc[shifts['index'].values, [f'scaf{h}' for h in range(ploidy)]].values) < 0) |
                            (np.where(scaffold_paths.loc[shifts['new_index'].values, [f'phase{h}' for h in range(ploidy)]].values < 0, scaffold_paths.loc[shifts['new_index'].values, ['scaf0' for h in range(ploidy)]].values, scaffold_paths.loc[shifts['new_index'].values, [f'scaf{h}' for h in range(ploidy)]].values) < 0)).all(axis=1) &
                           (scaffold_paths.loc[shifts['new_index'].values, 'pos'] > 0) )
                if np.sum(further) == 0:
                    break
                else:
                    shifts.loc[further, 'new_index'] -= 1
            shifts['new_index'] += 1
            for h in range(ploidy):
                cur_shifts = (scaffold_paths.loc[shifts['index'].values, f'phase{h}'].values >= 0) & (scaffold_paths.loc[shifts['index'].values, f'scaf{h}'].values >= 0)
                if np.sum(cur_shifts):
                    scaffold_paths.loc[shifts.loc[cur_shifts, 'new_index'].values, f'phase{h}'] = np.abs(scaffold_paths.loc[shifts.loc[cur_shifts, 'index'].values, f'phase{h}'].values)
                    scaffold_paths.loc[shifts.loc[cur_shifts, 'new_index'].values, [f'scaf{h}',f'strand{h}',f'dist{h}']] = scaffold_paths.loc[shifts.loc[cur_shifts, 'index'].values, [f'scaf{h}',f'strand{h}',f'dist{h}']].values
                cur_shifts = (scaffold_paths.loc[shifts['index'].values, f'phase{h}'].values < 0) & (scaffold_paths.loc[shifts['index'].values, 'scaf0'].values >= 0)
                if np.sum(cur_shifts):
                    scaffold_paths.loc[shifts.loc[cur_shifts, 'new_index'].values, f'phase{h}'] = np.abs(scaffold_paths.loc[shifts.loc[cur_shifts, 'index'].values, f'phase{h}'].values)
                    scaffold_paths.loc[shifts.loc[cur_shifts, 'new_index'].values, [f'scaf{h}',f'strand{h}',f'dist{h}']] = scaffold_paths.loc[shifts.loc[cur_shifts, 'index'].values, ['scaf0','strand0','dist0']].values
            scaffold_paths.drop(shifts['index'].values, inplace=True)
            scaffold_paths['pos'] = scaffold_paths.groupby(['pid'], sort=False).cumcount()
            scaffold_paths.reset_index(drop=True, inplace=True)
#
    return scaffold_paths

def MergeHaplotypes(scaffold_paths, graph_ext, scaf_bridges, ploidy, ends_in=[]):
    if len(ends_in):
        ends = ends_in
    else:
        # Find start and end scaffolds for paths
        ends = scaffold_paths.loc[scaffold_paths['pos'] == 0, ['pid','scaf0','strand0']].rename(columns={'scaf0':'sscaf','strand0':'sside'})
        ends = ends.merge(scaffold_paths.loc[scaffold_paths['pid'] != scaffold_paths['pid'].shift(-1), ['pid','scaf0','strand0']].rename(columns={'scaf0':'escaf','strand0':'eside'}), on=['pid'], how='left')
        ends['sside'] = np.where(ends['sside'] == '+','l','r')
        ends['eside'] = np.where(ends['eside'] == '+','r','l')
        # Reverse scaffold ends if start has higher scaffold than end such that paths with identical ends always look the same
        ends['reverse'] = (ends['sscaf'] > ends['escaf']) | ((ends['sscaf'] == ends['escaf']) & (ends['sside'] == 'r') & (ends['eside'] == 'l'))
        ends.loc[ends['reverse'], [f'{p}{n}' for p in ['s','e'] for n in ['scaf','side']]] = ends.loc[ends['reverse'], [f'{p}{n}' for p in ['e','s'] for n in ['scaf','side']]].values
#
        # Find paths that start and end with the same scaffold
        ends.sort_values(['sscaf','sside','escaf','eside','pid'], inplace=True)
        groups = ends.groupby(['sscaf','sside','escaf','eside'], sort=False)['reverse'].agg(['sum','size'])
        ends['group'] = np.repeat(np.arange(len(groups)), groups['size'].values)
        ends['gsize'] = np.repeat(groups['size'].values, groups['size'].values)
        ends['grev'] = np.repeat(groups['sum'].values, groups['size'].values)
        ends = ends[ends['gsize'] > 1].copy()
        # Reverse the minority such that all scaffolds with identical start and end are facing in the same direction
        ends['reverse'] = np.where(ends['grev'] > ends['gsize']/2, ends['reverse'] == False, ends['reverse'])
        scaffold_paths['reverse'] = scaffold_paths[['pid']].merge(ends[['pid','reverse']], on=['pid'], how='left')['reverse'].fillna(False).values.astype(bool)
        scaffold_paths = ReverseScaffolds(scaffold_paths, scaffold_paths['reverse'], ploidy)
        scaffold_paths.drop(columns=['reverse'], inplace=True)
        ends.drop(columns=['gsize','grev'], inplace=True)
#
    # Get duplications within the groups
    duplications = GetDuplications(scaffold_paths, ploidy, ends[['pid','group']])
    # Require same strand (easy here, because we made them the same direction earlier)
    duplications = AddStrandToDuplications(duplications, scaffold_paths, ploidy)
    duplications = duplications[duplications['astrand'] == duplications['bstrand']].drop(columns=['astrand','bstrand'])
    # Require continuous direction of position change
    duplications = SeparateDuplicationsByHaplotype(duplications, scaffold_paths, ploidy)
    duplications['samedir'] = True
    duplications = RequireContinuousDirectionForDuplications(duplications)
    # When assuring continuous positions sometimes the duplication at the start/end gets assign to a place after/before the start/end, which means that those two paths should not be in the same group
    path_len = scaffold_paths.groupby(['pid'])['pos'].last().reset_index(name='max_pos')
    duplications['del'] = False
    for p in ['a','b']:
        duplications.loc[(duplications['did'] != duplications['did'].shift(1)) & (duplications[f'{p}pos'] > 0), 'del'] = True
        duplications['max_pos'] = duplications[[f'{p}pid']].rename(columns={f'{p}pid':'pid'}).merge(path_len, on=['pid'], how='left')['max_pos'].values
        duplications.loc[(duplications['did'] != duplications['did'].shift(-1)) & (duplications[f'{p}pos'] != duplications['max_pos']), 'del'] = True
    duplications['del']  = duplications[['apid','bpid']].merge(duplications.groupby(['apid','bpid'])['del'].max().reset_index(), on=['apid','bpid'], how='left')['del'].values
    duplications = duplications[duplications['del'] == False].drop(columns=['max_pos','del'])
#
#    # Check if the haplotypes to be merged share an extension (reduces continuity without reducing misassemblies)
#    if len(duplications):
#        # Get both ends for the paths
#        ends = duplications[['apid','ahap','bpid','bhap']].drop_duplicates()
#        ends = ends.loc[np.repeat(ends.index.values, 2)].reset_index(drop=True)
#        for p in ['a','b']:
#            ends[f'{p}side'] = np.tile(['l','r'], len(ends)//2)
#        ends, patha = GetPathAFromEnds(ends, scaffold_paths, ploidy)
#        # Get matching origins
#        cur_org = MatchOriginsToPathA(patha, graph_ext)
#        ends = ends.merge(cur_org.rename(columns={'pid':'opid'}), on='opid', how='inner')
#        # Get paired extensions (and -1 if no extension exists at all)
#        ends = ends.merge(graph_ext['pairs'], on='oindex', how='inner').merge(ends[['apid','ahap','bpid','bhap','aside','bside']].drop_duplicates(), on=['apid','ahap','bpid','bhap','aside','bside'], how='right')
#        ends = ends.drop(columns=['opid','oindex']).drop_duplicates()
#        ends['eindex'] = ends['eindex'].fillna(-1).astype(int)
#        ends = ends.rename(columns={'eindex':'aindex'}).merge(ends.rename(columns={'apid':'bpid','bpid':'apid','ahap':'bhap','bhap':'ahap','aside':'bside','bside':'aside','eindex':'bindex'}), on=['apid','ahap','bpid','bhap','aside','bside'], how='inner')
#        # Check if the pair shares at least one extension on both sides
#        ends = ends[ends['aindex'] == ends['bindex']].drop(columns=['aindex','bindex']).drop_duplicates().groupby(['apid','ahap','bpid','bhap']).size().reset_index(name='nsides')
#        ends = ends[ends['nsides'] == 2].drop(columns='nsides')
#        # Only keep the duplications between pairs that share an extension on both sides
#        duplications = duplications.merge(ends, on=['apid','ahap','bpid','bhap'], how='inner')
#
    # Add the duplications between haplotypes of the same path
    if len(duplications):
        new_duplications = [duplications]
        for h in range(1,ploidy):
            # Only take scaffold_paths that are not exactly identical to main
            new_dups = scaffold_paths.groupby(['pid'])[f'phase{h}'].max()
            new_dups = new_dups[new_dups >= 0].reset_index()
            new_dups = scaffold_paths[np.isin(scaffold_paths['pid'], new_dups['pid'])].copy()
            for h1 in range(h):
                # Add one direction (deletions cannot be duplications to be consistent)
                if h1 == 0:
                    add_dups = new_dups.loc[(new_dups['scaf0'] >= 0) & ((new_dups[f'phase{h}'] < 0) | ((new_dups[f'scaf{h}'] == new_dups['scaf0']) & (new_dups[f'strand{h}'] == new_dups['strand0']))), ['pid','pos']].copy()
                else:
                    add_dups = scaffold_paths.groupby(['pid'])[f'phase{h1}'].max()
                    add_dups = add_dups[add_dups >= 0].reset_index()
                    add_dups = new_dups[np.isin(new_dups['pid'], add_dups['pid'])].copy()
                    add_dups = add_dups.loc[((new_dups['scaf0'] >= 0) & (add_dups[f'phase{h}'] < 0) & (add_dups[f'phase{h1}'] < 0)) | ((new_dups[f'scaf{h}'] >= 0) & (add_dups[f'scaf{h}'] == add_dups[f'scaf{h1}']) & (new_dups[f'strand{h}'] == new_dups[f'strand{h1}'])) |
                                            ((add_dups[f'phase{h}'] < 0) & (new_dups['scaf0'] >= 0) & (add_dups[f'scaf{h1}'] == add_dups['scaf0']) & (new_dups[f'strand{h1}'] == new_dups['strand0'])) |
                                            ((add_dups[f'phase{h1}'] < 0) & (new_dups['scaf0'] >= 0) & (add_dups[f'scaf{h}'] == add_dups['scaf0']) & (new_dups[f'strand{h}'] == new_dups['strand0'])), ['pid','pos']].copy()
                add_dups.rename(columns={'pid':'apid','pos':'apos'}, inplace=True)
                add_dups['ahap'] = h1
                add_dups['bpid'] = add_dups['apid'].values
                add_dups['bpos'] = add_dups['apos'].values
                add_dups['bhap'] = h
                add_dups['group'] = add_dups[['apid','ahap']].merge(duplications[['apid','ahap','group']].drop_duplicates(), on=['apid','ahap'], how='left')['group'].values
                add_dups = add_dups[np.isnan(add_dups['group']) == False].copy() # We do not need to add scaffold that do not have duplications with other scaffolds
                add_dups['group'] = add_dups['group'].astype(int)
                add_dups['samedir'] = True
                if len(add_dups):
                    new_duplications.append(add_dups.copy())
                    # Add other direction
                    add_dups[['ahap','bhap']] = add_dups[['bhap','ahap']].values
                    add_dups['group'] = add_dups[['apid','ahap']].merge(duplications[['apid','ahap','group']].drop_duplicates(), on=['apid','ahap'], how='left')['group'].values
                    new_duplications.append(add_dups)
        duplications = pd.concat(new_duplications, ignore_index=True)
        duplications.sort_values(['apid','ahap','bpid','bhap','apos'], inplace=True)
        duplications['did'] = ((duplications['apid'] != duplications['apid'].shift(1)) | (duplications['ahap'] != duplications['ahap'].shift(1)) | (duplications['bpid'] != duplications['bpid'].shift(1)) | (duplications['bhap'] != duplications['bhap'].shift(1))).cumsum()
    # Assign new groups after some groups might have been split when we removed the duplications where the start/end duplication got reassigned to another position (always join the group with the scaffold with most matches as long as all scaffolds match with all other scaffolds)
    if len(duplications):
        duplications['agroup'] = duplications['apid']
        duplications['bgroup'] = duplications['bpid']
        while True:
            # Count the matches(scaffold duplications) in each pid pair with duplicate
            matches = duplications.groupby(['agroup','apid','ahap','bgroup','bpid','bhap']).size().reset_index(name='matches')
            # Get size(number of pid pairs) of each group
            for p in ['a','b']:
                matches[f'{p}size'] = matches[[f'{p}group']].merge(matches[[f'{p}group',f'{p}pid',f'{p}hap']].drop_duplicates().groupby([f'{p}group']).size().reset_index(name='size'), on=[f'{p}group'], how='left')['size'].values
            # Get min, median, max number of matches between groups
            matches = matches[matches['agroup'] != matches['bgroup']].copy()
            groups = matches.groupby(['agroup','bgroup','asize','bsize'])['matches'].agg(['size','min','median','max']).reset_index()
            # Delete duplications between groups, where not all pids match all pids of the other group
            delete = groups.loc[groups['size'] != groups['asize']*groups['bsize'], ['agroup','bgroup']].copy()
            if len(delete):
                duplications = duplications[duplications[['agroup','bgroup']].merge(delete, on=['agroup','bgroup'], how='left', indicator=True)['_merge'].values == "left_only"].copy()
            groups = groups[groups['size'] == groups['asize']*groups['bsize']].drop(columns=['size','asize','bsize'])
            if len(groups):
                # Merge the groups with the most min, median, max matches between them
                groups.sort_values(['agroup','min','median','max','bgroup'], ascending=[True,False,False,False,True], inplace=True)
                groups = groups.groupby(['agroup'], sort=False).first().reset_index()
                groups.drop(columns=['min','median','max'], inplace=True)
                groups = groups.merge(groups.rename(columns={'agroup':'bgroup','bgroup':'agroup'}), on=['agroup','bgroup'], how='inner')
                groups = groups[groups['agroup'] < groups['bgroup']].rename(columns={'bgroup':'group','agroup':'new_group'})
                if len(groups) == 0:
                    raise RuntimeError("Error: Stuck in an endless loop while regrouping in MergeHaplotypes.")
                for p in ['a','b']:
                    duplications['new_group'] = duplications[[f'{p}group']].rename(columns={f'{p}group':'group'}).merge(groups, on=['group'], how='left')['new_group'].values
                    duplications.loc[np.isnan(duplications['new_group']) == False, f'{p}group'] = duplications.loc[np.isnan(duplications['new_group']) == False, 'new_group'].astype(int)
                duplications.drop(columns=['new_group'], inplace=True)
            else:
                break
        duplications['group'] = duplications['agroup']
        duplications.drop(columns=['agroup','bgroup'], inplace=True)
#
    if len(duplications):
        # Get minimum difference to another haplotype in group
        duplications = GetPositionsBeforeDuplication(duplications, scaffold_paths, ploidy, False)
        duplications['scaf_diff'] = (duplications['aprev_pos'] != duplications['apos'].shift(1)) | (duplications['bprev_pos'] != duplications['bpos'].shift(1))
        duplications['dist_diff'] = (duplications['scaf_diff'] == False) & (duplications['adist'] != duplications['bdist'])
        duplications.loc[duplications['apos'] == 0, 'scaf_diff'] = False
        differences = GetDuplicationDifferences(duplications)
#
        # Remove all except one version of haplotypes with no differences
        if len(differences):
            no_diff = differences[(differences['scaf_diff'] == 0) & (differences['dist_diff'] == 0)].copy()
            if len(no_diff):
                # Remove all except the one with the lowest pid
                for h in range(ploidy-1, -1, -1): # We need to go through in the inverse order, because we still need the main for the alternatives
                    rem_pid = np.unique(no_diff.loc[(no_diff['bpid'] > no_diff['apid']) & (no_diff['bhap'] == h), 'bpid'].values)
                    duplications = duplications[((np.isin(duplications['apid'], rem_pid) == False) | (duplications['ahap'] != h)) & ((np.isin(duplications['bpid'], rem_pid) == False) | (duplications['bhap'] != h))].copy()
                    rem_pid = np.isin(scaffold_paths['pid'], rem_pid)
                    if h==0:
                        scaffold_paths = RemoveMainPath(scaffold_paths, rem_pid, ploidy)
                        duplications = AssignLowestHaplotypeToMain(duplications)
                    else:
                        scaffold_paths = RemoveHaplotype(scaffold_paths, rem_pid, h)
                differences = GetDuplicationDifferences(duplications)
#
    if len(duplications):
        # Get bridge counts for different path/haplotypes to base decisions on it
        bsupp = duplications[['group','apid','ahap']].drop_duplicates()
        bsupp.rename(columns={'apid':'pid','ahap':'hap'}, inplace=True)
        bsupp = GetBridgeSupport(bsupp, scaffold_paths, scaf_bridges, ploidy)
#
        # Remove distance only variants with worst bridge support as long as we are above ploidy haplotypes
        while True:
            if len(duplications):
                groups = duplications.groupby(['group','apid','ahap']).size().groupby(['group']).size().reset_index(name='nhaps')
                rem_groups = groups[groups['nhaps'] > ploidy].copy()
            else:
                rem_groups = []
            if len(rem_groups) == 0:
                break
            else:
                # For each group find the path/haplotype with the lowest bridge support and remove it
                rem_dups = duplications[np.isin(duplications['did'], differences.loc[np.isin(differences['group'], rem_groups['group'].values) & (differences['scaf_diff'] == 0), 'did'].values)].copy()
                if len(rem_dups) == 0:
                    break
                else:
                    rem_haps = rem_dups[['group','apid','ahap']].drop_duplicates()
                    rem_haps.rename(columns={'apid':'pid','ahap':'hap'}, inplace=True)
                    scaffold_paths, duplications, differences = RemoveDuplicatedHaplotypesWithLowestSupport(scaffold_paths, duplications, rem_haps, bsupp, ploidy)
#
        # Remove variants that are identical to other haplotypes except of deletions(missing scaffolds) and distances as long as we are above ploidy haplotypes
        while True:
            if len(duplications):
                groups = duplications.groupby(['group','apid','ahap']).size().groupby(['group']).size().reset_index(name='nhaps')
                rem_groups = groups[groups['nhaps'] > ploidy].copy()
            else:
                rem_groups = []
            if len(rem_groups) == 0:
                break
            else:
                # For each group find the path/haplotype with the lowest bridge support and remove it
                rem_dups = duplications[np.isin(duplications['group'], rem_groups['group'].values)].copy()
                rem_dups['complete'] = (rem_dups['apos'].shift(1) == rem_dups['aprev_pos']) | (rem_dups['aprev_pos'] < 0)
                rem_dups['complete'] = rem_dups[['did']].merge(rem_dups.groupby(['did'])['complete'].min().reset_index(), on=['did'], how='left')['complete'].values
                rem_dups = rem_dups[rem_dups['complete']].copy()
                if len(rem_dups) == 0:
                    break
                else:
                    rem_haps = rem_dups[['group','apid','ahap']].drop_duplicates()
                    rem_haps.rename(columns={'apid':'pid','ahap':'hap'}, inplace=True)
                    scaffold_paths, duplications, differences = RemoveDuplicatedHaplotypesWithLowestSupport(scaffold_paths, duplications, rem_haps, bsupp, ploidy)
#
    if len(duplications) == 0:
        group_info = []
    else:
        # Merge all groups that do not have more than ploidy haplotypes
        groups = duplications.groupby(['group','apid','ahap']).size().groupby(['group']).size().reset_index(name='nhaps')
        groups = groups[groups['nhaps'] <= ploidy].copy()
        duplications = duplications[np.isin(duplications['group'], groups['group'].values)].copy()
        # Define insertion order by amount of bridge support (highest support == lowest new haplotype)
        if len(bsupp):
            bsupp = bsupp.merge(duplications[['group','apid','ahap']].drop_duplicates().rename(columns={'apid':'pid','ahap':'hap'}), on=['group','pid','hap'], how='inner')
            bsupp.sort_values(['group','bplace'], ascending=[True,False], inplace=True)
            for h in range(ploidy):
                groups[[f'pid{h}',f'hap{h}']] = groups[['group']].merge( bsupp.groupby(['group'])[['pid','hap']].last().reset_index(), on=['group'], how='left')[['pid','hap']].fillna(-1).values.astype(int)
                bsupp = bsupp[bsupp['group'].shift(-1) == bsupp['group']].copy()
        # Separate the individual haplotypes and update positions in duplications after removal of deletions
        haps = duplications[['group','apid','ahap']].drop_duplicates()
        haps.rename(columns={'apid':'pid','ahap':'hap'}, inplace=True)
        haps = GetHaplotypes(haps, scaffold_paths, ploidy)
        haps.sort_values(['group','pid','hap','pos'], inplace=True)
        haps['new_pos'] = haps.groupby(['group','pid','hap'], sort=False).cumcount()
        duplications['apos'] = duplications[['apid','apos','ahap']].rename(columns={'apid':'pid','apos':'pos','ahap':'hap'}).merge(haps[['pid','pos','hap','new_pos']], on=['pid','pos','hap'], how='left')['new_pos'].values
        duplications['bpos'] = duplications[['bpid','bpos','bhap']].rename(columns={'bpid':'pid','bpos':'pos','bhap':'hap'}).merge(haps[['pid','pos','hap','new_pos']], on=['pid','pos','hap'], how='left')['new_pos'].values
        duplications.drop(columns=['samedir','aprev_pos','adist','bprev_pos','bdist','scaf_diff','dist_diff'], inplace=True)
        haps['pos'] = haps['new_pos']
        haps.drop(columns={'new_pos'}, inplace=True)
        # Create new merged paths with most supported haplotype (so far only with the positions in haps)
        new_paths = groups[['group','pid0','hap0']].rename(columns={'pid0':'pid','hap0':'hap'})
        new_paths = new_paths.merge(haps[['pid','hap','pos']], on=['pid','hap'], how='left')
        new_paths.drop(columns=['hap'], inplace=True)
        plen = new_paths.groupby(['group'], sort=False).size().values
        new_paths['pid'] = np.repeat( np.arange(len(plen)) + scaffold_paths['pid'].max() + 1, plen )
        new_paths['pos0'] = new_paths['pos']
        for hadd in range(1, ploidy):
            new_paths[f'pos{hadd}'] = -1 # fill everything by default as a deletion and later replace the duplicated scaffolds with the correct position
            for hcomp in range(hadd):
                # Get all duplications relevant for this combination
                dups = groups[['group',f'pid{hcomp}',f'hap{hcomp}',f'pid{hadd}',f'hap{hadd}']].rename(columns={f'pid{hcomp}':'apid',f'hap{hcomp}':'ahap',f'pid{hadd}':'bpid',f'hap{hadd}':'bhap'}).merge( duplications, on=['group','apid','ahap','bpid','bhap'], how='left' )
                dups.drop(columns=['apid','ahap','bpid','bhap','did'], inplace=True)
                # Assing duplications to the place they belong (they might be different for different hcomp, but then we just take the last. For the polyploid case a better algorithm would be helpful, but the most important is that we do not introduce bugs, because a non-optimal alignment for the merging only increases path size, but no errors)
                new_paths[f'pos{hadd}'] = new_paths[['group',f'pos{hcomp}']].rename(columns={f'pos{hcomp}':'apos'}).merge(dups, on=['group','apos'], how='left')['bpos'].fillna(-1).values.astype(int)
            # If the order of the added positions is inverted, see if we can fix it
            while True:
                # Find conflicts
                new_paths['posmax'] =  new_paths.groupby(['group'])[f'pos{hadd}'].cummax()
                new_paths['conflict'] = (new_paths['posmax'] > new_paths[f'pos{hadd}']) & (0 <= new_paths[f'pos{hadd}'])
                if np.sum(new_paths['conflict']) == 0:
                    break
                else:
                    # Check if we can fix the problem
                    conflicts = new_paths.loc[new_paths['conflict'], ['group','pos',f'pos{hadd}']].rename(columns={f'pos{hadd}':'conpos'})
                    conflicts['oindex'] = conflicts.index.values
                    conflicts['cindex'] = conflicts.index.values-1
                    conflicts['nbefore'] = 0
                    conflicts[[f'in{h}' for h in range(hadd)]] = new_paths.loc[new_paths['conflict'], [f'pos{h}' for h in range(hadd)]].values >= 0
                    conflicts['done'] = False
                    conflicts['fixable'] = True
                    while np.sum(conflicts['done'] == False):
                        cons = new_paths.loc[conflicts['cindex'].values].copy()
                        cons['in'] = ((cons[[f'pos{h}' for h in range(hadd)]] >= 0) & (conflicts[[f'in{h}' for h in range(hadd)]].values)).any(axis=1)
                        conflicts.loc[cons['in'].values, 'nbefore'] += 1
                        conflicts.loc[cons['in'].values, [f'in{h}' for h in range(hadd)]] = conflicts.loc[cons['in'].values, [f'in{h}' for h in range(hadd)]] | (cons.loc[cons['in'], [f'pos{h}' for h in range(hadd)]] >= 0)
                        conflicts.loc[cons[f'pos{hadd}'].values >= 0, 'done'] = True
                        conflicts.loc[conflicts['done'] & cons['in'].values, 'fixable'] = False # The ordering in the previous haplotypes prevents a switch
                        conflicts.loc[conflicts['done'] & (cons[f'pos{hadd}'].values <= conflicts['conpos']), 'fixable'] = False # If the previous position is not the reason for the conflict, we need to fix that position first
                        conflicts = conflicts[conflicts['fixable']].copy()
                        conflicts.loc[conflicts['done'] == False, 'cindex'] -= 1
                    conflicts['newpos'] = new_paths.loc[conflicts['cindex'].values, 'pos'].values
                    # Remove conflicts that overlap with a previous conflict
                    conflicts = conflicts[ (conflicts['group'] != conflicts['group'].shift(1)) | (conflicts['newpos'] > conflicts['pos'].shift(1)) ].drop(columns=['conpos','done','fixable'])
                    # Fix the fixable conflicts
                    if len(conflicts) == 0:
                        break
                    else:
                        conflicts.rename(columns={'cindex':'sindex','oindex':'cindex','newpos':'pos_before','pos':'pos_after'}, inplace=True)
                        conflicts['pos_before'] += conflicts['nbefore']
                        conflicts[[f'in{h}' for h in range(hadd)]] = new_paths.loc[new_paths['conflict'], [f'pos{h}' for h in range(hadd)]].values >= 0
                        while len(conflicts):
                            cons = new_paths.loc[conflicts['cindex'].values].copy()
                            cons['in'] = ((cons[[f'pos{h}' for h in range(hadd)]] >= 0) & (conflicts[[f'in{h}' for h in range(hadd)]].values)).any(axis=1)
                            new_paths.loc[conflicts.loc[cons['in'].values, 'cindex'].values, 'pos'] = conflicts.loc[cons['in'].values, 'pos_before'].values
                            conflicts.loc[cons['in'].values, 'pos_before'] -= 1
                            new_paths.loc[conflicts.loc[cons['in'].values == False, 'cindex'].values, 'pos'] = conflicts.loc[cons['in'].values == False, 'pos_after'].values
                            conflicts.loc[cons['in'].values == False, 'pos_before'] -= 1
                            conflicts = conflicts[conflicts['cindex'] > conflicts['sindex']].copy()
                            conflicts['cindex'] -= 1
                        new_paths.sort_values(['group','pos'], inplace=True)
                        new_paths.reset_index(drop=True, inplace=True)
            # Remove the new positions that cannot be fixed
            while True:
                # Find conflicts (this time equal positions are also conflicts, they cannot be fixed by swapping, thus were ignored before)
                new_paths['posmax'] = new_paths.groupby(['group'])[f'pos{hadd}'].cummax().shift(1, fill_value=-1)
                new_paths.loc[new_paths['group'] != new_paths['group'].shift(1), 'posmax'] = -1
                new_paths['conflict'] = (new_paths['posmax'] >= new_paths[f'pos{hadd}']) & (0 <= new_paths[f'pos{hadd}'])
                if np.sum(new_paths['conflict']) == 0:
                    break
                else:
                    # Remove first conflict in each group
                    new_paths.loc[new_paths[['group', 'pos']].merge( new_paths.loc[new_paths['conflict'], ['group','pos']].groupby(['group'], sort=False).first().reset_index(), on=['group', 'pos'], how='left', indicator=True)['_merge'].values == "both", f'pos{hadd}'] = -1
            new_paths.drop(columns=['conflict'], inplace=True)
            # Add missing positions
            new_paths['posmax'] = new_paths.groupby(['group'])[f'pos{hadd}'].cummax().shift(1, fill_value=-1)
            new_paths.loc[new_paths['group'] != new_paths['group'].shift(1), 'posmax'] = -1
            new_paths['index'] = new_paths.index.values
            new_paths['repeat'] = new_paths[f'pos{hadd}'] - new_paths['posmax']
            new_paths.loc[new_paths[f'pos{hadd}'] < 0, 'repeat'] = 1
            new_paths.drop(columns=['posmax'], inplace=True)
            new_paths = new_paths.loc[np.repeat(new_paths.index.values, new_paths['repeat'].values)].copy()
            new_paths['ipos'] = new_paths.groupby(['index'], sort=False).cumcount()+1
            new_paths.loc[new_paths['ipos'] != new_paths['repeat'], [f'pos{h}' for h in range(hadd)]] = -1
            new_paths[f'pos{hadd}'] += new_paths['ipos'] - new_paths['repeat']
            new_paths['pos'] = new_paths.groupby(['group'], sort=False).cumcount()
            new_paths.reset_index(drop=True, inplace=True)
            new_paths.drop(columns=['index','repeat','ipos'], inplace=True)
        # Compress path by filling deletions
        while True:
            shifts = new_paths.loc[ ((new_paths[[f'pos{h}' for h in range(ploidy)]].values < 0) | (new_paths[[f'pos{h}' for h in range(ploidy)]].shift(1).values < 0)).all(axis=1) ].drop(columns=['pid','pos'])
            if len(shifts) == 0:
                break
            else:
                shifts['index'] = shifts.index.values
                shifts = shifts.groupby(['group'], sort=False).first() # We can only take the first in each group, because otherwise we might block the optimal solution
                shifts['new_index'] = shifts['index']-2
                while True:
                    further = ((new_paths.loc[shifts['index'].values, [f'pos{h}' for h in range(ploidy)]].values < 0) | (new_paths.loc[shifts['new_index'].values, [f'pos{h}' for h in range(ploidy)]].values < 0)).all(axis=1)
                    if np.sum(further) == 0: # We do not need to worry to go into another group, because the first position in every group must be a duplication for all included haplotypes, thus blocks continuation
                        break
                    else:
                        shifts.loc[further, 'new_index'] -= 1
                shifts['new_index'] += 1
                for h in range(ploidy):
                    cur_shifts = new_paths.loc[shifts['index'].values, f'pos{h}'].values >= 0
                    new_paths.loc[shifts.loc[cur_shifts, 'new_index'].values, f'pos{h}'] = new_paths.loc[shifts.loc[cur_shifts, 'index'].values, f'pos{h}'].values
                new_paths.drop(shifts['index'].values, inplace=True)
                new_paths['pos'] = new_paths.groupby(['group'], sort=False).cumcount()
                new_paths.reset_index(drop=True, inplace=True)
        # Insert scaffold information into new path
        new_paths[[f'{n}{h}' for h in range(ploidy) for n in ['pid','hap']]] = new_paths[['group']].merge(groups.drop(columns=['nhaps']), on=['group'], how='left')[[f'{n}{h}' for h in range(ploidy) for n in ['pid','hap']]].values
        group_info = new_paths.copy()
        for h in range(ploidy):
            new_paths[[f'phase{h}',f'scaf{h}',f'strand{h}',f'dist{h}']] = new_paths[[f'pid{h}',f'hap{h}',f'pos{h}']].rename(columns={f'{n}{h}':n for n in ['pid','hap','pos']}).merge(haps.drop(columns=['group']), on=['pid','hap','pos'], how='left')[['phase','scaf','strand','dist']].values
            while np.sum(new_paths[f'phase{h}'].isnull()):
                new_paths[f'phase{h}'] = np.where(new_paths[f'phase{h}'].isnull(), new_paths[f'phase{h}'].shift(-1), new_paths[f'phase{h}'])
            new_paths[[f'phase{h}']] = new_paths[[f'phase{h}']].astype(int)
            new_paths[[f'scaf{h}']] = new_paths[[f'scaf{h}']].fillna(-1).astype(int)
            new_paths[[f'strand{h}']] = new_paths[[f'strand{h}']].fillna('')
            new_paths[[f'dist{h}']] = new_paths[[f'dist{h}']].fillna(0).astype(int)
            new_paths.drop(columns=[f'pos{h}',f'pid{h}',f'hap{h}'], inplace=True)
        new_paths.drop(columns=['group'], inplace=True)
        new_paths = TrimAlternativesConsistentWithMain(new_paths, ploidy)
        # Remove the old version of the merged haplotypes from scaffold_path and add the new merged path
        haps = pd.concat([groups[[f'pid{h}',f'hap{h}']].rename(columns={f'pid{h}':'pid',f'hap{h}':'hap'}) for h in range(ploidy)], ignore_index=True)
        for h in range(ploidy-1, -1, -1): # We need to go through in the inverse order, because we still need the main for the alternatives
            rem_pid = np.isin(scaffold_paths['pid'], haps.loc[haps['hap'] == h, 'pid'].values)
            if h==0:
                scaffold_paths = RemoveMainPath(scaffold_paths, rem_pid, ploidy)
            else:
                scaffold_paths = RemoveHaplotype(scaffold_paths, rem_pid, h)
        scaffold_paths = pd.concat([scaffold_paths, new_paths], ignore_index=True)
        # Clean up at the end
        scaffold_paths = ShiftHaplotypesToLowestPossible(scaffold_paths, ploidy) # Do not do this earlier because it invalides haplotypes stored in duplications
        scaffold_paths = CompressPaths(scaffold_paths, ploidy)
#
    if len(ends_in):
        if len(group_info):
            group_info = group_info[['pid']+[f'pid{h}' for h in range(ploidy)]].drop_duplicates()
        return scaffold_paths, group_info
    else:
        return scaffold_paths

def RequireDuplicationAtPathEnd(duplications, scaf_len, mcols, patha_only=False):
    ends = duplications.groupby(mcols)[['apos','bpos']].agg(['min','max']).reset_index()
    for p in ['a'] if patha_only else ['a','b']:
        ends[f'{p}max'] = ends[[f'{p}pid']].droplevel(1,axis=1).rename(columns={f'{p}pid':'pid'}).merge(scaf_len, on=['pid'], how='left')['pos'].values
    ends = ends.loc[((ends['apos','min'] == 0) | (ends['apos','max'] == ends['amax'])) & (True if patha_only else ((ends['bpos','min'] == 0) | (ends['bpos','max'] == ends['bmax']))), mcols].droplevel(1,axis=1)
    duplications = duplications.merge(ends, on=mcols, how='inner')
#
    return duplications

def GetEndDuplicationsBelongTo(duplications, scaf_len):
    ends = duplications.groupby(['did','apid','ahap','bpid','bhap'])[['apos','bpos']].agg(['min','max','size']).reset_index()
    ends.columns = [col[0]+col[1] for col in ends.columns]
    ends.rename(columns={'aposmin':'amin','aposmax':'amax','bposmin':'bmin','bposmax':'bmax','bpossize':'matches'}, inplace=True)
    ends.drop(columns=['apossize'], inplace=True)
    for p in ['a','b']:
        ends[f'{p}len'] = ends[[f'{p}pid']].rename(columns={f'{p}pid':'pid'}).merge(scaf_len, on=['pid'], how='left')['pos'].values
        ends[f'{p}left'] = ends[f'{p}min'] == 0
        ends[f'{p}right'] = ends[f'{p}max'] == ends[f'{p}len']
#
    return ends

def GetDuplicatedPathEnds(scaffold_paths, ploidy):
    # Get duplications that contain a path end for both sides
    duplications = GetDuplications(scaffold_paths, ploidy)
    scaf_len = scaffold_paths.groupby(['pid'])['pos'].max().reset_index()
    duplications = RequireDuplicationAtPathEnd(duplications, scaf_len, ['apid','bpid'])
    # Check if they are on the same or opposite strand (alone it is useless, but it sets the requirements for the direction of change for the positions)
    duplications = AddStrandToDuplications(duplications, scaffold_paths, ploidy)
    duplications['samedir'] = duplications['astrand'] == duplications['bstrand']
    duplications.drop(columns=['astrand','bstrand'], inplace=True)
    # Extend the duplications with valid distances from each end
    duplications = SeparateDuplicationsByHaplotype(duplications, scaffold_paths, ploidy)
    ldups = ExtendDuplicationsFromEnd(duplications, scaffold_paths, 'l', scaf_len, ploidy)
    rdups = ExtendDuplicationsFromEnd(duplications, scaffold_paths, 'r', scaf_len, ploidy)
    rdups['did'] += 1 + ldups['did'].max()
    duplications = pd.concat([ldups,rdups], ignore_index=True)
#
    # Check at what end the duplications are
    ends = GetEndDuplicationsBelongTo(duplications, scaf_len)
    # Filter the duplications that are either at no end or at both ends (both ends means one of the paths is fully covered by the other, so no extension of that one is possible. We keep the shorter one as a separate scaffold, because it might belong to an alternative haplotype)
    ends = ends[(ends['aleft'] != ends['aright']) & (ends['bleft'] != ends['bright'])].copy()
    ends['aside'] = np.where(ends['aleft'], 'l', 'r')
    ends['bside'] = np.where(ends['bleft'], 'l', 'r')
    ends.drop(columns=['aleft','aright','bleft','bright'], inplace=True)
    duplications = duplications.merge(ends[['did']], on=['did'], how='inner')
    ends['samedir'] = duplications.groupby(['did'])['samedir'].first().values
#
    return ends

def SelectingHaplotypeAndApplyingSideToPath(cpath, ploidy, p):
    for h in range(1,ploidy):
        cur = (cpath[f'{p}hap'] == h) & (cpath[f'phase{h}'] > 0)
        cpath.loc[cur, ['scaf0','strand0','dist0']] = cpath.loc[cur, [f'scaf{h}',f'strand{h}',f'dist{h}']].values
    cpath.drop(columns=[f'{n}{h}' for h in range(1,ploidy) for n in ['phase','scaf','strand','dist']], inplace=True)
    cpath = cpath[cpath['scaf0'] >= 0].copy()
    cpath['pos'] = cpath.groupby(['pid'], sort=False).cumcount()
    cpath = ReverseScaffolds(cpath, cpath[f'{p}side'] == 'r', 1)
    cpath.rename(columns={'scaf0':'scaf','strand0':'strand','dist0':'dist'}, inplace=True)
    cpath.drop(columns=['phase0'], inplace=True)
#
    return cpath

def GetPathAFromEnds(ends, scaffold_paths, ploidy):
    patha = ends[['apid','ahap','aside']].drop_duplicates()
    patha.sort_values(['apid','ahap','aside'])
    patha['pid'] = np.arange(len(patha))
    ends['opid'] = ends[['apid','ahap','aside']].merge(patha, on=['apid','ahap','aside'], how='left')['pid'].values
    patha = patha.merge(scaffold_paths.rename(columns={'pid':'apid'}), on=['apid'], how='left')
    patha = SelectingHaplotypeAndApplyingSideToPath(patha, ploidy, 'a')
    patha.drop(columns=['apid','ahap','aside'], inplace=True)
#
    return ends, patha

def MatchOriginsToPathA(patha, graph_ext):
    # Find origins with full length matches. If we do not find one at the end go back along patha until we find a full length match.
    patha['strand'] = np.where(patha['strand'] == '+', '-', '+') # origin has the opposite direction of patha, so fix strand in patha here and during the comparison take shifted distances from origin to match them (the order/positions of scaffolds are identical though)
    patha_len = patha.groupby(['pid'])['pos'].max().reset_index(name='len')
    patha_len['len'] += 1
    missing_pids = np.unique(patha['pid'].values)
    cut = 0
    org_storage = []
    while len(missing_pids):
        cur_org = graph_ext['org'][['olength','scaf0','strand0']].reset_index().rename(columns={'index':'oindex','olength':'length','scaf0':'scaf','strand0':'strand'}).merge(patha.loc[(patha['pos'] == cut) & np.isin(patha['pid'], missing_pids), ['pid','scaf','strand']], on=['scaf','strand'], how='inner').drop(columns=['scaf','strand'])
        cur_org['length'] = np.minimum(cur_org['length'], cur_org[['pid']].merge(patha_len, on=['pid'], how='left')['len'].values-cut)
        cur_org['matches'] = 1
        for s in range(1, cur_org['length'].max()):
            comp = (cur_org['matches'] == s) & (cur_org['length'] > s)
            cur_org.loc[comp,'matches'] += (graph_ext['org'].loc[cur_org.loc[comp, 'oindex'].values, [f'oscaf{s}',f'ostrand{s}',f'odist{s-1}']].values == cur_org.loc[comp, ['pid']].merge(patha[patha['pos'] == s+cut], on=['pid'], how='left')[['scaf','strand','dist']].values).all(axis=1).astype(int)
        cur_org = cur_org[cur_org['matches'].values == cur_org['length'].values].drop(columns=['length','matches'])
        missing_pids = np.setdiff1d(missing_pids, np.unique(cur_org['pid'].values))
        cur_org['cut'] = cut
        cut += 1
        org_storage.append(cur_org)
    org_storage = pd.concat(org_storage, ignore_index=True)
    # Propagate the origin forward to the end again
    pairs = graph_ext['pairs'].copy()
    pairs[['nscaf','nstrand','ndist']] = graph_ext['ext'].loc[pairs['eindex'].values, ['scaf1','strand1','dist1']].values
    has_valid_ext = pairs.drop(columns=['eindex']).drop_duplicates()
    patha['dist'] = patha['dist'].shift(-1, fill_value=0) # Shift the distances additionally to the previous strand flip to get the reverse direction
    cur_org = []
    for cut in range(org_storage['cut'].max(), 0, -1):
        # Start from the origins with the longest cut and add every round the ones that need to be additionally included
        if len(cur_org) == 0:
            cur_org = org_storage[org_storage['cut'] == cut].drop(columns=['cut'])
        else:
            cur_org = pd.concat([cur_org, org_storage[org_storage['cut'] == cut].drop(columns=['cut'])], ignore_index=True)
        # Only keep cur_org that have an extension that match at least one position
        cur_org[['nscaf','nstrand','ndist']] = cur_org[['pid']].merge(patha[patha['pos'] == cut-1], on=['pid'], how='left')[['scaf','strand','dist']].values
        cur_org = cur_org.merge(has_valid_ext, on=['oindex','nscaf','nstrand','ndist'], how='inner')
        # Prepare next round by stepping one scaffold forward to also cover branches that do not reach up to the end
        cur_org = cur_org.rename(columns={'oindex':'oindex1'}).merge(graph_ext['ocont'], on=['oindex1','nscaf','nstrand','ndist'], how='inner').drop(columns=['oindex1','nscaf','nstrand','ndist']).rename(columns={'oindex2':'oindex'})
        cur_org.drop_duplicates(inplace=True)
    if len(cur_org):
        cur_org = pd.concat([org_storage[org_storage['cut'] == 0].drop(columns=['cut']), cur_org], ignore_index=True)
    else:
        cur_org = org_storage.drop(columns=['cut'])
    patha['strand'] = np.where(patha['strand'] == '+', '-', '+') # Revert the strand flip for the previous comparison
    patha['dist'] = patha['dist'].shift(1, fill_value=0) # And also shift the distances back to the correct position
#
    return cur_org

def GetPathBFromEnds(ends, scaffold_paths, ploidy):
    pathb = ends[['bpid','bhap','bside','bmin','bmax']].drop_duplicates()
    pathb['pid'] = np.arange(len(pathb)) # We need a pid that separates the different bhap and bside from the same bpid
    ends['epid'] = ends[['bpid','bhap','bside','bmin','bmax']].merge(pathb, on=['bpid','bhap','bside','bmin','bmax'], how='left')['pid'].values
    pathb = pathb.merge(scaffold_paths.rename(columns={'pid':'bpid'}), on=['bpid'], how='left').drop(columns=['bpid'])
    pathb['keep'] = (pathb['pos'] < pathb['bmin']) | (pathb['pos'] > pathb['bmax'])
    pathb.drop(columns=['bmin','bmax'], inplace=True)
    pathb = SelectingHaplotypeAndApplyingSideToPath(pathb, ploidy, 'b')
    pathb = pathb[pathb['keep']].drop(columns=['bhap','bside','keep']) # Only remove the overlapping scaffolds here, because we are interested in the distance at position 0
    pathb['pos'] = pathb.groupby(['pid'], sort=False).cumcount()
#
    return ends, pathb

def FilterInvalidConnections(ends, scaffold_paths, graph_ext, ploidy):
    if len(ends):
        # Find origins with full length matches. If we do not find one at the end go back along patha until we find a full length match.
        ends, patha = GetPathAFromEnds(ends, scaffold_paths, ploidy)
        cur_org = MatchOriginsToPathA(patha, graph_ext)
        # Get the path to which the extensions of the origins should match
        ends, pathb = GetPathBFromEnds(ends, scaffold_paths, ploidy)
        cur_org = cur_org.rename(columns={'pid':'opid'}).merge(ends[['opid','epid']].reset_index().rename(columns={'index':'endindex'}), on=['opid'], how='left')
        cur_org.drop(columns=['opid'], inplace=True)
        cur_org.rename(columns={'epid':'pid'}, inplace=True)
        # Get extensions that pair with the valid origins
        cur_org[['nscaf','nstrand','ndist']] = cur_org[['pid']].merge(pathb[pathb['pos'] == 0], on=['pid'], how='left')[['scaf','strand','dist']].values
        pairs = graph_ext['pairs'].copy()
        pairs[['nscaf','nstrand','ndist']] = graph_ext['ext'].loc[pairs['eindex'].values, ['scaf1','strand1','dist1']].values
        cur_ext = cur_org.merge(pairs, on=['oindex','nscaf','nstrand','ndist'], how='inner').drop(columns=['nscaf','nstrand','ndist']).drop_duplicates()
        valid_ends = []
        while len(cur_ext):
            # Check how long the extensions match pathb
            cur_ext['length'] = np.minimum(graph_ext['ext'].loc[cur_ext['eindex'].values, 'length'].values-1, cur_ext[['pid']].merge(pathb.groupby(['pid'])['pos'].max().reset_index(), on=['pid'], how='left')['pos'].values+1) # The +1 is because it is the max pos not one after it and the -1 for the extensions is because pathb has scaf0 from extension removed, so that the positions are shifted by one
            cur_ext['matches'] = 1
            for s in range(1, cur_ext['length'].max()):
                comp = (cur_ext['matches'] == s) & (cur_ext['length'] > s)
                cur_ext.loc[comp,'matches'] += (graph_ext['ext'].loc[cur_ext.loc[comp, 'eindex'].values, [f'scaf{s+1}',f'strand{s+1}',f'dist{s+1}']].values == cur_ext.loc[comp, ['pid']].merge(pathb[pathb['pos'] == s], on=['pid'], how='left')[['scaf','strand','dist']].values).all(axis=1).astype(int)
            # Handle valid ends, where we have a full length match between extension and pathb
            valid_ids = np.unique(cur_ext.loc[cur_ext['length'] == cur_ext['matches'], 'endindex'].values)
            valid_ends.append( ends.loc[valid_ids, ['apid','ahap','aside','bpid','bhap','bside']].copy() )
            cur_ext = cur_ext[np.isin(cur_ext['endindex'], valid_ids) == False].copy()
            cur_org = cur_org.merge(cur_ext[['oindex','endindex']].drop_duplicates(), on=['oindex','endindex'], how='inner') # Taking only the endindex still in cur_ext also filters the cur_org that do not have any matching extension with the allowed paths in end
            # Prepare next round by stepping one scaffold forward to also cover branches that do not reach up to the end
            pathb = pathb[pathb['pos'] > 0].copy()
            pathb['pos'] -= 1
            cur_org = cur_org.rename(columns={'oindex':'oindex1'}).merge(graph_ext['ocont'], on=['oindex1','nscaf','nstrand','ndist'], how='inner').drop(columns=['oindex1']).rename(columns={'oindex2':'oindex'})
            cur_org[['nscaf','nstrand','ndist']] = cur_org[['pid']].merge(pathb[pathb['pos'] == 0], on=['pid'], how='left')[['scaf','strand','dist']].values
            cur_org.drop_duplicates(inplace=True)
            cur_ext = cur_org.merge(pairs, on=['oindex','nscaf','nstrand','ndist'], how='inner').drop(columns=['nscaf','nstrand','ndist']).drop_duplicates()
        # Overwrite ends with valid_end
        if len(valid_ends):
            valid_ends = pd.concat(valid_ends, ignore_index=True)
            valid_ends.sort_values(['apid','bpid','ahap','bhap','aside','bside'], inplace=True)
            valid_ends = valid_ends.merge(valid_ends.rename(columns={'apid':'bpid','ahap':'bhap','aside':'bside','bpid':'apid','bhap':'ahap','bside':'aside'}), on=['apid','ahap','aside','bpid','bhap','bside'], how='outer', indicator=True)
            valid_ends.rename(columns={'_merge':'valid_path'}, inplace=True)
            valid_ends['valid_path'] = np.where(valid_ends['valid_path'] == "both", 'ab', np.where(valid_ends['valid_path'] == "left_only", 'a', 'b'))
            ends = ends.merge(valid_ends, on=[col for col in valid_ends.columns if col != 'valid_path'], how='inner').drop(columns=['opid','epid'])
        else:
            ends = []
#
    return ends

def SelectBestConnections(ends, scaffold_paths, scaffold_graph, ploidy):
    if len(ends):
        ends = ends[ends['valid_path'] == 'ab'].drop(columns=['valid_path'])
        # Find branching points in alternative patha
        branch_points = []
        extending = []
        ends, patha = GetPathAFromEnds(ends, scaffold_paths, ploidy)
        pairwise = ends[['opid','bpid','bside','bhap','matches']].copy()
        pairwise = pairwise.groupby(['opid','bpid','bside','bhap'])['matches'].max().reset_index()
        pairwise = pairwise.rename(columns={'opid':'opid1','matches':'matches1'}).merge(pairwise.rename(columns={'opid':'opid2','matches':'matches2'}), on=['bpid','bside','bhap'], how='inner').drop(columns=['bpid','bside','bhap'])
        pairwise = pairwise[pairwise['opid1'] != pairwise['opid2']].drop_duplicates()
        pairwise.rename(columns={'matches1':'pos1','matches2':'pos2'}, inplace=True)
        pairwise['min_pos'] = np.minimum(pairwise['pos1'], pairwise['pos2'])
        for p in [1,2]:
            pairwise[f'pos{p}'] -= pairwise['min_pos']
        pairwise.drop(columns=['min_pos'], inplace=True)
        patha_len = patha.groupby(['pid'])['pos'].max().reset_index(name='len')
        patha_len['len'] += 1
        for p in [1,2]:
            pairwise[f'len{p}'] = pairwise[[f'opid{p}']].rename(columns={f'opid{p}':'pid'}).merge(patha_len, on=['pid'], how='left')['len'].values
        while len(pairwise):
            pairwise['pos1'] += 1
            pairwise['pos2'] += 1
            extending.append( pairwise.loc[(pairwise['pos1'] >= pairwise['len1']) & (pairwise['pos2'] < pairwise['len2']), ['opid1','opid2']].drop_duplicates() )
            pairwise = pairwise[(pairwise['pos1'] < pairwise['len1']) & (pairwise['pos2'] < pairwise['len2'])].copy()
            if len(pairwise):
                pairwise['match'] = ( pairwise[['opid1','pos1']].rename(columns={'opid1':'pid','pos1':'pos'}).merge(patha, on=['pid','pos'], how='left')[['scaf','strand','dist']].values == pairwise[['opid2','pos2']].rename(columns={'opid2':'pid','pos2':'pos'}).merge(patha, on=['pid','pos'], how='left')[['scaf','strand','dist']].values ).all(axis=1)
                branch_points.append( pairwise.loc[pairwise['match'] == False, ['opid1','pos1']].drop_duplicates() )
                pairwise = pairwise[pairwise['match']].copy()
        if len(branch_points):
            branch_points = pd.concat(branch_points, ignore_index=True)
            branch_points = branch_points[branch_points['pos1'] < scaffold_graph['length'].max()-1].copy() # Branch points that cannot have a connection out of the path are irrelevant
        if len(extending):
            extending = pd.concat(extending, ignore_index=True)
        # Check from the branch_points beginning with the furthest if some ends are not directly supported by the scaffold_graph
        if len(branch_points):
            # Build vertical paths up to furthest branch_point
            branch_points.rename(columns={'opid1':'opid','pos1':'pos'}, inplace=True)
            branch_points.sort_values(['opid','pos'], ascending=[True,False], inplace=True)
            branch_points.drop_duplicates(inplace=True)
            vpatha = branch_points.groupby(['opid'], sort=False)['pos'].max().reset_index()
            vpatha['length'] = vpatha['pos'] + 1
            patha['strand'] = np.where(patha['strand'] == '+', '-', '+') # We need patha in the opposite direction
            patha['dist'] = patha['dist'].shift(-1, fill_value=0) # Shift the distances additionally to the previous strand flip to get the reverse direction
            vpatha[['scaf0','strand0']] = vpatha[['opid','pos']].rename(columns={'opid':'pid'}).merge(patha, on=['pid','pos'], how='left')[['scaf','strand']].values
            vpatha['pos'] -= 1
            s = 1
            while np.sum(vpatha['pos'] >= 0):
                vpatha[[f'scaf{s}',f'strand{s}',f'dist{s}']] = vpatha[['opid','pos']].rename(columns={'opid':'pid'}).merge(patha, on=['pid','pos'], how='left')[['scaf','strand','dist']].values
                vpatha['pos'] -= 1
                s += 1
            vpatha.drop(columns=['pos'], inplace=True)
            vpatha.rename(columns={'scaf0':'from','strand0':'from_side','length':'olen'}, inplace=True)
            vpatha['from_side'] = np.where(vpatha['from_side'] == '+', 'r', 'l')
            # Check from the branch_points beginning with the furthest if some ends are not directly supported by the scaffold_graph
            ends, pathb = GetPathBFromEnds(ends, scaffold_paths, ploidy)
            pathb_len = pathb.groupby(['pid'])['pos'].max().reset_index(name='len')
            pathb_len['len'] += 1
            while len(branch_points):
                # Get the extensions for each patha from scaffold_graph
                patha_ext = []
                for l in np.unique(vpatha['olen'].values):
                    mcols = ['from','from_side'] + [f'{n}{s}' for s in range(1,l) for n in ['scaf','strand','dist']]
                    cur_ext = vpatha.loc[vpatha['olen'] == l, ['opid']+mcols].merge(scaffold_graph.loc[scaffold_graph['length'] > l], on=mcols, how='inner').drop(columns=mcols)
                    if len(cur_ext):
                        cur_ext = RemoveEmptyColumns(cur_ext)
                        cur_ext.rename(columns={f'{n}{s}':f'{n}{s-l}' for s in range(l,cur_ext['length'].max()) for n in ['scaf','strand','dist']}, inplace=True)
                        cur_ext['length'] -= l
                        patha_ext.append( cur_ext )
                if len(patha_ext):
                    patha_ext = pd.concat(patha_ext, ignore_index=True)
                # Check if pathb matches at least one of the extensions
                if len(patha_ext):
                    # Compare pathb with extensions
                    pairs = ends[['opid','epid']].reset_index().rename(columns={'index':'endindex'}).merge(patha_ext[['opid','length']].reset_index().rename(columns={'index':'eindex'}), on='opid', how='inner')
                    if len(pairs):
                        pairs['length'] = np.minimum(pairs['length'], pairs[['epid']].merge(pathb_len, left_on='epid', right_on='pid', how='left')['len'].values)
                        pairs['match'] = True
                        for s in range(pairs['length'].max()):
                            cur = (pairs['length'] > s) & pairs['match']
                            pairs.loc[cur, 'match'] = ( pairs.loc[cur, ['epid']].rename(columns={'epid':'pid'}).merge(pathb[pathb['pos'] == s], on=['pid'], how='left')[['scaf','strand','dist']].values == patha_ext.loc[pairs.loc[cur, 'eindex'].values, [f'scaf{s}',f'strand{s}',f'dist{s}']].values ).all(axis=1)
                        # Remove ends without a matching pairing
                        pairs = pairs.groupby(['endindex'])['match'].max().reset_index()
                        pairs = pairs[pairs['match'] == False].drop(columns=['match'])
                        ends.drop(pairs['endindex'].values, inplace=True)
                # Remove handled branch_points and go to next ones
                branch_points['trim'] = np.where(branch_points['opid'] == branch_points['opid'].shift(1), branch_points['pos'].shift(1, fill_value=0) - branch_points['pos'], -1)
                branch_points = branch_points[branch_points['trim'] > 0].copy()
                vpatha = vpatha.merge(branch_points.groupby(['opid'], sort=False).first().reset_index()[['opid','trim']], on='opid', how='inner')
                if len(vpatha):
                    vpatha.drop(columns=['from','from_side'], inplace=True)
                    vpatha = pd.concat([ vpatha[vpatha['trim'] == t].drop(columns=[f'{n}{s}' for s in range(1,t) for n in ['scaf','strand','dist']]).rename(columns={f'{n}{s}':f'{n}{s-t}' for s in range(t,vpatha['olen'].max()) for n in ['scaf','strand','dist']}) for t in np.unique(vpatha['trim'].values)], ignore_index=True)
                    vpatha['olen'] -= vpatha['trim']
                    vpatha.drop(columns=['trim','dist0'], inplace=True)
                    vpatha = RemoveEmptyColumns(vpatha)
                    vpatha.rename(columns={'scaf0':'from','strand0':'from_side'}, inplace=True)
                    vpatha['from_side'] = np.where(vpatha['from_side'] == '+', 'r', 'l')
            ends.drop(columns=['epid'], inplace=True)
        ends.drop(columns=['opid'], inplace=True)
        # Require ends to be valid from both sides
        ends = ends.merge(ends[['apid','ahap','aside','bpid','bhap','bside']].rename(columns={'apid':'bpid','ahap':'bhap','aside':'bside','bpid':'apid','bhap':'ahap','bside':'aside'}), on=['apid','ahap','aside','bpid','bhap','bside'], how='inner')
        # Take the ones that support more haplotypes
        support = ends[['apid','aside','bpid','bside','bhap']].drop_duplicates().groupby(['bpid','bside','apid','aside']).size().reset_index(name='bhaps')
        support['max_supp'] = support[['bpid','bside']].merge(support.groupby(['bpid','bside'], sort=False)['bhaps'].max().reset_index(), on=['bpid','bside'], how='left')['bhaps'].values
        support = support[support['bhaps'] == support['max_supp']].drop(columns=['bhaps','max_supp'])
        ends = ends.merge(support, on=['apid','aside','bpid','bside'], how='inner')
        ends = ends.merge(ends[['apid','ahap','aside','bpid','bhap','bside']].rename(columns={'apid':'bpid','ahap':'bhap','aside':'bside','bpid':'apid','bhap':'ahap','bside':'aside'}), on=['apid','ahap','aside','bpid','bhap','bside'], how='inner')
#
    return ends

def GetDeduplicatedHaplotypesAtPathEnds(path_sides, scaffold_paths, scaffold_graph, ploidy):
    # Reverse the scaffolds, where we are interested on the right side, so that the position 0 is always the first on the interesting side
    cur_paths = scaffold_paths.merge(path_sides, on=['pid'], how='inner')
    rcur = cur_paths[cur_paths['side'] == 'r'].copy()
    rcur['reverse'] = True
    cur_paths = [ cur_paths[cur_paths['side'] == 'l'] ]
    for m in np.unique(rcur['matches']):
        cur_paths.append( ReverseScaffolds(rcur[rcur['matches'] == m].copy(), rcur.loc[rcur['matches'] == m, 'reverse'].values, ploidy).drop(columns=['reverse']) )
    cur_paths = pd.concat(cur_paths, ignore_index=True)
    cur_paths.sort_values(['pid','side','matches','pos'], inplace=True)
    # Get the deduplicated haplotypes
    haps = []
    dedup_haps = [ path_sides ] # The main paths will never be removed through deduplication
    dedup_haps[-1]['hap'] = 0
    for h in range(ploidy):
        # Get all haplotypes that differ from the main (and the main) without any deletions
        cur = cur_paths[np.isin(cur_paths['pid'], np.unique(cur_paths.loc[cur_paths[f'phase{h}'] > 0, 'pid'].values))].copy()
        if np.sum(cur[f'phase{h}'] < 0):
            cur.loc[cur[f'phase{h}'] < 0, [f'scaf{h}',f'strand{h}',f'dist{h}']] = cur.loc[cur[f'phase{h}'] < 0, ['scaf0','strand0','dist0']].values
        cur = cur.loc[cur[f'scaf{h}'] >= 0, ['pid','side','matches','pos',f'scaf{h}',f'strand{h}',f'dist{h}']].rename(columns={f'scaf{h}':'scaf',f'strand{h}':'strand',f'dist{h}':'dist'})
        cur['pos'] = cur.groupby(['pid','side','matches'], sort=False).cumcount()
        cur['hap'] = h
        # Compare to with all previous haplotypes if it differs
        if h == 0:
            haps.append(cur) # The main does not have any to compare
        else:
            cur_dedups = cur[['pid','side','matches','hap']].drop_duplicates()
            for hap in haps:
                # Get the first position from where it differs to the currently compared haplotype
                cur[['cscaf','cstrand','cdist']] = cur[['pid','side','matches','pos']].merge(hap, on=['pid','side','matches','pos'], how='left')[['scaf','strand','dist']].values
                cur['diff'] = np.isnan(cur['cscaf']) | (cur[['cscaf','cstrand','cdist']].values != cur[['scaf','strand','dist']].values).any(axis=1)
                cur['diff'] = cur.groupby(['pid','side','matches'], sort=False)['diff'].cummax()
                # Also get it the other way round
                chap = hap.merge(cur[['pid']].drop_duplicates(), on=['pid'], how='inner')
                chap[['cscaf','cstrand','cdist']] = chap[['pid','side','matches','pos']].merge(cur, on=['pid','side','matches','pos'], how='left')[['scaf','strand','dist']].values
                chap['diff'] = np.isnan(chap['cscaf']) | (chap[['cscaf','cstrand','cdist']].values != chap[['scaf','strand','dist']].values).any(axis=1)
                chap['diff'] = chap.groupby(['pid','side','matches'], sort=False)['diff'].cummax()
                # Do the next tests on both haplotypes combined and if one of them passes all the two haplotypes are no duplicates
                chap = pd.concat([chap,cur], ignore_index=True)
                # Keep paths up to first difference
                chap['diff'] = chap['diff'] & (chap['diff'].shift(1) | (chap[['pid','side','matches','hap']] != chap[['pid','side','matches','hap']].shift(1)).any(axis=1))
                chap = chap[chap['diff'] == False].drop(columns=['diff'])
                # Make it a vertical paths starting from the highest position
                vpaths = chap.groupby(['pid','side','matches','hap'])['pos'].max().reset_index()
                vpaths['length'] = 0
                s = 0
                while vpaths['length'].max() == s:
                    vpaths[[f'scaf{s}',f'strand{s}',f'dist{s+1}']] = vpaths[['pid','side','matches','hap','pos']].merge(chap, on=['pid','side','matches','hap','pos'], how='left')[['scaf','strand','dist']].values
                    if np.sum(np.isnan(vpaths[f'scaf{s}']) == False):
                        vpaths.loc[np.isnan(vpaths[f'scaf{s}']) == False, 'length'] += 1
                        vpaths.loc[np.isnan(vpaths[f'scaf{s}']) == False, f'strand{s}'] = np.where(vpaths.loc[np.isnan(vpaths[f'scaf{s}']) == False, f'strand{s}'] == '+', '-', '+') # We go in reverse order so we have to flip the strand
                    vpaths['pos'] -= 1
                    s += 1
                # Check scaffold_graph to see if the haplotype extends from its first unique position over the end
                vpaths = vpaths[vpaths['length'] < scaffold_graph['length'].max()].drop(columns=['pos']) # If a path is as long or longer than any in scaffold_graph, scaffold_graph cannot extend on this path
                vpaths.rename(columns={'scaf0':'from','strand0':'from_side'}, inplace=True)
                vpaths['from_side'] = np.where(vpaths['from_side'] == '+', 'r', 'l')
                vpaths['slen'] = -1
                for l in range(2,vpaths['length'].max()+1):
                    vpaths['slen2'] = np.nan
                    mcols = ['from','from_side']+[f'{n}{s}' for s in range(1,l) for n in ['scaf','strand','dist']]
                    vpaths.loc[vpaths['length'] >= l, 'slen2'] = vpaths.loc[vpaths['length'] >= l, mcols].merge(scaffold_graph.groupby(mcols)['length'].max().reset_index(), on=mcols, how='left')['length'].values
                    vpaths.loc[np.isnan(vpaths['slen2']) == False, 'slen'] = vpaths.loc[np.isnan(vpaths['slen2']) == False, 'slen2']
                cur_dedups = cur_dedups.merge(vpaths.loc[vpaths['length'] < np.maximum(vpaths['slen'],vpaths['matches']+1), ['pid','side','matches']].drop_duplicates(), on=['pid','side','matches'], how='inner')
            # Store the haplotypes that are different to all other haplotypes
            dedup_haps.append(cur_dedups)
            # Store the haplotype to compare it to the next haplotypes in the loop
            haps.append(cur.drop(columns=['cscaf','cstrand','cdist','diff']))
    dedup_haps = pd.concat(dedup_haps, ignore_index=True)
#
    return dedup_haps

def RemoveHaplotypes(scaffold_paths, delete_haps, ploidy):
    for h in range(ploidy-1, -1, -1): # We need to go through in the inverse order, because we still need the main for the alternatives
        remove = np.isin(scaffold_paths['pid'], delete_haps.loc[delete_haps['hap'] == h, 'pid'].values)
        if np.sum(remove):
            if h==0:
                scaffold_paths = RemoveMainPath(scaffold_paths, remove, ploidy)
            else:
                scaffold_paths = RemoveHaplotype(scaffold_paths, remove, h)
#
    return scaffold_paths

def GetNumberOfHaplotypes(scaffold_paths, ploidy):
    return (((scaffold_paths.groupby(['pid'])[[f'phase{h}' for h in range(ploidy)]].max() >= 0)*[h for h in range(ploidy)]).max(axis=1)+1).reset_index(name='nhaps')

def SetDistanceAtFirstPositionToZero(scaffold_paths, ploidy):
    scaffold_paths.loc[scaffold_paths['pos'] == 0, [f'dist{h}' for h in range(ploidy)]] = 0
    # Handle first positions that are not at zero due to deletions in that haplotype
    del_start = []
    for h in range(ploidy):
        del_start.append(scaffold_paths.loc[(scaffold_paths['pos'] == 0) & ( ((scaffold_paths[f'scaf{h}'] < 0) & (scaffold_paths[f'phase{h}'] > 0)) |
                                                                             ((scaffold_paths['scaf0'] < 0) & (scaffold_paths[f'phase{h}'] < 0)) ), ['pid','pos']].copy())
        del_start[-1]['hap'] = h
    del_start = pd.concat(del_start, ignore_index=True)
    del_start['pos'] = 1
    while len(del_start):
        new_start = []
        for h in range(ploidy):
            scaffold_paths['hap'] = scaffold_paths[['pid','pos']].merge(del_start[del_start['hap'] == h], on=['pid','pos'], how='left')['hap'].values
            if h == 0:
                # When we set the main paths to zero, first store it in all the ones that are equal to the main paths
                for h2 in range(1,ploidy):
                    cur = (np.isnan(scaffold_paths['hap']) == False) & (scaffold_paths[f'phase{h2}'] < 0)
                    if np.sum(cur):
                        scaffold_paths.loc[cur, [f'scaf{h2}',f'strand{h2}',f'dist{h2}']] = scaffold_paths.loc[cur, ['scaf0','strand0','dist0']].values
                        scaffold_paths.loc[cur, f'phase{h2}'] = -scaffold_paths.loc[cur, f'phase{h2}']
            scaffold_paths.loc[np.isnan(scaffold_paths['hap']) == False, f'dist{h}'] = 0
            new_start.append( scaffold_paths.loc[(np.isnan(scaffold_paths['hap']) == False) & ( ((scaffold_paths[f'scaf{h}'] < 0) & (scaffold_paths[f'phase{h}'] > 0)) |
                                                                                                ((scaffold_paths['scaf0'] < 0) & (scaffold_paths[f'phase{h}'] < 0)) ), ['pid','pos','hap']].copy() )
        scaffold_paths.drop(columns=['hap'], inplace=True)
        del_start = pd.concat(new_start, ignore_index=True)
        del_start['pos'] += 1
    # Clean up    
    scaffold_paths = TrimAlternativesConsistentWithMain(scaffold_paths, ploidy)
#
    return scaffold_paths

def TurnHorizontalHaplotypeIntoVertical(haps):
    vhaps = haps[['pid']].drop_duplicates()
    vhaps['length'] = 0
    if len(haps):
        for p in range(haps['pos'].max()+1):
            vhaps[[f'scaf{p}',f'strand{p}',f'dist{p}']] = vhaps[['pid']].merge(haps[haps['pos'] == p], on=['pid'], how='left')[['scaf','strand','dist']].values
            vhaps[[f'scaf{p}',f'dist{p}']] = vhaps[[f'scaf{p}',f'dist{p}']].astype(float)
            vhaps.loc[np.isnan(vhaps[f'scaf{p}']) == False, 'length'] += 1
        vhaps.drop(columns=['dist0'], inplace=True)
#
    return vhaps

def FindInvalidOverlaps(ends, scaffold_paths, ploidy):
    # Check first position, where distance does not matter
    for p in ['a','b']:
        cur = (ends[f'{p}side'] == 'r') == (p == 'a')
        ends[f'{p}pos'] = np.where(cur, ends[f'{p}min'], ends[f'{p}max'])
        ends[f'{p}dir'] = np.where(cur, 1, -1)
        ends = GetPositionFromPaths(ends, scaffold_paths, ploidy, f'{p}scaf', 'scaf', f'{p}pid', f'{p}pos', f'{p}hap')
        ends = GetPositionFromPaths(ends, scaffold_paths, ploidy, f'{p}strand', 'strand', f'{p}pid', f'{p}pos', f'{p}hap')
    ends['valid_overlap'] = (ends['ascaf'] == ends['bscaf']) & ((ends['astrand'] == ends['bstrand']) == ends['samedir'])
    # Check following positions
    ends['unfinished'] = True
    switch_cols = {'apid':'bpid','ahap':'bhap','bpid':'apid','bhap':'ahap'}
    while np.sum(ends['unfinished']):
        for p in ['a','b']:
            ends.rename(columns=switch_cols, inplace=True)
            ends['mpos'] = ends[f'{p}pos']
            ends['dir'] = ends[f'{p}dir']
            ends = GetFullNextPositionInPathB(ends, scaffold_paths, ploidy)
            ends[f'{p}pos'] = ends['mpos']
            ends[f'{p}scaf'] = ends['next_scaf']
            ends[f'{p}strand'] = ends['next_strand']
            ends[f'{p}dist'] = ends['next_dist']
            ends[f'{p}unfinished'] = (ends[f'{p}min'] <= ends[f'{p}pos']) & (ends[f'{p}pos'] <= ends[f'{p}max'])
        ends['unfinished'] = ends['aunfinished'] | ends['bunfinished']
        ends.loc[ends['aunfinished'] != ends['bunfinished'], 'valid_overlap'] = False
        ends.loc[ends['unfinished'], 'valid_overlap'] = ends.loc[ends['unfinished'], 'valid_overlap'] & (ends.loc[ends['unfinished'], ['ascaf','astrand','adist']].values == ends.loc[ends['unfinished'], ['bscaf','bstrand','bdist']].values).all(axis=1)
    ends.drop(columns=['apos','adir','ascaf','astrand','aunfinished','bpos','bdir','bscaf','bstrand','bunfinished','unfinished','mpos','dir','opos','next_scaf','next_strand','dist_pos','next_dist','adist','bdist'], inplace=True)
    # If the haplotype is identical to a lower haplotype at the overlap, ignore this test (to be able to combine non-haploid overlaps)
    pids = pd.concat([ends.loc[ends['valid_overlap'] == False, ['apid','amin','amax']].rename(columns={'apid':'pid','amin':'min','amax':'max'}), ends.loc[ends['valid_overlap'] == False, ['bpid','bmin','bmax']].rename(columns={'bpid':'pid','bmin':'min','bmax':'max'})], ignore_index=True).drop_duplicates()
    pids = scaffold_paths.merge(pids, on=['pid'], how='inner')
    pids = pids[(pids['min'] <= pids['pos']) & (pids['pos'] <= pids['max'])].drop(columns=['min','max'])
    pids['pos'] = pids.groupby(['pid']).cumcount()
    pids = SetDistanceAtFirstPositionToZero(pids, ploidy)
    haps = []
    dedup_haps = []
    for h in range(1,ploidy):
        cur = pids.groupby(['pid'])[f'phase{h}'].max()
        dedup_haps.append(cur[cur < 0].reset_index()[['pid']])
        dedup_haps[-1]['hap'] = h
        cur = pids[np.isin(pids['pid'], cur[cur > 0].reset_index()['pid'].values)].copy()
        cur.loc[cur[f'phase{h}'] < 0, [f'scaf{h}',f'strand{h}',f'dist{h}']] = cur.loc[cur[f'phase{h}'] < 0, ['scaf0','strand0','dist0']].values
        cur = cur.loc[cur[f'scaf{h}'] >= 0, ['pid','pos',f'scaf{h}',f'strand{h}',f'dist{h}']].rename(columns={f'scaf{h}':'scaf',f'strand{h}':'strand',f'dist{h}':'dist'})
        cur['pos'] = cur.groupby(['pid']).cumcount()
        new_haps = TurnHorizontalHaplotypeIntoVertical(cur)
        if len(haps):
            cols = list(np.intersect1d(new_haps.columns, haps.columns))
            new_haps['dups'] = new_haps[cols].merge(haps[cols], on=cols, how='left', indicator=True)['_merge'].values == "both"
            dedup_haps.append(new_haps.loc[new_haps['dups'], ['pid']].copy())
            dedup_haps[-1]['hap'] = h
            new_haps = new_haps[new_haps['dups'] == False].drop(columns=['dups'])
            haps = pd.concat([haps, new_haps], ignore_index=True)
        else:
            haps = new_haps
    dedup_haps = pd.concat(dedup_haps, ignore_index=True)
    ends['dup_hap'] = ''
    for p in ['a','b']:
        ends.loc[ends[[f'{p}pid',f'{p}hap']].rename(columns={f'{p}pid':'pid',f'{p}hap':'hap'}).merge(dedup_haps, on=['pid','hap'], how='left', indicator=True)['_merge'].values == "both", 'dup_hap'] += p
#
    return ends

def SwitchHaplotypesToConnectPerfectMatches(scaffold_paths, connected_ends, scaffold_graph, ploidy):
    # We only need to check entries with multiple haplotypes for both paths, otherwise nothing can be switched anyways
    valid_matches = connected_ends.copy()
    valid_matches['anhaps'] = valid_matches[['apid','bpid']].merge(valid_matches[['apid','bpid','ahap']].drop_duplicates().groupby(['apid','bpid']).size().reset_index(name='nhaps'), on=['apid','bpid'], how='left')['nhaps'].values
    valid_matches['bnhaps']  = valid_matches[['apid','bpid']].merge(valid_matches[['apid','bpid','bhap']].drop_duplicates().groupby(['apid','bpid']).size().reset_index(name='nhaps'), on=['apid','bpid'], how='left')['nhaps'].values
    valid_matches = valid_matches[ (valid_matches['anhaps'] > 1) & (valid_matches['bnhaps'] > 1)].copy()
    # Check which of them are perfect_matches
    perfect_matches = valid_matches.copy()
    perfect_matches['matches'] = perfect_matches['amax'] - perfect_matches['amin'] + 1
    long_matches = perfect_matches.loc[perfect_matches['matches'] > 1, ['apid','ahap','amin','amax','matches']].drop_duplicates()
    if len(long_matches):
        long_matches = long_matches.loc[np.repeat(long_matches.index.values, long_matches['matches'].values)].reset_index(drop=True)
        long_matches.drop(columns=['matches'], inplace=True)
        long_matches['pos'] = long_matches.groupby(['apid','ahap','amin','amax']).cumcount() + long_matches['amin']
        long_matches = GetPositionFromPaths(long_matches, scaffold_paths, ploidy, 'scaf', 'scaf', 'apid', 'pos', 'ahap')
        long_matches = long_matches[long_matches['scaf'] != 1].copy()
        long_matches = long_matches.groupby(['apid','ahap','amin','amax']).size().reset_index(name='matches')
        perfect_matches.loc[perfect_matches['matches'] > 1, 'matches'] = perfect_matches.loc[perfect_matches['matches'] > 1, ['apid','ahap','amin','amax']].merge(long_matches, on=['apid','ahap','amin','amax'], how='left')['matches'].values
    perfect_matches = SelectBestConnections(perfect_matches, scaffold_paths, scaffold_graph, ploidy)
    # Only keep the apath, bpath constellation of the merge not the opposite (we only needed it up to here for SelectBestConnections)
    perfect_matches = perfect_matches.loc[perfect_matches['aside'] == 'r', ['apid','bpid','ahap','bhap','anhaps','bnhaps']].copy()
    valid_matches = valid_matches.loc[valid_matches['aside'] == 'r', ['apid','bpid','ahap','bhap']].copy()
    # If all haplotypes are already perfect, we do not need to do anything
    perfect_matches['nhaps'] = np.minimum(perfect_matches['anhaps'], perfect_matches['bnhaps'])
    perfect_matches['nperfect'] = perfect_matches[['apid','bpid']].merge( perfect_matches[perfect_matches['ahap'] == perfect_matches['bhap']].groupby(['apid','bpid']).size().reset_index(name='nperfect'), on=['apid','bpid'], how='left')['nperfect'].fillna(0).astype(int).values
    perfect_matches = perfect_matches[perfect_matches['nperfect'] < perfect_matches['nhaps']].copy()
    # Check which haplotypes are valid to switch
    valid_matches = valid_matches.merge(perfect_matches[['apid','bpid']].drop_duplicates(), on=['apid','bpid'], how='inner').drop_duplicates()
    switchable = valid_matches[valid_matches['ahap'] != valid_matches['bhap']].drop(columns=['apid'])
    switchable = switchable.merge(switchable.rename(columns={'ahap':'bhap','bhap':'ahap'}), on=['bpid','ahap','bhap'], how='inner')
    switchable = switchable[switchable['ahap'] > switchable['bhap']].rename(columns={'ahap':'to_hap','bhap':'from_hap'}) # We only need them in one direction
    # Switch haplotypes if this results in more perfect_matches, but does not make any match invalid
    new_haps = valid_matches[['bpid','bhap']].drop_duplicates()
    new_haps['new_bhap'] = new_haps['bhap']
    new_haps['ahap'] = new_haps['bhap']
    while len(new_haps):
        # Check for all valid switches if this improves the amount of perfect matches
        switches = switchable[np.isin(switchable['bpid'], new_haps['bpid'].values)].copy()
        switches['to_ahap'] = switches[['bpid','to_hap']].rename(columns={'to_hap':'new_bhap'}).merge(new_haps[['bpid','new_bhap','ahap']], on=['bpid','new_bhap'], how='left')['ahap'].values
        switches['from_ahap'] = switches[['bpid','from_hap']].rename(columns={'from_hap':'new_bhap'}).merge(new_haps[['bpid','new_bhap','ahap']], on=['bpid','new_bhap'], how='left')['ahap'].values
        switches['improvement'] = (switches[['bpid','from_ahap','to_hap']].rename(columns={'from_ahap':'ahap','to_hap':'bhap'}).merge(perfect_matches[['bpid','ahap','bhap']], on=['bpid','ahap','bhap'], how='left', indicator=True)['_merge'].values == "both").astype(int)
        switches['improvement'] += switches[['bpid','from_hap','to_ahap']].rename(columns={'from_hap':'bhap','to_ahap':'ahap'}).merge(perfect_matches[['bpid','ahap','bhap']], on=['bpid','ahap','bhap'], how='left', indicator=True)['_merge'].values == "both"
        switches['improvement'] -= switches[['bpid','from_hap','from_ahap']].rename(columns={'from_hap':'bhap','from_ahap':'ahap'}).merge(perfect_matches[['bpid','ahap','bhap']], on=['bpid','ahap','bhap'], how='left', indicator=True)['_merge'].values == "both"
        switches['improvement'] -= switches[['bpid','to_hap','to_ahap']].rename(columns={'to_hap':'bhap','to_ahap':'ahap'}).merge(perfect_matches[['bpid','ahap','bhap']], on=['bpid','ahap','bhap'], how='left', indicator=True)['_merge'].values == "both"
        # Only one change per path per round to avoid conflicts (The one with the largest improvement)
        switches.drop(columns=['to_ahap','from_ahap'], inplace=True)
        switches.sort_values(['bpid','to_hap','from_hap','improvement'], ascending=[True,True,True,False], inplace=True)
        switches = switches.groupby(['bpid']).first().reset_index()
        # Apply changes to haplotypes that cannot be improved anymore
        new_haps = new_haps.merge(switches, on='bpid', how='left')
        ap_switches = new_haps.loc[(new_haps['improvement'] <= 0) & (new_haps['bhap'] < new_haps['new_bhap']), ['bpid','bhap','new_bhap']].rename(columns={'bpid':'pid','bhap':'hap1','new_bhap':'hap2'})
        if len(ap_switches):
            scaffold_paths = SwitchHaplotypes(scaffold_paths, ap_switches, ploidy)
        # Mark switch where it leads to an improvement
        new_haps = new_haps[new_haps['improvement'] > 0].drop(columns=['improvement'])
        cur_switch = (new_haps['new_bhap'] == new_haps['to_hap']) | (new_haps['new_bhap'] == new_haps['from_hap'])
        new_haps.loc[cur_switch, 'new_bhap'] = np.where(new_haps.loc[cur_switch, 'new_bhap'] == new_haps.loc[cur_switch, 'to_hap'], new_haps.loc[cur_switch, 'from_hap'], new_haps.loc[cur_switch, 'to_hap'])
        new_haps.drop(columns=['to_hap','from_hap'], inplace=True)
#
    return scaffold_paths

def SetConnectablePathsInMetaScaffold(meta_scaffolds, ends, connectable):
    meta_scaffolds['connectable'] = meta_scaffolds['connectable'] | (meta_scaffolds[['new_pid','bpid']].rename(columns={'new_pid':'apid'}).merge(connectable[['apid','bpid']], on=['apid','bpid'], how='left', indicator=True)['_merge'].values == "both")
    ends = ends[ends[['apid','bpid']].merge(connectable[['apid','bpid']], on=['apid','bpid'], how='left', indicator=True)['_merge'].values == "left_only"].copy()
#
    return meta_scaffolds, ends

def FillHaplotypes(scaffold_paths, fill, ploidy):
    for h in range(1,ploidy):
        need_fill = fill & (scaffold_paths[f'phase{h}'] < 0)
        scaffold_paths.loc[need_fill, f'phase{h}'] = -1*scaffold_paths.loc[need_fill, f'phase{h}']
        scaffold_paths.loc[need_fill, [f'scaf{h}',f'strand{h}',f'dist{h}']] = scaffold_paths.loc[need_fill, ['scaf0','strand0','dist0']].values
#
    return scaffold_paths

def SwitchHaplotypes(scaffold_paths, switches, ploidy):
     # Guarantee that all haplotypes are later still present
    missing = switches[['pid','hap2']].drop_duplicates()
    missing.rename(columns={'hap2':'hap1'}, inplace=True)
    missing = missing[missing.merge(switches[['pid','hap1']].drop_duplicates(), on=['pid','hap1'], how='left', indicator=True)['_merge'].values == "left_only"].copy()
    missing.sort_values(['pid','hap1'], inplace=True)
    free = switches[['pid','hap1']].drop_duplicates()
    free.rename(columns={'hap1':'hap2'}, inplace=True)
    free = free[free.merge(switches[['pid','hap2']].drop_duplicates(), on=['pid','hap2'], how='left', indicator=True)['_merge'].values == "left_only"].copy()
    free.sort_values(['pid','hap2'], inplace=True)
    missing['hap2'] = free['hap2'].values
    switches = pd.concat([switches, missing], ignore_index=True)
    # Apply switches
    scaffold_paths['change'] = np.isin(scaffold_paths['pid'], np.unique(switches['pid'].values))
    if np.sum(scaffold_paths['change']):
        org_paths = scaffold_paths[scaffold_paths['change']].copy()
        org_paths = FillHaplotypes(org_paths, org_paths['change'], ploidy)
        for h1 in range(ploidy):
            tmp_paths = org_paths[[f'phase{h1}',f'scaf{h1}',f'strand{h1}',f'dist{h1}']].copy()
            for h2 in range(ploidy):
                switch = np.isin(org_paths['pid'], switches.loc[(switches['hap1'] == h1) & (switches['hap2'] == h2), 'pid'].values)
                tmp_paths.loc[switch, [f'phase{h1}',f'scaf{h1}',f'strand{h1}',f'dist{h1}']] = org_paths.loc[switch, [f'phase{h2}',f'scaf{h2}',f'strand{h2}',f'dist{h2}']].values
            tmp_paths[[f'phase{h1}',f'scaf{h1}',f'dist{h1}']] = tmp_paths[[f'phase{h1}',f'scaf{h1}',f'dist{h1}']].astype(int)
            scaffold_paths.loc[scaffold_paths['change'], [f'phase{h1}',f'scaf{h1}',f'strand{h1}',f'dist{h1}']] = tmp_paths.values
    scaffold_paths = TrimAlternativesConsistentWithMain(scaffold_paths, ploidy)
    scaffold_paths.drop(columns=['change'], inplace=True)
#
    return scaffold_paths

def BringAllConnectableHaplotypesToTheLowerHaplotypes(scaffold_paths, ends, ploidy):
    # The first step is to bring everything that is valid from both sides to the lower haplotypes
    for p1, p2 in zip(['a','b'],['b','a']):
        connectable = ends[ends['valid_overlap'] & (ends[f'{p1}nhaps'] > ends[f'{p2}nhaps'])].drop(columns=['valid_overlap','dup_hap','valid_path'])
        connectable.sort_values(['apid','bpid',f'{p1}hap',f'{p2}hap'], inplace=True)
        connectable['fixed'] = False
        while np.sum(connectable['fixed'] == False):
            # Lowest haplotypes of p2 are fixed for lowest haplotypes of p1
            connectable.loc[(connectable['apid'] != connectable['apid'].shift(1)) | connectable['fixed'].shift(1), 'fixed'] = True
            # Remove all other haplotypes of p2 for fixed haplotypes of p1
            connectable['delete'] = False
            connectable.loc[(connectable['apid'] == connectable['apid'].shift(1)) & (connectable[f'{p1}hap'] == connectable[f'{p1}hap'].shift(1)) & connectable['fixed'].shift(1), 'delete'] = True
            while True:
                old_ndels = np.sum(connectable['delete'])
                connectable.loc[(connectable['apid'] == connectable['apid'].shift(1)) & (connectable[f'{p1}hap'] == connectable[f'{p1}hap'].shift(1)) & connectable['delete'].shift(1), 'delete'] = True
                if old_ndels == np.sum(connectable['delete']):
                    break
            # Remove haplotypes of p2 that are fixed in a haplotype of p1 in all other haplotypes of p1
            connectable.loc[connectable[['apid','bpid',f'{p2}hap']].merge(connectable.loc[connectable['fixed'], ['apid','bpid',f'{p2}hap']], on=['apid','bpid',f'{p2}hap'], how='left', indicator=True)['_merge'].values == "both", 'delete'] = True
            connectable.loc[connectable['fixed'], 'delete'] = False
            connectable = connectable[connectable['delete'] == False].copy()
        switches = connectable.loc[connectable[f'{p1}hap'] != connectable[f'{p2}hap'], [f'{p1}pid',f'{p1}hap',f'{p2}hap']].rename(columns={f'{p1}pid':'pid',f'{p1}hap':'hap1',f'{p2}hap':'hap2'})
        scaffold_paths = SwitchHaplotypes(scaffold_paths, switches, ploidy)
#
    return scaffold_paths

def DuplicateHaplotypes(scaffold_paths, duplicate, ploidy):
    for h1 in range(1,ploidy):
        for h2 in range(h1+1, ploidy):
            copy = np.isin(scaffold_paths['pid'], duplicate.loc[(duplicate['hap1'] == h1) & (duplicate['hap2'] == h2), 'pid'].values)
            if np.sum(copy):
                scaffold_paths.loc[copy, [f'phase{h2}',f'scaf{h2}', f'strand{h2}', f'dist{h2}']] = scaffold_paths.loc[copy, [f'phase{h1}',f'scaf{h1}', f'strand{h1}', f'dist{h1}']].values
#
    return scaffold_paths

def DuplicateHaplotypesToCreateMatches(scaffold_paths, ends, ploidy):
    # When all haplotypes either match to the corresponding haplotype or, if the corresponding haplotype does not exist, to another haplotype, scaffolds can be made connectable by duplicating haplotypes on the paths with less haplotypes(p1)
    for p1, p2 in zip(['a','b'],['b','a']):
        connectable = ends[ (ends[f'{p2}hap'] >= ends[f'{p1}nhaps']) & np.isin(ends['valid_path'],['ab',p2])].drop(columns=['valid_overlap','dup_hap','valid_path'])
        connectable.sort_values(['apid','bpid',f'{p2}hap',f'{p1}hap'], inplace=True)
        connectable = connectable.groupby(['apid','bpid',f'{p2}hap']).first().reset_index() # Take the lowest haplotype that can be duplicated to fill the missing one
        connectable = connectable.loc[connectable[f'{p1}hap'] > 0, [f'{p1}pid',f'{p1}hap',f'{p2}hap']].rename(columns={f'{p1}pid':'pid', f'{p1}hap':'hap1', f'{p2}hap':'hap2'}) # If the lowest is the main, we do not need to do anything
        scaffold_paths = DuplicateHaplotypes(scaffold_paths, connectable, ploidy)
#
    return scaffold_paths

def SwitchHaplotypesToCreateMatches(scaffold_paths, ends, ploidy):
    # Switching haplotypes might also help to make paths connectable
    connectable = ends[ends['valid_overlap'] | (ends['dup_hap'] != '')].drop(columns=['valid_overlap','valid_path'])
    connectable.rename(columns={'dup_hap':'switchcol'}, inplace=True)
    dupa = connectable[['apid','bpid']].merge(connectable.loc[connectable['switchcol'] == 'b', ['apid','bpid']].drop_duplicates(), on=['apid','bpid'], how='left', indicator=True)['_merge'].values == "both"
    dupb = connectable[['apid','bpid']].merge(connectable.loc[connectable['switchcol'] == 'a', ['apid','bpid']].drop_duplicates(), on=['apid','bpid'], how='left', indicator=True)['_merge'].values == "both"
    connectable.loc[dupa & (dupb == False), 'switchcol'] = 'b' # We need consistent switchcols within pids
    connectable = connectable[(dupa & dupb) == False].copy() 
    connectable['switchcol'] = np.where(connectable['switchcol'] != 'b', 'b', 'a') # Switch the column that is not a duplicated haplotype, because switching a duplicated haplotype to a position lower than the duplicate would remove that status (as we always want to keep one version and chose the lowest haplotype with that version for it)
    connectable['nhaps'] = np.minimum(connectable['anhaps'], connectable['bnhaps'])
    connectable = connectable[(connectable['ahap'] < connectable['nhaps']) & (connectable['bhap'] < connectable['nhaps'])].drop(columns=['anhaps','bnhaps'])
    connectable['nmatches'] = connectable[['apid','bpid']].merge(connectable[connectable['ahap'] == connectable['bhap']].groupby(['apid','bpid']).size().reset_index(name='matches'), on=['apid','bpid'], how='left')['matches'].fillna(0).values.astype(int)
    connectable = connectable[connectable['nmatches'] < connectable['nhaps']].copy()
    connectable['new_ahap'] = connectable['ahap']
    connectable['new_bhap'] = connectable['bhap']
    while len(connectable):
        connectable['match'] = connectable['new_ahap'] == connectable['new_bhap']
        connectable['amatch'] = connectable[['apid','bpid','new_ahap']].merge(connectable.groupby(['apid','bpid','new_ahap'])['match'].max().reset_index(), on=['apid','bpid','new_ahap'], how='left')['match'].values
        connectable['bmatch'] = connectable[['apid','bpid','new_bhap']].merge(connectable.groupby(['apid','bpid','new_bhap'])['match'].max().reset_index(), on=['apid','bpid','new_bhap'], how='left')['match'].values
        connectable['switchable'] = connectable[['apid','bpid','new_ahap','new_bhap']].merge( connectable[['apid','bpid','new_ahap','new_bhap']].rename(columns={'new_ahap':'new_bhap','new_bhap':'new_ahap'}), on=['apid','bpid','new_ahap','new_bhap'], how='left', indicator=True)['_merge'].values == "both"
        for p1, p2 in zip(['a','b'],['b','a']):
            switches = connectable.loc[(connectable[f'{p1}match'] == False) & (connectable['switchcol'] == p2) & ((connectable[f'{p2}match'] == False) | connectable['switchable']), ['apid','new_ahap','bpid','new_bhap','switchcol']].copy()
            switches = switches.groupby(['apid','bpid']).first().reset_index() # Only one switch per meta_paths per round to avoid conflicts
            switches.rename(columns={f'new_{p2}hap':f'{p2}hap',f'new_{p1}hap':f'new_{p2}hap'}, inplace=True)
            switches = pd.concat([switches.rename(columns={f'new_{p2}hap':f'{p2}hap',f'{p2}hap':f'new_{p2}hap'}), switches], ignore_index=True)
            switches = connectable[['apid','bpid',f'new_{p2}hap']].rename(columns={f'new_{p2}hap':f'{p2}hap'}).merge(switches.drop_duplicates(), on=['apid','bpid',f'{p2}hap'], how='left')[f'new_{p2}hap'].values
            connectable[f'new_{p2}hap'] = np.where(np.isnan(switches), connectable[f'new_{p2}hap'], switches).astype(int)
        connectable['old_nmatches'] = connectable['nmatches']
        connectable['nmatches'] = connectable[['apid','bpid']].merge(connectable[connectable['new_ahap'] == connectable['new_bhap']].groupby(['apid','bpid']).size().reset_index(name='matches'), on=['apid','bpid'], how='left')['matches'].fillna(0).values.astype(int)
        improvable = (connectable['old_nmatches'] < connectable['nmatches']) & (connectable['nmatches'] < connectable['nhaps'])
        for p in ['a','b']:
            switches = connectable.loc[(improvable == False) & (connectable[f'{p}hap'] != connectable[f'new_{p}hap']), [f'{p}pid',f'{p}hap',f'new_{p}hap']].drop_duplicates()
            switches.rename(columns={f'{p}pid':'pid',f'{p}hap':'hap1',f'new_{p}hap':'hap2'}, inplace=True)
            scaffold_paths = SwitchHaplotypes(scaffold_paths, switches, ploidy)
        connectable = connectable[improvable].copy()
#
    return scaffold_paths

def GetHaplotypesThatDifferOnlyByDistance(scaffold_paths, check_pids, ploidy):
    # Find haplotypes that are identical except distance differences
    check_paths = scaffold_paths[np.isin(scaffold_paths['pid'], check_pids)].copy()
    dist_diff_only = []
    for h in range(1, ploidy):
        for h1 in range(h):
            if h1==0:
                check_paths['identical'] = (check_paths[f'phase{h}'] < 0) | ((check_paths[f'scaf{h}'] == check_paths['scaf0']) & (check_paths[f'strand{h}'] == check_paths['strand0']))
            else:
                check_paths['identical'] = ( ((check_paths[f'phase{h}'] < 0) & (check_paths[f'phase{h1}'] < 0)) |
                                             ((check_paths[f'phase{h}'] >= 0) & (check_paths[f'phase{h1}'] >= 0) & (check_paths[f'scaf{h}'] == check_paths[f'scaf{h1}']) & (check_paths[f'strand{h}'] == check_paths[f'strand{h1}'])) | # Here we need the phase checks to make sure we not consider a deletion the same as a strand equal to main
                                             ((check_paths[f'phase{h}'] < 0) & (check_paths[f'scaf{h1}'] == check_paths['scaf0']) & (check_paths[f'strand{h1}'] == check_paths['strand0'])) |
                                             ((check_paths[f'phase{h1}'] < 0) & (check_paths[f'scaf{h}'] == check_paths['scaf0']) & (check_paths[f'strand{h}'] == check_paths['strand0'])) )
            identical = check_paths.groupby(['pid'])['identical'].min().reset_index()
            identical = identical[identical['identical']].drop(columns=['identical'])
            identical['hap1'] = h1
            identical['hap2'] = h
            dist_diff_only.append(identical)
    # Enter haplotypes in both directions
    dist_diff_only = pd.concat(dist_diff_only, ignore_index=True)
    dist_diff_only = pd.concat([dist_diff_only, dist_diff_only.rename(columns={'hap1':'hap2','hap2':'hap1'})], ignore_index=True)
#
    return dist_diff_only

def RemoveDistanceOnlyDifferences(scaffold_paths, ends, ploidy):
    # Remove haplotypes that block a connection if they differ only by distance from a valid haplotype
    delete_haps = []
    connectable_pids = []
    for p in ['a','b']:
        dist_diff_only = GetHaplotypesThatDifferOnlyByDistance(scaffold_paths, ends[f'{p}pid'].drop_duplicates().values, ploidy)
        valid_haps = ends.loc[ends['valid_overlap'], [f'{p}pid',f'{p}hap']].drop_duplicates()
        valid_haps.rename(columns={f'{p}pid':'pid',f'{p}hap':'hap'}, inplace=True)
        dist_diff_only = dist_diff_only.merge(valid_haps.rename(columns={'hap':'hap1'}), on=['pid','hap1'], how='inner')
        invalid_haps = ends[[f'{p}pid',f'{p}nhaps']].drop_duplicates()
        invalid_haps.rename(columns={f'{p}pid':'pid'}, inplace=True)
        invalid_haps = invalid_haps.loc[np.repeat(invalid_haps.index.values, invalid_haps[f'{p}nhaps'].values), ['pid']].copy()
        invalid_haps['hap'] = invalid_haps.groupby(['pid']).cumcount()
        invalid_haps = invalid_haps[invalid_haps.merge(valid_haps, on=['pid','hap'], how='left', indicator=True)['_merge'].values == "left_only"].copy()
        invalid_haps['dist_diff_only'] = invalid_haps.merge(dist_diff_only[['pid','hap2']].rename(columns={'hap2':'hap'}), on=['pid','hap'], how='left', indicator=True)['_merge'].values == "both"
        delete_haps.append(invalid_haps.loc[invalid_haps['dist_diff_only'], ['pid','hap']])
        valid_haps['valid'] = True
        valid_haps = pd.concat([valid_haps, invalid_haps.rename(columns={'dist_diff_only':'valid'})], ignore_index=True)
        valid_haps = valid_haps.groupby(['pid'])['valid'].min().reset_index()
        valid_haps = valid_haps.loc[valid_haps['valid'], ['pid']].rename(columns={'pid':f'{p}pid'})
        valid_haps = valid_haps.merge(ends[['apid','bpid']].drop_duplicates(), on=[f'{p}pid'], how='left')
        connectable_pids.append(valid_haps)
    connectable_pids = connectable_pids[0].merge(connectable_pids[1], on=['apid','bpid'], how='inner')
    delete_haps[0] = delete_haps[0][np.isin(delete_haps[0]['pid'], connectable_pids['apid'].values)].copy()
    delete_haps[1] = delete_haps[1][np.isin(delete_haps[1]['pid'], connectable_pids['bpid'].values)].copy()
    delete_haps = pd.concat(delete_haps, ignore_index=True)
    scaffold_paths = RemoveHaplotypes(scaffold_paths, delete_haps, ploidy)
    scaffold_paths = ShiftHaplotypesToLowestPossible(scaffold_paths, ploidy)
#
    return scaffold_paths

def CombinePathAccordingToMetaParts(scaffold_paths, meta_parts_in, conns, graph_ext, scaffold_graph, scaf_bridges, scaf_len, ploidy):
    # Combine scaffold_paths from lowest to highest position in meta scaffolds
    meta_scaffolds = meta_parts_in.loc[meta_parts_in['pos'] == 0].drop(columns=['pos'])
    scaffold_paths['reverse'] = scaffold_paths[['pid']].merge(meta_scaffolds[['pid','reverse']], on=['pid'], how='left')['reverse'].fillna(False).values.astype(bool)
    scaffold_paths = ReverseScaffolds(scaffold_paths, scaffold_paths['reverse'], ploidy)
    scaffold_paths.drop(columns=['reverse'], inplace=True)
    meta_scaffolds['start_pos'] = meta_scaffolds[['pid']].merge(scaf_len, on=['pid'], how='left')['pos'].values + 1
    meta_scaffolds.rename(columns={'pid':'new_pid'}, inplace=True)
    meta_scaffolds['apid'] = meta_scaffolds['new_pid']
    meta_scaffolds['aside'] = np.where(meta_scaffolds['reverse'], 'l', 'r')
    meta_scaffolds.drop(columns=['reverse'], inplace=True)
    meta_parts = meta_parts_in[meta_parts_in['pos'] > 0].copy()
    pos=1
    while len(meta_parts):
        # Get next connection
        meta_scaffolds = meta_scaffolds.merge(meta_parts.loc[meta_parts['pos'] == pos].drop(columns=['pos']), on=['meta'], how='inner')
        scaffold_paths['reverse'] = scaffold_paths[['pid']].merge(meta_scaffolds[['pid','reverse']], on=['pid'], how='left')['reverse'].fillna(False).values.astype(bool)
        scaffold_paths = ReverseScaffolds(scaffold_paths, scaffold_paths['reverse'], ploidy)
        scaffold_paths.drop(columns=['reverse'], inplace=True)
        meta_scaffolds.rename(columns={'pid':'bpid'}, inplace=True)
        meta_scaffolds['bside'] = np.where(meta_scaffolds['reverse'], 'r', 'l')
        meta_scaffolds[['aoverlap','boverlap']] = meta_scaffolds[['apid','aside','bpid','bside']].merge(conns, on=['apid','aside','bpid','bside'], how='left')[['aoverlap','boverlap']].values
        # Check which haplotypes are connectable (we cannot take the previous checks, because the scaffolds might be longer now)
        meta_scaffolds['connectable'] = False
        for iteration in range(4):
            nhaps = GetNumberOfHaplotypes(scaffold_paths, ploidy)
            meta_scaffolds['anhaps'] = meta_scaffolds[['new_pid']].rename(columns={'new_pid':'pid'}).merge(nhaps, on=['pid'], how='left')['nhaps'].values # apid does not exist anymore, because we merged it into new_pid
            meta_scaffolds['bnhaps'] = meta_scaffolds[['bpid']].rename(columns={'bpid':'pid'}).merge(nhaps, on=['pid'], how='left')['nhaps'].values
            ends = meta_scaffolds.loc[np.repeat(meta_scaffolds[meta_scaffolds['connectable'] == False].index.values, meta_scaffolds.loc[meta_scaffolds['connectable'] == False, 'anhaps'].values*meta_scaffolds.loc[meta_scaffolds['connectable'] == False, 'bnhaps'].values), ['new_pid','bpid','anhaps','bnhaps','aoverlap','boverlap','start_pos']].rename(columns={'new_pid':'apid'})
            meta_scaffolds.drop(columns=['anhaps','bnhaps'], inplace=True)
            ends.sort_values(['apid','bpid'], inplace=True)
            ends.reset_index(drop=True, inplace=True)
            ends['ahap'] = ends.groupby(['apid','bpid'], sort=False).cumcount()
            ends['bhap'] = ends['ahap'] % ends['bnhaps']
            ends['ahap'] = ends['ahap'] // ends['bnhaps']
            ends['aside'] = 'r'
            ends['bside'] = 'l'
            ends['amin'] = ends['start_pos'] - ends['aoverlap']
            ends['amax'] = ends['start_pos'] - 1
            ends['alen'] = ends['amax']
            ends['bmin'] = 0
            ends['bmax'] = ends['boverlap']-1
            ends['blen'] = ends[['bpid']].rename(columns={'bpid':'pid'}).merge(scaf_len, on=['pid'], how='left')['pos'].values
            ends.drop(columns=['anhaps','bnhaps','aoverlap','boverlap','start_pos'], inplace=True)
            cols = ['pid','hap','side','min','max','len']
            ends = pd.concat([ends, ends.rename(columns={**{f'a{n}':f'b{n}' for n in cols},**{f'b{n}':f'a{n}' for n in cols}})], ignore_index=True)
            ends = FilterInvalidConnections(ends, scaffold_paths, graph_ext, ploidy)
            if len(ends) == 0:
                break
            else:
                ends['samedir'] = True
                connected_ends = ends.copy() # Store ends for a later check
                ends = FindInvalidOverlaps(ends, scaffold_paths, ploidy)
                ends = ends.loc[ends['aside'] == 'r', ['apid','ahap','bpid','bhap','valid_overlap','dup_hap','valid_path']].copy()
                ends['valid_overlap'] = ends['valid_overlap'] & (ends['valid_path'] == 'ab')
                ends.loc[ends['valid_path'] == 'a', 'dup_hap'] = np.where(np.isin(ends.loc[ends['valid_path'] == 'a', 'dup_hap'],['ab','b']), 'b', '')
                ends.loc[ends['valid_path'] == 'b', 'dup_hap'] = np.where(np.isin(ends.loc[ends['valid_path'] == 'b', 'dup_hap'],['ab','a']), 'a', '')
                for p in ['a','b']:
                    ends[f'{p}nhaps'] = ends[[f'{p}pid']].rename(columns={f'{p}pid':'pid'}).merge(nhaps, on=['pid'], how='left')['nhaps'].values
                ends['nhaps'] = np.maximum(ends['anhaps'], ends['bnhaps'])
                 # When all haplotypes either match to the corresponding haplotype or, if the corresponding haplotype does not exist, to the main, scaffolds are connectable
                connectable = ends.copy()
                connectable = ends[ ((ends['ahap'] == ends['bhap']) & (ends['valid_overlap'] | (ends['dup_hap'] != ''))) | 
                                    ((ends['ahap'] == 0) & (ends['bhap'] >= ends['anhaps']) & np.isin(ends['valid_path'],['ab','b'])) |
                                    ((ends['bhap'] == 0) & (ends['ahap'] >= ends['bnhaps']) & np.isin(ends['valid_path'],['ab','a'])) ].drop(columns=['valid_overlap','dup_hap','valid_path'])
                connectable = connectable.groupby(['apid','bpid','nhaps']).size().reset_index(name='matches')
                connectable = connectable[connectable['nhaps'] == connectable['matches']].copy()
                connected_ends = connected_ends.merge(pd.concat([connectable[['apid','bpid']], connectable[['apid','bpid']].rename(columns={'apid':'bpid','bpid':'apid'})], ignore_index=True).drop_duplicates(), on=['apid','bpid'], how='inner')
                scaffold_paths = SwitchHaplotypesToConnectPerfectMatches(scaffold_paths, connected_ends, scaffold_graph, ploidy)
                meta_scaffolds, ends = SetConnectablePathsInMetaScaffold(meta_scaffolds, ends, connectable)
                if 0 == iteration:
                    scaffold_paths = BringAllConnectableHaplotypesToTheLowerHaplotypes(scaffold_paths, ends, ploidy)
                elif 1 == iteration or 3 == iteration:
                    scaffold_paths = DuplicateHaplotypesToCreateMatches(scaffold_paths, ends, ploidy)
                    scaffold_paths = SwitchHaplotypesToCreateMatches(scaffold_paths, ends, ploidy)
                elif 2 == iteration:
                    scaffold_paths = RemoveDistanceOnlyDifferences(scaffold_paths, ends, ploidy)
        meta_scaffolds.drop(columns=['bside'], inplace=True)
#
        # Since all the existing haplotypes must match take the paths with more haplotypes in the overlapping region (only relevant starting at the second position, since the first cannot have a variant or they would not have been merged)
        long_overlaps = meta_scaffolds.loc[meta_scaffolds['connectable'] & (np.maximum(meta_scaffolds['aoverlap'],meta_scaffolds['boverlap']) > 1), ['new_pid','bpid','aoverlap','boverlap','start_pos']].reset_index()
        overhaps = long_overlaps[['new_pid','aoverlap','start_pos','index']].rename(columns={'new_pid':'pid','aoverlap':'min_pos','start_pos':'max_pos'})
        overhaps['min_pos'] = overhaps['max_pos'] - overhaps['min_pos']
        overhaps['max_pos'] -= 1
        overhaps['path'] = 'a'
        overhaps = [overhaps]
        overhaps.append(long_overlaps[['bpid','boverlap','index']].rename(columns={'bpid':'pid','boverlap':'max_pos'}))
        overhaps[-1]['min_pos'] = 0
        overhaps[-1]['max_pos'] -= 1
        overhaps[-1]['path'] = 'b'
        overhaps = pd.concat(overhaps, ignore_index=True)
        overhaps['len'] = overhaps['max_pos'] - overhaps['min_pos'] + 1
        overhaps.sort_values(['index','path'], inplace=True)
        overhaps = overhaps.loc[np.repeat(overhaps.index.values, overhaps['len'].values), ['index','path','pid','min_pos']].reset_index(drop=True)
        overhaps['pos'] = overhaps['min_pos'] + overhaps.groupby(['index','path'], sort=False).cumcount()
        overhaps[[f'hap{h}' for h in range(ploidy)]] = overhaps[['pid','pos']].merge(scaffold_paths[['pid','pos']+[f'phase{h}' for h in range(ploidy)]], on=['pid','pos'], how='left')[[f'phase{h}' for h in range(ploidy)]] >= 0
        for h in range(1,ploidy):
            # Distance variants at the first position do not matter
            overhaps.loc[(overhaps['min_pos'] == overhaps['pos']), f'hap{h}'] = overhaps.loc[(overhaps['min_pos'] == overhaps['pos']), f'hap{h}'].values & (overhaps.loc[(overhaps['min_pos'] == overhaps['pos']), ['pid','pos']].merge(scaffold_paths[['pid','pos']+[f'scaf{h}',f'strand{h}']], on=['pid','pos'], how='left')[[f'scaf{h}',f'strand{h}']].values != overhaps.loc[(overhaps['min_pos'] == overhaps['pos']), ['pid','pos']].merge(scaffold_paths[['pid','pos']+['scaf0','strand0']], on=['pid','pos'], how='left')[['scaf0','strand0']].values).any(axis=1)
        overhaps = overhaps.groupby(['index','path'], sort=False)[[f'hap{h}' for h in range(ploidy)]].max().reset_index()
        overhaps['nhaps'] = overhaps[[f'hap{h}' for h in range(ploidy)]].sum(axis=1)
        meta_scaffolds[['ahaps','bhaps']] = 0
        for p in ['a','b']:
            meta_scaffolds.loc[ overhaps.loc[overhaps['path'] == p, 'index'].values, f'{p}haps'] = overhaps.loc[overhaps['path'] == p, 'nhaps'].values
        meta_scaffolds.loc[meta_scaffolds['ahaps'] < meta_scaffolds['bhaps'], 'boverlap'] = 1 # The first scaffold in pathb will also be removed, because it cannot have an alternative and does not contain the distance information
        meta_scaffolds.loc[meta_scaffolds['ahaps'] < meta_scaffolds['bhaps'], 'start_pos'] -= meta_scaffolds.loc[meta_scaffolds['ahaps'] < meta_scaffolds['bhaps'], 'aoverlap'] - 1
        meta_scaffolds.drop(columns=['aoverlap','ahaps','bhaps'], inplace=True)
        # Connect scaffolds
        scaffold_paths[['overlap','shift']] = scaffold_paths[['pid']].merge( meta_scaffolds.loc[meta_scaffolds['connectable'], ['bpid','boverlap','start_pos']].rename(columns={'bpid':'pid'}), on=['pid'], how='left')[['boverlap','start_pos']].fillna(0).values.astype(int)
        scaffold_paths['shift'] -= scaffold_paths['overlap']
        scaffold_paths = scaffold_paths[ scaffold_paths['pos'] >= scaffold_paths['overlap'] ].drop(columns=['overlap'])
        scaffold_paths['pos'] += scaffold_paths['shift']
        scaffold_paths.drop(columns=['shift'], inplace=True)
        scaffold_paths['trim'] = scaffold_paths[['pid']].merge( meta_scaffolds.loc[meta_scaffolds['connectable'], ['new_pid','start_pos']].rename(columns={'new_pid':'pid'}), on=['pid'], how='left')['start_pos'].fillna(sys.maxsize*0.9).values.astype(int) # sys.maxsize*0.9 to avoid variable overrun due to type conversion
        scaffold_paths = scaffold_paths[scaffold_paths['pos'] < scaffold_paths['trim']].drop(columns=['trim'])
        scaffold_paths['new_pid'] = scaffold_paths[['pid']].merge( meta_scaffolds.loc[meta_scaffolds['connectable'], ['bpid','new_pid']].rename(columns={'bpid':'pid'}), on=['pid'], how='left')['new_pid'].values
        scaffold_paths.loc[np.isnan(scaffold_paths['new_pid']) == False, 'pid'] = scaffold_paths.loc[np.isnan(scaffold_paths['new_pid']) == False, 'new_pid'].astype(int)
        scaffold_paths.drop(columns=['new_pid'], inplace=True)
        scaffold_paths.sort_values(['pid','pos'], inplace=True)
#
        # The unconnectable paths in meta_scaffolds might have had haplotypes duplicated in an attempt to make them connectable. Remove those duplications
        for h1 in range(1, ploidy):
            for h2 in range(h1+1, ploidy):
                scaffold_paths['remove'] =  ( (np.sign(scaffold_paths[f'phase{h1}']) == np.sign(scaffold_paths[f'phase{h2}'])) & (scaffold_paths[f'scaf{h1}'] == scaffold_paths[f'scaf{h2}']) &
                                              (scaffold_paths[f'strand{h1}'] == scaffold_paths[f'strand{h2}']) & (scaffold_paths[f'dist{h1}'] == scaffold_paths[f'dist{h2}']) )
                remove = scaffold_paths.groupby(['pid'])['remove'].min().reset_index()
                remove = remove.loc[remove['remove'], 'pid'].values
                scaffold_paths['remove'] = np.isin(scaffold_paths['pid'], remove)
                scaffold_paths = RemoveHaplotype(scaffold_paths, scaffold_paths['remove'], h2)
                scaffold_paths.drop(columns=['remove'], inplace=True)
#
        # Make sure the haplotypes are still sorted by highest support for bridges
        bsupp = pd.concat([ meta_scaffolds[['new_pid']].rename(columns={'new_pid':'pid'}), meta_scaffolds.loc[meta_scaffolds['connectable'] == False, ['bpid']].rename(columns={'bpid':'pid'})], ignore_index=True)
        bsupp.sort_values(['pid'], inplace=True)
        nhaps = GetNumberOfHaplotypes(scaffold_paths, ploidy)
        bsupp = bsupp.merge(nhaps, on=['pid'], how='left')
        bsupp = bsupp[bsupp['nhaps'] > 1].copy()
        bsupp = bsupp.loc[np.repeat(bsupp.index.values, bsupp['nhaps'].values)].drop(columns=['nhaps'])
        bsupp.reset_index(drop=True, inplace=True)
        bsupp['hap'] = bsupp.groupby(['pid'], sort=False).cumcount()
        bsupp['group'] = 0
        bsupp = GetBridgeSupport(bsupp, scaffold_paths, scaf_bridges, ploidy)
        if len(bsupp):
            bsupp.drop(columns=['group'], inplace=True)
            bsupp.sort_values(['pid','bplace'], inplace=True)
            bsupp['new_hap'] = bsupp.groupby(['pid']).cumcount()
            bsupp = bsupp.loc[bsupp['hap'] != bsupp['new_hap'], ['pid','hap','new_hap']].rename(columns={'hap':'hap1','new_hap':'hap2'})
            scaffold_paths = SwitchHaplotypes(scaffold_paths, bsupp, ploidy)
            scaffold_paths = ShiftHaplotypesToLowestPossible(scaffold_paths, ploidy)
#
        # Break unconnectable meta_scaffolds
        meta_scaffolds.loc[meta_scaffolds['connectable'] == False, 'new_pid'] = meta_scaffolds.loc[meta_scaffolds['connectable'] == False, 'bpid']
        meta_scaffolds.loc[meta_scaffolds['connectable'] == False, ['start_pos','boverlap']] = 0
#
        # Prepare next round
        meta_scaffolds['start_pos'] += meta_scaffolds[['bpid']].rename(columns={'bpid':'pid'}).merge(scaf_len, on=['pid'], how='left')['pos'].values + 1 - meta_scaffolds['boverlap']
        meta_scaffolds['apid'] = meta_scaffolds['bpid']
        meta_scaffolds['aside'] = np.where(meta_scaffolds['reverse'], 'l', 'r')
        meta_scaffolds.drop(columns=['bpid','reverse','boverlap'], inplace=True)
        meta_parts = meta_parts[meta_parts['pos'] > pos].copy()
        pos += 1
#
    # Check that positions are consistent in scaffold_paths
    inconsistent = scaffold_paths[(scaffold_paths['pos'] < 0) |
                                  ((scaffold_paths['pos'] == 0) & (scaffold_paths['pid'] == scaffold_paths['pid'].shift(1))) |
                                  ((scaffold_paths['pos'] > 0) & ((scaffold_paths['pid'] != scaffold_paths['pid'].shift(1)) | (scaffold_paths['pos'] != scaffold_paths['pos'].shift(1)+1)))].copy()
    if len(inconsistent):
        print("Warning: Scaffold paths is inconsistent after CombinePathAccordingToMetaParts.")
        print(inconsistent)
#
    return scaffold_paths

def CombinePathOnUniqueOverlap(scaffold_paths, scaffold_graph, graph_ext, scaf_bridges, ploidy):
    ends = GetDuplicatedPathEnds(scaffold_paths, ploidy)
    # If the paths continue in the same direction on the non-end side they are alternatives and not combinable
    ends = ends[ends['samedir'] == (ends['aside'] != ends['bside'])].copy()
#
    # Check that combining the paths does not violate scaffold_graph
    ends = FilterInvalidConnections(ends, scaffold_paths, graph_ext, ploidy)
    ends = SelectBestConnections(ends, scaffold_paths, scaffold_graph, ploidy)
#
    if len(ends):
        # Combine all ends that describe the same connection (multiple haplotypes of same path and multiple mapping options): Take the lowest overlap (to not compress repeats) supported by most haplotypes (requiring all haplotypes to be present would remove connections, where one haplotype reaches further into the other scaffold than other haplotypes)
        for p in ['a','b']:
            ends[f'{p}overlap'] = np.where(ends[f'{p}side'] == 'l', ends[f'{p}max']+1, ends[f'{p}len']-ends[f'{p}min']+1)
        ends.drop(columns=['did','amin','amax','bmin','bmax','alen','blen'], inplace=True)
        conns = ends.groupby(['apid','aside','bpid','bside','aoverlap','boverlap'])['matches'].max().reset_index()
        matches = conns.groupby(['apid','aside','bpid','bside'])['matches'].agg(['max','size'])
        conns['max_matches'] = np.repeat(matches['max'].values, matches['size'].values)
        conns['anhaps'] = ends[['apid','aside','bpid','bside','aoverlap','boverlap','ahap']].drop_duplicates().groupby(['apid','aside','bpid','bside','aoverlap','boverlap']).size().values
        conns['bnhaps'] = ends[['apid','aside','bpid','bside','aoverlap','boverlap','bhap']].drop_duplicates().groupby(['apid','aside','bpid','bside','aoverlap','boverlap']).size().values
        conns['minhaps'] = np.minimum(conns['anhaps'], conns['bnhaps'])
        conns['maxhaps'] = np.maximum(conns['anhaps'], conns['bnhaps'])
        conns['maxoverlap'] = np.maximum(conns['aoverlap'], conns['boverlap'])
        conns['minoverlap'] = np.minimum(conns['aoverlap'], conns['boverlap'])
        conns['tiebreakhaps'] = np.where(conns['apid'] < conns['bpid'], conns['anhaps'], conns['bnhaps'])
        conns['tiebreakoverlap'] = np.where(conns['apid'] < conns['bpid'], conns['aoverlap'], conns['boverlap'])
        conns.sort_values(['apid','aside','bpid','bside','minhaps','maxhaps','maxoverlap','minoverlap','tiebreakhaps','tiebreakoverlap'], ascending=[True,True,True,True,False,False,True,True,False,True], inplace=True)
        conns = conns.groupby(['apid','aside','bpid','bside'])[['aoverlap','boverlap']].first().reset_index()
        for p in ['a','b']:
            conns.sort_values([f'{p}pid',f'{p}side'], inplace=True)
            alts = conns.groupby([f'{p}pid',f'{p}side'], sort=False).size().values
            conns[f'{p}alts'] = np.repeat(alts, alts)
        conns = conns[(conns['aalts'] == 1) & (conns['balts'] == 1)].drop(columns=['aalts','balts'])
#
        # Assign all connected paths to a meta scaffold to define connection order
        conns.sort_values(['apid','aside','bpid','bside'], inplace=True)
        meta_scaffolds = pd.DataFrame({'meta':np.unique(conns['apid']), 'size':1, 'lcon':-1, 'lcon_side':'', 'rcon':-1, 'rcon_side':''})
        meta_scaffolds.index = meta_scaffolds['meta'].values
        meta_parts = pd.DataFrame({'scaffold':meta_scaffolds['meta'], 'meta':meta_scaffolds['meta'], 'pos':0, 'reverse':False})
        meta_parts.index = meta_parts['scaffold'].values
        meta_scaffolds.loc[conns.loc[conns['aside'] == 'l', 'apid'].values, ['lcon','lcon_side']] = conns.loc[conns['aside'] == 'l', ['bpid','bside']].values
        meta_scaffolds.loc[conns.loc[conns['aside'] == 'r', 'apid'].values, ['rcon','rcon_side']] = conns.loc[conns['aside'] == 'r', ['bpid','bside']].values
        # Rename some columns and create extra columns just to call the same function as used for the contig scaffolding on unique bridges
        meta_scaffolds['left'] = meta_scaffolds['meta']
        meta_scaffolds['lside'] = 'l'
        meta_scaffolds['right'] = meta_scaffolds['meta']
        meta_scaffolds['rside'] = 'r'
        meta_scaffolds['lextendible'] = True
        meta_scaffolds['rextendible'] = True
        meta_scaffolds['circular'] = False
        meta_scaffolds.rename(columns={'meta':'scaffold','lcon':'lscaf','lcon_side':'lscaf_side','rcon':'rscaf','rcon_side':'rscaf_side'}, inplace=True)
        meta_parts.rename(columns={'scaffold':'conpart','meta':'scaffold'}, inplace=True)
        meta_scaffolds, meta_parts = ScaffoldAlongGivenConnections(meta_scaffolds, meta_parts)
        meta_parts.rename(columns={'conpart':'pid','scaffold':'meta'}, inplace=True)
        meta_parts.sort_values(['meta','pos'], inplace=True)
#
        scaf_len = scaffold_paths.groupby(['pid'])['pos'].max().reset_index()
        scaffold_paths = CombinePathAccordingToMetaParts(scaffold_paths, meta_parts, conns, graph_ext, scaffold_graph, scaf_bridges, scaf_len, ploidy)

    return scaffold_paths

def GetExistingHaplotypesFromPaths(scaffold_paths, ploidy):
    haps = [scaffold_paths[['pid']].drop_duplicates()]
    haps[0]['hap'] = 0
    for h in range(1, ploidy):
        haps.append( scaffold_paths.loc[scaffold_paths[f'phase{h}'] > 0, ['pid']].drop_duplicates() )
        haps[-1]['hap'] = h
    haps = pd.concat(haps, ignore_index=True).sort_values(['pid','hap']).reset_index(drop=True)
#
    return haps

def GetEndPosForEachHaplotypes(scaffold_paths, ploidy):
    scaf_len = scaffold_paths.groupby(['pid'])['pos'].max().reset_index(name='max_pos')
    scaf_len['min_pos'] = 0
    scaf_len = GetExistingHaplotypesFromPaths(scaffold_paths, ploidy).merge(scaf_len[['pid','min_pos','max_pos']], on=['pid'], how='left')
    scaf_len['finished'] = False
    while np.sum(scaf_len['finished'] == False):
        for h in range(ploidy):
            cur = (scaf_len['finished'] == False) & (scaf_len['hap'] == h)
            scaf_len.loc[cur, ['phase','scaf','scaf0']] = scaf_len.loc[cur, ['pid','min_pos']].rename(columns={'min_pos':'pos'}).merge(scaffold_paths[['pid','pos',f'phase{h}',f'scaf{h}']+([] if h==0 else ['scaf0'])], on=['pid','pos'], how='left')[[f'phase{h}',f'scaf{h}','scaf0']].values
        found_del = (scaf_len['scaf'] < 0) & (scaf_len['phase'] > 0) | (scaf_len['scaf0'] < 0) & (scaf_len['phase'] < 0)
        scaf_len.loc[found_del, 'min_pos'] += 1
        scaf_len.loc[scaf_len['min_pos'] > scaf_len['max_pos'], 'finished'] = True
        scaf_len.loc[found_del == False, 'finished'] = True
    scaf_len['finished'] = scaf_len['max_pos'] < scaf_len['min_pos']
    while np.sum(scaf_len['finished'] == False):
        for h in range(ploidy):
            cur = (scaf_len['finished'] == False) & (scaf_len['hap'] == h)
            scaf_len.loc[cur, ['phase','scaf','scaf0']] = scaf_len.loc[cur, ['pid','max_pos']].rename(columns={'max_pos':'pos'}).merge(scaffold_paths[['pid','pos',f'phase{h}',f'scaf{h}']+([] if h==0 else ['scaf0'])], on=['pid','pos'], how='left')[[f'phase{h}',f'scaf{h}','scaf0']].values
        found_del = (scaf_len['scaf'] < 0) & (scaf_len['phase'] > 0) | (scaf_len['scaf0'] < 0) & (scaf_len['phase'] < 0)
        scaf_len.loc[found_del, 'max_pos'] -= 1
        scaf_len.loc[found_del == False, 'finished'] = True
    scaf_len.drop(columns=['finished','phase','scaf','scaf0'], inplace=True)
#
    return scaf_len

def ExtendDuplicationsFromEnd(duplications, scaffold_paths, end, scaf_len, ploidy):
    # Get duplications starting at end
    ext_dups = []
    if 'min_pos' in scaf_len.columns:
        edups = duplications[duplications['apos'] == (duplications[['apid','ahap']].rename(columns={'apid':'pid','ahap':'hap'}).merge(scaf_len, on=['pid','hap'], how='left')['min_pos' if end == 'l' else 'max_pos'].values)].copy()
    else:
        edups = duplications[duplications['apos'] == (0 if end == 'l' else duplications[['apid']].rename(columns={'apid':'pid'}).merge(scaf_len, on=['pid'], how='left')['pos'].values)].copy()
    edups['did'] = np.arange(len(edups))
    ext_dups.append(edups.copy())
    while len(edups):
        # Get next position to see if we can extend the duplication
        edups = GetPositionsBeforeDuplication(edups, scaffold_paths, ploidy, end == 'l')
        # Remove everything that does not fit
        edups = edups[(edups['adist'] == edups['bdist']) & (edups['aprev_pos'] >= 0) & (edups['bprev_pos'] >= 0)].drop(columns=['adist','bdist'])
        edups['apos'] = edups['aprev_pos']
        edups['bpos'] = edups['bprev_pos']
        # Check if we have a duplication at the new position
        edups = edups[['apid','apos','ahap','bpid','bpos','bhap','samedir','did']].merge(duplications, on=['apid','apos','ahap','bpid','bpos','bhap','samedir'], how='inner')
        # Insert the valid extensions
        ext_dups.append(edups.copy())
    ext_dups = pd.concat(ext_dups, ignore_index=True)
    ext_dups.sort_values(['did','apos'], inplace=True)
#
    return ext_dups

def RemoveDuplicates(scaffold_paths, remove_all, ploidy):
    # Get min/max position for each haplotype in paths (excluding deletions at ends)
    scaf_len = GetEndPosForEachHaplotypes(scaffold_paths, ploidy)
    # Remove haplotypes that are only deletions
    rem_paths = scaf_len.loc[scaf_len['max_pos'] < scaf_len['min_pos'], ['pid','hap']].copy()
    scaf_len = scaf_len[scaf_len['max_pos'] >= scaf_len['min_pos']].copy()
    # Remove haplotype that are a complete duplication of another haplotype
    scaf_len['len'] = scaf_len['max_pos'] - scaf_len['min_pos'] + 1
    check = scaf_len.drop(columns=['max_pos']).rename(columns={'hap':'hap1','len':'len1'}).merge(scaf_len[['pid','hap','len']].rename(columns={'hap':'hap2','len':'len2'}), on=['pid'], how='left')
    scaf_len.drop(columns=['len'], inplace=True)
    check = check[ (check['len1'] < check['len2']) | ((check['len1'] == check['len2']) & (check['hap1'] > check['hap2'])) ].drop(columns=['len2']) # Only the shorter one can be a (partial) duplicate of the other one and if they are the same length we want to keep one of the duplications, so chose the higher haplotype for potential removal
    if len(check):
        check['index'] = check.index.values
        check = check.loc[np.repeat(check['index'].values, check['len1'].values)].reset_index(drop=True)
        check['pos'] = check.groupby(['index'], sort=False).cumcount() + check['min_pos']
        check_paths = check[['pid','pos']].merge(scaffold_paths, on=['pid','pos'], how='left')
        for i in [1,2]:
            check[[f'scaf{i}',f'strand{i}',f'dist{i}']] = check_paths[['scaf0','strand0','dist0']].values
            for h in range(1,ploidy):
                cur = (check[f'hap{i}'] == h) & (check_paths[f'phase{h}'] > 0)
                check.loc[cur, [f'scaf{i}',f'strand{i}',f'dist{i}']] = check_paths.loc[cur, [f'scaf{h}',f'strand{h}',f'dist{h}']].values
        check['identical'] = (check['scaf1'] == check['scaf2']) & (check['strand1'] == check['strand2']) & ((check['dist1'] == check['dist2']) | (check['pos'] == check['min_pos']))
        check = check.groupby(['pid','hap1','hap2'])['identical'].min().reset_index()
        check = check[check['identical']].drop(columns=['identical','hap2']).drop_duplicates()
        rem_paths = pd.concat([rem_paths, check.rename(columns={'hap1':'hap'})], ignore_index=True)
    if len(rem_paths):
        scaffold_paths = RemoveHaplotypes(scaffold_paths, rem_paths, ploidy)
        scaf_len = GetEndPosForEachHaplotypes(scaffold_paths, ploidy)
#
    # Get duplications that contain both path ends for side a (we cannot go down to haplotype level here, because we have not separated the haplotypes yet, so they miss the duplications they share with main)
    duplications = GetDuplications(scaffold_paths, ploidy)
    ends = duplications.groupby(['apid','bpid'])['apos'].agg(['min','max']).reset_index()
    ends['amin'] = ends[['apid']].rename(columns={'apid':'pid'}).merge(scaf_len.groupby(['pid'])['min_pos'].max().reset_index(), on=['pid'], how='left')['min_pos'].values
    ends['amax'] = ends[['apid']].rename(columns={'apid':'pid'}).merge(scaf_len.groupby(['pid'])['max_pos'].min().reset_index(), on=['pid'], how='left')['max_pos'].values
    ends = ends.loc[(ends['min'] <= ends['amin']) & (ends['max'] >= ends['amax']), ['apid','bpid']].copy()
    duplications = duplications.merge(ends, on=['apid','bpid'], how='inner')
    # Add length of haplotype a
    duplications['alen']  = duplications[['apid','ahap']].rename(columns={'apid':'pid','ahap':'hap'}).merge(scaf_len[['pid','hap','max_pos']], on=['pid','hap'], how='left')['max_pos'].values
    # Check if they are on the same or opposite strand (alone it is useless, but it sets the requirements for the direction of change for the positions)
    duplications = AddStrandToDuplications(duplications, scaffold_paths, ploidy)
    duplications['samedir'] = duplications['astrand'] == duplications['bstrand']
    duplications.drop(columns=['astrand','bstrand'], inplace=True)
    # Extend the duplications with valid distances from start and only keep the ones that reach end
    duplications = SeparateDuplicationsByHaplotype(duplications, scaffold_paths, ploidy)
    duplications = ExtendDuplicationsFromEnd(duplications, scaffold_paths, 'l', scaf_len, ploidy)
    duplications = duplications.groupby(['apid','ahap','bpid','bhap','alen'])['apos'].max().reset_index(name='amax')
    duplications = duplications[duplications['amax'] == duplications['alen']].drop(columns=['amax','alen'])
    # Remove duplicated scaffolds (Depending on the setting only remove duplicates with the same length, because we need the other ends later for merging)
    rem_paths = duplications.merge(duplications.rename(columns={'apid':'bpid','ahap':'bhap','bpid':'apid','bhap':'ahap'}), on=['apid','ahap','bpid','bhap'], how='inner') # Paths that are exactly the same (required same length of the haplotype is not the same as same length of paths, thus we cannot use paths length)
    duplications = duplications.loc[duplications.merge(rem_paths, on=['apid','ahap','bpid','bhap'], how='left', indicator=True)['_merge'].values == "left_only", ['apid','ahap']].copy() # Paths that are part of a larger part
    rem_paths = rem_paths.loc[rem_paths['apid'] < rem_paths['bpid'], ['apid','ahap']].copy() # Only remove the lower pid (because we add merged path with new, larger pids at the end)
    if remove_all:
        rem_paths = pd.concat([rem_paths, duplications], ignore_index=True)
    rem_paths.drop_duplicates(inplace=True)
    rem_paths.rename(columns={'apid':'pid','ahap':'hap'}, inplace=True)
    scaffold_paths = RemoveHaplotypes(scaffold_paths, rem_paths, ploidy)
    scaffold_paths = CompressPaths(scaffold_paths, ploidy)
#
    return scaffold_paths

def PlacePathAInPathB(duplications, scaffold_paths, graph_ext, scaffold_graph, scaf_bridges, ploidy):
    includes = duplications.groupby(['ldid','rdid','apid','ahap','bpid','bhap'])['bpos'].agg(['min','max']).reset_index()
    scaf_len = scaffold_paths.groupby(['pid'])['pos'].max().reset_index()
    includes['blen'] = includes[['bpid']].rename(columns={'bpid':'pid'}).merge(scaf_len, on=['pid'], how='left')['pos'].values
    includes['reverse'] = includes[['ldid','rdid']].merge(duplications.groupby(['ldid','rdid'])['samedir'].first().reset_index(), on=['ldid','rdid'], how='left')['samedir'].values == False
    includes['tpid1'] = np.arange(0,3*len(includes),3)
    includes['tpid2'] = includes['tpid1'] + 1
    includes['tpid3'] = includes['tpid1'] + 2
    includes['group'] = np.arange(len(includes))
    # Merge middle section of path b (where path a is inserted) and path a
    test_paths = pd.concat([ includes[['apid','tpid1']].rename(columns={'apid':'pid','tpid1':'tpid'}), includes[['bpid','tpid2']].rename(columns={'bpid':'pid','tpid2':'tpid'}) ], ignore_index=True)
    test_paths = test_paths.merge(scaffold_paths, on=['pid'], how='left')
    test_paths['pid'] = test_paths['tpid']
    test_paths.drop(columns=['tpid'], inplace=True)
    test_paths.sort_values(['pid','pos'], inplace=True)
    test_paths['reverse'] = test_paths[['pid']].merge(includes[['tpid1','reverse']].rename(columns={'tpid1':'pid'}), on=['pid'], how='left')['reverse'].fillna(False).values.astype(bool)
    test_paths = ReverseScaffolds(test_paths, test_paths['reverse'], ploidy)
    test_paths.drop(columns=['reverse'], inplace=True)
    test_paths[['min','max']] = test_paths[['pid']].merge(includes[['min','max','tpid2']].rename(columns={'tpid2':'pid'}), on=['pid'], how='left')[['min','max']].values
    test_paths['min'] = test_paths['min'].fillna(0).astype(int)
    test_paths['max'] = test_paths['max'].fillna(scaf_len['pos'].max()).astype(int)
    test_paths = test_paths[(test_paths['min'] <= test_paths['pos']) & (test_paths['pos'] <= test_paths['max'])].drop(columns=['min','max'])
    test_paths['pos'] = test_paths.groupby(['pid'], sort=False).cumcount()
    ends = pd.concat([ includes[['tpid1','group']].rename(columns={'tpid1':'pid'}), includes[['tpid2','group']].rename(columns={'tpid2':'pid'}) ], ignore_index=True)
    if len(ends) == 0:
        includes['success'] = False
    else:
        SetDistanceAtFirstPositionToZero(test_paths, ploidy)
        test_paths, group_info = MergeHaplotypes(test_paths, graph_ext, scaf_bridges, ploidy, ends)
        pids = np.unique(test_paths['pid'])
        includes['success'] = (np.isin(includes['tpid1'], pids) == False) & (np.isin(includes['tpid2'], pids) == False) # If we still have the original pids they could not be merged
        group_info['new_pid'] = group_info[[f'pid{h}' for h in range(ploidy)]].max(axis=1) # We get here tpid2 from includes, which is the pid we want to assign to the now merged middle part
        test_paths['new_pid'] = test_paths[['pid']].merge(group_info[['pid','new_pid']], on=['pid'], how='left')['new_pid'].values
        test_paths = test_paths[np.isnan(test_paths['new_pid']) == False].copy()
        test_paths['pid'] = test_paths['new_pid'].astype(int)
        test_paths.drop(columns=['new_pid'], inplace=True)
        # Add part of path b before and after the merged region
        test_paths = [test_paths]
        spaths = scaffold_paths.merge(includes.loc[includes['success'] & (includes['min'] > 0), ['bpid','min','tpid1']].rename(columns={'bpid':'pid'}), on=['pid'], how='inner')
        spaths = spaths[spaths['pos'] <= spaths['min']].copy() # We need an overlap of 1 for the combining function to work properly
        spaths['pid'] = spaths['tpid1']
        spaths.drop(columns=['min','tpid1'], inplace=True)
        test_paths.append(spaths)
        epaths = scaffold_paths.merge(includes.loc[includes['success'] & (includes['max'] < includes['blen']), ['bpid','max','tpid3']].rename(columns={'bpid':'pid'}), on=['pid'], how='inner')
        epaths = epaths[epaths['pos'] >= epaths['max']].copy() # We need an overlap of 1 for the combining function to work properly
        epaths['pid'] = epaths['tpid3']
        epaths.drop(columns=['max','tpid3'], inplace=True)
        test_paths.append(epaths)
        test_paths = pd.concat(test_paths, ignore_index=True)
        test_paths.sort_values(['pid','pos'], inplace=True)
        test_paths.loc[test_paths['pos'] == 0, [f'dist{h}' for h in range(ploidy)]] = 0
        test_paths = TrimAlternativesConsistentWithMain(test_paths, ploidy)
        # Prepare additional information necessary for combining the individual parts in test_paths
        meta_parts = [ includes.loc[includes['success'] & (includes['min'] > 0), ['tpid1','tpid1']] ]
        meta_parts[-1].columns = ['pid','meta']
        meta_parts[-1]['pos'] = 0
        meta_parts.append( includes.loc[includes['success'], ['tpid2','tpid1']].rename(columns={'tpid2':'pid','tpid1':'meta'}) )
        meta_parts[-1]['pos'] = 1
        meta_parts.append( includes.loc[includes['success'] & (includes['max'] < includes['blen']), ['tpid3','tpid1']].rename(columns={'tpid3':'pid','tpid1':'meta'}) )
        meta_parts[-1]['pos'] = 2
        meta_parts = pd.concat(meta_parts, ignore_index=True)
        meta_parts.sort_values(['pid'], inplace=True)
        meta_parts.index = meta_parts['pid'].values
        meta_parts = meta_parts[(meta_parts['meta'] == meta_parts['meta'].shift(-1)) | (meta_parts['meta'] == meta_parts['meta'].shift(1))].copy() # If we do not have anything to combine we do not need to add it to meta_parts
        meta_parts['pos'] = meta_parts.groupby(['meta']).cumcount()
        meta_parts['reverse'] = False
        conns = meta_parts.drop(columns=['pos','reverse'])
        conns.rename(columns={'pid':'apid'}, inplace=True)
        conns['aside'] = 'r'
        conns['bpid'] = conns['apid'].shift(-1, fill_value=0)
        conns = conns[conns['meta'] == conns['meta'].shift(-1)].copy()
        conns['bside'] = 'l'
        conns.drop(columns=['meta'], inplace=True)
        conns = pd.concat([ conns, conns.rename(columns={'apid':'bpid','aside':'bside','bpid':'apid','bside':'aside'})], ignore_index=True)
        conns['aoverlap'] = 1
        conns['boverlap'] = 1
        scaf_len = test_paths.groupby(['pid'])['pos'].max().reset_index()
        test_paths = CombinePathAccordingToMetaParts(test_paths, meta_parts, conns, graph_ext, scaffold_graph, scaf_bridges, scaf_len, ploidy)
        pids = np.unique(test_paths['pid'])
        includes['success'] = (np.isin(includes[['tpid1','tpid2','tpid3']], pids).sum(axis=1) == 1)
        test_paths = test_paths.merge(pd.concat([ includes.loc[includes['success'], [f'tpid{i}','tpid3']].rename(columns={f'tpid{i}':'pid'}) for i in [1,2] ], ignore_index=True), on=['pid'], how='inner')
        test_paths['pid'] = test_paths['tpid3']
        test_paths.drop(columns=['tpid3'], inplace=True)
        test_paths.sort_values(['pid','pos'], inplace=True)
    includes = includes.loc[includes['success'], ['ldid','rdid','apid','bpid','tpid3']].rename(columns={'tpid3':'tpid'})
#
    return test_paths, includes

def PlaceUnambigouslyPlaceablePathsAsAlternativeHaplotypes(scaffold_paths, graph_ext, scaffold_graph, scaf_bridges, ploidy):
    duplications = GetDuplications(scaffold_paths, ploidy)
    # Only insert a maximum of one path a per path b per round to avoid conflicts between inserted path
    while len(duplications):
        # Get duplications that contain both path ends for side a
        scaf_len = scaffold_paths.groupby(['pid'])['pos'].max().reset_index()
        ends = duplications.groupby(['apid','bpid'])['apos'].agg(['min','max']).reset_index()
        ends['alen'] = ends[['apid']].rename(columns={'apid':'pid'}).merge(scaf_len, on=['pid'], how='left')['pos'].values
        ends = ends.loc[(ends['min'] == 0) & (ends['max'] == ends['alen']), ['apid','bpid','alen']].copy()
        duplications = duplications.merge(ends, on=['apid','bpid'], how='inner')
        # Only keep duplications, where path a is haploid
        nhaps = GetNumberOfHaplotypes(scaffold_paths, ploidy)
        duplications['anhaps'] = duplications[['apid']].rename(columns={'apid':'pid'}).merge(nhaps, on=['pid'], how='left')['nhaps'].values
        duplications = duplications[duplications['anhaps'] == 1].drop(columns=['anhaps'])
        # Check if they are on the same or opposite strand (alone it is useless, but it sets the requirements for the direction of change for the positions)
        duplications = AddStrandToDuplications(duplications, scaffold_paths, ploidy)
        duplications['samedir'] = duplications['astrand'] == duplications['bstrand']
        duplications.drop(columns=['astrand','bstrand'], inplace=True)
        # Extend the duplications with valid distances from both ends and only keep the ones that have both ends on the same path in the same direction
        duplications = SeparateDuplicationsByHaplotype(duplications, scaffold_paths, ploidy)
        ldups = ExtendDuplicationsFromEnd(duplications, scaffold_paths, 'l', scaf_len, ploidy)
        ldups.rename(columns={'did':'ldid'}, inplace=True)
        rdups = ExtendDuplicationsFromEnd(duplications, scaffold_paths, 'r', scaf_len, ploidy)
        rdups.rename(columns={'did':'rdid'}, inplace=True)
        mcols = ['apid','ahap','bpid','bhap']
        ldups = ldups.merge(rdups[mcols+['rdid']].drop_duplicates(), on=mcols, how='inner')
        rdups = rdups.merge(ldups[mcols+['ldid']].drop_duplicates(), on=mcols, how='inner')
        duplications = pd.concat([ldups,rdups], ignore_index=True)
        duplications.drop_duplicates(inplace=True)
        includes = duplications.groupby(['ldid','rdid'])['samedir'].agg(['min','max']).reset_index()
        includes = includes[includes['min'] == includes['max']].drop(columns=['min','max'])
        duplications = duplications.merge(includes, on=['ldid','rdid'], how='inner')
        # We cannot insert an alternative haplotype if the start and end map to the same position
        includes = duplications.groupby(['apid','ahap','bpid','bhap','ldid','rdid'])['bpos'].agg(['min','max']).reset_index()
        includes.sort_values(['apid','ahap','bpid','min','max','bhap','ldid','rdid'], inplace=True)
        includes = includes.groupby(['apid','ahap','bpid','min','max'], sort=False).first().reset_index() # We add it to a path not a single haplotype, so only take the lowest haplotype to have the corresponding duplications
        includes = includes[includes['min'] != includes['max']].copy()
        # Check that path b has a free haplotype to include path a
        includes = includes.loc[np.repeat(includes.index.values, includes['max']-includes['min'])].reset_index(drop=True)
        includes['pos'] = includes.groupby(['apid','ahap','bpid','min','max'], sort=False).cumcount() + includes['min'] + 1
        includes[[f'free{h}' for h in range(1,ploidy)]] = (includes[['bpid','pos']].rename(columns={'bpid':'pid'}).merge(scaffold_paths[['pid','pos']+[f'phase{h}' for h in range(1,ploidy)]], on=['pid','pos'], how='left')[[f'phase{h}' for h in range(1,ploidy)]].values < 0)
        includes = includes.groupby(['ldid','rdid','bpid','min','max'])[[f'free{h}' for h in range(1,ploidy)]].min().reset_index()
        includes = includes.merge(scaffold_paths.rename(columns={'pid':'bpid','pos':'min'}), on=['bpid','min'], how='left')
        for h in range(1,ploidy):
            includes[f'free{h}'] = includes[f'free{h}'] & ((includes[f'phase{h}'] < 0) | ((includes[f'scaf{h}'] == includes['scaf0']) & (includes[f'strand{h}'] == includes['strand0'])))
        includes['nfree'] = includes[[f'free{h}' for h in range(1,ploidy)]].sum(axis=1)
        includes = includes.loc[includes['nfree'] > 0, ['ldid','rdid','min','max']].copy()
        duplications = duplications.merge(includes, on=['ldid','rdid'], how='inner')
        # The positions for a and b must match samedir
        duplications['valid'] = (duplications['bpos'] != duplications['min']) | (duplications['samedir'] == (duplications['apos'] == 0)) # If samedir at lowest position of path b it must also be lowest for path a
        duplications['valid'] = duplications[['ldid','rdid']].merge(duplications.groupby(['ldid','rdid'])['valid'].min().reset_index(), on=['ldid','rdid'], how='left')['valid'].values
        duplications = duplications[duplications['valid']].copy()
        # Check that including path a as haplotype of path b does not violate scaffold_graph
        test_paths, includes = PlacePathAInPathB(duplications, scaffold_paths, graph_ext, scaffold_graph, scaf_bridges, ploidy)
        # Require that path a has a unique placement
        includes.sort_values(['apid'], inplace=True)
        includes = includes[(includes['apid'] != includes['apid'].shift(1)) & (includes['apid'] != includes['apid'].shift(-1))].copy()
        duplications = duplications.merge(includes[['ldid','rdid']], on=['ldid','rdid'], how='inner')
        # A path a cannot be at the same time a path b
        includes = includes[np.isin(includes['apid'],includes['bpid'].values) == False].copy()
        duplications = duplications[ duplications[['apid','bpid']].merge(duplications[['apid','bpid']].drop_duplicates().rename(columns={'apid':'bpid','bpid':'apid'}), on=['apid','bpid'], how='left', indicator=True)['_merge'].values == "left_only" ].copy()
        # Only one include per path b per round
        includes.sort_values(['bpid'], inplace=True)
        includes = includes[includes['bpid'] != includes['bpid'].shift(1)].copy()
        # Include those scaffolds
        if len(test_paths):
            test_paths = test_paths.merge(includes[['tpid','bpid']].rename(columns={'tpid':'pid'}), on=['pid'], how='inner')
            test_paths['pid'] = test_paths['bpid']
            test_paths.drop(columns=['bpid'], inplace=True)
            scaffold_paths = scaffold_paths[np.isin(scaffold_paths['pid'], np.concatenate([includes['apid'].values, includes['bpid'].values])) == False].copy()
            scaffold_paths = pd.concat([scaffold_paths, test_paths], ignore_index=True)
            scaffold_paths.sort_values(['pid','pos'], inplace=True)
        # Get the not yet used duplications and update them
        duplications = duplications[ duplications.merge(includes[['ldid','rdid']], on=['ldid','rdid'], how='left', indicator=True)['_merge'].values == "left_only"].copy()
        includes = duplications[['apid','bpid']].drop_duplicates()
        duplications = GetDuplications(scaffold_paths, ploidy)
        duplications = duplications.merge(includes, on=['apid','bpid'], how='inner')
#
    return scaffold_paths

def RemoveNearlyDuplicatedPaths(scaffold_paths, ploidy):
    # Get duplications that contain both path ends for side a
    duplications = GetDuplications(scaffold_paths, ploidy)
    scaf_len = scaffold_paths.groupby(['pid'])['pos'].max().reset_index()
    ends = duplications.groupby(['apid','bpid'])['apos'].agg(['min','max']).reset_index()
    ends['alen'] = ends[['apid']].rename(columns={'apid':'pid'}).merge(scaf_len, on=['pid'], how='left')['pos'].values
    ends = ends.loc[(ends['min'] == 0) & (ends['max'] == ends['alen']), ['apid','bpid','alen']].copy()
    duplications = duplications.merge(ends, on=['apid','bpid'], how='inner')
    # Only keep duplications, where path a is haploid
    nhaps = GetNumberOfHaplotypes(scaffold_paths, ploidy)
    duplications['anhaps'] = duplications[['apid']].rename(columns={'apid':'pid'}).merge(nhaps, on=['pid'], how='left')['nhaps'].values
    duplications = duplications[duplications['anhaps'] == 1].drop(columns=['anhaps'])
    # Check if they are on the same or opposite strand (alone it is useless, but it sets the requirements for the direction of change for the positions)
    duplications = AddStrandToDuplications(duplications, scaffold_paths, ploidy)
    duplications['samedir'] = duplications['astrand'] == duplications['bstrand']
    duplications.drop(columns=['astrand','bstrand'], inplace=True)
    # Start at position 0 and check if we can extend only accepting differences in distance
    duplications = SeparateDuplicationsByHaplotype(duplications, scaffold_paths, ploidy)
    ext_dups = []
    edups = duplications[duplications['apos'] == 0].copy()
    edups['did'] = np.arange(len(edups))
    ext_dups.append(edups.copy())
    while len(edups):
        # Get next position to see if we can extend the duplication
        edups = GetPositionsBeforeDuplication(edups, scaffold_paths, ploidy, True)
        # Remove everything that does not fit (different distances are allowed)
        edups = edups[(edups['aprev_pos'] >= 0) & (edups['bprev_pos'] >= 0)].drop(columns=['adist','bdist'])
        edups['apos'] = edups['aprev_pos']
        edups['bpos'] = edups['bprev_pos']
        # Check if we have a duplication at the new position
        edups = edups[['apid','apos','ahap','bpid','bpos','bhap','samedir','did']].merge(duplications, on=['apid','apos','ahap','bpid','bpos','bhap','samedir'], how='inner')
        # Insert the valid extensions
        ext_dups.append(edups.copy())
    ext_dups = pd.concat(ext_dups, ignore_index=True)
    # Check if the whole patha is duplicated
    ext_dups.sort_values(['did','apos'], inplace=True)
    duplications = ext_dups.drop_duplicates()
    duplications = duplications.groupby(['did','apid','alen','samedir'])['apos'].agg(['min','max']).reset_index()
    duplications = duplications[(duplications['min'] == 0) & (duplications['max'] == duplications['alen'])].drop_duplicates()
    # Remove the completely duplicated paths
    scaffold_paths = scaffold_paths[np.isin(scaffold_paths['pid'], duplications['apid'].values) == False].copy()
#
    return scaffold_paths

def CombineOnMatchingExtensions(scaffold_paths, graph_ext, scaffold_graph, scaf_bridges, ploidy):
    # Extend scaffold_paths, but keep the old version and only take the new paths if it connects two uniquely placeable old paths
    old_paths = scaffold_paths.copy()
    extendable_pids = np.unique(scaffold_paths['pid'].values)
    extendable_sides = pd.DataFrame( {'apid':np.repeat(extendable_pids, 2), 'aside':np.tile(['l','r'], len(extendable_pids)), 'max_len':scaffold_graph['length'].max()} ) # Only allow to extend as long as a single read spans it (The length of the single read will be put in later, but it cannot be longer than max span of a single read) 
    ext_phase = extendable_sides[['apid','aside']].copy()
    ext_phase['phase'] = scaffold_paths[[f'phase{h}' for h in range(ploidy)]].max().max() + 1 + np.arange(len(ext_phase))
    while len(extendable_sides):
        # Get all paths
        extendable_pids = np.unique(extendable_sides['apid'].values)
        ends = scaffold_paths[np.isin(scaffold_paths['pid'], extendable_pids)].groupby(['pid']).size().reset_index(name='len').rename(columns={'pid':'apid'})
        # Get present haplotypes
        ends = ends.loc[np.repeat(ends.index.values, ploidy)]
        ends['ahap'] = ends.groupby(['apid'], sort=False).cumcount()
        ends['phase'] = 1
        for h in range(1,ploidy):
            ends.loc[ends['ahap'] == h, 'phase'] = scaffold_paths[np.isin(scaffold_paths['pid'], extendable_pids)].groupby(['pid'])[f'phase{h}'].max().values
        ends = ends[ends['phase'] > 0].drop(columns=['phase'])
        ends.reset_index(drop=True, inplace=True)
        # Keep only path with at least two scaffolds (all scaffolds with connections must be in at least a two scaffold path until we trim)
        ends = ends[ends['len'] > 1].drop(columns='len')
        # Duplicate path to have both sides of it
        ends = ends.loc[np.repeat(ends.index.values, 2)]
        ends['aside'] = np.where(ends.groupby(['apid','ahap'], sort=False).cumcount() == 0, 'l', 'r')
        ends.reset_index(drop=True, inplace=True)
        ends = ends.merge(extendable_sides[['apid','aside']], on=['apid','aside'], how='inner')
        ends, patha = GetPathAFromEnds(ends, scaffold_paths, ploidy)
        # Get matching origins
        cur_org = MatchOriginsToPathA(patha, graph_ext)
        extensions = ends.merge(cur_org.rename(columns={'pid':'opid'}), on='opid', how='inner')
        # Get paired extensions and remove haplotype, because the extension has to be unique for both together
        extensions = extensions.drop(columns=['ahap','opid']).drop_duplicates()
        extensions = extensions.merge(graph_ext['pairs'], on='oindex', how='inner')
        extensions = extensions.drop(columns=['oindex']).drop_duplicates()
        cols = ['length']+[f'{n}{s}' for s in range(1,graph_ext['ext']['length'].max()) for n in ['scaf','strand','dist']]
        extensions[cols] = graph_ext['ext'].loc[extensions['eindex'].values, cols].values
        extensions.sort_values(['apid','aside'], inplace=True)
        if len(extensions) == 0:
            extendable_sides = []
        else:
            # Get consistent part of extensions
            con_ext = extensions.groupby(['apid','aside'], sort=False)['length'].min().reset_index()
            con_ext.rename(columns={'length':'max_len'}, inplace=True) # This is the maximum length it can be consistent (cannot be longer consistent as the shortest extension is long)
            con_ext['max_len'] = np.minimum( con_ext['max_len'], con_ext[['apid','aside']].merge(extendable_sides, on=['apid','aside'], how='left')['max_len'].values ) # Only allow to extend as long as a single read spans it starting from the unextended path end
            con_ext['len'] = 1
            for s in range(1,con_ext['max_len'].max()):
                cur_ext = extensions[['apid','aside',f'scaf{s}',f'strand{s}',f'dist{s}']].drop_duplicates()
                cur_ext = cur_ext[((cur_ext[['apid','aside']] == cur_ext[['apid','aside']].shift(1)).all(axis=1) == False) & ((cur_ext[['apid','aside']] == cur_ext[['apid','aside']].shift(-1)).all(axis=1) == False)].copy()
                if len(cur_ext) == 0:
                    con_ext = con_ext[con_ext['len'] > 1].copy()
                    break
                else:
                    con_ext[[f'scaf{s}',f'strand{s}',f'dist{s}']] = con_ext[['apid','aside']].merge(cur_ext, on=['apid','aside'], how='left')[[f'scaf{s}',f'strand{s}',f'dist{s}']].values
                    con_ext[[f'scaf{s}',f'dist{s}']] = con_ext[[f'scaf{s}',f'dist{s}']].astype(float)
                    con_ext.loc[np.isnan(con_ext[f'scaf{s}']) == False, 'len'] += 1
                    if s==1:
                        # Only keep the sides that have any consistent extension at all
                        con_ext = con_ext[con_ext['len'] > 1].copy()
                    # Filter the extensions for next round
                    extensions = extensions.merge( cur_ext[['apid','aside']].merge(con_ext.loc[con_ext['max_len'] > s+1, ['apid','aside']], on=['apid','aside'], how='inner'), on=['apid','aside'], how='inner')
                    if len(extensions) == 0:
                        break
            con_ext.drop(columns=['max_len'], inplace=True)
            # Extend scaffold_paths by one scaffold in each direction according to con_ext (do not extend for more in a single iteration to make sure we do not miss any alternatives)
            con_ext['phase'] = con_ext[['apid','aside']].merge(ext_phase, on=['apid','aside'], how='left')['phase'].values
            if len(con_ext):
                new_paths = []
                if np.sum(con_ext['aside'] == 'l'):
                    cur_ext = con_ext[con_ext['aside'] == 'l'].copy()
                    # Enter the distance at first position
                    scaffold_paths['new_dist'] = scaffold_paths[['pid']].merge(cur_ext[['apid','dist1']].rename(columns={'apid':'pid'}), on='pid', how='left')['dist1'].values
                    scaffold_paths.loc[(scaffold_paths['pos'] == 0) & (np.isnan(scaffold_paths['new_dist']) == False), 'dist0'] = scaffold_paths.loc[(scaffold_paths['pos'] == 0) & (np.isnan(scaffold_paths['new_dist']) == False), 'new_dist'].astype(int).values
                    # Shift existing scaffolds to make space
                    scaffold_paths['pos'] += np.where(np.isnan(scaffold_paths['new_dist']), 0, 1)
                    scaffold_paths.drop(columns=['new_dist'], inplace=True)
                    # Prepare the new scaffolds on the left
                    new_paths = cur_ext[['apid','phase','scaf1','strand1']].rename(columns={'apid':'pid','phase':'phase0','scaf1':'scaf0','strand1':'strand0'})
                    new_paths['dist0'] = 0 # First scaffold in path always has dist 0 (if we later extend more to it, we will fix it then)
                    new_paths['pos'] = 0
                    # The left extensions need to be inverted
                    new_paths['strand0'] = np.where(new_paths['strand0'] == '+', '-', '+')
                    new_paths = [new_paths]
                if np.sum(con_ext['aside'] == 'r'):
                    cur_ext = con_ext[con_ext['aside'] == 'r'].copy()
                    # Get position in path for extensions
                    cur_ext['pos'] = cur_ext[['apid']].rename(columns={'apid':'pid'}).merge(scaffold_paths.groupby(['pid'])['pos'].max().reset_index(), on='pid', how='left')['pos'].values + 1
                    # Prepare the new scaffolds on the right
                    new_paths.append( cur_ext[['apid','pos','phase','scaf1','strand1','dist1']].rename(columns={'apid':'pid','phase':'phase0','scaf1':'scaf0','strand1':'strand0','dist1':'dist0'}) )
                # Add missing columns and convert to int
                new_paths = pd.concat(new_paths, ignore_index=True)
                for h in range(1, ploidy):
                    new_paths[f'phase{h}'] = -new_paths['phase0']
                    new_paths[[f'scaf{h}',f'strand{h}',f'dist{h}']] = [-1, '', 0]
                new_paths[[f'{n}{h}' for h in range(ploidy) for n in ['scaf','dist']]] = new_paths[[f'{n}{h}' for h in range(ploidy) for n in ['scaf','dist']]].astype(int).values
                # Join new_paths with scaffold_paths
                scaffold_paths = pd.concat([scaffold_paths, new_paths], ignore_index=True)
                scaffold_paths.sort_values(['pid','pos'], inplace=True)
            # Next round we only need to look for extensions for sides that could be extended this round and we keep track here how long it is allowed to be extended so we do not exceed the reach of the longest read starting from the unextended paths
            extendable_sides = con_ext[['apid','aside','len']].rename(columns={'len':'max_len'})
            extendable_sides['max_len'] -= 1
            extendable_sides = extendable_sides[extendable_sides['max_len'] > 1].copy() # Remove unextendible ones already here
    # Remove duplicates and try to combine paths
    scaffold_paths = RemoveDuplicates(scaffold_paths, True, ploidy)
    scaffold_paths = CombinePathOnUniqueOverlap(scaffold_paths, scaffold_graph, graph_ext, scaf_bridges, ploidy)
    # Only keep extended paths and assign new pids
    scaf_len = old_paths.groupby(['pid'])['pos'].max().reset_index(name='old_len')
    scaf_len['new_len'] = scaf_len[['pid']].merge(scaffold_paths.groupby(['pid'])['pos'].max().reset_index(), on='pid', how='left')['pos'].fillna(0).astype(int)
    scaf_len = scaf_len[scaf_len['old_len'] != scaf_len['new_len']].copy()
    max_pid = old_paths['pid'].max()
    scaf_len['new_pid'] = np.where(scaf_len['new_len'] > 0, (scaf_len['new_len'] > 0).cumsum() + max_pid, -1)
    scaffold_paths = scaffold_paths.merge(scaf_len[['pid','new_pid']], on=['pid'], how='inner')
    scaffold_paths['pid'] = scaffold_paths['new_pid']
    scaffold_paths.drop(columns=['new_pid'], inplace=True)
    # Merge old and new paths and find duplications to identify where the old paths is now included in the new paths
    scaffold_paths = pd.concat([old_paths, scaffold_paths], ignore_index=True)
    duplications = GetDuplications(scaffold_paths, ploidy)
    duplications = duplications[np.isin(duplications['apid'], scaf_len['pid'].values) & (duplications['bpid'] > max_pid)].copy()
    tmp_len = scaffold_paths.groupby(['pid'])['pos'].max().reset_index()
    duplications = RequireDuplicationAtPathEnd(duplications, tmp_len, ['apid','bpid'], patha_only=True)
    # Check if they are on the same or opposite strand (alone it is useless, but it sets the requirements for the direction of change for the positions)
    duplications = AddStrandToDuplications(duplications, scaffold_paths, ploidy)
    duplications['samedir'] = duplications['astrand'] == duplications['bstrand']
    duplications.drop(columns=['astrand','bstrand'], inplace=True)
    # Extend the duplications with valid distances from each end
    duplications = SeparateDuplicationsByHaplotype(duplications, scaffold_paths, ploidy)
    ldups = ExtendDuplicationsFromEnd(duplications, scaffold_paths, 'l', tmp_len, ploidy)
    rdups = ExtendDuplicationsFromEnd(duplications, scaffold_paths, 'r', tmp_len, ploidy)
    rdups['did'] += 1 + ldups['did'].max()
    duplications = pd.concat([ldups,rdups], ignore_index=True)
    # Require that both ends of patha are included
    ends = GetEndDuplicationsBelongTo(duplications, tmp_len)
    ends = ends[(ends['aleft'] & ends['aright'])].drop(columns=['did','amin','amax','alen','aleft','aright','bleft','bright','bhap','matches','blen'])
    ends.drop_duplicates(inplace=True)
    # Require that all haplotypes are found at that position
    ends['ahap'] = (np.repeat("hap", len(ends)).astype(np.object) + ends['ahap'].astype(str)).astype(str)
    ends['present'] = True
    ends = ends.pivot(index=['apid','bpid','bmin','bmax'], columns='ahap', values='present').reset_index().rename_axis(None, axis=1)
    for h in range(ploidy):
        if f'hap{h}' in ends.columns:
            ends[f'hap{h}'] = ends[f'hap{h}'].fillna(False)
        else:
            ends[f'hap{h}'] = False
    ends = ends.merge(scaffold_paths[np.isin(scaffold_paths['pid'], scaf_len['pid'].values)].groupby(['pid'])[[f'phase{h}' for h in range(ploidy)]].max().reset_index().rename(columns={'pid':'apid'}), on='apid', how='left')
    ends = ends[ (ends[[f'hap{h}' for h in range(ploidy)]].values == (ends[[f'phase{h}' for h in range(ploidy)]] > 0).values).all(axis=1) ].drop(columns=[f'{n}{h}' for h in range(ploidy) for n in ['hap','phase']])
    # Require unique placement
    ends = ends[ ends[['apid']].merge(ends.groupby(['apid']).size().reset_index(name='nalt'), on='apid', how='left')['nalt'].values == 1 ].copy()
    # Require multiple original paths on one new paths
    ends = ends[ ends[['bpid']].merge(ends.groupby(['bpid']).size().reset_index(name='count'), on='bpid', how='left')['count'].values > 1 ].copy()
    # Insert the new combined paths (without the non-combining extensions) and remove duplicates
    ends = ends.groupby(['bpid']).agg({'bmin':'min','bmax':'max'}).reset_index()
    ends.rename(columns={'bpid':'pid','bmin':'minpos','bmax':'maxpos'}, inplace=True)
    scaffold_paths = scaffold_paths.merge(ends, on='pid', how='inner')
    scaffold_paths = scaffold_paths[(scaffold_paths['pos'] >= scaffold_paths['minpos']) & (scaffold_paths['pos'] <= scaffold_paths['maxpos'])].drop(columns=['minpos','maxpos'])
    scaffold_paths['pos'] = scaffold_paths.groupby(['pid']).cumcount()
    scaffold_paths = SetDistanceAtFirstPositionToZero(scaffold_paths, ploidy)
    scaffold_paths = pd.concat([old_paths, scaffold_paths], ignore_index=True)
    scaffold_paths = RemoveDuplicates(scaffold_paths, True, ploidy)
#
    return scaffold_paths

def GetNextPositionInPathB(ends, scaffold_paths, ploidy):
    # Get next position (jumping over deletions)
    ends['opos'] = ends['mpos']
    update = np.repeat(True, len(ends))
    while np.sum(update):
        ends.loc[update, 'mpos'] += ends.loc[update, 'dir']
        ends = GetPositionFromPaths(ends, scaffold_paths, ploidy, 'next_scaf', 'scaf', 'bpid', 'mpos', 'bhap')
        update = ends['next_scaf'] < 0
#
    return ends

def GetFullNextPositionInPathB(ends, scaffold_paths, ploidy):
    ends = GetNextPositionInPathB(ends, scaffold_paths, ploidy)
    ends = GetPositionFromPaths(ends, scaffold_paths, ploidy, 'next_strand', 'strand', 'bpid', 'mpos', 'bhap')
    ends['next_strand'] = ends['next_strand'].fillna('')
    if np.sum(ends['dir'] == -1):
        ends.loc[ends['dir'] == -1, 'next_strand'] = np.where(ends.loc[ends['dir'] == -1, 'next_strand'] == '+', '-', '+')
    ends['dist_pos'] = np.where(ends['dir'] == 1, ends['mpos'], ends['opos'])
    ends = GetPositionFromPaths(ends, scaffold_paths, ploidy, 'next_dist', 'dist', 'bpid', 'dist_pos', 'bhap')
#
    return ends

def TrimAmbiguousOverlap(scaffold_paths, scaffold_graph, ploidy):
    # Get duplications that contain a path ends for side a
    duplications = GetDuplications(scaffold_paths, ploidy)
    scaf_len = scaffold_paths.groupby(['pid'])['pos'].max().reset_index()
    duplications = RequireDuplicationAtPathEnd(duplications, scaf_len, ['apid','bpid'], patha_only=True)
    # Check if they are on the same or opposite strand (alone it is useless, but it sets the requirements for the direction of change for the positions)
    duplications = AddStrandToDuplications(duplications, scaffold_paths, ploidy)
    duplications['samedir'] = duplications['astrand'] == duplications['bstrand']
    duplications.drop(columns=['astrand','bstrand'], inplace=True)
    # Extend the duplications with valid distances from each end
    duplications = SeparateDuplicationsByHaplotype(duplications, scaffold_paths, ploidy)
    ldups = ExtendDuplicationsFromEnd(duplications, scaffold_paths, 'l', scaf_len, ploidy)
    rdups = ExtendDuplicationsFromEnd(duplications, scaffold_paths, 'r', scaf_len, ploidy)
    rdups['did'] += 1 + ldups['did'].max()
    duplications = pd.concat([ldups,rdups], ignore_index=True)
    # Check at what end the duplications are
    ends = GetEndDuplicationsBelongTo(duplications, scaf_len)
    # Filter the duplications that are either at no end (allowed for pathb) or at both ends (not allowed for pathb) (both ends means one of the paths is fully covered by the other, thus will be removed as a duplication and is ignored here)
    ends = ends[(ends['aleft'] != ends['aright']) & ((ends['bleft'] & ends['bright']) == False)].copy()
    ends['aside'] = np.where(ends['aleft'], 'l', 'r')
    ends['bside'] = np.where(ends['bleft'], 'l', np.where(ends['bright'], 'r', 'm'))
    ends.drop(columns=['aleft','aright','bleft','bright','bmin','bmax','matches','blen'], inplace=True)
    duplications = duplications.merge(ends[['did']], on=['did'], how='inner')
    ends['samedir'] = duplications.groupby(['did'])['samedir'].first().values
    # Filter connectable overlaps (coming from different sides), since they will be removed on the side with multiple options (thus we keep the single option side)
    ends = ends[(ends['samedir'] == (ends['aside'] == ends['bside'])) | (ends['bside'] == 'm')].copy()
    # Get the maximum overlap for each haplotype
    ends = pd.concat([ ends[ends['aside'] == 'l'].groupby(['apid','ahap','aside'])['amax'].max().reset_index(name='apos'), ends[ends['aside'] == 'r'].groupby(['apid','ahap','aside'])['amin'].min().reset_index(name='apos') ], ignore_index=True)
    ends['dir'] = np.where(ends['aside'] == 'l', -1, +1)
    ends.drop(columns=['aside'], inplace=True)
    # Find first unambiguous position (by going one in the opposite direction and skipping deletions)
    ends['apos'] -= ends['dir'] # First go in the opposite direction
    while True:
        ends = GetPositionFromPaths(ends, scaffold_paths, ploidy, 'from', 'scaf', 'apid', 'apos', 'ahap')
        if np.sum(ends['from'] < 0) == 0:
            break
        else:
            ends.loc[ends['from'] < 0, 'apos'] -= ends.loc[ends['from'] < 0, 'dir'] # Start after deletions
    ends.rename(columns={'apid':'pid','ahap':'hap'}, inplace=True)
    # Take the haplotype with the longest support
    ends = ends.groupby(['pid','dir'])['apos'].agg(['min','max']).reset_index()
    ends['pos'] = np.where(ends['dir'] == 1, ends['max'], ends['min'])
    ends.drop(columns=['min','max'], inplace=True)
    # In case the ambiguous part from both sides overlaps, break up the path into the individual scaffolds (which are later removed in case they are already present somewhere else)
    ends['overlap'] = False
    ends.loc[(ends['pid'] == ends['pid'].shift(1)) & (ends['pos'] < ends['pos'].shift(1)), 'overlap'] = True
    ends.loc[(ends['pid'] == ends['pid'].shift(-1)) & (ends['pos'] > ends['pos'].shift(-1)), 'overlap'] = True
    rem_pid = np.unique(ends.loc[ends['overlap'], 'pid'].values)
    ends = ends[ends['overlap'] == False].drop(columns=['overlap'])
    single_scaffolds = np.unique(scaffold_paths.loc[np.isin(scaffold_paths['pid'], rem_pid), [f'scaf{h}' for h in range(ploidy)]].values)
    scaffold_paths = scaffold_paths[np.isin(scaffold_paths['pid'], rem_pid) == False].copy()
    single_scaffolds = pd.DataFrame({'pid':np.arange(len(single_scaffolds))+ 1 + scaffold_paths['pid'].max(), 'pos':0, 'phase0': -1, 'scaf0':single_scaffolds, 'strand0':'+', 'dist0':0})
    single_scaffolds = single_scaffolds[single_scaffolds['scaf0'] >= 0].copy()
    single_scaffolds['phase0'] = single_scaffolds['pid'] + 1
    for h in range(1,ploidy):
        single_scaffolds[f'phase{h}'] = -single_scaffolds['phase0']
        single_scaffolds[f'scaf{h}'] = -1
        single_scaffolds[f'strand{h}'] = ''
        single_scaffolds[f'dist{h}'] = 0
    scaffold_paths = pd.concat([scaffold_paths, single_scaffolds], ignore_index=True)
    # Separate the ambiguous overlap on both sides into their own paths and remove duplicates
    ends.rename(columns={'new_pos':'new_pid'}, inplace=True)
    ends['new_pid'] = np.arange(len(ends)) + 1 + scaffold_paths['pid'].max()
    max_len = scaffold_paths.groupby(['pid'])['pos'].max().max()
    scaffold_paths[['new_pid','end']] = scaffold_paths[['pid']].merge(ends.loc[ends['dir'] == -1, ['pid','new_pid','pos']], on=['pid'], how='left')[['new_pid','pos']].fillna(0).values.astype(int)
    scaffold_paths.loc[scaffold_paths['pos'] < scaffold_paths['end'], 'pid'] = scaffold_paths.loc[scaffold_paths['pos'] < scaffold_paths['end'], 'new_pid']
    scaffold_paths[['new_pid','end']] = scaffold_paths[['pid']].merge(ends.loc[ends['dir'] == 1, ['pid','new_pid','pos']], on=['pid'], how='left')[['new_pid','pos']].fillna(max_len).values.astype(int)
    scaffold_paths.loc[scaffold_paths['pos'] > scaffold_paths['end'], 'pid'] = scaffold_paths.loc[scaffold_paths['pos'] > scaffold_paths['end'], 'new_pid']
    scaffold_paths.drop(columns=['new_pid','end'], inplace=True)
    scaffold_paths.sort_values(['pid','pos'], inplace=True)
    scaffold_paths['pos'] = scaffold_paths.groupby(['pid']).cumcount()
    scaffold_paths = SetDistanceAtFirstPositionToZero(scaffold_paths, ploidy)
    scaffold_paths = RemoveDuplicates(scaffold_paths, True, ploidy)
#
    return scaffold_paths

def TrimCircularPaths(scaffold_paths, ploidy):
    # Find circular paths (Check where the first scaffold in paths is duplicated and see if the duplications reaches the path end from there)
    circular = [ pd.concat( [scaffold_paths.loc[(scaffold_paths['pos'] > 0) & (scaffold_paths[f'scaf{h1}'] >= 0), ['pid','pos',f'scaf{h1}',f'strand{h1}']].rename(columns={f'scaf{h1}':'scaf',f'strand{h1}':'strand'}).merge(scaffold_paths.loc[(scaffold_paths['pos'] == 0) & (scaffold_paths[f'scaf{h2}'] >= 0), ['pid',f'scaf{h2}',f'strand{h2}']].rename(columns={f'scaf{h2}':'scaf',f'strand{h2}':'strand'}), on=['pid','scaf','strand'], how='inner') for h1 in range(ploidy)], ignore_index=True ).drop_duplicates() for h2 in range(ploidy) ]
    circular = pd.concat(circular, ignore_index=True).groupby(['pid','pos']).size().reset_index(name='nhaps')
    circular['start_pos'] = circular['pos']
    scaf_len = scaffold_paths.groupby(['pid'])['pos'].max().reset_index()
    circular['len'] = circular[['pid']].merge(scaf_len, on=['pid'], how='left')['pos'].values
    circular['circular'] = False
    p = 0
    while True:
        # All haplotypes need to be duplicated
        circular = circular[circular['circular'] | (circular['nhaps'] == circular[['pid']].merge((scaffold_paths.loc[scaffold_paths['pos'] == p, ['pid']+[f'phase{h}' for h in range(ploidy)]].set_index(['pid']) >= 0).sum(axis=1).reset_index(name='nhaps'), on=['pid'], how='left')['nhaps'].values)].copy()
        # If we reach the end of path it is a valid circular duplication
        circular.loc[circular['pos'] == circular['len'], 'circular'] = True
        if np.sum(circular['circular'] == False):
            # Check next position
            circular['pos'] += 1
            p += 1
            circular['nhaps'] = 0
            for h2 in range(ploidy):
                cur_test = circular.loc[circular['circular'] == False, ['pid','pos']].reset_index().merge(scaffold_paths.loc[(scaffold_paths['pos'] == p) & (scaffold_paths[f'scaf{h2}'] >= 0), ['pid',f'scaf{h2}',f'strand{h2}',f'dist{h2}']].rename(columns={f'scaf{h2}':'scaf',f'strand{h2}':'strand',f'dist{h2}':'dist'}), on=['pid'], how='inner')
                circular.loc[pd.concat([cur_test.merge(scaffold_paths[['pid','pos',f'scaf{h1}',f'strand{h1}',f'dist{h1}']].rename(columns={f'scaf{h1}':'scaf',f'strand{h1}':'strand',f'dist{h1}':'dist'}), on=['pid','pos','scaf','strand','dist'], how='inner') for h1 in range(ploidy)], ignore_index=True).drop_duplicates()['index'].values, 'nhaps'] += 1
        else:
            break
    # Remove longest circular duplication for each path
    circular = circular.groupby(['pid'])['start_pos'].min().reset_index()
    scaffold_paths['trim'] = scaffold_paths[['pid']].merge(circular, on=['pid'], how='left')['start_pos'].fillna(-1).values.astype(int)
    scaffold_paths = scaffold_paths[(scaffold_paths['pos'] < scaffold_paths['trim']) | (scaffold_paths['trim'] == -1)].drop(columns=['trim'])
    # Check if the remainder is duplicated somewhere else
    scaffold_paths = RemoveDuplicates(scaffold_paths, True, ploidy)
#
    return scaffold_paths

def TestPrint(scaffold_paths):
    #print(scaffold_paths)
    #print( scaffold_paths[np.isin(scaffold_paths['pid'], scaffold_paths.loc[np.isin(scaffold_paths['scaf0'], [57465,57466]), 'pid'].values )] )
    #print( scaffold_paths[np.isin(scaffold_paths['pid'], scaffold_paths.loc[np.isin(scaffold_paths['scaf0'], [9064, 41269, 51925, 67414, 123224, 123225, 123226, 123227, 123228, 123229, 123230, 123231, 123236, 123237, 123238]), 'pid'].values )] ) # Test 6
    #print( scaffold_paths[np.isin(scaffold_paths['pid'], scaffold_paths.loc[np.isin(scaffold_paths['scaf0'], [7, 1440, 7349, 10945, 11769, 23515, 29100, 30446, 31108, 31729, 31737, 31758, 32135, 32420, 45782, 45783, 47750, 49372, 54753, 74998, 76037, 86633, 93920, 95291, 105853, 110006, 113898]), 'pid'].values )] ) # Test 5
    return

def TraverseScaffoldGraph(scaffolds, scaffold_graph, graph_ext, scaf_bridges, org_scaf_conns, ploidy, max_loop_units):
    # Get phased haplotypes
    if len(scaffold_graph):
        scaffold_paths, handled_origins = FollowUniquePathsThroughGraph(graph_ext)
        scaffold_paths, handled_scaf_conns = AddPathThroughLoops(scaffold_paths, scaffold_graph, graph_ext, scaf_bridges, org_scaf_conns, ploidy, max_loop_units)
        scaffold_paths, handled_scaf_conns = AddPathThroughInvertedRepeats(scaffold_paths, handled_scaf_conns, scaffold_graph, scaf_bridges, ploidy)
        scaffold_paths = AddUntraversedConnectedPaths(scaffold_paths, graph_ext, handled_origins, handled_scaf_conns)
    scaffold_paths = AddUnconnectedPaths(scaffold_paths, scaffolds, scaffold_graph)
#
    # Turn them into full path with ploidy
    scaffold_paths.insert(2, 'phase0', scaffold_paths['pid'].values+1) #+1, because phases must be larger than zero to be able to be positive and negative (negative means identical to main paths at that position)
    for h in range(1,ploidy):
        scaffold_paths[f'phase{h}'] = -scaffold_paths['phase0'].values
        scaffold_paths[f'scaf{h}'] = -1
        scaffold_paths[f'strand{h}'] = ''
        scaffold_paths[f'dist{h}'] = 0
    CheckScaffoldPathsConsistency(scaffold_paths)
    if len(scaf_bridges):
        CheckIfScaffoldPathsFollowsValidBridges(scaffold_paths, scaf_bridges, ploidy)
#
    # Combine paths as much as possible
    if len(scaffold_graph):
        print("Start")
        print(len(np.unique(scaffold_paths['pid'].values)))
        TestPrint(scaffold_paths)
        old_nscaf = 0
        for i in range(3):
            n = 1
            while old_nscaf != len(np.unique(scaffold_paths['pid'].values)):
                time0 = process_time()
                print(f"Iteration {n}")
                n+=1
                 # First Merge then Combine to not accidentially merge a haplotype, where the other haplotype of the paths is not compatible and thus joining wrong paths
                scaffold_paths = MergeHaplotypes(scaffold_paths, graph_ext, scaf_bridges, ploidy)
                time1 = process_time()
                print(str(timedelta(seconds=time1-time0)), len(np.unique(scaffold_paths['pid'].values)))
                TestPrint(scaffold_paths)
                old_nscaf = len(np.unique(scaffold_paths['pid'].values))
                scaffold_paths = CombinePathOnUniqueOverlap(scaffold_paths, scaffold_graph, graph_ext, scaf_bridges, ploidy)
                print(str(timedelta(seconds=process_time()-time1)), len(np.unique(scaffold_paths['pid'].values)))
                TestPrint(scaffold_paths)
                CheckScaffoldPathsConsistency(scaffold_paths)
                CheckIfScaffoldPathsFollowsValidBridges(scaffold_paths, scaf_bridges, ploidy)
            if i==0:
                print("RemoveDuplicates")
                scaffold_paths = RemoveDuplicates(scaffold_paths, True, ploidy)
            elif i==1:
                print("PlaceUnambigouslyPlaceables")
                scaffold_paths = PlaceUnambigouslyPlaceablePathsAsAlternativeHaplotypes(scaffold_paths, graph_ext, scaffold_graph, scaf_bridges, ploidy)
                scaffold_paths = RemoveNearlyDuplicatedPaths(scaffold_paths, ploidy)
            if i != 2:
                print(len(np.unique(scaffold_paths['pid'].values)))
                TestPrint(scaffold_paths)
                CheckScaffoldPathsConsistency(scaffold_paths)
                CheckIfScaffoldPathsFollowsValidBridges(scaffold_paths, scaf_bridges, ploidy)
        print("CombineOnMatchingExtensions")
        scaffold_paths = CombineOnMatchingExtensions(scaffold_paths, graph_ext, scaffold_graph, scaf_bridges, ploidy)
        print(len(np.unique(scaffold_paths['pid'].values)))
        TestPrint(scaffold_paths)
        print("TrimAmbiguousOverlap")
        scaffold_paths = TrimAmbiguousOverlap(scaffold_paths, scaffold_graph, ploidy)
        print(len(np.unique(scaffold_paths['pid'].values)))
        TestPrint(scaffold_paths)
        print("TrimCircularPaths")
        scaffold_paths = TrimCircularPaths(scaffold_paths, ploidy)
        print(len(np.unique(scaffold_paths['pid'].values)))
        TestPrint(scaffold_paths)
        CheckScaffoldPathsConsistency(scaffold_paths)
        CheckIfScaffoldPathsFollowsValidBridges(scaffold_paths, scaf_bridges, ploidy)

    return scaffold_paths

def AssignNewPhases(scaffold_paths, phase_change_in, ploidy):
    phase_change = phase_change_in.copy()
    phase_change['new_phase'] = phase_change['from_phase']
    while True:
        phase_change['gphase'] = phase_change[['new_phase']].rename(columns={'new_phase':'to_phase'}).merge(phase_change[['to_phase','new_phase']], on=['to_phase'], how='left')['new_phase'].values
        if np.sum(np.isnan(phase_change['gphase']) == False):
            phase_change.loc[np.isnan(phase_change['gphase']) == False, 'new_phase'] = phase_change.loc[np.isnan(phase_change['gphase']) == False, 'gphase'].astype(int)
        else:
            break
    phase_change.drop(columns=['from_phase','gphase'], inplace=True)
    phase_change = pd.concat([phase_change, phase_change*-1], ignore_index=True)
    for h in range(ploidy):
        scaffold_paths['new_phase'] = scaffold_paths[[f'phase{h}']].rename(columns={f'phase{h}':'to_phase'}).merge(phase_change, on=['to_phase'], how='left')['new_phase'].values
        scaffold_paths.loc[np.isnan(scaffold_paths['new_phase']) == False, f'phase{h}'] = scaffold_paths.loc[np.isnan(scaffold_paths['new_phase']) == False, 'new_phase'].astype(int).values
    scaffold_paths.drop(columns=['new_phase'], inplace=True)
#
    return scaffold_paths

def PhaseScaffoldsWithScafBridges(scaffold_paths, scaf_bridges, ploidy):
    ## Combine phases where scaf_bridges leaves only one option
    # First get all bridges combining phases (ignoring deletions)
    test_bridges = []
    for h2 in range(ploidy):
        cur = scaffold_paths[['pid','pos',f'phase{h2}',f'scaf{h2}',f'strand{h2}',f'dist{h2}']].rename(columns={'pos':'to_pos',f'phase{h2}':'to_phase',f'scaf{h2}':'to',f'strand{h2}':'to_side',f'dist{h2}':'mean_dist'})
        cur['to_side'] = np.where(cur['to_side'] == '+', 'l', 'r')
        cur['to_hap'] = h2
        cur['update'] = True
        cur['from_phase'] = 0
        cur['from'] = -1
        cur['from_side'] = ''
        cur['from_pos'] = -1
        s = 1
        while np.sum(cur['update']):
            cur['deletion'] = False
            for h1 in range(ploidy):
                cur['from_hap'] = h1
                cur['from_phase'] = np.where(scaffold_paths['pid'] == scaffold_paths['pid'].shift(s), scaffold_paths[f'phase{h1}'].shift(s, fill_value=0), 0)
                cur['from'] = np.where(cur['from_phase'] > 0, scaffold_paths[f'scaf{h1}'].shift(s, fill_value=-2), np.where(cur['from_phase'] == 0, -2, scaffold_paths['scaf0'].shift(s, fill_value=-2)))
                cur['from_side'] = np.where(scaffold_paths[f'strand{h1}'].shift(s, fill_value='') == '+', 'r', 'l')
                cur['from_pos'] = cur['to_pos']-s
                cur['deletion'] = cur['deletion'] | (cur['from'] == -1)
                test_bridges.append( cur.loc[cur['update'] & (cur['from_phase'] > 0) & (cur['to_phase'] > 0) & (cur['to'] >= 0) & (cur['from'] >= 0) & (cur['from_phase'] != cur['to_phase']), ['pid','from_pos','from_hap','from_phase','from','from_side','to_pos','to_hap','to_phase','to','to_side','mean_dist']].copy() )
            s += 1
            cur['update'] = cur['deletion']
    test_bridges = pd.concat(test_bridges, ignore_index=True)
    # Filter to have only valid bridges
    test_bridges = test_bridges.merge(scaf_bridges[['from','from_side','to','to_side','mean_dist']], on=['from','from_side','to','to_side','mean_dist'], how='inner')
    test_bridges.sort_values(['pid','from_pos','from_hap','to_pos','to_hap'], inplace=True)
    # Combine phases without alternatives
    alt_count = test_bridges.groupby(['pid','from_pos','from_hap'], sort=False).size().values
    test_bridges['from_alts'] = np.repeat(alt_count, alt_count)
    test_bridges['to_alts'] = test_bridges[['pid','to_pos','to_hap']].merge(test_bridges.groupby(['pid','to_pos','to_hap']).size().reset_index(name='alts'), on=['pid','to_pos','to_hap'], how='left')['alts'].values
    test_bridges = test_bridges.loc[(test_bridges['from_alts'] == 1) & (test_bridges['to_alts'] == 1), ['from_phase','to_phase']].copy()
    scaffold_paths = AssignNewPhases(scaffold_paths, test_bridges, ploidy)
    # Handle deletions by assigning them to the following phase if that phase cannot connect to an alternative to going through the deletion
    deletions = []
    for h in range(ploidy):
        scaffold_paths['deletion'] = np.where(scaffold_paths[f'phase{h}'] > 0, scaffold_paths[f'scaf{h}'], scaffold_paths['scaf0']) < 0
        scaffold_paths['add'] = (scaffold_paths['deletion'] == False) & (scaffold_paths[f'phase{h}'] > 0)
        s = 1
        while True:
            scaffold_paths['add'] = scaffold_paths['add'] & scaffold_paths['deletion'].shift(s) & (scaffold_paths['pid'] == scaffold_paths['pid'].shift(s))
            scaffold_paths['from_phase'] = np.abs(scaffold_paths[f'phase{h}'].shift(s, fill_value=0)) # We need the phase of the deletion later, not the one of the checked connection, since we are looking for no valid connections
            if np.sum(scaffold_paths['add']):
                for h2 in range(ploidy):
                    if h2 != h:
                        for s2 in range(1,s+1):
                            scaffold_paths['from'] = scaffold_paths[f'scaf{h2}'].shift(s2, fill_value=-1)
                            scaffold_paths['from_side'] = scaffold_paths[f'strand{h2}'].shift(s2, fill_value='')
                            deletions.append( scaffold_paths.loc[scaffold_paths['add'] & (scaffold_paths[f'phase{h2}'].shift(s2) > 0) & (scaffold_paths['from'] >= 0), ['pid','pos','from_phase','from','from_side',f'phase{h}',f'scaf{h}',f'strand{h}',f'dist{h}']].rename(columns={f'phase{h}':'to_phase',f'scaf{h}':'to',f'strand{h}':'to_side',f'dist{h}':'mean_dist'}) )
                s += 1
                scaffold_paths.drop(columns=['from','from_side'], inplace=True)
            else:
                break
    scaffold_paths.drop(columns=['deletion','add','from_phase'], inplace=True)
    if len(deletions):
        deletions = pd.concat(deletions, ignore_index=True)
    if len(deletions):
        deletions['from_side'] = np.where(deletions['from_side'] == '+', 'r', 'l')
        deletions['to_side'] = np.where(deletions['to_side'] == '+', 'l', 'r')
        deletions['valid'] = deletions[['from','from_side','to','to_side','mean_dist']].merge(scaf_bridges[['from','from_side','to','to_side','mean_dist']], on=['from','from_side','to','to_side','mean_dist'], how='left', indicator=True)['_merge'].values == "both"
        deletions.sort_values(['pid','pos','from_phase'], inplace=True)
        valid = deletions.groupby(['pid','pos','from_phase'], sort=False)['valid'].agg(['max','size'])
        deletions['valid'] = np.repeat(valid['max'].values, valid['size'].values)
        deletions = deletions.loc[deletions['valid'] == False, ['from_phase','to_phase']].drop_duplicates()
        deletions.rename(columns={'from_phase':'to_phase','to_phase':'from_phase'}, inplace=True) # We need to assign 'to_phase' to 'from_phase', because from_phase is unique, but to_phase not necessarily
        scaffold_paths = AssignNewPhases(scaffold_paths, deletions, ploidy)
    # Combine adjacent deletions if the previous phase has no connection into the deletion (to a position of the deletion that is not the first)
    deletions = []
    for h in range(ploidy):
        scaffold_paths['deletion'] = np.where(scaffold_paths[f'phase{h}'] > 0, scaffold_paths[f'scaf{h}'], scaffold_paths['scaf0']) < 0
        scaffold_paths['add'] = (scaffold_paths['deletion'] == False) & (scaffold_paths['deletion'].shift(-1) == True) & (scaffold_paths[f'phase{h}'] > 0)
        scaffold_paths['from_phase'] = scaffold_paths[f'phase{h}'].shift(-1, fill_value=0)
        s = 2
        while True:
            scaffold_paths['add'] = scaffold_paths['add'] & scaffold_paths['deletion'].shift(-s) & (scaffold_paths['pid'] == scaffold_paths['pid'].shift(-s))
            scaffold_paths['to_phase'] = np.abs(scaffold_paths[f'phase{h}'].shift(-s, fill_value=0)) # We need the phase of the deletion later, not the one of the checked connection, since we are looking for no valid connections
            if np.sum(scaffold_paths['add']):
                for h2 in range(ploidy):
                    if h2 != h:
                        for s2 in range(2,s+1):
                            scaffold_paths['to'] = scaffold_paths[f'scaf{h2}'].shift(-s2, fill_value=-1)
                            scaffold_paths['to_side'] = scaffold_paths[f'strand{h2}'].shift(-s2, fill_value='')
                            scaffold_paths['mean_dist'] = scaffold_paths[f'dist{h2}'].shift(-s2, fill_value=0)
                            deletions.append( scaffold_paths.loc[scaffold_paths['add'] & (scaffold_paths[f'phase{h2}'].shift(-s2) > 0) & (scaffold_paths['to'] >= 0), ['pid','pos','from_phase',f'scaf{h}',f'strand{h}','to_phase','to','to_side','mean_dist']].rename(columns={f'scaf{h}':'from',f'strand{h}':'from_side'}) )
                s += 1
            else:
                break
    scaffold_paths.drop(columns=['deletion','add','from_phase','to_phase','to','to_side','mean_dist'], inplace=True, errors='ignore')
    if len(deletions):
        deletions = pd.concat(deletions, ignore_index=True)
        deletions = deletions[deletions['from_phase'] != deletions['to_phase']].copy()
    if len(deletions):
        deletions['from_side'] = np.where(deletions['from_side'] == '+', 'r', 'l')
        deletions['to_side'] = np.where(deletions['to_side'] == '+', 'l', 'r')
        deletions['valid'] = deletions[['from','from_side','to','to_side','mean_dist']].merge(scaf_bridges[['from','from_side','to','to_side','mean_dist']], on=['from','from_side','to','to_side','mean_dist'], how='left', indicator=True)['_merge'].values == "both"
        deletions.sort_values(['pid','pos','from_phase','to_phase'], inplace=True)
        valid = deletions.groupby(['pid','pos','from_phase','to_phase'], sort=False)['valid'].agg(['max','size'])
        deletions['valid'] = np.repeat(valid['max'].values, valid['size'].values)
        deletions = deletions.loc[deletions['valid'] == False, ['from_phase','to_phase']].drop_duplicates()
        scaffold_paths = AssignNewPhases(scaffold_paths, deletions, ploidy)
#
    return scaffold_paths

def FindNextValidPositionOnBothSidesOfTestConnection(test_conns, scaffold_paths, ploidy):
    for s1 in ['l','r']:
        for s2 in ['l','r']:
            test_conns[f'{s1}{s2}spos'] = test_conns['pos']
            while True:
                test_conns = GetPositionFromPaths(test_conns, scaffold_paths, ploidy, f'{s1}{s2}scaf', 'scaf', 'pid', f'{s1}{s2}spos', f'{s1}hap')
                if np.sum(test_conns[f'{s1}{s2}scaf'] == -1):
                    test_conns.loc[test_conns[f'{s1}{s2}scaf'] == -1, f'{s1}{s2}spos'] += -1 if s2 == 'l' else 1
                else:
                    break
            # Check if we have another scaffold between the first valid one and the end of the scaffold
            for d in [-1,+1]:
                test_conns['test_pos'] = test_conns[f'{s1}{s2}spos'] + d
                while True:
                    test_conns = GetPositionFromPaths(test_conns, scaffold_paths, ploidy, 'test', 'scaf', 'pid', 'test_pos', f'{s1}hap')
                    if np.sum(test_conns['test'] == -1):
                        test_conns.loc[test_conns['test'] == -1, 'test_pos'] += d
                    else:
                        break
                test_conns.loc[np.isnan(test_conns['test']), f'{s1}{s2}scaf'] = np.nan
    test_conns[['invalid','lspos','rspos']] = [True,-1,-1]
    for s1 in ['r','l']:
        for s2 in ['r','l']:
            valid = (test_conns[f'l{s1}scaf'] == test_conns[f'r{s2}scaf'])
            test_conns.loc[valid, 'invalid'] = False
            test_conns.loc[valid, ['lspos','rspos']] = test_conns.loc[valid, [f'l{s1}spos',f'r{s2}spos']].values
    test_conns.drop(columns=['llspos','llscaf','test_pos','test','lrspos','lrscaf','rlspos','rlscaf','rrspos','rrscaf'], inplace=True)
#
    return test_conns

def PhaseScaffoldsWithScaffoldGraph(scaffold_paths, graph_ext, ploidy):
    ## Combine phases where scaffold_graph leaves only one option
    # Get all remaining phase_breaks
    phase_breaks = []
    for h in range(ploidy):
        phase_breaks.append( scaffold_paths.loc[(scaffold_paths[f'phase{h}'] != scaffold_paths[f'phase{h}'].shift(1)) & (scaffold_paths['pid'] == scaffold_paths['pid'].shift(1)), ['pid','pos']].rename(columns={'pos':'rpos'}) )
        phase_breaks[-1]['hap'] = h
    phase_breaks = pd.concat(phase_breaks, ignore_index=True)
    phase_breaks['lpos'] = phase_breaks['rpos'] - 1
    test_conns = []
    for h in range(ploidy):
        test_conns.append(phase_breaks[['pid','lpos','rpos','hap']].rename(columns={'hap':'lhap'}))
        test_conns[-1]['rhap'] = h
        test_conns.append(phase_breaks[['pid','lpos','rpos','hap']].rename(columns={'hap':'rhap'}))
        test_conns[-1]['lhap'] = h
    test_conns = pd.concat(test_conns, ignore_index=True)
    test_conns.drop_duplicates(inplace=True)
    # Filter out phases breaks where we cannot create an overlap between left and right side, which is not at the path end (if we create an overlap at the paths end, we do not have two sides to check a connection anymore)
    test_conns['pos'] = test_conns['lpos']
    test_conns = FindNextValidPositionOnBothSidesOfTestConnection(test_conns, scaffold_paths, ploidy)
    test_conns.loc[test_conns['invalid'], 'pos'] = test_conns.loc[test_conns['invalid'], 'rpos']
    test_conns = FindNextValidPositionOnBothSidesOfTestConnection(test_conns, scaffold_paths, ploidy)
    phase_breaks = phase_breaks[ phase_breaks[['pid','lpos','rpos']].merge(test_conns.loc[test_conns['invalid'], ['pid','lpos','rpos']].drop_duplicates(), on=['pid','lpos','rpos'], how='left', indicator=True)['_merge'].values == "left_only" ].copy()
    test_conns.drop(columns=['invalid'], inplace=True)
    test_conns = test_conns.merge(phase_breaks[['pid','lpos','rpos']].drop_duplicates(), on=['pid','lpos','rpos'], how='inner')
    # Test connections
    test_conns.sort_values(['pid','pos','lspos','rspos'], inplace=True, ignore_index=True)
    test_conns['apid'] = (((test_conns['pid'] != test_conns['pid'].shift(1)) | (test_conns['pos'] != test_conns['pos'].shift(1)) |
                           (test_conns['lspos'] != test_conns['lspos'].shift(1)) | (test_conns['rspos'] != test_conns['rspos'].shift(1))).cumsum() - 1 ) * 2
    test_conns['bpid'] = test_conns['apid']+1
    test_paths = scaffold_paths.merge(test_conns[['pid','pos','apid','bpid']].drop_duplicates().rename(columns={'pos':'split_pos'}), on=['pid'], how='inner')
    test_paths = test_paths.loc[ np.repeat(test_paths.index.values, 1+(test_paths['pos'] == test_paths['split_pos']).values) ].reset_index()
    test_paths['pid'] = np.where((test_paths['pos'] < test_paths['split_pos']) | (test_paths['index'] == test_paths['index'].shift(-1)), test_paths['apid'], test_paths['bpid'])
    test_paths.drop(columns=['index','apid','bpid'], inplace=True)
    test_paths.sort_values(['pid','pos'], inplace=True, ignore_index=True)
    switch_pos = pd.concat( [test_conns.loc[test_conns['lspos'] != test_conns['pos'], ['apid','lspos','pid','lhap']].rename(columns={'apid':'pid','lspos':'switch_pos','pid':'opid','lhap':'hap'}), 
                             test_conns.loc[test_conns['rspos'] != test_conns['pos'], ['bpid','rspos','pid','rhap']].rename(columns={'bpid':'pid','rspos':'switch_pos','pid':'opid','rhap':'hap'})], ignore_index=True).drop_duplicates()
    if len(switch_pos):
        test_paths[['switch_pos','opid','hap']] = test_paths[['pid']].merge(switch_pos, on=['pid'], how='left')[['switch_pos','opid','hap']].fillna(-1).astype(int).values
        for h in range(ploidy):
            if np.sum(test_paths['hap'] == h):
                # Replace deletion with scaffold next to deletion (we need to take it from scaffold_paths, because it might be on the other side of the split)
                cur = (test_paths['hap'] == h) & (test_paths['pos'] == test_paths['split_pos'])
                cols = [f'scaf{h}',f'strand{h}',f'dist{h}']
                if h == 0:
                    test_paths.loc[cur, cols] = test_paths.loc[cur, ['opid','switch_pos']].rename(columns={'opid':'pid','switch_pos':'pos'}).merge(scaffold_paths[['pid','pos']+cols], on=['pid','pos'], how='left')[cols].values
                else:
                    tmp = test_paths.loc[cur, ['opid','switch_pos']].rename(columns={'opid':'pid','switch_pos':'pos'}).merge(scaffold_paths[['pid','pos',f'phase{h}','scaf0','strand0','dist0']+cols], on=['pid','pos'], how='left')
                    test_paths.loc[cur, cols] = np.where(tmp[[f'phase{h}',f'phase{h}',f'phase{h}']].values > 0, tmp[cols].values, tmp[['scaf0','strand0','dist0']].values)
                # Make sure we do not affect the other haplotypes by changing the main path
                cur = (test_paths['hap'] == h) & (test_paths['pos'] == test_paths['switch_pos'])
                if h == 0:
                    for h1 in range(1,ploidy):
                        test_paths.loc[cur & (test_paths[f'phase{h1}'] < 0), [f'scaf{h1}',f'strand{h1}',f'dist{h1}']] = test_paths.loc[cur & (test_paths[f'phase{h1}'] < 0), ['scaf0','strand0','dist0']].values
                # Delete the scaffold we shifted
                test_paths.loc[cur, cols] = [-1,'',0]
                test_paths.loc[cur, f'phase{h}'] = np.abs(test_paths.loc[cur, f'phase{h}'])
        test_paths.drop(columns=['switch_pos','opid','hap'], inplace=True)
    test_paths.drop(columns=['split_pos'], inplace=True)  
    test_paths['pos'] = test_paths.groupby(['pid'], sort=False).cumcount()
    test_paths = SetDistanceAtFirstPositionToZero(test_paths, ploidy)
    test_conns.rename(columns={'lhap':'ahap','rhap':'bhap'}, inplace=True)
    test_conns['aside'] = 'r'
    test_conns['bside'] = 'l'
    test_conns['amin'] = test_conns['pos']
    test_conns['amax'] = test_conns['pos']
    test_conns['alen'] = test_conns['amax']
    test_conns['bmin'] = 0
    test_conns['bmax'] = 0
    scaf_len = scaffold_paths.groupby(['pid'])['pos'].max().reset_index()
    test_conns['blen'] = test_conns[['pid']].merge(scaf_len, on=['pid'], how='left')['pos'].values - test_conns['pos']
    ends = test_conns.drop(columns=['pid','lpos','rpos','pos','lspos','rspos']).drop_duplicates()
    cols = ['pid','hap','side','min','max','len']
    ends = pd.concat([ends, ends.rename(columns={**{f'a{n}':f'b{n}' for n in cols},**{f'b{n}':f'a{n}' for n in cols}})], ignore_index=True)
    ends = FilterInvalidConnections(ends, test_paths, graph_ext, ploidy)
    if len(ends):
        ends = ends[ends['valid_path'] == 'ab'].drop(columns=['valid_path'])
    if len(ends) == 0:
        path_errors = phase_breaks
        phase_breaks = []
    else:
        test_conns = test_conns.merge(ends, on=list(ends.columns), how='inner')
        test_conns = test_conns[['pid','lpos','rpos','ahap','bhap']].rename(columns={'ahap':'lhap','bhap':'rhap'})
        # Check if we have evidence for errors in paths (same haplotype is not a valid connection)
        phase_breaks['rhap'] = phase_breaks['hap']
        path_errors = phase_breaks[ phase_breaks.rename(columns={'hap':'lhap'}).merge(test_conns, on=['pid','lpos','rpos','lhap','rhap'], how='left', indicator=True)['_merge'].values == "left_only" ].copy()
    if len(path_errors):
        print(path_errors)
        raise RuntimeError("Found scaffold_paths that violate scaffold_graph.")
    # Combine phases where we do not have alternative options
    if len(phase_breaks):
        phase_breaks = phase_breaks.merge(test_conns.rename(columns={'lhap':'hap'}), on=['pid','lpos','rpos','hap','rhap'], how='inner').drop(columns=['rhap'])
    if len(phase_breaks):
        for s in ['l','r']:
            cols = ['pid','lpos','rpos','hap']
            phase_breaks[f'{s}alts'] = phase_breaks[cols].merge(test_conns.rename(columns={f'{s}hap':'hap'}).groupby(cols).size().reset_index(name='alts'), on=cols, how='left')['alts'].values
        phase_breaks = phase_breaks[(phase_breaks['lalts'] == 1) & (phase_breaks['ralts'] == 1)].copy()
        phase_breaks[['from_phase','to_phase']] = 1
        for h in range(ploidy):
            phase_breaks.loc[phase_breaks['hap'] == h, 'from_phase'] = np.abs(phase_breaks.loc[phase_breaks['hap'] == h, ['pid','lpos']].rename(columns={'lpos':'pos'}).merge(scaffold_paths[['pid','pos',f'phase{h}']], on=['pid','pos'], how='left')[f'phase{h}'].values)
            phase_breaks.loc[phase_breaks['hap'] == h, 'to_phase'] = np.abs(phase_breaks.loc[phase_breaks['hap'] == h, ['pid','rpos']].rename(columns={'rpos':'pos'}).merge(scaffold_paths[['pid','pos',f'phase{h}']], on=['pid','pos'], how='left')[f'phase{h}'].values)
        phase_breaks = phase_breaks[['from_phase','to_phase']].drop_duplicates()
        scaffold_paths = AssignNewPhases(scaffold_paths, phase_breaks, ploidy)
#
    return scaffold_paths

def PhaseScaffolds(scaffold_paths, graph_ext, scaf_bridges, ploidy):
    ## Set initial phasing by giving every polyploid positions its own phase
    scaffold_paths['polyploid'] = (scaffold_paths[[f'phase{h}' for h in range(ploidy)]].values > 0).sum(axis=1) > 1
    scaffold_paths['polyploid'] = scaffold_paths['polyploid'] | (scaffold_paths['polyploid'].shift(1) & (scaffold_paths['pid'] == scaffold_paths['pid'].shift(1)))
    scaffold_paths['new_phase'] = scaffold_paths['polyploid'] | (scaffold_paths['pid'] != scaffold_paths['pid'].shift(1))
    scaffold_paths['new_phase'] = scaffold_paths['new_phase'].cumsum() - 1
    for h in range(ploidy):
        scaffold_paths[f'phase{h}'] = np.sign(scaffold_paths[f'phase{h}']) * (scaffold_paths['new_phase'] * ploidy + 1 + h)
    scaffold_paths.drop(columns=['polyploid','new_phase'], inplace=True)
#
    if len(scaf_bridges):
        scaffold_paths = PhaseScaffoldsWithScafBridges(scaffold_paths, scaf_bridges, ploidy)
        if len(graph_ext['org']):
            scaffold_paths = PhaseScaffoldsWithScaffoldGraph(scaffold_paths, graph_ext, ploidy)
#
    # Merge phases, where no other phase has a break
    while True:
        for h in range(ploidy):
            scaffold_paths[f'from_phase{h}'] = np.abs(scaffold_paths[f'phase{h}'].shift(1, fill_value=0))
            scaffold_paths[f'to_phase{h}'] = np.abs(scaffold_paths[f'phase{h}'])
        scaffold_paths.loc[scaffold_paths['pid'] != scaffold_paths['pid'].shift(1), [f'from_phase{h}' for h in range(ploidy)]] = 0
        merge_phase = scaffold_paths.loc[(scaffold_paths['from_phase0'] != 0) & ((scaffold_paths[[f'from_phase{h}' for h in range(ploidy)]].values != scaffold_paths[[f'to_phase{h}' for h in range(ploidy)]].values).sum(axis=1) == 1), [f'{n}_phase{h}' for h in range(ploidy) for n in ['from','to']]].copy()
        scaffold_paths.drop(columns=[f'{n}_phase{h}' for h in range(ploidy) for n in ['from','to']], inplace=True)
        if len(merge_phase):
            merge_phase[['from_phase','to_phase']] = merge_phase[['from_phase0','to_phase0']].values
            for h in range(1, ploidy):
                merge_phase.loc[merge_phase[f'from_phase{h}'] != merge_phase[f'to_phase{h}'], ['from_phase','to_phase']] = merge_phase.loc[merge_phase[f'from_phase{h}'] != merge_phase[f'to_phase{h}'], [f'from_phase{h}',f'to_phase{h}']].values
            merge_phase = merge_phase[['from_phase','to_phase']].drop_duplicates()
            scaffold_paths = AssignNewPhases(scaffold_paths, merge_phase, ploidy)
        else:
            break
#
    return scaffold_paths

def ExpandScaffoldsWithContigs(scaffold_paths, scaffolds, scaffold_parts, ploidy):
    for h in range(0,ploidy):
        scaffold_paths = scaffold_paths.merge(scaffolds[['scaffold','size']].rename(columns={'scaffold':f'scaf{h}','size':f'size{h}'}), on=[f'scaf{h}'], how='left')
        scaffold_paths[f'size{h}'] = scaffold_paths[f'size{h}'].fillna(0).astype(int)
        
    scaffold_paths = scaffold_paths.loc[np.repeat(scaffold_paths.index.values, scaffold_paths[[f'size{h}' for h in range(ploidy)]].max(axis=1).values)]
    scaffold_paths['spos'] = scaffold_paths.groupby(['pid','pos'], sort=False).cumcount()
    for h in range(ploidy-1,-1,-1): # We need to go in the opposite direction, because we need the main to handle the alternatives
        scaffold_paths['mpos'] = np.where(scaffold_paths[f'strand{h}'] == '-', scaffold_paths[f'size{h}']-scaffold_paths['spos']-1, scaffold_paths['spos'])
        not_first = scaffold_paths['mpos'].values != np.where(scaffold_paths[f'strand{h}'] == '+', 0, scaffold_paths[f'size{h}']-1)
        if h>0:
            # Remove alternatives that are the same except for distance for all position except the first, where we need the distance
            scaffold_paths.loc[(scaffold_paths[f'scaf{h}'] == scaffold_paths['scaf0']) & (scaffold_paths[f'strand{h}'] == scaffold_paths['strand0']) & not_first, [f'phase{h}',f'scaf{h}',f'strand{h}',f'dist{h}']] = [-1,-1,'',0]
        scaffold_paths = scaffold_paths.merge(scaffold_parts.rename(columns={'scaffold':f'scaf{h}','pos':'mpos'}), on=[f'scaf{h}','mpos'], how='left')
        scaffold_paths.loc[np.isnan(scaffold_paths['conpart']), [f'scaf{h}',f'strand{h}',f'dist{h}']] = [-1,'',0] # Set positions that do not have a match (scaffold is shorter than for other haplotypes) to a deletion
        scaffold_paths.loc[np.isnan(scaffold_paths['conpart']) == False, f'scaf{h}'] = scaffold_paths.loc[np.isnan(scaffold_paths['conpart']) == False, 'conpart'].astype(int)
        scaffold_paths.rename(columns={f'scaf{h}':f'con{h}'}, inplace=True)
        apply_dist = (np.isnan(scaffold_paths['prev_dist']) == False) & not_first
        scaffold_paths.loc[apply_dist, f'dist{h}'] = np.where(scaffold_paths.loc[apply_dist, f'strand{h}'] == '-', scaffold_paths.loc[apply_dist, 'next_dist'], scaffold_paths.loc[apply_dist, 'prev_dist']).astype(int)
        scaffold_paths.loc[scaffold_paths['reverse'] == True, f'strand{h}'] = np.where(scaffold_paths.loc[scaffold_paths['reverse'] == True, f'strand{h}'] == '+', '-', '+')
        scaffold_paths.drop(columns=['conpart','reverse','next_dist','prev_dist'],inplace=True)
    scaffold_paths = scaffold_paths[['pid','pos'] + [f'{col}{h}' for h in range(ploidy) for col in ['phase','con','strand','dist']]].rename(columns={'pid':'scaf'})
    scaffold_paths['pos'] = scaffold_paths.groupby(['scaf']).cumcount()
#
    return scaffold_paths

def GetOriginalConnections(scaffold_paths, contig_parts, ploidy):
    # Find original connections in contig_parts
    org_cons = contig_parts.reset_index().loc[contig_parts['org_dist_right'] > 0, ['index','org_dist_right']].copy() # Do not include breaks (org_dist_right==0). If they could not be resealed, they should be separated for good
    org_cons.rename(columns={'index':'from','org_dist_right':'distance'}, inplace=True)
    org_cons['from_side'] = 'r'
    org_cons['to'] = org_cons['from']+1
    org_cons['to_side'] = 'l'

    # Lift connections to scaffolds while dropping connections that don't end at scaffold ends
    scaffold_ends_left = []
    scaffold_ends_right = []
    for h in range(ploidy):
        ends = scaffold_paths.loc[(scaffold_paths[f'phase{h}'] < 0) | (scaffold_paths[f'con{h}'] >= 0), :] # Remove deletions at this haplotype, because they cannot be an end
        lends = ends.groupby(['scaf'], sort=False).first().reset_index()
        lends.loc[lends[f'phase{h}'] < 0, [f'con{h}',f'strand{h}']] = lends.loc[lends[f'phase{h}'] < 0, ['con0','strand0']].values
        lends = lends[['scaf',f'con{h}',f'strand{h}']].rename(columns={'scaf':'scaffold',f'con{h}':'from',f'strand{h}':'fstrand'})
        lends['from_side'] = np.where(lends['fstrand'] == '+', 'l', 'r')
        lends['scaf_side'] = 'l'
        scaffold_ends_left.append( lends.drop(columns=['fstrand']) )
        rends = ends.groupby(['scaf'], sort=False).last().reset_index()
        rends.loc[rends[f'phase{h}'] < 0, [f'con{h}',f'strand{h}']] = rends.loc[rends[f'phase{h}'] < 0, ['con0','strand0']].values
        rends = rends[['scaf',f'con{h}',f'strand{h}']].rename(columns={'scaf':'scaffold',f'con{h}':'from',f'strand{h}':'fstrand'})
        rends['from_side'] = np.where(rends['fstrand'] == '+', 'r', 'l')
        rends['scaf_side'] = 'r'
        scaffold_ends_right.append( rends.drop(columns=['fstrand']) )
    scaffold_ends_left = pd.concat(scaffold_ends_left, ignore_index=True).drop_duplicates()
    scaffold_ends_right = pd.concat(scaffold_ends_right, ignore_index=True).drop_duplicates()
    scaffold_ends = pd.concat([ scaffold_ends_left, scaffold_ends_right ], ignore_index=True)
    org_cons = org_cons.merge(scaffold_ends, on=['from','from_side'], how='inner')
    org_cons.drop(columns=['from','from_side'], inplace=True)
    org_cons.rename(columns={'scaffold':'from','scaf_side':'from_side'}, inplace=True)
    org_cons = org_cons.merge(scaffold_ends.rename(columns={'from':'to', 'from_side':'to_side'}), on=['to','to_side'], how='inner')
    org_cons.drop(columns=['to','to_side'], inplace=True)
    org_cons.rename(columns={'scaffold':'to','scaf_side':'to_side'}, inplace=True)

    # Also insert reversed connections
    org_cons = pd.concat( [org_cons, org_cons.rename(columns={'from':'to','from_side':'to_side','to':'from','to_side':'from_side'})], ignore_index=True )

    # Remove scaffolding connections, where different haplotypes have different connections
    org_cons = org_cons.groupby(['from','from_side','to','to_side'])['distance'].mean().reset_index()
    fdups = org_cons.groupby(['from','from_side']).size()
    fdups = fdups[fdups > 1].reset_index().drop(columns=[0])
    tdups = org_cons.groupby(['to','to_side']).size()
    tdups = tdups[tdups > 1].reset_index().drop(columns=[0])
    org_cons = org_cons[ (org_cons.merge(fdups, on=['from','from_side'], how='left', indicator=True)['_merge'].values == "left_only") & 
                         (org_cons.merge(tdups, on=['to','to_side'], how='left', indicator=True)['_merge'].values == "left_only") ].copy()

    return org_cons

def OrderByUnbrokenOriginalScaffolds(scaffold_paths, contig_parts, ploidy):
    ## Bring scaffolds in order of unbroken original scaffolding and remove circularities
    # Every scaffold starts as its own metascaffold
    meta_scaffolds = pd.DataFrame({'meta':np.unique(scaffold_paths['scaf']), 'size':1, 'lcon':-1, 'lcon_side':'', 'rcon':-1, 'rcon_side':''})
    meta_scaffolds.index = meta_scaffolds['meta'].values
    meta_parts = pd.DataFrame({'scaffold':meta_scaffolds['meta'], 'meta':meta_scaffolds['meta'], 'pos':0, 'reverse':False})
    meta_parts.index = meta_parts['scaffold'].values
#
    # Prepare meta scaffolding
    org_cons = GetOriginalConnections(scaffold_paths, contig_parts, ploidy)
    meta_scaffolds.loc[org_cons.loc[org_cons['from_side'] == 'l', 'from'].values, 'lcon'] = org_cons.loc[org_cons['from_side'] == 'l', 'to'].values
    meta_scaffolds.loc[org_cons.loc[org_cons['from_side'] == 'l', 'from'].values, 'lcon_side'] = org_cons.loc[org_cons['from_side'] == 'l', 'to_side'].values
    meta_scaffolds.loc[org_cons.loc[org_cons['from_side'] == 'r', 'from'].values, 'rcon'] = org_cons.loc[org_cons['from_side'] == 'r', 'to'].values
    meta_scaffolds.loc[org_cons.loc[org_cons['from_side'] == 'r', 'from'].values, 'rcon_side'] = org_cons.loc[org_cons['from_side'] == 'r', 'to_side'].values
#
    # Rename some columns and create extra columns just to call the same function as used for the normal scaffolding
    meta_scaffolds['left'] = meta_scaffolds['meta']
    meta_scaffolds['lside'] = 'l'
    meta_scaffolds['right'] = meta_scaffolds['meta']
    meta_scaffolds['rside'] = 'r'
    meta_scaffolds['lextendible'] = True
    meta_scaffolds['rextendible'] = True
    meta_scaffolds['circular'] = False
    meta_scaffolds.rename(columns={'meta':'scaffold','lcon':'lscaf','lcon_side':'lscaf_side','rcon':'rscaf','rcon_side':'rscaf_side'}, inplace=True)
    meta_parts.rename(columns={'scaffold':'conpart','meta':'scaffold'}, inplace=True)
    meta_scaffolds, meta_parts = ScaffoldAlongGivenConnections(meta_scaffolds, meta_parts)
    meta_parts.rename(columns={'conpart':'scaffold','scaffold':'meta'}, inplace=True)
#
    # Set new continuous scaffold ids consistent with original scaffolding and apply reversions
    meta_parts.sort_values(['meta','pos'], inplace=True)
    meta_parts['new_scaf'] = range(len(meta_parts))
    scaffold_paths = scaffold_paths.merge(meta_parts[['scaffold','reverse','new_scaf']].rename(columns={'scaffold':'scaf'}), on=['scaf'], how='left')
    scaffold_paths['scaf'] = scaffold_paths['new_scaf']
    col_rename = {**{'scaf':'pid'}, **{f'con{h}':f'scaf{h}' for h in range(ploidy)}}
    scaffold_paths = ReverseScaffolds(scaffold_paths.rename(columns=col_rename), scaffold_paths['reverse'], ploidy).rename(columns={v: k for k, v in col_rename.items()})
    scaffold_paths.drop(columns=['reverse','new_scaf'], inplace=True)

    # Set org_dist_left/right based on scaffold (not the contig anymore)
    org_cons = GetOriginalConnections(scaffold_paths, contig_parts, ploidy)
    org_cons = org_cons.loc[(org_cons['from_side'] == 'l') & (org_cons['from']-1 == org_cons['to']), :]
    scaffold_paths['sdist_left'] = -1
    scaffold_paths.loc[np.isin(scaffold_paths['scaf'], org_cons['from'].values) & (scaffold_paths['pos'] == 0), 'sdist_left'] = org_cons['distance'].values
    scaffold_paths['sdist_left'] = scaffold_paths['sdist_left'].astype(int)
    scaffold_paths['sdist_right'] = scaffold_paths['sdist_left'].shift(-1, fill_value=-1)

    return scaffold_paths

def ScaffoldContigs(contig_parts, bridges, mappings, cov_probs, repeats, prob_factor, min_mapping_length, max_dist_contig_end, prematurity_threshold, ploidy, max_loop_units):
    # Each contig starts with being its own scaffold
    scaffold_parts = pd.DataFrame({'conpart': contig_parts.index.values, 'scaffold': contig_parts.index.values, 'pos': 0, 'reverse': False})
    scaffolds = pd.DataFrame({'scaffold': contig_parts.index.values, 'left': contig_parts.index.values, 'lside':'l', 'right': contig_parts.index.values, 'rside':'r', 'lextendible':True, 'rextendible':True, 'circular':False, 'size':1})
#
    # Combine contigs into scaffolds on unique bridges
    unique_bridges = bridges.loc[(bridges['to_alt'] == 1) & (bridges['from_alt'] == 1), ['from','from_side','to','to_side']]
    scaffolds = scaffolds.merge(unique_bridges[unique_bridges['from_side']=='l'].drop(columns=['from_side']).rename(columns={'from':'scaffold', 'to':'lscaf', 'to_side':'lscaf_side'}), on=['scaffold'], how='left')
    scaffolds = scaffolds.merge(unique_bridges[unique_bridges['from_side']=='r'].drop(columns=['from_side']).rename(columns={'from':'scaffold', 'to':'rscaf', 'to_side':'rscaf_side'}), on=['scaffold'], how='left')
    scaffolds[['lscaf','rscaf']] = scaffolds[['lscaf','rscaf']].fillna(-1).astype(int)
    scaffolds[['lscaf_side','rscaf_side']] = scaffolds[['lscaf_side','rscaf_side']].fillna('')
    scaffolds, scaffold_parts = ScaffoldAlongGivenConnections(scaffolds, scaffold_parts)
    scaffolds.drop(columns=['lscaf','lscaf_side','rscaf','rscaf_side'], inplace = True)
    scaffold_parts = AddDistaneInformation(scaffold_parts, bridges[(bridges['to_alt'] == 1) & (bridges['from_alt'] == 1)]) # Do not use unique_bridges, because we require 'mean_dist' column
    scaf_bridges = LiftBridgesFromContigsToScaffolds(bridges, scaffolds)
    org_scaf_conns = GetOriginalScaffoldConnections(contig_parts, scaffolds)
#
    # Build scaffold graph to find unique bridges over scaffolds with alternative connections
    long_range_connections = GetLongRangeConnections(bridges, mappings, contig_parts, max_dist_contig_end)
    long_range_connections = TransformContigConnectionsToScaffoldConnections(long_range_connections, scaffold_parts)
    long_range_connections, long_range_extlen = SummarizeLongRangeConnections(long_range_connections)
    long_range_connections = FilterLongRangeConnections(long_range_connections, long_range_extlen, scaffold_parts, contig_parts, cov_probs, prob_factor, min_mapping_length, prematurity_threshold)
    scaffold_graph = BuildScaffoldGraph(long_range_connections, scaf_bridges)
    scaffold_graph, trim_repeats = DeduplicateScaffoldsInGraph(scaffold_graph, repeats, scaffold_parts, scaf_bridges)
    scaf_bridges = UpdateScafBridgesAccordingToScaffoldGraph(scaf_bridges, scaffold_graph)
    graph_ext = FindValidExtensionsInScaffoldGraph(scaffold_graph)
    scaffold_paths = TraverseScaffoldGraph(scaffolds, scaffold_graph, graph_ext, scaf_bridges, org_scaf_conns, ploidy, max_loop_units)
    scaffold_paths = PhaseScaffolds(scaffold_paths, graph_ext, scaf_bridges, ploidy)
#
    # Finish Scaffolding
    scaffold_paths = ExpandScaffoldsWithContigs(scaffold_paths, scaffolds, scaffold_parts, ploidy)
    scaffold_paths = OrderByUnbrokenOriginalScaffolds(scaffold_paths, contig_parts, ploidy)
#
    return scaffold_paths, trim_repeats

def RemoveScaffoldsFromAssembly(scaffold_paths, contig_parts, del_scafs):
    if len(del_scafs) == 0:
        removed_length = []
    else:
        # Update the original scaffolding in case of deleted contigs
        scaffold_paths['del'] = np.isin(scaffold_paths['scaf'], del_scafs)
        scaffold_paths.loc[scaffold_paths['del'].shift(1, fill_value=False), 'sdist_left'] = -1
        scaffold_paths['conlen'] = 0
        scaffold_paths.loc[scaffold_paths['con0'] >= 0, 'conlen'] = contig_parts.loc[scaffold_paths.loc[scaffold_paths['con0'] >= 0, 'con0'].values, 'end'].values - contig_parts.loc[scaffold_paths.loc[scaffold_paths['con0'] >= 0, 'con0'].values, 'start'].values
        scaffold_paths.reset_index(drop=True, inplace=True)
        cur_entries = scaffold_paths.loc[(scaffold_paths['sdist_right'] >= 0) & scaffold_paths['del'].shift(-1, fill_value=False) & (scaffold_paths['del'] == False), ['sdist_right']].reset_index()
        cur_entries.index = cur_entries['index'].values
        s = 1
        while len(cur_entries):
            bridged = cur_entries[scaffold_paths.loc[cur_entries['index'].values+s, 'del'].values == False].copy()
            if len(bridged):
                scaffold_paths.loc[bridged['index'].values, 'sdist_right'] = bridged['sdist_right'].values
                scaffold_paths.loc[bridged['index'].values+s, 'sdist_left'] = bridged['sdist_right'].values
                cur_entries.drop(bridged['index'].values, inplace=True)
            broken = cur_entries[scaffold_paths.loc[cur_entries['index'].values+s, 'sdist_right'].values < 0].copy()
            if len(broken):
                scaffold_paths.loc[broken['index'].values, 'sdist_right'] = -1
                cur_entries.drop(broken['index'].values, inplace=True)
            if len(cur_entries):
                cur_entries['sdist_right'] += scaffold_paths.loc[cur_entries['index'].values+s, 'sdist_right'].values + scaffold_paths.loc[cur_entries['index'].values+s, 'conlen'].values
                s += 1
        # Store removed length
        removed_length = scaffold_paths.loc[scaffold_paths['del'], 'conlen'].values
        # Remove the unconnected contigs scheduled for deletion
        scaffold_paths = scaffold_paths[scaffold_paths['del'] == False].drop(columns=['del','conlen'])
#
    return scaffold_paths, removed_length

def RemoveUnconnectedLowlyMappedOrDuplicatedContigs(scaffold_paths, result_info, mappings, contig_parts, cov_probs, repeats, ploidy, lowmap_threshold):
    # Find unconnected scaffolds
    unconnected = scaffold_paths.groupby(['scaf']).size()
    unconnected = unconnected[unconnected == 1].reset_index()['scaf'].values
    unconnected = scaffold_paths.loc[np.isin(scaffold_paths['scaf'], unconnected) & (scaffold_paths[[f'con{h}' for h in range(1,ploidy)]].max(axis=1) < 0), ['scaf','con0']].rename(columns={'con0':'con'})
#    
    # Check unconnected for lowly mapped contigs
    #unconnected['nmapped'] = unconnected[['con']].merge(mappings.groupby(['conpart']).size().reset_index(name='nmapped').rename(columns={'conpart':'con'}), on='con', how='left')['nmapped'].fillna(0).astype(int).values
    #unconnected['lowmap'] = GetConProb(cov_probs, 1, unconnected['nmapped']) <= lowmap_threshold
    unconnected['lowmap'] = False
#
    # check unconnected for completely duplicated contigs
    unconnected['fullrep'] = False
    uncon_reps = repeats[(repeats['side'] == 'lr') & (repeats['q_con'] != repeats['t_con']) & np.isin(repeats['q_con'], unconnected['con'].values)].copy()
    full_reps = np.unique(uncon_reps.loc[np.isin(uncon_reps['t_con'].values, uncon_reps['q_con'].values) == False, 'q_con'].values) # Make sure we do not remove both copies of a repeated contig
    unconnected.loc[np.isin(unconnected['con'].values, full_reps), 'fullrep'] = True
    uncon_reps = uncon_reps[(np.isin(uncon_reps['q_con'].values, full_reps) == False) & (np.isin(uncon_reps['t_con'].values, full_reps) == False)].copy()
    # Handle the repeats, where all versions are fully repeated by taking the one with the longest unique sequence
    for s in ['q','t']:
        uncon_reps[f'{s}_uniq'] = (uncon_reps[f'{s}_start'] - uncon_reps[f'{s}_confrom']) + (uncon_reps[f'{s}_conto'] - uncon_reps[f'{s}_end'])
    full_reps = np.unique(uncon_reps.loc[uncon_reps['q_uniq'] < uncon_reps['t_uniq'], 'q_con'].values)
    unconnected.loc[np.isin(unconnected['con'].values, full_reps), 'fullrep'] = True
    uncon_reps = uncon_reps[(np.isin(uncon_reps['q_con'].values, full_reps) == False) & (np.isin(uncon_reps['t_con'].values, full_reps) == False)].copy()
    # To break ties in case the unique length is identical remove the ones with the higher contig id
    full_reps = np.unique(uncon_reps.loc[uncon_reps['q_con'] > uncon_reps['t_con'], 'q_con'].values)
    unconnected.loc[np.isin(unconnected['con'].values, full_reps), 'fullrep'] = True
#
    # Remove the unconnected contigs scheduled for deletion
    scaffold_paths, removed_length = RemoveScaffoldsFromAssembly(scaffold_paths, contig_parts, unconnected.loc[unconnected['lowmap'] | unconnected['fullrep'], 'scaf'].values)
    # Update result_info
    result_info['removed2'] = {}
    result_info['removed2']['num'] = len(removed_length)
    result_info['removed2']['total'] = np.sum(removed_length) if len(removed_length) else 0
    result_info['removed2']['min'] = np.min(removed_length) if len(removed_length) else 0
    result_info['removed2']['max'] = np.max(removed_length) if len(removed_length) else 0
    result_info['removed2']['mean'] = np.mean(removed_length) if len(removed_length) else 0

#
    return scaffold_paths, result_info

def TrimDuplicatedEnds(contig_parts, scaffold_paths, mappings, repeats, trim_repeats, min_mapping_length, alignment_precision):
    if len(trim_repeats):
        # Trim previously selected contig sides
        trim = repeats.merge(trim_repeats, on=['q_con'], how='inner')
        trim = trim[(trim['side'] == 'lr') | (trim['side'] == trim['rep_side'])].drop(columns='side')
        trim.rename(columns={'rep_side':'side'}, inplace=True)
        # Trim the longest duplicated regions
        trim = trim.groupby(['q_con','side','q_confrom','q_conto']).agg({'q_start':'min', 'q_end':'max'}).reset_index()
        # Remove the contigs that have been already removed for other reasons
        trim = trim.merge(scaffold_paths[['con0','scaf']].rename(columns={'con0':'q_con'}), on='q_con', how='inner') # Since they were disconnected from the graph before they must be on a separate path and thus the main haplotype
        # Make sure they are also unconnected (in case not all paths were disconnected)
        trim = trim[ trim[['scaf']].merge(scaffold_paths.groupby('scaf')['pos'].max().reset_index(name='len'), on='scaf', how='left')['len'] == 0 ].copy()
        if len(trim):
            # Trim left sides
            if np.sum(trim['side'] == 'l'):
                contig_parts.loc[trim.loc[trim['side'] == 'l', 'q_con'].values, 'start'] = trim.loc[trim['side'] == 'l', 'q_end'].values
            # Trim right sides
            if np.sum(trim['side'] == 'r'):
                contig_parts.loc[trim.loc[trim['side'] == 'r', 'q_con'].values, 'end'] = trim.loc[trim['side'] == 'r', 'q_start'].values
#
        # Trim unconnected contigs that are duplicated on both sides by the same contig (likely alternative haplotypes)
        ctrim = scaffold_paths.groupby(['scaf'])['pos'].max().reset_index(name='len')
        ctrim = ctrim[ctrim['len'] == 0].drop(columns='len')
        if len(ctrim):
            ctrim['q_con'] = ctrim[['scaf']].merge(scaffold_paths[['scaf','con0']], on='scaf', how='left')['con0'].values
            ctrim = ctrim.merge(repeats.loc[repeats['side'] == 'l', ['q_con','q_end','strand','t_con','t_start','t_end']].rename(columns={'q_end':'new_start','t_start':'ltstart','t_end':'ltend'}), on='q_con', how='inner')
            ctrim = ctrim.merge(repeats.loc[repeats['side'] == 'r', ['q_con','q_start','strand','t_con','t_start','t_end']].rename(columns={'q_start':'new_end','t_start':'rtstart','t_end':'rtend'}), on=['q_con','strand','t_con'], how='inner')
            # Remove self-mappings
            ctrim = ctrim[ctrim['q_con'] != ctrim['t_con']].copy()
        if len(ctrim):
            # Remove overlapping duplications (we do not want to remove the contigs that passed the thresholds for not fully duplicated)
            ctrim = ctrim[ np.where(ctrim['strand'] == '+', ctrim['ltend'] < ctrim['rtstart'], ctrim['rtend'] < ctrim['ltstart']) ].copy()
        if len(ctrim):
            # Collect the trimming caused by different targets
            ctrim = ctrim.groupby(['scaf','q_con']).agg({'new_start':'min','new_end':'max'}).reset_index()
            if len(ctrim):
                contig_parts.loc[ctrim['q_con'].values, ['start','end']] = ctrim[['new_start','new_end']].values
#
        # Remove contig_parts that became shorter than min_mapping_length+alignment_precision completely
        scaffold_paths, removed_length = RemoveScaffoldsFromAssembly(scaffold_paths, contig_parts, scaffold_paths.loc[np.isin(scaffold_paths['con0'], contig_parts[contig_parts['end'] - contig_parts['start'] < min_mapping_length+alignment_precision].index.values), 'scaf'].values)
        # Remove mappings for trimmed contigs, so that they will not be extended (must happen after RemoveUnconnectedLowlyMappedOrDuplicatedContigs or we will remove all of them)
        mappings = mappings[np.isin(mappings['conpart'], np.concatenate([ctrim['q_con'].values, trim['q_con'].values])) == False].copy()
#
    return contig_parts, scaffold_paths, mappings

def PrepareScaffoldPathForMapping(scaffold_paths, bridges, ploidy):
    # Prepare entries in scaffold_paths for mapping
    all_scafs = []
    for h in range(ploidy):
        hap_paths = scaffold_paths[['scaf','pos']+[f'phase{h}',f'con{h}',f'strand{h}',f'dist{h}']].rename(columns={f'phase{h}':'phase',f'con{h}':'conpart',f'strand{h}':'scaf_strand',f'dist{h}':'ldist'})
        if np.sum(hap_paths['phase'] < 0):
            hap_paths.loc[hap_paths['phase'] < 0, ['conpart','scaf_strand','ldist']] = scaffold_paths.loc[hap_paths['phase'].values < 0, ['con0','strand0','dist0']].values
        ## Get haplotype of connected contigs
        hap_paths['lhap'] = h # If the only difference is the distance, an alternative haplotype could only be alternative on one side
        hap_paths['rhap'] = h
        if 0 < h:
            hap_paths['lphase'] = hap_paths['phase'].shift(1, fill_value=-1)
            hap_paths.loc[hap_paths['scaf'] != hap_paths['scaf'].shift(1), 'lphase'] = -1
            hap_paths['rphase'] = hap_paths['phase'].shift(-1, fill_value=-1)
            hap_paths.loc[hap_paths['scaf'] != hap_paths['scaf'].shift(-1), 'rphase'] = -1
            # The important phase is the one storing the distance, but sometimes the distances to different contigs are the same and then we need the other phase
            hap_paths['main'] = (hap_paths['conpart'] == scaffold_paths['con0'].values) & (hap_paths['scaf_strand'] == scaffold_paths['strand0'].values) # Get the alternative haplotypes identical to main
            hap_paths['lphase'] = np.where((hap_paths['phase'] >= 0) | hap_paths['main'].shift(1, fill_value=True), hap_paths['phase'], hap_paths['lphase'])
            hap_paths['rphase'] = np.where((hap_paths['rphase'] >= 0) | hap_paths['main'], hap_paths['rphase'], hap_paths['phase'])
            # Get haplotypes based on phase
            hap_paths.loc[hap_paths['lphase'] < 0, 'lhap'] = 0
            hap_paths.loc[hap_paths['rphase'] < 0, 'rhap'] = 0
        else:
            hap_paths['main'] = False # While the main path is by definition identical to main, we do not want to remove it, thus set it to False
        ## Remove deletions (we needed them to get the proper phase, but now they are only making things complicated)
        hap_paths = hap_paths.loc[(hap_paths['conpart'] >= 0), :]
        ## Get connected contigs with strand and distance (we already have ldist)
        hap_paths['lcon'] = hap_paths['conpart'].shift(1, fill_value=-1)
        hap_paths['lstrand'] = hap_paths['scaf_strand'].shift(1, fill_value='')
        hap_paths['lpos'] = hap_paths['pos'].shift(1, fill_value=-1)
        hap_paths.loc[hap_paths['scaf'] != hap_paths['scaf'].shift(1), ['lcon','lstrand','lpos']] = [-1,'',-1]
        hap_paths['rcon'] = hap_paths['conpart'].shift(-1, fill_value=-1)
        hap_paths['rstrand'] = hap_paths['scaf_strand'].shift(-1, fill_value='')
        hap_paths['rdist'] = hap_paths['ldist'].shift(-1, fill_value=0)
        hap_paths['rpos'] = hap_paths['pos'].shift(-1, fill_value=-1)
        hap_paths.loc[hap_paths['scaf'] != hap_paths['scaf'].shift(-1), ['rcon','rstrand','rdist','rpos']] = [-1,'',0,-1]
        ## Filter the ones completely identical to main
        if 0 < h:
            hap_paths = hap_paths.loc[(hap_paths['lhap'] > 0) | (hap_paths['rhap'] > 0), :]
        all_scafs.append(hap_paths[['scaf','pos','conpart','scaf_strand','lpos','lhap','lcon','lstrand','ldist','rpos','rhap','rcon','rstrand','rdist','main']])
    all_scafs = pd.concat(all_scafs, ignore_index=True)
    # Get allowed distance range for contig bridges
    all_scafs['from_side'] = np.where(all_scafs['scaf_strand'] == '+', 'l', 'r')
    all_scafs['to_side'] = np.where(all_scafs['lstrand'] == '+', 'r', 'l')
    all_scafs = all_scafs.merge(bridges[['from','from_side','to','to_side','mean_dist','min_dist','max_dist']].rename(columns={'from':'conpart','to':'lcon','mean_dist':'ldist','min_dist':'ldmin','max_dist':'ldmax'}), on=['conpart','from_side','lcon','to_side','ldist'], how='left')
    all_scafs['from_side'] = np.where(all_scafs['scaf_strand'] == '+', 'r', 'l')
    all_scafs['to_side'] = np.where(all_scafs['rstrand'] == '+', 'l', 'r')
    all_scafs = all_scafs.merge(bridges[['from','from_side','to','to_side','mean_dist','min_dist','max_dist']].rename(columns={'from':'conpart','to':'rcon','mean_dist':'rdist','min_dist':'rdmin','max_dist':'rdmax'}), on=['conpart','from_side','rcon','to_side','rdist'], how='left')
    consitency_check = all_scafs[(np.isnan(all_scafs['ldmin']) & (all_scafs['lcon'] >= 0)) | (np.isnan(all_scafs['rdmin']) & (all_scafs['rcon'] >= 0))].copy()
    if len(consitency_check):
        print(consitency_check)
        raise RuntimeError("Scaffold paths contains connections that are not supported by a bridge.")
    all_scafs[['ldmin','rdmin']] = all_scafs[['ldmin','rdmin']].fillna(-sys.maxsize*0.99).values.astype(int) # Use *0.99 to avoid overflow through type convertion from float to int (should be the lcon/rcon -1, where we do not want to set any constraints)
    all_scafs[['ldmax','rdmax']] = all_scafs[['ldmax','rdmax']].fillna(sys.maxsize*0.99).values.astype(int)
    all_scafs.drop(columns=['from_side','to_side'], inplace=True)
#
    return all_scafs

def BasicMappingToScaffolds(mappings, all_scafs):
    # Mapping to matching contigs and reverse if the contig is reversed on the scaffold
    mappings = mappings.merge(all_scafs, on=['conpart'], how='inner')
    mappings.loc[mappings['scaf_strand'] == '-', 'strand'] = np.where(mappings.loc[mappings['scaf_strand'] == '-', 'strand'].values == '+', '-', '+')
    tmp = mappings.loc[mappings['scaf_strand'] == '-', ['right_con','right_con_dist','rmapq','rmatches']].values
    mappings.loc[mappings['scaf_strand'] == '-', ['right_con','right_con_dist','rmapq','rmatches']] = mappings.loc[mappings['scaf_strand'] == '-', ['left_con','left_con_dist','lmapq','lmatches']].values
    mappings.loc[mappings['scaf_strand'] == '-', ['left_con','left_con_dist','lmapq','lmatches']] = tmp
    tmp = mappings.loc[mappings['scaf_strand'] == '-', 'right_con_side'].values
    mappings.loc[mappings['scaf_strand'] == '-', 'right_con_side'] = mappings.loc[mappings['scaf_strand'] == '-', 'left_con_side'].values
    mappings.loc[mappings['scaf_strand'] == '-', 'left_con_side'] = tmp
    # Check that connections to left and right fit with the scaffold
    mappings = mappings[((mappings['left_con'] == mappings['lcon']) | (mappings['left_con'] < 0) | (mappings['lcon'] < 0)) & ((mappings['right_con'] == mappings['rcon']) | (mappings['right_con'] < 0) | (mappings['rcon'] < 0))].drop(columns=['rcon','lcon']).rename(columns={'left_con':'lcon','right_con':'rcon'})
    mappings = mappings[(((mappings['left_con_side'] == 'r') == (mappings['lstrand'] == '+')) | (mappings['left_con_side'] == '') | (mappings['lstrand'] == '')) & (((mappings['right_con_side'] == 'l') == (mappings['rstrand'] == '+')) | (mappings['right_con_side'] == '') | (mappings['rstrand'] == ''))].drop(columns=['left_con_side','right_con_side','lstrand','rstrand'])
    mappings = mappings[(((mappings['left_con_dist'] <= mappings['ldmax']) & (mappings['left_con_dist'] >= mappings['ldmin'])) | (mappings['lcon'] < 0)) & (((mappings['right_con_dist'] <= mappings['rdmax']) & (mappings['right_con_dist'] >= mappings['rdmin'])) | (mappings['rcon'] < 0))].drop(columns=['ldmin','ldmax','rdmin','rdmax','ldist','rdist']).rename(columns={'left_con_dist':'ldist','right_con_dist':'rdist','scaf_strand':'con_strand'})
    # Check for alternative haplotypes that the mapping is not only supporting a side that is identical to main
    mappings = mappings.loc[(mappings['lhap'] == mappings['rhap']) | ((mappings['lhap'] > 0) & (mappings['lcon'] >= 0)) | ((mappings['rhap'] > 0) & (mappings['rcon'] >= 0)),:]
    mappings = mappings[(mappings['main'] == False) | (mappings['lcon'] >= 0) | (mappings['rcon'] >= 0)].drop(columns=['main']) # Remove alternative haplotypes, where both sides, but not the contig itself, are distinct from the main path and the read does not reach either side
    # Check that we do not have another haplotype with a longer scaffold supported by the read
    mappings['nconns'] = (mappings[['lpos','rpos']] >= 0).sum(axis=1)
    mappings.sort_values(['read_id','read_start','read_pos','scaf','pos','lhap','rhap'], inplace=True)
    mappings = mappings[ ( (mappings[['read_id','read_start','read_pos','scaf','pos']] != mappings[['read_id','read_start','read_pos','scaf','pos']].shift(1)).any(axis=1) | (mappings['nconns'] >= mappings['nconns'].shift(1)) ) &
                         ( (mappings[['read_id','read_start','read_pos','scaf','pos']] != mappings[['read_id','read_start','read_pos','scaf','pos']].shift(-1)).any(axis=1) | (mappings['nconns'] >= mappings['nconns'].shift(-1)) ) ].copy()
    mappings.drop(columns=['nconns'], inplace=True)
#
    # Assign groups to separate parts of a read mapping to two separate (maybe overlapping) locations on the scaffold
    mappings['group'] = ( (mappings['read_id'] != mappings['read_id'].shift(1)) | (mappings['read_start'] != mappings['read_start'].shift(1)) | (mappings['read_pos'] != mappings['read_pos'].shift(1)+1) | # We do not follow the read
                          (mappings['scaf'] != mappings['scaf'].shift(1)) | (np.where(mappings['strand'] == '-', mappings['rpos'], mappings['lpos']) != mappings['pos'].shift(1)) | # We do not follow the scaffold
                          np.where(mappings['strand'] == '-', mappings['rhap'] != mappings['lhap'].shift(1), mappings['lhap'] != mappings['rhap'].shift(1)) ) # We do not follow the haplotype
    mappings['unigroup'] = ( mappings['group'] | ((mappings['read_id'] == mappings['read_id'].shift(2)) & (mappings['read_pos'] == mappings['read_pos'].shift(2)+1)) | 
                                                 ((mappings['read_id'] == mappings['read_id'].shift(-1)) & (mappings['read_pos'] == mappings['read_pos'].shift(-1))) ).cumsum() # We have more than one option (thus while we want to group them if everything is correct, we do not want to propagate errors through those branchings, because the other side might correctly continue along another branch)
    mappings['group'] = mappings['group'].cumsum()
    # Remove mappings, where reads do not continue, allthough they should
    old_len = 0
    while old_len != len(mappings):
        old_len = len(mappings)
        for d, d2 in zip(['l','r'],['r','l']):
            mappings['check_pos'] = mappings['read_pos'] + np.where(mappings['strand'] == '+', -1, 1) * (1 if d == 'l' else -1)
            invalid = mappings.loc[(mappings[f'{d}con'] >= 0) & (mappings[f'{d}pos'] >= 0), ['read_id','read_start','check_pos','scaf',f'{d}pos',f'{d}hap','unigroup']].merge(mappings[['read_id','read_start','read_pos','scaf','pos',f'{d2}hap']].rename(columns={'read_pos':'check_pos','pos':f'{d}pos',f'{d2}hap':f'{d}hap'}), on=['read_id','read_start','check_pos','scaf',f'{d}pos',f'{d}hap'], how='left', indicator=True)[['unigroup','_merge']]
            invalid = np.unique(invalid.loc[invalid['_merge'] == "left_only", 'unigroup'].values)
            mappings = mappings[np.isin(mappings['unigroup'], invalid) == False].copy()
    mappings.drop(columns=['unigroup'], inplace=True)
    # Assign the groups matching the left/right connection to combine groups that are not in neighbouring rows
    groups = []
    for d, d2 in zip(['l','r'],['r','l']):
        mappings['check_pos'] = mappings['read_pos'] + np.where(mappings['strand'] == '+', -1, 1) * (1 if d == 'l' else -1)
        new_groups = mappings[['read_id','read_start','check_pos','scaf',f'{d}pos',f'{d}hap','group']].merge(mappings[['read_id','read_start','read_pos','scaf','pos',f'{d2}hap','group']].rename(columns={'read_pos':'check_pos','pos':f'{d}pos',f'{d2}hap':f'{d}hap','group':f'{d}group'}), on=['read_id','read_start','check_pos','scaf',f'{d}pos',f'{d}hap'], how='inner')[['group',f'{d}group']] # We can have multiple matches here, when haplotypes split, we will handle that later
        new_groups = new_groups[new_groups['group'] != new_groups[f'{d}group']].drop_duplicates()
        if len(groups) == 0:
            groups = new_groups
        else:
            groups = groups.merge(new_groups, on=['group'], how='outer')
    mappings.drop(columns=['check_pos'], inplace=True)
    if len(groups):
        for d in ['l','r']:
            if f'{d}group' in groups.columns:
                groups.loc[np.isnan(groups[f'{d}group']), f'{d}group'] = groups.loc[np.isnan(groups[f'{d}group']), 'group']
            else:
                groups[f'{d}group'] = groups['group']
        groups = groups.astype(int)
        groups['new_group'] = groups.min(axis=1)
        while True:
            # Assign lowest connected group to all groups
            groups['check'] = groups['new_group']
            groups = groups.merge(groups[['group','new_group']].rename(columns={'group':'lgroup','new_group':'lngroup'}), on=['lgroup'], how='left')
            groups = groups.merge(groups[['group','new_group']].rename(columns={'group':'rgroup','new_group':'rngroup'}), on=['rgroup'], how='left')
            groups['new_group'] = groups[['new_group','lngroup','rngroup']].min(axis=1).astype(int)
            groups.drop(columns=['lngroup','rngroup'], inplace=True)
            groups.drop_duplicates(inplace=True)
            groups = groups.groupby(['group','lgroup','rgroup','check'])['new_group'].min().reset_index()
            tmp = groups.groupby(['group'], sort=False)['new_group'].agg(['min','size'])
            groups['new_group'] = np.repeat(tmp['min'].values, tmp['size'].values)
            if np.sum(groups['new_group'] != groups['check']) == 0:
                groups = groups.groupby(['group'])['new_group'].max().reset_index()
                break
        groups = groups[groups['group'] != groups['new_group']].copy()
        mappings['new_group'] = mappings[['group']].merge(groups, on=['group'], how='left')['new_group'].values
        if np.sum(np.isnan(mappings['new_group']) == False):
            mappings.loc[np.isnan(mappings['new_group']) == False, 'group'] = mappings.loc[np.isnan(mappings['new_group']) == False, 'new_group'].astype(int)
        mappings.drop(columns=['new_group'], inplace=True)
    # If a read matches multiple haplotypes of the same read they might end up in the same group. To resolve this, duplicate the groups.
    merged_groups = mappings.groupby(['group','read_pos']).size()
    merged_groups = merged_groups[merged_groups > 1].reset_index()[['group']].drop_duplicates()
    if len(merged_groups):
        mappings['mindex'] = mappings.index.values
        old_groups = mappings.loc[np.isin(mappings['group'].values, merged_groups['group'].values), ['group','read_pos','pos','rhap','lhap','mindex','strand','rpos','lpos']].copy()
        # Start from the lowest read_pos in the group and expand+duplicate on valid connections
        merged_groups = merged_groups.merge(old_groups.groupby(['group'])['read_pos'].min().reset_index(), on=['group'], how='left')
        merged_groups['max_pos'] = merged_groups[['group']].merge(old_groups.groupby(['group'])['read_pos'].max().reset_index(), on=['group'], how='left')['read_pos'].values
        merged_groups['new_group'] = merged_groups['group']
        merged_groups = merged_groups.merge(old_groups, on=['group','read_pos'], how='left')
        start_group_id = mappings['group'].max() + 1
        split = merged_groups['new_group'] == merged_groups['new_group'].shift(1)
        nsplit = np.sum(split)
        if nsplit:
            merged_groups.loc[split, 'new_group'] = np.arange(start_group_id, start_group_id+nsplit)
            start_group_id += nsplit
        sep_groups = merged_groups[['group','new_group','mindex']].copy()
        merged_groups = merged_groups[merged_groups['read_pos'] < merged_groups['max_pos']].drop(columns=['mindex','strand','rpos','lpos'])
        while len(merged_groups):
            # Get the valid extensions for the next position
            merged_groups['read_pos'] += 1
            merged_groups.rename(columns={col:f'o{col}' for col in ['pos','rhap','lhap']}, inplace=True)
            merged_groups = merged_groups.merge(old_groups, on=['group','read_pos'], how='left')
            merged_groups = merged_groups[ (np.where(merged_groups['strand'] == '-', merged_groups['rpos'], merged_groups['lpos']) == merged_groups['opos']) &
                                           np.where(merged_groups['strand'] == '-', merged_groups['rhap'] == merged_groups['olhap'], merged_groups['lhap'] == merged_groups['orhap']) ].drop(columns=['opos','olhap','orhap'])
            # Split groups
            merged_groups.sort_values(['group','new_group'], inplace=True)
            split = merged_groups['new_group'] == merged_groups['new_group'].shift(1)
            nsplit = np.sum(split)
            if nsplit:
                merged_groups['new_group2'] =  merged_groups['new_group']
                merged_groups.loc[split, 'new_group2'] = np.arange(start_group_id, start_group_id+nsplit)
                start_group_id += nsplit
                sep_groups = sep_groups.merge(merged_groups[['new_group','new_group2']], on=['new_group'], how='left')
                sep_groups.loc[np.isnan(sep_groups['new_group2']) == False, 'new_group'] = sep_groups.loc[np.isnan(sep_groups['new_group2']) == False, 'new_group2'].astype(int) # If the new_group is no longer in merged_groups (because the group finished) we get NaNs
                sep_groups.drop(columns=['new_group2'], inplace=True)
                merged_groups['new_group'] = merged_groups['new_group2']
                merged_groups.drop(columns=['new_group2'], inplace=True)
            # Check if a new groups starts here (unconnected entry at this position for the group)
            new_groups = merged_groups[['group','read_pos','max_pos']].drop_duplicates().merge(old_groups, on=['group','read_pos'], how='left')
            new_groups = new_groups[np.isin(new_groups['mindex'].values, merged_groups['mindex'].values) == False].copy()
            if len(new_groups):
                new_groups['new_group'] = np.arange(start_group_id, start_group_id+len(new_groups))
                start_group_id += len(new_groups)
                merged_groups = pd.concat([merged_groups, new_groups], ignore_index=True)
            # Store valid extensions and prepare next round
            sep_groups = pd.concat([sep_groups, merged_groups[['group','new_group','mindex']].copy()], ignore_index=True)
            merged_groups = merged_groups[merged_groups['read_pos'] < merged_groups['max_pos']].drop(columns=['mindex','strand','rpos','lpos'])
        sep_groups.sort_values(['mindex','group','new_group'], inplace=True)
        mappings = mappings.merge(sep_groups, on=['mindex','group'], how='left')
        mappings.loc[np.isnan(mappings['new_group']) == False, 'group'] = mappings.loc[np.isnan(mappings['new_group']) == False, 'new_group'].astype(int)
        mappings.drop(columns=['mindex','new_group'], inplace=True)
    # Check that groups are consistent now
    merged_groups = mappings.groupby(['group','read_pos']).size()
    merged_groups = merged_groups[merged_groups > 1].reset_index()[['group']].drop_duplicates()
    if len(merged_groups):
        print( mappings.loc[np.isin(mappings['group'].values, merged_groups['group'].values)] )
        raise RuntimeError("Groups have duplicated positions after BasicMappingToScaffolds.")
#
    return mappings

def MapReadsToScaffolds(mappings, scaffold_paths, overlap_bridges, bridges, ploidy):
    # Preserve some information on the neighbouring mappings that we need later
    mappings['read_pos'] = mappings.groupby(['read_id','read_start'], sort=False).cumcount()
    mappings[['lmapq','lmatches']] = mappings[['mapq','matches']].shift(1, fill_value=-1).values
    tmp =  mappings[['mapq','matches']].shift(-1, fill_value=-1)
    mappings['rmapq'] = np.where(mappings['strand'] == '+', tmp['mapq'].values, mappings['lmapq'].values)
    mappings['rmatches'] = np.where(mappings['strand'] == '+', tmp['matches'].values, mappings['lmatches'].values)
    mappings.loc[mappings['strand'] == '-', ['lmapq','lmatches']] = tmp[mappings['strand'].values == '-'].values
    mappings.loc[mappings['left_con'] < 0, ['lmapq','lmatches']] = -1
    mappings.loc[mappings['right_con'] < 0, ['rmapq','rmatches']] = -1
    mappings.drop(columns=['num_mappings'], inplace=True)
#
    # Store full mappings, such that we can later go back and check if we can improve scaffold path to fit the reads
    org_mappings = mappings.copy()
#
    while True:
        all_scafs = PrepareScaffoldPathForMapping(scaffold_paths, bridges, ploidy)
        mappings = BasicMappingToScaffolds(mappings, all_scafs)
#
        # Get coverage for connections
        conn_cov = mappings.loc[(mappings['lcon'] >= 0) & (mappings['lpos'] >= 0), ['scaf','pos','lpos','lhap']].rename(columns={'pos':'rpos','lhap':'hap'}).groupby(['scaf','lpos','rpos','hap']).size().reset_index(name='cov')
        conn_cov = all_scafs.loc[(all_scafs['lpos'] >= 0), ['scaf','lpos','pos','lhap']].drop_duplicates().rename(columns={'pos':'rpos','lhap':'hap'}).sort_values(['scaf','lpos','rpos','hap']).merge(conn_cov, on=['scaf','lpos','rpos','hap'], how='left')
        conn_cov['cov'] = conn_cov['cov'].fillna(0).astype(int)
#
        # Remove reads, where they map to multiple locations (keep them separately, so that we can restore them if it does leave a connection between contigs without reads)
        dups_maps = mappings[['read_id','read_start','read_pos','scaf','pos']].drop_duplicates().groupby(['read_id','read_start','read_pos'], sort=False).size().reset_index(name='count')
        mcols = ['read_id','read_start','read_pos']
        mappings['count'] = mappings[mcols].merge(dups_maps, on=mcols, how='left')['count'].values
        dups_maps = mappings[['group','count']].groupby(['group'])['count'].min().reset_index(name='gcount')
        mappings.rename(columns={'count':'gcount'}, inplace=True)
        mappings['gcount'] = mappings[['group']].merge(dups_maps, on=['group'], how='left')['gcount'].values
        dups_maps = mappings[mappings['gcount'] > 1].copy()
        mappings = mappings[mappings['gcount'] == 1].drop(columns=['gcount'])
        mappings.rename(columns={'group':'mapid'}, inplace=True)
        conn_cov = conn_cov.merge(mappings.loc[(mappings['lcon'] >= 0) & (mappings['lpos'] >= 0), ['scaf','pos','lpos','lhap']].rename(columns={'pos':'rpos','lhap':'hap'}).groupby(['scaf','lpos','rpos','hap']).size().reset_index(name='ucov'), on=['scaf','lpos','rpos','hap'], how='left')
        conn_cov['ucov'] = conn_cov['ucov'].fillna(0).astype(int)
#
        # Set coverage to 1 for  overlap_bridges, so that they are not removed
        if len(overlap_bridges):
            zero_cov = conn_cov.loc[conn_cov['cov'] == 0, ['scaf','lpos','rpos','hap']].copy()
            cols = [f'{n}{h}' for h in range(ploidy) for n in ['phase','con','strand']]
            for s in ['l','r']:
                zero_cov[cols] = zero_cov[['scaf',f'{s}pos']].rename(columns={f'{s}pos':'pos'}).merge(scaffold_paths[['scaf','pos']+cols], on=['scaf','pos'], how='left')[cols].values
                zero_cov[[f'{s}con',f'{s}strand']] = zero_cov[['con0','strand0']].values
                for h in range(1,ploidy):
                    cur = (zero_cov['hap'] == h) & (zero_cov[f'phase{h}'] > 0)
                    if np.sum(cur):
                        zero_cov.loc[cur, [f'{s}con',f'{s}strand']] = zero_cov.loc[cur, [f'con{h}',f'strand{h}']].values
                    zero_cov.drop(columns=cols, inplace=True)
            zero_cov.rename(columns={'lcon':'q_con','lstrand':'q_side','rcon':'t_con','rstrand':'t_side'}, inplace=True)
            zero_cov['q_side'] = np.where(zero_cov['q_side'] == '+', 'r', 'l')
            zero_cov['t_side'] = np.where(zero_cov['t_side'] == '+', 'l', 'r')
            zero_cov = zero_cov.merge(overlap_bridges[['q_con','q_side','t_con','t_side']], on=['q_con','q_side','t_con','t_side'], how='inner')
            conn_cov.loc[ conn_cov[['scaf','lpos','rpos','hap']].merge(zero_cov[['scaf','lpos','rpos','hap']], on=['scaf','lpos','rpos','hap'], how='left', indicator=True)['_merge'].values == "both", ['cov','ucov']] = 1
#
#        # Try to fix scaffold_paths, where no reads support the connection
#        # Start by getting mappings that have both sides in them
#        unsupp_conns = conn_cov[conn_cov['ucov'] == 0].copy()
#        if len(unsupp_conns):
#            unsupp_conns = unsupp_conns.merge(scaffold_paths[['scaf','pos']+[f'con{h}' for h in range(ploidy)]].rename(columns={'pos':'lpos','con0':'lcon'}), on=['scaf','lpos'], how='left')
#            for h in range(1,ploidy):
#                sel = (h == unsupp_conns['hap']) & (unsupp_conns[f'con{h}'] >= 0)
#                unsupp_conns.loc[sel, 'lcon'] = unsupp_conns.loc[sel, f'con{h}'] 
#            unsupp_conns.drop(columns=[f'con{h}' for h in range(1,ploidy)], inplace=True)
#            unsupp_conns = unsupp_conns.merge(scaffold_paths[['scaf','pos']+[f'con{h}' for h in range(ploidy)]].rename(columns={'pos':'rpos','con0':'rcon'}), on=['scaf','rpos'], how='left')
#            for h in range(1,ploidy):
#                sel = (h == unsupp_conns['hap']) & (unsupp_conns[f'con{h}'] >= 0)
#                unsupp_conns.loc[sel, 'rcon'] = unsupp_conns.loc[sel, f'con{h}'] 
#            unsupp_conns.drop(columns=[f'con{h}' for h in range(1,ploidy)], inplace=True)
#            lreads = unsupp_conns[['lcon']].reset_index().merge(org_mappings[['conpart','read_id']].rename(columns={'conpart':'lcon'}), on=['lcon'], how='inner')
#            rreads = unsupp_conns[['rcon']].reset_index().merge(org_mappings[['conpart','read_id']].rename(columns={'conpart':'rcon'}), on=['rcon'], how='inner')
#            supp_reads = lreads.drop(columns=['lcon']).merge(rreads.drop(columns=['rcon']), on=['index','read_id'], how='inner')[['read_id']].drop_duplicates()
#            # Remove reads that already have a valid mapping to scaffold_paths
#            supp_reads = supp_reads.loc[supp_reads.merge(mappings[['read_id']].drop_duplicates(), on=['read_id'], how='left', indicator=True)['_merge'].values == "left_only", :]
#            supp_reads = supp_reads.loc[supp_reads.merge(dups_maps[['read_id']].drop_duplicates(), on=['read_id'], how='left', indicator=True)['_merge'].values == "left_only", :]
#            supp_reads = supp_reads.merge(org_mappings, on=['read_id'], how='inner')
#            supp_reads.sort_values(['read_id','read_start','read_pos'], inplace=True)
#            # Remove the unsupported connections from all_scafs and try mapping supp_reads again
#            all_scafs.loc[all_scafs[['scaf','pos','rhap']].merge(unsupp_conns[['scaf','lpos','hap']].rename(columns={'lpos':'pos','hap':'rhap'}), on=['scaf','pos','rhap'], how='left', indicator=True)['_merge'].values == "both", ['rpos','rcon','rstrand','rdist','rdmin','rdmax']] = [-1,-1,'',0,-int(sys.maxsize*0.99),int(sys.maxsize)*0.99]
#            all_scafs.loc[all_scafs[['scaf','pos','lhap']].merge(unsupp_conns[['scaf','rpos','hap']].rename(columns={'rpos':'pos','hap':'lhap'}), on=['scaf','pos','lhap'], how='left', indicator=True)['_merge'].values == "both", ['lpos','lcon','lstrand','ldist','ldmin','ldmax']] = [-1,-1,'',0,-int(sys.maxsize*0.99),int(sys.maxsize)*0.99]
#            supp_reads = BasicMappingToScaffolds(supp_reads, all_scafs)
#            # Only keep supp_reads that still map to both sides of a the unsupported connection
#            lreads = unsupp_conns[['lcon']].reset_index().merge(supp_reads[['conpart','read_id']].rename(columns={'conpart':'lcon'}), on=['lcon'], how='inner')
#            rreads = unsupp_conns[['rcon']].reset_index().merge(supp_reads[['conpart','read_id']].rename(columns={'conpart':'rcon'}), on=['rcon'], how='inner')
#            supp_reads = lreads.drop(columns=['lcon']).merge(rreads.drop(columns=['rcon']), on=['index','read_id'], how='inner')[['read_id']].drop_duplicates().merge(supp_reads, on=['read_id'], how='inner')
#            supp_reads.sort_values(['read_id','read_start','read_pos'], inplace=True)
            # Remove supp_reads, where read_pos have been filtered out between both sides of the unsupported connection
            
            # supp_reads[supp_reads['scaf'] == 214].drop(columns=['read_start','read_end','read_from','read_to','con_from','con_to','matches','lmatches','rmatches'])
            # unsupp_conns[unsupp_conns['scaf'] == 214]
            # scaffold_paths[scaffold_paths['scaf'] == 214].head(30).tail(10)
            
            # Do something about connected alternatives and read truncation
            
            # supp_reads[supp_reads['scaf'] == 59].drop(columns=['read_start','read_end','read_from','read_to','con_from','con_to','matches','lmatches','rmatches'])
            # unsupp_conns[unsupp_conns['scaf'] == 59]
            # scaffold_paths[scaffold_paths['scaf'] == 59].head(15).tail(15)
            
            # Do something about connected haplotypes, where only one is bad
            
            # supp_reads[supp_reads['scaf'] == 3043].drop(columns=['read_start','read_end','read_from','read_to','con_from','con_to','matches','lmatches','rmatches'])
            # unsupp_conns[unsupp_conns['scaf'] == 3043]
            # scaffold_paths[scaffold_paths['scaf'] == 3043].head(10)
            
            # Remove unsupp_conns, where the difference between haplotypes is only the distance and one is supported
            
            # supp_reads[supp_reads['scaf'] == 266].drop(columns=['read_start','read_end','read_from','read_to','con_from','con_to','matches','lmatches','rmatches'])
            # unsupp_conns[unsupp_conns['scaf'] == 266]
            # scaffold_paths[scaffold_paths['scaf'] == 266].head(10)
        
        # Put back duplicated reads, where otherwise no read support exists for the connection
        dcons = conn_cov[(conn_cov['ucov'] == 0) & (conn_cov['cov'] > 0)].drop(columns=['cov','ucov']).reset_index(drop=True).reset_index().rename(columns={'index':'ci'})
        dups_maps['lci'] = dups_maps[['scaf','pos','lhap']].merge(dcons[['scaf','rpos','hap','ci']].rename(columns={'rpos':'pos','hap':'lhap'}), on=['scaf','pos','lhap'], how='left')['ci'].fillna(-1).astype(int).values
        dups_maps['rci'] = dups_maps[['scaf','pos','rhap']].merge(dcons[['scaf','lpos','hap','ci']].rename(columns={'lpos':'pos','hap':'rhap'}), on=['scaf','pos','rhap'], how='left')['ci'].fillna(-1).astype(int).values
        dups_maps.loc[dups_maps['lcon'] < 0, 'lci'] = -1
        dups_maps.loc[dups_maps['rcon'] < 0, 'rci'] = -1
        dups_maps = dups_maps.loc[(dups_maps['lci'] >= 0) | (dups_maps['rci'] >= 0), :]
        # Duplicate entries that have a left and a right connection that can only be filled with a duplicated read
        dups_maps.sort_values(['read_id','read_start','group','read_pos','scaf','pos','lhap','rhap'], inplace=True)
        dups_maps.reset_index(inplace=True)
        dups_maps = dups_maps.loc[np.repeat(dups_maps.index.values, (dups_maps['lci'] >= 0).values.astype(int) + (dups_maps['rci'] >= 0).values.astype(int))].reset_index(drop=True)
        dups_maps.loc[dups_maps['index'] == dups_maps['index'].shift(1), 'lci'] = -1
        dups_maps.loc[dups_maps['index'] == dups_maps['index'].shift(-1), 'rci'] = -1
        dups_maps.drop(columns=['index'], inplace=True)
        # Only keep the groups for a ci that covers both sides
        dups_maps = dups_maps[ (dups_maps['lci'] < 0) | (dups_maps[['group','lci']].rename(columns={'lci':'rci'}).merge(dups_maps[['group','rci']].drop_duplicates(), on=['group','rci'], how='left', indicator=True)['_merge'].values == "both") ].copy()
        dups_maps = dups_maps[ (dups_maps['rci'] < 0) | (dups_maps[['group','rci']].rename(columns={'rci':'lci'}).merge(dups_maps[['group','lci']].drop_duplicates(), on=['group','lci'], how='left', indicator=True)['_merge'].values == "both") ].copy()
        # Choose the reads with the lowest amount of duplication for the connections
        for col in ['lci','rci']:
            dups_maps.sort_values([col], inplace=True)
            mcount = dups_maps.groupby([col], sort=False)['gcount'].agg(['min','size'])
            dups_maps.loc[dups_maps['gcount'] > np.repeat(mcount['min'].values, mcount['size'].values), col] = -1
        dups_maps = dups_maps[(dups_maps['lci'] >= 0) | (dups_maps['rci'] >= 0)].drop(columns=['gcount'])
        # Remove the unused connections, such that we do not use them for other connections
        dups_maps.loc[dups_maps['lci'] < 0, ['lcon','ldist','lmapq','lmatches']] = [-1,0,-1,-1]
        dups_maps.loc[dups_maps['rci'] < 0, ['rcon','rdist','rmapq','rmatches']] = [-1,0,-1,-1]
        # In rare cases a read can map on the forward and reverse strand, chose one of those arbitrarily
        dups_maps['ci'] = dups_maps[['lci','rci']].max(axis=1) # Only one is != -1 at this point
        dups_maps.drop(columns=['lci','rci'], inplace=True)
        dups_maps.sort_values(['read_id','read_start','group','ci'], inplace=True)
        tmp = dups_maps.groupby(['read_id','read_start','group','ci'], sort=False).size().reset_index(name='mappings')
        if np.sum(tmp['mappings'] != 2):
            print("Error: A connection between two mappings of a read has more or less than two mappings associated.")
            print(tmp[tmp['mappings'] != 2])
        tmp = tmp.drop(columns=['mappings']).groupby(['read_id','read_start','ci'], sort=False).first().reset_index() # Take here the group with the lower id to resolve the strand conflict
        dups_maps = dups_maps.merge(tmp, on=['read_id','read_start','group','ci'], how='inner')
        # Trim the length of the duplicated reads, such taht we do not use them for extending
        tmp = dups_maps.groupby(['read_id','read_start','group','ci'], sort=False).agg({'read_from':['min'], 'read_to':['max']})
        dups_maps['read_start'] = np.repeat( tmp['read_from','min'].values, 2 )
        dups_maps['read_end'] = np.repeat( tmp['read_to','max'].values, 2 )
        # Assing a map id to uniquely identify the individual mapping groups
        dups_maps.sort_values(['group','ci'], inplace=True)
        dups_maps['mapid'] = ((dups_maps['group'] != dups_maps['group'].shift(1)) | (dups_maps['ci'] != dups_maps['ci'].shift(1))).cumsum() + mappings['mapid'].max()
        dups_maps.drop(columns=['group','ci'], inplace=True)
        # Update read positions
        dups_maps.sort_values(['read_id','read_start','mapid','read_pos'], inplace=True)
        dups_maps['read_pos'] = dups_maps.groupby(['read_id','read_start','mapid'], sort=False).cumcount()
        # Merge unique and duplicated mappings
        mappings = pd.concat([mappings, dups_maps], ignore_index=True).sort_values(['read_id','read_start','mapid','read_pos'], ignore_index=True)
        mappings['mapid'] = (mappings['mapid'] != mappings['mapid'].shift(1)).cumsum()-1
#
        # Break connections where they are not supported by reads even with multi mapping reads and after fixing attemps(should never happen, so give a warning)
        if len(conn_cov[conn_cov['cov'] == 0]) == 0:
            break
        else:
            print( len(conn_cov[conn_cov['cov'] == 0]), "gaps were created for which no read for filling can be found. The connections will be broken up again.")
            # Start with alternative paths
            for h in range(1,ploidy):
                rem = conn_cov.loc[(conn_cov['cov'] == 0) & (conn_cov['hap'] == h), ['scaf','rpos']].rename(columns={'rpos':'pos'}).merge(scaffold_paths[['scaf','pos',f'phase{h}']], on=['scaf','pos'], how='inner')[f'phase{h}'].values
                rem = np.isin(scaffold_paths[f'phase{h}'], np.abs(rem))
                if np.sum(rem):
                    scaffold_paths.loc[rem, [f'con{h}',f'strand{h}',f'dist{h}']] = [-1,'',0]
                    scaffold_paths.loc[rem, f'phase{h}'] = -scaffold_paths.loc[rem, f'phase{h}']
            # Continue with main paths that has alternatives
            rem_conns = conn_cov[(conn_cov['cov'] == 0) & (conn_cov['hap'] == 0)].copy()
            for h in range(1,ploidy):
                rem = rem_conns[['scaf','rpos']].reset_index().rename(columns={'rpos':'pos'}).merge(scaffold_paths.loc[scaffold_paths[f'phase{h}'] >= 0, ['scaf','pos',f'phase{h}']], on=['scaf','pos'], how='inner')
                rem_conns.drop(rem['index'].values, inplace=True)
                rem = np.isin(scaffold_paths[f'phase{h}'], rem[f'phase{h}'].values)
                if np.sum(rem):
                    scaffold_paths.loc[rem, ['phase0','con0','strand0','dist0']] = scaffold_paths.loc[rem, [f'phase{h}',f'con{h}',f'strand{h}',f'dist{h}']].values
                    scaffold_paths.loc[rem, [f'con{h}',f'strand{h}',f'dist{h}']] = [-1,'',0]
                    scaffold_paths.loc[rem, f'phase{h}'] = -scaffold_paths.loc[rem, f'phase{h}']
            # Finally split contig, where we do not have any valid connection
            scaffold_paths['split'] = scaffold_paths[['scaf','pos']].merge(rem_conns[['scaf','rpos']].rename(columns={'rpos':'pos'}), on=['scaf','pos'], how='left', indicator=True)['_merge'].values == "both"
            scaffold_paths['scaf'] = ((scaffold_paths['scaf'] != scaffold_paths['scaf'].shift(1)) | scaffold_paths['split']).cumsum()-1
            scaffold_paths.drop(columns=['split'], inplace=True)
            # Clean up
            scaffold_paths = scaffold_paths[(scaffold_paths[[f'con{h}' for h in range(ploidy)]] == -1).all(axis=1) == False].copy()
            scaffold_paths['pos'] = scaffold_paths.groupby(['scaf'], sort=False).cumcount()
            scaffold_paths.loc[scaffold_paths['pos'] == 0, 'dist0'] = 0
            mappings = org_mappings.copy()
    # Store the position of overlap_bridges
    if len(overlap_bridges):
        overlap_bridges = zero_cov.merge(overlap_bridges, on=['q_con','q_side','t_con','t_side'], how='inner')
#
    ## Handle circular scaffolds
    mcols = [f'con{h}' for h in range(ploidy)]+[f'strand{h}' for h in range(ploidy)]
    mappings[mcols] = mappings[['scaf']].merge(scaffold_paths.loc[scaffold_paths['pos'] == 0, ['scaf']+mcols], on=['scaf'], how='left')[mcols].values
    mappings.loc[(mappings['rpos'] >= 0) | (mappings['rcon'] < 0), [f'con{h}' for h in range(ploidy)]] = -1
    # Check if the read continues at the beginning of the scaffold
    mappings['check_pos'] = mappings['read_pos'] + np.where(mappings['strand'] == '+', 1, -1)
    mappings['rmapid'] = mappings[['read_id','read_start','scaf','strand','check_pos']].merge(mappings.loc[mappings['pos'] == 0, ['read_id','read_start','scaf','strand','read_pos','mapid']].rename(columns={'read_pos':'check_pos'}).drop_duplicates(), on=['read_id','read_start','scaf','strand','check_pos'], how='left')['mapid'].values
    mappings.loc[np.isnan(mappings['rmapid']), [f'con{h}' for h in range(ploidy)]] = -1
    mappings['rmapid'] = mappings['rmapid'].fillna(-1).astype(int)
    mappings.drop(columns=['check_pos'], inplace=True)
    # Check available bridges
    mappings['index'] = mappings.index.values
    for h in range(ploidy):
        mappings.loc[(mappings[f'con{h}'] != mappings['rcon']), f'con{h}'] = -1
        mappings = mappings.merge(bridges[['from','from_side','to','to_side','min_dist','max_dist','mean_dist']].rename(columns={'from':'conpart','to':f'con{h}','mean_dist':f'rmdist{h}'}), on=['conpart',f'con{h}'], how='left')
        mappings.loc[mappings['from_side'].isnull() | ((mappings['from_side'] == 'r') != (mappings['con_strand'] == '+')) | ((mappings['to_side'] == 'l') != (mappings[f'strand{h}'] == '+')), f'con{h}'] = -1
        mappings.loc[(mappings['rdist'] < mappings['min_dist']) | (mappings['rdist'] > mappings['max_dist']), f'con{h}'] = -1
        mappings.drop(columns=[f'strand{h}','from_side','to_side','min_dist','max_dist'], inplace=True)
        mappings[f'rmdist{h}'] = mappings[f'rmdist{h}'].fillna(0).astype(int)
        mappings.sort_values(['index',f'con{h}',f'rmdist{h}'], inplace=True)
        mappings = mappings.groupby(['index'], sort=False).last().reset_index()
    mappings.drop(columns=['index'], inplace=True)
    mappings.sort_values(['mapid','read_pos','scaf','pos','lhap','rhap'], inplace=True)
    # Summarize the circular connections and filter
    circular = pd.concat([mappings.loc[mappings[f'con{h}'] >= 0, ['scaf','pos','rhap',f'rmdist{h}']].rename(columns={f'rmdist{h}':'rmdist'}) for h in range(ploidy)], ignore_index=True)
    circular['lhap'] = np.repeat([h for h in range(ploidy)], [np.sum(mappings[f'con{h}'] >= 0) for h in range(ploidy)])
    circular = circular.groupby(['scaf','pos','rhap','lhap','rmdist']).size().reset_index(name='count')
    tmp = circular.groupby(['scaf'])['count'].agg(['sum','size'])
    circular['tot'] = np.repeat(tmp['sum'].values, tmp['size'].values)
    # Filter circular connections with more than ploidy options
    circular = circular[np.repeat(tmp['size'].values<=ploidy, tmp['size'].values)].copy()
    # Filter scaffolds where non-circular options are dominant
    circular = circular.merge(circular[['scaf','pos']].drop_duplicates().merge(mappings.loc[(mappings['rcon'] >= 0) & (mappings[[f'con{h}' for h in range(ploidy)]] < 0).all(axis=1), ['scaf','pos']], on=['scaf','pos'], how='inner').groupby(['scaf','pos']).size().reset_index(name='veto'), on=['scaf','pos'], how='left')
    circular = circular[circular['count'] > circular['veto']].copy()
    # Ensure that left and right haplotypes match
    circular = circular[(circular['rhap'] == circular['lhap']) | (circular['rhap'] == 0) | (circular['lhap'] == 0)].copy()
    circular['nhap'] = circular[['rhap','lhap']].max(axis=1)
    circular[[f'rphase{h}' for h in range(1,ploidy)]] = circular[['scaf','pos']].merge(scaffold_paths[['scaf','pos']+[f'phase{h}' for h in range(1,ploidy)]], on=['scaf','pos'], how='left')[[f'phase{h}' for h in range(1,ploidy)]].values
    circular[[f'lphase{h}' for h in range(1,ploidy)]] = circular[['scaf']].merge(scaffold_paths.loc[scaffold_paths['pos'] == 0, ['scaf']+[f'phase{h}' for h in range(1,ploidy)]], on=['scaf'], how='left')[[f'phase{h}' for h in range(1,ploidy)]].values
    for h in range(1,ploidy):
        circular = circular[ ((circular['rhap'] == circular['nhap']) | (circular['nhap'] != h) | (circular[f'rphase{h}'] < 0)) &
                             ((circular['lhap'] == circular['nhap']) | (circular['nhap'] != h) | (circular[f'lphase{h}'] < 0)) ].drop(columns=[f'rphase{h}',f'lphase{h}'])
    # If we have multiple connections for the same haplotype take the one with most counts
    circular.sort_values(['scaf','pos','nhap','count'], inplace=True)
    circular = circular.groupby(['scaf','pos','nhap']).last().reset_index()
    # Add information to accepted circular mappings
    if len(circular):
        mappings[['hap','rmdist','nhap']] = mappings[['scaf','pos','rhap']].merge(circular[['scaf','pos','rhap','lhap','rmdist','nhap']].rename(columns={'lhap':'hap'}), on=['scaf','pos','rhap'], how='left')[['hap','rmdist','nhap']].values
        mappings['circular'] = False
        for h in range(ploidy):
            mappings.loc[(mappings['hap'] == h) & (mappings[f'rmdist{h}'] == mappings['rmdist']) & (mappings[f'con{h}'] >= 0), 'circular'] = True
        mappings.loc[mappings['circular'], 'rpos'] = 0
        mappings.loc[mappings['circular'], 'rhap'] = mappings.loc[mappings['circular'], 'nhap'].astype(int)
        mappings.loc[mappings['circular'] == False, 'rmapid'] = mappings.loc[mappings['circular'] == False, 'mapid']
        mappings.drop(columns=[f'con{h}' for h in range(ploidy)] + [f'rmdist{h}' for h in range(ploidy)] + ['hap','rmdist','circular','nhap'], inplace=True)
        mappings['check_pos'] = mappings['read_pos'] + np.where(mappings['strand'] == '+', 1, -1)
        mappings[['circular','nhap']] = mappings[['read_id','read_start','read_pos','scaf','pos','strand','conpart','lcon','ldist','mapid']].merge(mappings.loc[mappings['rpos'] == 0, ['read_id','read_start','scaf','strand','check_pos','rpos','conpart','rcon','rdist','rmapid','pos','rhap']].rename(columns={'check_pos':'read_pos','rpos':'pos','conpart':'lcon','rcon':'conpart','rdist':'ldist','rmapid':'mapid','pos':'circular'}), on=['read_id','read_start','read_pos','scaf','pos','strand','conpart','lcon','ldist','mapid'], how='left')[['circular','rhap']].values
        mappings.loc[np.isnan(mappings['circular']) == False, ['lpos','lhap']] = mappings.loc[np.isnan(mappings['circular']) == False, ['circular','nhap']].astype(int).values
        mappings.drop(columns=['check_pos','circular','nhap'], inplace=True)
    else:
        mappings['rmapid'] = mappings['mapid']
        mappings.drop(columns=[f'rmdist{h}' for h in range(ploidy)], inplace=True)
#
    return mappings, scaffold_paths, overlap_bridges

def CountResealedBreaks(result_info, scaffold_paths, contig_parts, ploidy):
    ## Get resealed breaks (Number of total contig_parts - minimum number of scaffold chunks needed to have them all included in the proper order)
    broken_contigs = contig_parts.reset_index().rename(columns={'index':'conpart'}).loc[(contig_parts['part'] > 0) | (contig_parts['part'].shift(-1) > 0), ['contig','part','conpart']]
    result_info['breaks']['resealed'] = 0
    if len(broken_contigs):
        # For this treat haplotypes as different scaffolds
        resealed_breaks = scaffold_paths.loc[np.repeat(scaffold_paths.index.values, ploidy), ['scaf','pos']+[f'{n}{h}' for h in range(ploidy) for n in ['phase','con','strand']]]
        if len(resealed_breaks):
            resealed_breaks['hap'] = list(range(ploidy))*len(scaffold_paths)
            resealed_breaks.rename(columns={'con0':'con','strand0':'strand'}, inplace=True)
            for h in range(1,ploidy):
                change = (resealed_breaks['hap'] == h) & (resealed_breaks[f'phase{h}'] >= 0)
                if np.sum(change):
                    resealed_breaks.loc[change, ['con','strand']] = resealed_breaks.loc[change, [f'con{h}',f'strand{h}']].values
            resealed_breaks['scaf'] = resealed_breaks['scaf']*ploidy + resealed_breaks['hap']
            resealed_breaks.drop(columns=['hap','phase0'] + [f'{n}{h}' for h in range(1,ploidy) for n in ['phase','con','strand']], inplace=True)
            resealed_breaks = resealed_breaks[resealed_breaks['con'] >= 0].copy()
        if len(resealed_breaks):
            resealed_breaks.sort_values(['scaf','pos'], inplace=True)
            resealed_breaks['pos'] = resealed_breaks.groupby(['scaf']).cumcount()
            # Reduce scaffolds to the chunks of connected broken contig parts
            resealed_breaks = resealed_breaks[np.isin(resealed_breaks['con'].values, broken_contigs['conpart'].values)].copy()
            if len(resealed_breaks):
                resealed_breaks['chunk'] = ( (resealed_breaks['scaf'] != resealed_breaks['scaf'].shift(1)) | (resealed_breaks['strand'] != resealed_breaks['strand'].shift(1)) |
                                             (resealed_breaks['pos'] != resealed_breaks['pos'].shift(1)+1) | (resealed_breaks['con'] != resealed_breaks['con'].shift(1)+np.where(resealed_breaks['strand'] == '-', -1, 1))).cumsum()
                resealed_breaks = resealed_breaks.groupby(['chunk'], sort=False)['con'].agg(['min','max']).reset_index(drop=True)
                resealed_breaks = resealed_breaks[resealed_breaks['min'] < resealed_breaks['max']].sort_values(['min','max']).drop_duplicates()
            if len(resealed_breaks):
                resealed_breaks['group'] = (resealed_breaks['min'] > resealed_breaks['max'].shift(1)).cumsum()
                resealed_breaks['length'] = resealed_breaks['max'] - resealed_breaks['min'] + 1
                # Count resealed parts
                resealed_parts = resealed_breaks.loc[np.repeat(resealed_breaks.index.values, resealed_breaks['length'].values)].reset_index()[['index','min']]
                resealed_parts['conpart'] = resealed_parts['min'] + resealed_parts.groupby(['index'], sort=False).cumcount()
        if len(resealed_breaks):
            # num_resealed = Number of total contig_parts - number of contig_parts not sealed at all - number of scaffold chunks uniquely sealing contigs
            num_resealed = len(broken_contigs) - len(np.setdiff1d(broken_contigs['conpart'].values, resealed_parts['conpart'].values)) - np.sum((resealed_breaks['group'] != resealed_breaks['group'].shift(1)) & (resealed_breaks['group'] != resealed_breaks['group'].shift(-1)))
#
            # The non-unqiue sealing events still have to be processed
            resealed_breaks = resealed_breaks[(resealed_breaks['group'] == resealed_breaks['group'].shift(1)) | (resealed_breaks['group'] == resealed_breaks['group'].shift(-1))].copy()
            resealed_breaks = resealed_breaks.reset_index(drop=True).reset_index()
            resealed_breaks['keep'] = False
            while np.sum(False == resealed_breaks['keep']):
                resealed_parts = resealed_breaks.loc[np.repeat(resealed_breaks.index.values, resealed_breaks['length'].values)].reset_index()[['index','min']]
                resealed_parts['conpart'] = resealed_parts['min'] + resealed_parts.groupby(['index'], sort=False).cumcount()
                resealed_parts = resealed_parts.groupby('conpart')['index'].agg(['first','size'])
                resealed_breaks['keep'] = np.isin(resealed_breaks['index'], resealed_parts.loc[resealed_parts['size'] == 1, ['first']].reset_index()['first'].values) # Keep all chunks with unique contig parts
                resealed_breaks.sort_values(['group','keep','length'], ascending=[True,False,False], inplace=True)
                resealed_breaks = resealed_breaks[resealed_breaks['keep'] | (resealed_breaks['group'] == resealed_breaks['group'].shift(-1))].copy() # Remove shortest non-unique chunk in each group
#
            result_info['breaks']['resealed'] = num_resealed - len(resealed_breaks)

    return result_info

def FindBestReadsToFillGaps(mappings, overlap_bridges):
    possible_reads = mappings.loc[(mappings['rcon'] >= 0) & (mappings['rpos'] >= 0), ['scaf','pos','rhap','rpos','read_pos','read_id','read_start','read_from','read_to','strand','rdist','mapq','rmapq','matches','rmatches','con_from','con_to','rmapid']].sort_values(['scaf','pos','rhap'])
#
    # First take the one with the highest mapping qualities on both sides
    possible_reads['cmapq'] = np.minimum(possible_reads['mapq'], possible_reads['rmapq'])*1000 + np.maximum(possible_reads['mapq'], possible_reads['rmapq'])
    tmp = possible_reads.groupby(['scaf','pos','rhap'], sort=False)['cmapq'].agg(['max','size'])
    possible_reads = possible_reads[possible_reads['cmapq'] == np.repeat(tmp['max'].values, tmp['size'].values)].copy()
    possible_reads.drop(columns=['cmapq','mapq','rmapq'], inplace=True)
#
    # Then take the one the closest to the mean distance to get the highest chance of the other reads mapping to it later for the consensus
    tmp = possible_reads.groupby(['scaf','pos','rhap'], sort=False)['rdist'].agg(['mean','size'])
    possible_reads['dist_diff'] = np.abs(possible_reads['rdist'] - np.repeat(tmp['mean'].values, tmp['size'].values))
    tmp = possible_reads.groupby(['scaf','pos','rhap'], sort=False)['dist_diff'].agg(['min','size'])
    possible_reads = possible_reads[possible_reads['dist_diff'] == np.repeat(tmp['min'].values, tmp['size'].values)].copy()
    possible_reads.drop(columns=['dist_diff'], inplace=True)
#
    # Use matches as a final tie-breaker
    possible_reads['cmatches'] = np.minimum(possible_reads['matches'], possible_reads['rmatches'])
    tmp = possible_reads.groupby(['scaf','pos','rhap'], sort=False)['cmatches'].agg(['max','size'])
    possible_reads = possible_reads[possible_reads['cmatches'] == np.repeat(tmp['max'].values, tmp['size'].values)].copy()
    possible_reads['cmatches'] = np.maximum(possible_reads['matches'], possible_reads['rmatches'])
    tmp = possible_reads.groupby(['scaf','pos','rhap'], sort=False)['cmatches'].agg(['max','size'])
    possible_reads = possible_reads[possible_reads['cmatches'] == np.repeat(tmp['max'].values, tmp['size'].values)].copy()
    possible_reads.drop(columns=['cmatches','matches','rmatches'], inplace=True)
#
    # If everything is equally good just take the first
    possible_reads = possible_reads.groupby(['scaf','pos','rhap'], sort=False).first().reset_index()
#
    # Get the read in the gap instead of the read on the mapping
    possible_reads.loc[possible_reads['strand'] == '+', 'read_from'] = possible_reads.loc[possible_reads['strand'] == '+', 'read_to']
    possible_reads.loc[possible_reads['strand'] == '+', 'read_to'] = possible_reads.loc[possible_reads['strand'] == '+', 'read_from'] + possible_reads.loc[possible_reads['strand'] == '+', 'rdist']
    possible_reads.loc[possible_reads['strand'] == '-', 'read_to'] = possible_reads.loc[possible_reads['strand'] == '-', 'read_from']
    possible_reads.loc[possible_reads['strand'] == '-', 'read_from'] = possible_reads.loc[possible_reads['strand'] == '-', 'read_to'] - possible_reads.loc[possible_reads['strand'] == '-', 'rdist']
    possible_reads.loc[possible_reads['rdist'] <= 0, ['read_from','read_to']] = 0
    possible_reads.drop(columns=['rdist'], inplace=True)
#
    # Get the mapping information for the other side of the gap
    possible_reads['read_pos'] += np.where(possible_reads['strand'] == '+', 1, -1)
    possible_reads[['rcon_from','rcon_to']] = possible_reads[['read_id','read_start','read_pos','scaf','pos','rpos','rmapid']].merge(mappings[['read_id','read_start','read_pos','scaf','pos','lpos','con_from','con_to','mapid']].rename(columns={'pos':'rpos','lpos':'pos','con_from':'rcon_from','con_to':'rcon_to','mapid':'rmapid'}), on=['read_id','read_start','read_pos','scaf','pos','rpos','rmapid'], how='left')[['rcon_from','rcon_to']].astype(int).values
    possible_reads.drop(columns=['read_pos','read_start','rmapid'], inplace=True)
#
    # Add overlap_bridges as pseudo reads
    if len(overlap_bridges):
        pseudo_reads = overlap_bridges[['scaf','lpos','hap','rpos','q_start','q_end','t_start','t_end']].rename(columns={'lpos':'pos','hap':'rhap','q_start':'con_from','q_end':'con_to','t_start':'rcon_from','t_end':'rcon_to'})
        pseudo_reads['read_id'] = mappings['read_id'].max() + 1 + np.arange(len(pseudo_reads))
        pseudo_reads['read_from'] = 0
        pseudo_reads['read_to'] = 0
        pseudo_reads['strand'] = '+'
        pseudo_reads['chap'] = pseudo_reads['rhap']
        possible_reads = pd.concat([possible_reads, pseudo_reads], ignore_index=True)
#
    return possible_reads

def StoreReadAndContingInformationInScaffoldPaths(scaffold_paths, contig_parts, possible_reads, read_names, ploidy):
    for h in range(ploidy):
        # Insert contig information
        scaffold_paths = scaffold_paths.merge(contig_parts[['name','start','end']].reset_index().rename(columns={'index':f'con{h}','name':f'name{h}','start':f'start{h}','end':f'end{h}'}), on=[f'con{h}'], how='left')
        # Insert read information
        scaffold_paths = scaffold_paths.merge(possible_reads.loc[possible_reads['rhap'] == h, ['scaf','pos','read_id','read_from','read_to','strand','con_from','con_to']].rename(columns={'read_id':f'read_name{h}','read_from':f'read_from{h}','read_to':f'read_to{h}','strand':f'read_strand{h}','con_from':f'con_from{h}','con_to':f'con_to{h}'}), on=['scaf','pos'], how='left')
        scaffold_paths.loc[scaffold_paths[f'read_to{h}'] > 0, f'read_name{h}'] = read_names.loc[ scaffold_paths.loc[scaffold_paths[f'read_to{h}'] > 0, f'read_name{h}'].values, 'read_name' ].values # Convert read id back into the name
        scaffold_paths.loc[scaffold_paths[f'read_to{h}'] == 0, f'read_name{h}'] = "__pseudo_read__"
        scaffold_paths = scaffold_paths.merge(possible_reads.loc[possible_reads['rhap'] == h, ['scaf','rpos','rcon_from','rcon_to']].rename(columns={'rpos':'pos','rcon_from':f'rcon_from{h}','rcon_to':f'rcon_to{h}'}), on=['scaf','pos'], how='left')
    # Split into contig and read part and apply information
    scaffold_paths = scaffold_paths.loc[np.repeat(scaffold_paths.index.values, 1+(scaffold_paths[[f'read_name{h}' for h in range(ploidy)]].isnull().all(axis=1) == False) )].reset_index()
    scaffold_paths['type'] = np.where(scaffold_paths.groupby(['index'], sort=False).cumcount() == 0, 'contig','read')
    scaffold_paths.drop(columns=['index'],inplace=True)
    for h in range(ploidy):
        scaffold_paths.loc[scaffold_paths['type'] == 'read', [f'name{h}',f'start{h}',f'end{h}',f'strand{h}']] = scaffold_paths.loc[scaffold_paths['type'] == 'read', [f'read_name{h}',f'read_from{h}',f'read_to{h}',f'read_strand{h}']].values
        scaffold_paths.drop(columns=[f'con{h}',f'dist{h}',f'read_name{h}',f'read_from{h}',f'read_to{h}',f'read_strand{h}'], inplace=True)
        scaffold_paths[[f'name{h}',f'strand{h}']] = scaffold_paths[[f'name{h}',f'strand{h}']].fillna('')
        scaffold_paths[[f'start{h}',f'end{h}']] = scaffold_paths[[f'start{h}',f'end{h}']].fillna(0).astype(int)
        # Trim contigs, where the reads stop mapping on both sides of the gap
        for side in ['','r']:
            trim = (scaffold_paths['type'] == 'contig') & (scaffold_paths[f'{side}con_from{h}'].isnull() == False)
            get_main = trim & (scaffold_paths[f'phase{h}'] < 0)
            if np.sum(get_main):
                scaffold_paths.loc[get_main, [f'name{h}',f'start{h}',f'end{h}',f'strand{h}']] = scaffold_paths.loc[get_main, ['name0','start0','end0','strand0']].values
                scaffold_paths.loc[get_main, f'phase{h}'] = -scaffold_paths.loc[get_main, f'phase{h}']
            trim_strand = trim & (scaffold_paths[f'strand{h}'] == ('-' if side=='r' else '+'))
            scaffold_paths.loc[trim_strand, f'end{h}'] = scaffold_paths.loc[trim_strand, f'{side}con_to{h}'].astype(int)
            trim_strand = trim & (scaffold_paths[f'strand{h}'] == ('+' if side=='r' else '-'))
            scaffold_paths.loc[trim_strand, f'start{h}'] = scaffold_paths.loc[trim_strand, f'{side}con_from{h}'].astype(int)
            scaffold_paths.drop(columns=[f'{side}con_to{h}',f'{side}con_from{h}'],inplace=True)
    # Remove reads that fill a negative gap length (so do not appear)
    for h in range(ploidy):
        scaffold_paths.loc[scaffold_paths[f'start{h}'] >= scaffold_paths[f'end{h}'], [f'name{h}',f'strand{h}']] = ''
    scaffold_paths = scaffold_paths.loc[(scaffold_paths[[f'name{h}' for h in range(ploidy)]] != '').any(axis=1), ['scaf','pos','type']+[f'{n}{h}' for h in range(ploidy) for n in ['phase','name','start','end','strand']]+['sdist_left','sdist_right']].copy()
    # Reads get the phase of the next contig
    for h in range(ploidy):
        scaffold_paths[f'phase{h}'] = np.where((scaffold_paths['type'] == 'read') & (scaffold_paths['scaf'] == scaffold_paths['scaf'].shift(-1)), scaffold_paths[f'phase{h}'].shift(-1, fill_value=0), scaffold_paths[f'phase{h}'])
        scaffold_paths.loc[scaffold_paths['type'] == 'read', f'phase{h}'] = np.abs(scaffold_paths.loc[scaffold_paths['type'] == 'read', f'phase{h}'])
    # Split alternative contigs that are identical to main into the overlapping part and the non-overlapping part
    scaffold_paths['smin'] = scaffold_paths['start0']
    scaffold_paths['smax'] = scaffold_paths['start0']
    scaffold_paths['emin'] = scaffold_paths['end0']
    scaffold_paths['emax'] = scaffold_paths['end0']
    for h in range(1,ploidy):
        identical = (scaffold_paths[f'name{h}'] == scaffold_paths['name0']) & (scaffold_paths[f'strand{h}'] == scaffold_paths['strand0']) & (scaffold_paths[f'phase{h}'] >= 0)
        if np.sum(identical):
            scaffold_paths.loc[identical, 'smin'] = scaffold_paths.loc[identical, ['smin',f'start{h}']].min(axis=1)
            scaffold_paths.loc[identical, 'smax'] = scaffold_paths.loc[identical, ['smax',f'start{h}']].max(axis=1)
            scaffold_paths.loc[identical, 'emin'] = scaffold_paths.loc[identical, ['emin',f'end{h}']].min(axis=1)
            scaffold_paths.loc[identical, 'emax'] = scaffold_paths.loc[identical, ['emax',f'end{h}']].max(axis=1)
    scaffold_paths.loc[(scaffold_paths['type'] == 'read') | (scaffold_paths['smax']>scaffold_paths['emin']), ['smin','smax','emin','emax']] = -1 # If we do not have an overlap in the middle we cannot use the split approach and for the reads we should not have identical reads for different haplotypes
    scaffold_paths['bsplit'] = np.where(scaffold_paths['strand0'] == '+', scaffold_paths['smin'] != scaffold_paths['smax'], scaffold_paths['emin'] != scaffold_paths['emax']) # Split before
    scaffold_paths['asplit'] = np.where(scaffold_paths['strand0'] == '+', scaffold_paths['emin'] != scaffold_paths['emax'], scaffold_paths['smin'] != scaffold_paths['smax']) # Split after
    scaffold_paths = scaffold_paths.loc[np.repeat(scaffold_paths.index.values,1+scaffold_paths['bsplit']+scaffold_paths['asplit'])].reset_index()
    scaffold_paths['split'] = scaffold_paths.groupby(['index'], sort=False).cumcount()+(scaffold_paths['bsplit'] == False)
    for h in range(ploidy-1,-1,-1):
        identical = (scaffold_paths[f'name{h}'] == scaffold_paths['name0']) & (scaffold_paths[f'strand{h}'] == scaffold_paths['strand0']) & (scaffold_paths[f'phase{h}'] >= 0)
        scaffold_paths.loc[(identical == False) & (scaffold_paths['split'] != 1), [f'name{h}',f'start{h}',f'end{h}',f'strand{h}']] = ['',0,0,''] # For the ones that we do not split, only keep the center
        mod = identical & (scaffold_paths['split'] == 0) & (scaffold_paths['strand0'] == '+')
        scaffold_paths.loc[mod, f'end{h}'] = scaffold_paths.loc[mod, 'smax']
        mod = identical & (scaffold_paths['split'] == 0) & (scaffold_paths['strand0'] == '-')
        scaffold_paths.loc[mod, f'start{h}'] = scaffold_paths.loc[mod, 'emin']
        mod = identical & (scaffold_paths['split'] == 1) & (scaffold_paths['smin'] != scaffold_paths['smax'])
        scaffold_paths.loc[mod, f'start{h}'] = scaffold_paths.loc[mod, 'smax']
        mod = identical & (scaffold_paths['split'] == 1) & (scaffold_paths['emin'] != scaffold_paths['emax'])
        scaffold_paths.loc[mod, f'end{h}'] = scaffold_paths.loc[mod, 'emin']
        mod = identical & (scaffold_paths['split'] == 2) & (scaffold_paths['strand0'] == '+')
        scaffold_paths.loc[mod, f'start{h}'] = scaffold_paths.loc[mod, 'emin']
        mod = identical & (scaffold_paths['split'] == 2) & (scaffold_paths['strand0'] == '-')
        scaffold_paths.loc[mod, f'end{h}'] = scaffold_paths.loc[mod, 'smax']
        scaffold_paths.loc[scaffold_paths[f'start{h}'] >= scaffold_paths[f'end{h}'], [f'name{h}',f'start{h}',f'end{h}',f'strand{h}']] = ['',0,0,'']
        # The end of the split contig gets the phase of the read or the next contig if the read was removed
        scaffold_paths[f'phase{h}'] = np.where((scaffold_paths['split'] == 2) & (scaffold_paths['scaf'] == scaffold_paths['scaf'].shift(-1)), np.abs(scaffold_paths[f'phase{h}'].shift(-1, fill_value=0)), scaffold_paths[f'phase{h}'])
    scaffold_paths.drop(columns=['index','smin','smax','emin','emax','bsplit','asplit','split'], inplace=True)
    # Remove alternative contigs, where they are identical to main
    for h in range(1,ploidy):
        identical = (scaffold_paths[[f'name{h}',f'start{h}',f'end{h}',f'strand{h}']] == scaffold_paths[['name0','start0','end0','strand0']].values).all(axis=1) & (scaffold_paths[f'phase{h}'] >= 0)
        scaffold_paths.loc[identical, [f'name{h}',f'start{h}',f'end{h}',f'strand{h}']] = ['',0,0,'']
        scaffold_paths.loc[identical, f'phase{h}'] = -scaffold_paths.loc[identical, f'phase{h}']
    # Update positions in scaffold to accomodate the reads and split contigs
    scaffold_paths['pos'] = scaffold_paths.groupby(['scaf'], sort=False).cumcount()
    # Move sdist_left and sdist_right to first or last entry of a scaffold only
    tmp = scaffold_paths[['scaf','sdist_left']].groupby(['scaf']).max().values
    scaffold_paths['sdist_left'] = -1
    scaffold_paths.loc[scaffold_paths['pos'] == 0, 'sdist_left'] = tmp
    tmp = scaffold_paths[['scaf','sdist_right']].groupby(['scaf']).max().values
    scaffold_paths['sdist_right'] = -1
    scaffold_paths.loc[scaffold_paths['scaf'] != scaffold_paths['scaf'].shift(-1), 'sdist_right'] = tmp
#
    return scaffold_paths

def RemoveNonExtendingMappings(mappings, contig_parts, max_dist_contig_end, min_extension, min_num_reads, pdf):
    ## Remove mappings that are not needed for scaffold extensions into gaps and update the rest
    mappings.drop(columns=['lmapq','rmapq','lmatches','rmatches','read_pos','pos'],inplace=True)
    circular = np.unique(mappings.loc[mappings['rpos'] == 0, 'scaf'].values) # Get them now, because the next step removes evidence, but remove them later to use the expensive np.isin() step on less entries
    mappings = mappings[(mappings['rpos'] < 0) | (mappings['lpos'] < 0)].copy() # Only keep mappings to ends of scaffolds
    mappings = mappings[np.isin(mappings['scaf'], circular) == False].copy() # We do not extend circular scaffolds
    # Check how much we extend in both directions (if extensions are negative set them to 0, such that min_extension==0 extends everything)
    mappings['con_start'] = contig_parts.iloc[mappings['conpart'].values, contig_parts.columns.get_loc('start')].values
    mappings['con_end'] = contig_parts.iloc[mappings['conpart'].values, contig_parts.columns.get_loc('end')].values
    mappings['ltrim'] = np.where('+' == mappings['con_strand'], mappings['con_from']-mappings['con_start'], mappings['con_end']-mappings['con_to'])
    mappings['rtrim'] = np.where('-' == mappings['con_strand'], mappings['con_from']-mappings['con_start'], mappings['con_end']-mappings['con_to'])
    mappings.drop(columns=['conpart','con_start','con_end','con_from','con_to','con_strand'],inplace=True)
    mappings['lext'] = np.maximum(0, np.where('+' == mappings['strand'], mappings['read_from']-mappings['read_start'], mappings['read_end']-mappings['read_to']) - mappings['ltrim'])
    mappings['rext'] = np.maximum(0, np.where('-' == mappings['strand'], mappings['read_from']-mappings['read_start'], mappings['read_end']-mappings['read_to']) - mappings['rtrim'])
    # We do not extend in the direction the scaffold continues
    mappings.loc[mappings['lpos'] >= 0, 'lext'] = -1
    mappings.loc[mappings['rpos'] >= 0, 'rext'] = -1
    mappings.drop(columns=['lpos','rpos'],inplace=True)
    # Set extensions to zero that are too far away from the contig_end
    mappings.loc[mappings['ltrim'] > max_dist_contig_end, 'lext'] = -1
    mappings.loc[mappings['rtrim'] > max_dist_contig_end, 'rext'] = -1
    # Only keep long enough extensions
    if pdf:
        extension_lengths = np.concatenate([mappings.loc[0<=mappings['lext'],'lext'], mappings.loc[0<=mappings['rext'],'rext']])
        if len(extension_lengths):
            if np.sum(extension_lengths < 10*min_extension):
                PlotHist(pdf, "Extension length", "# Extensions", np.extract(extension_lengths < 10*min_extension, extension_lengths), threshold=min_extension)
            PlotHist(pdf, "Extension length", "# Extensions", extension_lengths, threshold=min_extension, logx=True)
        del extension_lengths
    mappings = mappings[(mappings['lext'] >= min_extension) | (mappings['rext'] >= min_extension)].copy()
    # Separate mappings by side
    mappings = mappings.loc[np.repeat(mappings.index.values, 2)].reset_index(drop=True)
    mappings['side'] = np.tile(['l','r'], len(mappings)//2)
    mappings[['hap','ext','trim']] = mappings[['rhap','rext','rtrim']].values
    mappings.loc[mappings['side'] == 'l', ['hap','ext','trim']] = mappings.loc[mappings['side'] == 'l', ['lhap','lext','ltrim']].values
    mappings.drop(columns=['lhap','lext','ltrim','rhap','rext','rtrim'],inplace=True)
    mappings = mappings[mappings['ext'] >= min_extension].copy()
    # Get unmapped extension (This part is worth to add as a new scaffold if we do not have a unique extension, because it is not in the genome yet)
    mappings['unmap_ext'] = np.maximum(0,np.where(mappings['side'] == 'l', mappings['ldist'], mappings['rdist'])-mappings['trim'])
    no_connection = np.where(mappings['side'] == 'l', mappings['lcon'], mappings['rcon']) < 0
    mappings.loc[no_connection, 'unmap_ext'] = mappings.loc[no_connection, 'ext']
    mappings = mappings[['scaf','side','hap','ext','trim','unmap_ext','read_id','read_start','read_end','read_from','read_to','strand','mapq','matches']].copy()
    # Only keep extension, when there are enough of them
    mappings.sort_values(['scaf','side','hap'], inplace=True)
    count = mappings.groupby(['scaf','side','hap'], sort=False).size().values
    mappings = mappings[np.repeat(count >= min_num_reads, count)].copy()
#
    return mappings

def FillGapsWithReads(scaffold_paths, mappings, contig_parts, read_names, overlap_bridges, ploidy, max_dist_contig_end, min_extension, min_num_reads, pdf):
    possible_reads = FindBestReadsToFillGaps(mappings, overlap_bridges)
    scaffold_paths = StoreReadAndContingInformationInScaffoldPaths(scaffold_paths, contig_parts, possible_reads, read_names, ploidy)
    mappings = RemoveNonExtendingMappings(mappings, contig_parts, max_dist_contig_end, min_extension, min_num_reads, pdf)
    mappings['read_id'] = read_names.loc[ mappings['read_id'].values, 'read_name' ].values
    mappings.rename(columns={'read_id':'read_name'}, inplace=True)
#
    return scaffold_paths, mappings

def GetOutputInfo(result_info, scaffold_paths):
    # Calculate lengths
    con_len = scaffold_paths[['scaf', 'start0', 'end0']].copy()
    con_len['length'] = con_len['end0'] - con_len['start0']
    con_len = con_len.groupby('scaf', sort=False)['length'].sum().values

    scaf_len = scaffold_paths[['scaf','sdist_left']].groupby(['scaf'], sort=False)['sdist_left'].max().reset_index()
    scaf_len['length'] = con_len
    scaf_len.loc[scaf_len['sdist_left'] >= 0, 'length'] += scaf_len.loc[scaf_len['sdist_left'] >= 0, 'sdist_left']
    scaf_len['meta'] = (scaf_len['sdist_left'] == -1).cumsum()
    scaf_len = scaf_len.groupby('meta', sort=False)['length'].sum().values

    # Calculate N50
    con_len.sort()
    con_N50, con_total = calculateN50(con_len)

    scaf_len.sort()
    scaf_N50, scaf_total = calculateN50(scaf_len)

    # Store output stats
    result_info['output'] = {}
    result_info['output']['contigs'] = {}
    result_info['output']['contigs']['num'] = len(con_len)
    result_info['output']['contigs']['total'] = con_total
    result_info['output']['contigs']['min'] = con_len[0]
    result_info['output']['contigs']['max'] = con_len[-1]
    result_info['output']['contigs']['N50'] = con_N50

    result_info['output']['scaffolds'] = {}
    result_info['output']['scaffolds']['num'] = len(scaf_len)
    result_info['output']['scaffolds']['total'] = scaf_total
    result_info['output']['scaffolds']['min'] = scaf_len[0]
    result_info['output']['scaffolds']['max'] = scaf_len[-1]
    result_info['output']['scaffolds']['N50'] = scaf_N50

    return result_info

def PrintStats(result_info):
    print("Input assembly:     {:8,.0f} contigs   (Total sequence: {:,.0f} Min: {:,.0f} Max: {:,.0f} N50: {:,.0f})".format(result_info['input']['contigs']['num'], result_info['input']['contigs']['total'], result_info['input']['contigs']['min'], result_info['input']['contigs']['max'], result_info['input']['contigs']['N50']))
    print("                    {:8,.0f} scaffolds (Total sequence: {:,.0f} Min: {:,.0f} Max: {:,.0f} N50: {:,.0f})".format(result_info['input']['scaffolds']['num'], result_info['input']['scaffolds']['total'], result_info['input']['scaffolds']['min'], result_info['input']['scaffolds']['max'], result_info['input']['scaffolds']['N50']))
    print("Removed from input  {:8,.0f} contigs   (Total sequence: {:,.0f} Min: {:,.0f} Max: {:,.0f} Mean: {:,.0f})".format(result_info['removed']['num'], result_info['removed']['total'], result_info['removed']['min'], result_info['removed']['max'], result_info['removed']['mean']))
    print("Introduced {:,.0f} breaks of which {:,.0f} have been resealed".format(result_info['breaks']['opened'], result_info['breaks']['resealed']))
    print("Removed from output {:8,.0f} contigs   (Total sequence: {:,.0f} Min: {:,.0f} Max: {:,.0f} Mean: {:,.0f})".format(result_info['removed2']['num'], result_info['removed2']['total'], result_info['removed2']['min'], result_info['removed2']['max'], result_info['removed2']['mean']))
    print("Output assembly:    {:8,.0f} contigs   (Total sequence: {:,.0f} Min: {:,.0f} Max: {:,.0f} N50: {:,.0f})".format(result_info['output']['contigs']['num'], result_info['output']['contigs']['total'], result_info['output']['contigs']['min'], result_info['output']['contigs']['max'], result_info['output']['contigs']['N50']))
    print("                    {:8,.0f} scaffolds (Total sequence: {:,.0f} Min: {:,.0f} Max: {:,.0f} N50: {:,.0f})".format(result_info['output']['scaffolds']['num'], result_info['output']['scaffolds']['total'], result_info['output']['scaffolds']['min'], result_info['output']['scaffolds']['max'], result_info['output']['scaffolds']['N50']))

def GaplessScaffold(assembly_file, mapping_file, repeat_file, min_mapq, min_mapping_length, min_length_contig_break, prefix=False, stats=None):
    # Put in default parameters if nothing was specified
    if False == prefix:
        if ".gz" == assembly_file[-3:len(assembly_file)]:
            prefix = assembly_file.rsplit('.',2)[0]
        else:
            prefix = assembly_file.rsplit('.',1)[0]

    keep_all_subreads = False
    alignment_precision = 100
#
    proportion_duplicated = 0.95
    min_len_contig_overlap = 10000
    remove_duplicated_contigs = True
#
    remove_zero_hit_contigs = True
    remove_short_contigs = True
    min_extension = 500
    max_dist_contig_end = 500
    max_break_point_distance = 200
    merge_block_length = 10000
    prematurity_threshold = 0.05
#
    min_num_reads = 2
    borderline_removal = False
    allow_same_contig_breaks = True
    min_factor_alternatives = 1.1
    num_read_len_groups = 10
    prob_factor = 10
    min_distance_tolerance = 20
    rel_distance_tolerance = 0.2
    org_scaffold_trust = "basic" # blind: If there is a read that supports it use the org scaffold; Do not break contigs
                                 # full: If there is no confirmed other option use the org scaffold
                                 # basic: If there is no alternative bridge use the org scaffold
                                 # no: Do not give preference to org scaffolds
#
    ploidy = 2
    max_loop_units = 10
    lowmap_threshold = 0.05

    # Guarantee that outdir exists
    outdir = os.path.dirname(prefix)
    if outdir and not os.path.exists(outdir):
        os.makedirs(outdir)
    del outdir
    # Remove output files, such that we do not accidentially use an old one after a crash
    for f in [f"{prefix}_extensions.csv", f"{prefix}_scaffold_paths.csv", f"{prefix}_extending_reads.lst"]:
        if os.path.exists(f):
            os.remove(f)

    pdf = None
    if stats:
        pdf = PdfPages(stats)
        plt.ioff()

    print( str(timedelta(seconds=process_time())), "Reading in original assembly")
    contigs, contig_ids = ReadContigs(assembly_file)
    result_info = {}
    result_info = GetInputInfo(result_info, contigs)
#
    print( str(timedelta(seconds=process_time())), "Loading repeats")
    repeats = LoadRepeats(repeat_file, contig_ids)
#
    print( str(timedelta(seconds=process_time())), "Filtering mappings")
    mappings, cov_counts, cov_probs, read_names = ReadMappings(mapping_file, contig_ids, min_mapq, min_mapping_length, keep_all_subreads, alignment_precision, num_read_len_groups, pdf)
    contigs, covered_regions = RemoveUnmappedContigs(contigs, mappings, remove_zero_hit_contigs)
    mappings = BreakReadsAtAdapters(mappings, alignment_precision, keep_all_subreads)
#
    print( str(timedelta(seconds=process_time())), "Search for possible break points")
    if "blind" == org_scaffold_trust:
        # Do not break contigs
        break_groups, spurious_break_indexes, non_informative_mappings = CallAllBreaksSpurious(mappings, contigs, covered_regions, max_dist_contig_end, min_length_contig_break, min_extension, pdf)
    else:
        break_groups, spurious_break_indexes, non_informative_mappings, unconnected_breaks = FindBreakPoints(mappings, contigs, covered_regions, max_dist_contig_end, min_mapping_length, min_length_contig_break, max_break_point_distance, min_num_reads, min_extension, merge_block_length, org_scaffold_trust, cov_probs, prob_factor, allow_same_contig_breaks, prematurity_threshold, pdf)
    mappings.drop(np.concatenate([np.unique(spurious_break_indexes['map_index'].values), non_informative_mappings]), inplace=True) # Remove not-accepted breaks from mappings and mappings that do not contain any information (mappings inside of contigs that do not overlap with breaks)
    del spurious_break_indexes, non_informative_mappings
#
    contig_parts, contigs = GetContigParts(contigs, break_groups, covered_regions, remove_short_contigs, remove_zero_hit_contigs, min_mapping_length, alignment_precision, pdf)
    result_info = GetBreakAndRemovalInfo(result_info, contigs, contig_parts)
    mappings = UpdateMappingsToContigParts(mappings, contig_parts, min_mapping_length, max_dist_contig_end, min_extension)
    repeats = FindDuplicatedContigPartEnds(repeats, contig_parts, max_dist_contig_end, proportion_duplicated)
    del break_groups, contigs, contig_ids
#
    print( str(timedelta(seconds=process_time())), "Search for possible bridges")
    bridges = GetBridges(mappings, borderline_removal, min_factor_alternatives, min_num_reads, org_scaffold_trust, contig_parts, cov_probs, prob_factor, min_mapping_length, min_distance_tolerance, rel_distance_tolerance, prematurity_threshold, max_dist_contig_end, pdf)
    if min_len_contig_overlap == 0:
        overlap_bridges = []
    else:
        bridges, overlap_bridges = InsertBridgesOnUniqueContigOverlapsWithoutConnections(bridges, repeats, min_len_contig_overlap, min_num_reads)
#
    print( str(timedelta(seconds=process_time())), "Scaffold the contigs")
    scaffold_paths, trim_repeats = ScaffoldContigs(contig_parts, bridges, mappings, cov_probs, repeats, prob_factor, min_mapping_length, max_dist_contig_end, prematurity_threshold, ploidy, max_loop_units)
    scaffold_paths, result_info = RemoveUnconnectedLowlyMappedOrDuplicatedContigs(scaffold_paths, result_info, mappings, contig_parts, cov_probs, repeats, ploidy, lowmap_threshold)
    contig_parts, scaffold_paths, mappings = TrimDuplicatedEnds(contig_parts, scaffold_paths, mappings, repeats, trim_repeats, min_mapping_length, alignment_precision)
#
    print( str(timedelta(seconds=process_time())), "Fill gaps")
    mappings, scaffold_paths, overlap_bridges = MapReadsToScaffolds(mappings, scaffold_paths, overlap_bridges, bridges, ploidy) # Might break apart scaffolds again, if we cannot find a mapping read for a connection
    result_info = CountResealedBreaks(result_info, scaffold_paths, contig_parts, ploidy)
    scaffold_paths, mappings = FillGapsWithReads(scaffold_paths, mappings, contig_parts, read_names, overlap_bridges, ploidy, max_dist_contig_end, min_extension, min_num_reads, pdf) # Mappings are prepared for scaffold extensions
    result_info = GetOutputInfo(result_info, scaffold_paths)
#
    if pdf:
        pdf.close()

    print( str(timedelta(seconds=process_time())), "Writing output")
    mappings.to_csv(f"{prefix}_extensions.csv", index=False)
    scaffold_paths.to_csv(f"{prefix}_scaffold_paths.csv", index=False)
    np.savetxt(f"{prefix}_extending_reads.lst", np.unique(mappings['read_name']), fmt='%s')

    print( str(timedelta(seconds=process_time())), "Finished")
    PrintStats(result_info)

def GetPloidyFromPaths(scaffold_paths):
    return len([col for col in scaffold_paths.columns if col[:4] == 'name'])

def LoadExtensions(prefix, min_extension):
    # Load extending mappings
    mappings = pd.read_csv(prefix+"_extensions.csv")

    # Drop sides that do not fullfil min_extension
    mappings = mappings[min_extension <= mappings['ext']].reset_index(drop=True)
    
    return mappings

def RemoveDuplicatedReadMappings(extensions):
    # Minimap2 has sometimes two overlapping mappings: Remove the shorter mapping
    extensions.sort_values(['scaf','side','hap','q_index','t_index'], inplace=True)
    extensions['min_len'] = np.minimum(extensions['q_end']-extensions['q_start'], extensions['t_end']-extensions['t_start'])
    duplicates = extensions.groupby(['scaf','side','hap','q_index','t_index'], sort=False)['min_len'].agg(['max','size'])
    extensions['max_len'] = np.repeat(duplicates['max'].values,duplicates['size'].values)
    extensions = extensions[extensions['min_len'] == extensions['max_len']].copy()

    # Remove the more unequal one
    extensions['max_len'] = np.maximum(extensions['q_end']-extensions['q_start'], extensions['t_end']-extensions['t_start'])
    duplicates = extensions.groupby(['scaf','side','hap','q_index','t_index'], sort=False)['max_len'].agg(['min','size'])
    extensions['min_len'] = np.repeat(duplicates['min'].values,duplicates['size'].values)
    extensions = extensions[extensions['min_len'] == extensions['max_len']].copy()
    extensions.drop(columns=['min_len','max_len'], inplace=True)

    # Otherwise simply take the first
    extensions = extensions.groupby(['scaf','side','hap','q_index','t_index'], sort=False).first().reset_index()

    return extensions

def LoadReads(all_vs_all_mapping_file, mappings, min_length_contig_break):
    # Load all vs. all mappings for extending reads
    reads = ReadPaf(all_vs_all_mapping_file)
    reads.drop(columns=['matches','alignment_length','mapq'], inplace=True) # We don't need those columns
#
    # Get valid pairings on a smaller dataframe
    extensions = reads[['q_name','t_name']].drop_duplicates()
    extensions = extensions.merge(mappings[['read_name','read_start','scaf','side']].reset_index().rename(columns={'index':'q_index', 'read_name':'q_name', 'read_start':'q_read_start'}), on=['q_name'], how='inner')
    extensions = extensions.merge(mappings[['read_name','read_start','scaf','side']].reset_index().rename(columns={'index':'t_index', 'read_name':'t_name', 'read_start':'t_read_start'}), on=['t_name','scaf','side'], how='inner')
    extensions.drop(extensions.index[(extensions['q_name'] == extensions['t_name']) & (extensions['q_read_start'] == extensions['t_read_start'])].values, inplace=True) # remove reads that map to itself
#
    # Keep only reads that are part of a valid pairings
    extensions.drop(columns=['scaf','side','q_read_start', 't_read_start'], inplace=True)
    reads = reads.merge(extensions, on=['q_name','t_name'], how='inner')
    extensions = reads
    del reads
    extensions.drop(columns=['q_name','t_name'], inplace=True)
#
    # Add scaffold and side to which the query reads belong to
    extensions.reset_index(drop=True, inplace=True)
    extensions = pd.concat([extensions, mappings.loc[extensions['q_index'].values, ['read_start','read_end','read_from','read_to','scaf','side','hap','strand']].reset_index(drop=True).rename(columns={'read_from':'q_read_from', 'read_to':'q_read_to', 'hap':'q_hap', 'strand':'q_strand'})], axis=1)
#
    # Remove reads where the all vs. all mapping is not in the gap for the scaffold belonging to the query
    extensions.drop(extensions.index[np.where(np.logical_xor('+' == extensions['q_strand'], 'l' == extensions['side']), extensions['q_end'] <= extensions['q_read_to'], extensions['q_start'] >= extensions['q_read_from']) | (extensions['q_read_from'] >= extensions['read_end']) | (extensions['q_read_to'] <= extensions['read_start'])].values, inplace=True)
    extensions.drop(columns=['read_start','read_end'], inplace=True)
#
    # Repeat the last two steps for the target reads
    extensions.reset_index(drop=True, inplace=True)
    extensions = pd.concat([extensions, mappings.loc[extensions['t_index'].values, ['read_start','read_end','read_from','read_to','hap','strand']].reset_index(drop=True).rename(columns={'read_from':'t_read_from', 'read_to':'t_read_to', 'hap':'t_hap', 'strand':'t_strand'})], axis=1)
    extensions.drop(extensions.index[np.where(np.logical_xor('+' == extensions['t_strand'], 'l' == extensions['side']), extensions['t_end'] <= extensions['t_read_to'], extensions['t_start'] >= extensions['t_read_from']) | (extensions['t_read_from'] >= extensions['read_end']) | (extensions['t_read_to'] <= extensions['read_start'])].values, inplace=True)
    extensions.drop(columns=['read_start','read_end'], inplace=True)
#
    # Extensions: Remove reads that somehow do not fullfil strand rules and remove superfluous strand column
    extensions.drop(extensions.index[np.where(extensions['q_strand'] == extensions['t_strand'], '+', '-') != extensions['strand']].values, inplace=True)
    extensions.drop(columns=['strand'], inplace=True)
#
    # Split off hap_merger 
    hap_merger = extensions[extensions['q_hap'] != extensions['t_hap']].copy()
    extensions.drop(extensions.index[extensions['q_hap'] != extensions['t_hap']].values, inplace=True)
    extensions.drop(columns=['t_hap'], inplace=True)
    extensions.rename(columns={'q_hap':'hap'}, inplace=True)

    # Filter extensions where the all vs. all mapping does not touch the the contig mapping of query and target or the reads diverge more than min_length_contig_break of their mapping length within the contig mapping
    extensions['q_max_divergence'] = np.minimum(min_length_contig_break, np.where(np.logical_xor('+' == extensions['q_strand'], 'l' == extensions['side']), extensions['q_read_to']-extensions['q_start'], extensions['q_end']-extensions['q_read_from']))
    extensions['t_max_divergence'] = np.minimum(min_length_contig_break, np.where(np.logical_xor('+' == extensions['t_strand'], 'l' == extensions['side']), extensions['t_read_to']-extensions['t_start'], extensions['t_end']-extensions['t_read_from']))
    extensions = extensions[(0 < extensions['q_max_divergence']) & (0 < extensions['t_max_divergence'])].copy() # all vs. all mappings do not touch contig mapping
    extensions['read_divergence'] = np.minimum(np.where(np.logical_xor('+' == extensions['q_strand'], 'l' == extensions['side']), extensions['q_start'], extensions['q_len']-extensions['q_end']), np.where(np.logical_xor('+' == extensions['t_strand'], 'l' == extensions['side']), extensions['t_start'], extensions['t_len']-extensions['t_end']))
    extensions = extensions[( (0 < np.where(np.logical_xor('+' == extensions['q_strand'], 'l' == extensions['side']), extensions['q_read_from']-extensions['q_start'], extensions['q_end']-extensions['q_read_to']) + extensions['q_max_divergence']) |
                              (extensions['read_divergence'] < extensions['q_max_divergence']) ) &
                            ( (0 < np.where(np.logical_xor('+' == extensions['t_strand'], 'l' == extensions['side']), extensions['t_read_from']-extensions['t_start'], extensions['t_end']-extensions['t_read_to']) + extensions['t_max_divergence']) |
                              (extensions['read_divergence'] < extensions['t_max_divergence']) ) ].copy()
    extensions.drop(columns=['q_max_divergence','t_max_divergence','read_divergence'], inplace=True)

    # Add the flipped entries between query and strand
    extensions = pd.concat([extensions[['scaf','side','hap','t_index','t_strand','t_read_from','t_read_to','t_len','t_start','t_end','q_index','q_strand','q_read_from','q_read_to','q_len','q_start','q_end']].rename(columns={'t_index':'q_index','t_strand':'q_strand','t_read_from':'q_read_from','t_read_to':'q_read_to','t_len':'q_len','t_start':'q_start','t_end':'q_end','q_index':'t_index','q_strand':'t_strand','q_read_from':'t_read_from','q_read_to':'t_read_to','q_len':'t_len','q_start':'t_start','q_end':'t_end'}),
                            extensions[['scaf','side','hap','q_index','q_strand','q_read_from','q_read_to','q_len','q_start','q_end','t_index','t_strand','t_read_from','t_read_to','t_len','t_start','t_end']] ], ignore_index=True)
    hap_merger = pd.concat([hap_merger[['scaf','side','t_hap','t_index','t_strand','t_read_from','t_read_to','t_len','t_start','t_end','q_hap','q_index','q_strand','q_read_from','q_read_to','q_len','q_start','q_end']].rename(columns={'t_hap':'q_hap','t_index':'q_index','t_strand':'q_strand','t_read_from':'q_read_from','t_read_to':'q_read_to','t_len':'q_len','t_start':'q_start','t_end':'q_end','q_hap':'t_hap','q_index':'t_index','q_strand':'t_strand','q_read_from':'t_read_from','q_read_to':'t_read_to','q_len':'t_len','q_start':'t_start','q_end':'t_end'}),
                            hap_merger[['scaf','side','q_hap','q_index','q_strand','q_read_from','q_read_to','q_len','q_start','q_end','t_hap','t_index','t_strand','t_read_from','t_read_to','t_len','t_start','t_end']] ], ignore_index=True)

    # Haplotype merger are the first mapping of a read with a given read from the main haplotype (We can stop extending the alternative haplotype there, because it is identical to the main again)
    hap_merger = hap_merger[hap_merger['t_hap'] == 0].copy()
    hap_merger['max_ext'] = np.where((hap_merger['q_strand'] == '+') == (hap_merger['side'] == 'r'), hap_merger['q_start'] - hap_merger['q_read_to'], hap_merger['q_read_from'] - hap_merger['q_end'])
    hap_merger = hap_merger[['scaf','side','q_hap','q_index','t_index','max_ext']].copy()
    hap_merger['abs_est'] = np.abs(hap_merger['max_ext'])
    hap_merger.sort_values(['scaf','side','q_hap','q_index','t_index','abs_est'], inplace=True)
    hap_merger = hap_merger.groupby(['scaf','side','q_hap','q_index','t_index']).first().reset_index()
    hap_merger.drop(columns=['abs_est'], inplace=True)
    hap_merger.rename(columns={'q_hap':'hap'}, inplace=True)
    
    # Minimap2 has sometimes two overlapping mappings
    extensions = RemoveDuplicatedReadMappings(extensions)

    return extensions, hap_merger

def ClusterExtension(extensions, mappings, min_num_reads, min_scaf_len):
    # Remove indexes that cannot fulfill min_num_reads (have less than min_num_reads-1 mappings to other reads)
    extensions.sort_values(['scaf','side','hap','q_index'], inplace=True)
    org_len = len(extensions)+1
    while len(extensions) < org_len:
        org_len = len(extensions)
        num_mappings = extensions.groupby(['scaf','side','hap','q_index'], sort=False).size().values
        extensions = extensions[ np.minimum( extensions[['scaf','side','hap','t_index']].merge(extensions.groupby(['scaf','side','hap','t_index']).size().reset_index(name='num_mappings'), on=['scaf','side','hap','t_index'], how='left')['num_mappings'].values,
                                             np.repeat(num_mappings, num_mappings) ) >= min_num_reads-1 ].copy()
#
    # Cluster reads that share a mapping (A read in a cluster maps at least to one other read in this cluster)
    clusters = extensions.groupby(['scaf','side','hap','q_index'], sort=False).size().reset_index(name='size')
    clusters['cluster'] = np.arange(len(clusters))
    extensions['q_cluster_id'] = np.repeat(clusters.index.values, clusters['size'].values)
    extensions = extensions.merge(clusters[['scaf','side','hap','q_index']].reset_index().rename(columns={'q_index':'t_index','index':'t_cluster_id'}), on=['scaf','side','hap','t_index'], how='left')
#
    cluster_col = clusters.columns.get_loc('cluster')
    extensions['cluster'] = np.minimum(clusters.iloc[extensions['q_cluster_id'].values, cluster_col].values, clusters.iloc[extensions['t_cluster_id'].values, cluster_col].values)
    clusters['new_cluster'] = np.minimum( extensions.groupby('q_cluster_id')['cluster'].min().values, extensions.groupby('t_cluster_id')['cluster'].min().values )
    while np.sum(clusters['new_cluster'] != clusters['cluster']):
        clusters['cluster'] = clusters['new_cluster']
        extensions['cluster'] = np.minimum(clusters.iloc[extensions['q_cluster_id'].values, cluster_col].values, clusters.iloc[extensions['t_cluster_id'].values, cluster_col].values)
        clusters['new_cluster'] = np.minimum( extensions.groupby('q_cluster_id')['cluster'].min().values, extensions.groupby('t_cluster_id')['cluster'].min().values )
    extensions['cluster'] = clusters.iloc[extensions['q_cluster_id'].values, cluster_col].values
    clusters.drop(columns=['new_cluster'], inplace=True)
#
    # Remove sides with alternative clusters
    clusters.sort_values(['cluster','size','q_index'], ascending=[True,False,True], inplace=True)
    alternatives = clusters[['scaf','side','hap','cluster']].drop_duplicates().groupby(['scaf','side','hap']).size().reset_index(name='alternatives')
    clusters['cluster_id'] = clusters.index # We have to save it, because 'q_cluster_id' and 't_cluster_id' use them and merging removes the index
    clusters = clusters.merge(alternatives, on=['scaf','side','hap'], how='left')
    clusters.index = clusters['cluster_id'].values
    extensions['alternatives'] = clusters.loc[ extensions['q_cluster_id'].values, 'alternatives' ].values
    new_scaffolds = extensions.loc[ extensions['alternatives'] > 1, ['scaf','side','hap','cluster','q_index','q_strand','q_read_from','q_read_to','q_start','q_end']].copy()
    clusters = clusters[ clusters['alternatives'] == 1 ].copy()
    extensions = extensions[ extensions['alternatives'] == 1 ].copy()
    clusters.drop(columns=['alternatives'], inplace=True)
    extensions.drop(columns=['alternatives'], inplace=True)
#
    # Add how long the query agrees with the target in the gap
    extensions['q_agree'] = np.where( np.logical_xor('+' == extensions['q_strand'], 'l' == extensions['side']), extensions['q_end']-extensions['q_read_to'], extensions['q_read_from']-extensions['q_start'] )
    extensions.drop(columns=['cluster','q_cluster_id','t_cluster_id'], inplace=True)
#
    # Select the part of the removed clusters that is not in the assembly to add it as separate scaffolds that can later be connected in another round of scaffolding
    new_scaffolds['val_ext'] = np.where((new_scaffolds['q_strand'] == '+') == (new_scaffolds['side'] == 'r'), new_scaffolds['q_end'] - new_scaffolds['q_read_to'], new_scaffolds['q_read_from'] - new_scaffolds['q_start'])
    new_scaffolds.sort_values(['scaf','side','hap','cluster','q_index','val_ext'], ascending=[True,True,True,True,True,False], inplace=True)
    new_scaffolds['cov_sup'] = new_scaffolds.groupby(['scaf','side','hap','cluster','q_index']).cumcount()+1
    new_scaffolds = new_scaffolds[new_scaffolds['cov_sup'] == min_num_reads].copy()
    new_scaffolds.drop(columns=['cov_sup'], inplace=True)
    new_scaffolds.reset_index(drop=True, inplace=True)
    new_scaffolds = pd.concat([new_scaffolds, mappings.loc[new_scaffolds['q_index'],['ext','trim','unmap_ext','read_name','mapq','matches']].reset_index(drop=True)], axis=1)
    new_scaffolds['val_ext'] -= new_scaffolds['trim']
    new_scaffolds['gap_covered'] = np.minimum(new_scaffolds['val_ext']/new_scaffolds['unmap_ext'], 1.0)
    new_scaffolds['crosses_gap'] = new_scaffolds['unmap_ext'] < new_scaffolds['ext']
    new_scaffolds.loc[new_scaffolds['crosses_gap'] == False, 'unmap_ext'] *= -1
    new_scaffolds.sort_values(['scaf','side','hap','cluster','mapq','crosses_gap','unmap_ext','matches','gap_covered'], ascending=[True,True,True,True,False,False,True,False,False], inplace=True)
    new_scaffolds = new_scaffolds.groupby(['scaf','side','hap','cluster']).first().reset_index()
    new_scaffolds['unmap_ext'] = np.minimum(np.abs(new_scaffolds['unmap_ext']), new_scaffolds['val_ext']) + new_scaffolds['trim']
    new_scaffolds = new_scaffolds[new_scaffolds['unmap_ext'] >= min_scaf_len].copy()
    new_scaffolds['q_start'] = np.where((new_scaffolds['q_strand'] == '+') == (new_scaffolds['side'] == 'r'), new_scaffolds['q_read_to'], new_scaffolds['q_read_from']-new_scaffolds['unmap_ext'])
    new_scaffolds['q_end'] = np.where((new_scaffolds['q_strand'] == '+') == (new_scaffolds['side'] == 'r'), new_scaffolds['q_read_to']+new_scaffolds['unmap_ext'], new_scaffolds['q_read_from'])
    new_scaffolds = new_scaffolds[['read_name','q_start','q_end']].rename(columns={'read_name':'name0','q_start':'start0','q_end':'end0'})
#
    return extensions, new_scaffolds

def ExtendScaffolds(scaffold_paths, extensions, hap_merger, new_scaffolds, mappings, min_num_reads, max_mapping_uncertainty, min_scaf_len, ploidy):
    extension_info = {}
    extension_info['count'] = 0
    extension_info['new'] = 0

    if len(extensions) and len(mappings):
        # Create table on how long mappings agree in the gap with at least min_num_reads-1 (-1 because they always agree with themselves)
        len_agree = extensions[['scaf','side','hap','q_index','q_agree']].sort_values(['scaf','side','hap','q_index','q_agree'], ascending=[True,True,True,True,False])
        len_agree['n_longest'] = len_agree.groupby(['scaf','side','hap','q_index'], sort=False).cumcount()+1
        len_agree = len_agree[len_agree['n_longest'] == max(1,min_num_reads-1)].copy()
        len_agree.drop(columns=['n_longest'], inplace=True)
        len_mappings = mappings.iloc[len_agree['q_index'].values]
        len_agree['q_ext_len'] = np.where( np.logical_xor('+' == len_mappings['strand'], 'l' == len_mappings['side']), len_mappings['read_end']-len_mappings['read_to'], len_mappings['read_from'] )
        
        # Take the read that has the longest agreement with at least min_num_reads-1 and see if and at what position another bundle of min_num_reads diverge from it (extend until that position or as long as min_num_reads-1 agree)
        len_agree.sort_values(['scaf','side','hap','q_agree'], ascending=[True,True,True,False], inplace=True)
        extending_reads = len_agree.groupby(['scaf','side','hap'], sort=False).first().reset_index()
        len_agree = len_agree.merge(extending_reads[['scaf','side','hap','q_index']].rename(columns={'q_index':'t_index'}), on=['scaf','side','hap'], how='inner')
        len_agree = len_agree[len_agree['q_index'] != len_agree['t_index']].copy()
        len_agree = len_agree.merge(extensions[['scaf','side','hap','q_index','t_index','q_agree']].rename(columns={'q_agree':'qt_agree'}), on=['scaf','side','hap','q_index','t_index'], how='left')
        len_agree['qt_agree'].fillna(0, inplace=True)
        len_agree = len_agree[ len_agree['q_agree'] > len_agree['qt_agree']+max_mapping_uncertainty].copy()
        len_agree = len_agree.merge(extensions[['scaf','side','hap','q_index','t_index','q_agree']].rename(columns={'q_index':'t_index','t_index':'q_index','q_agree':'tq_agree'}), on=['scaf','side','hap','q_index','t_index'], how='left')
        len_agree['tq_agree'].fillna(0, inplace=True)
        
        len_agree.sort_values(['scaf','side','hap','tq_agree'], inplace=True)
        len_agree['n_disagree'] = len_agree.groupby(['scaf','side','hap'], sort=False).cumcount() + 1
        len_agree = len_agree[ len_agree['n_disagree'] == min_num_reads ].copy()
        extending_reads = extending_reads.merge(len_agree[['scaf','side','hap','tq_agree']].rename(columns={'tq_agree':'valid_ext'}), on=['scaf','side','hap'], how='left')
        extending_reads.loc[np.isnan(extending_reads['valid_ext']),'valid_ext'] = extending_reads.loc[np.isnan(extending_reads['valid_ext']),'q_agree'].values
        extending_reads['valid_ext'] = extending_reads['valid_ext'].astype(int)
        
        # Get max_ext to later trim extension of alternative haplotypes, when they get identical to main again
        extending_reads['t_hap'] = 0
        extending_reads = extending_reads.merge(extending_reads[['scaf','side','hap','q_index']].rename(columns={'hap':'t_hap','q_index':'t_index'}), on=['scaf','side','t_hap'], how='left')
        extending_reads['t_index'] = extending_reads['t_index'].fillna(-1).astype(int)
        extending_reads.drop(columns=['t_hap'], inplace=True)
        extending_reads = extending_reads.merge(hap_merger, on=['scaf','side','hap','q_index','t_index'], how='left')
        
        # Add the rest of the two reads after the split as new_scaffolds, up to unmap_ext (except if they are a trimmed alternative haplotype)
        add_scaffolds = []
        if len(extending_reads):
            extending_reads = pd.concat([extending_reads.reset_index(drop=True), mappings.loc[extending_reads['q_index'].values, ['trim','unmap_ext','read_name','read_from','read_to','strand']].reset_index(drop=True)], axis=1)
            add_scaffolds.append(extending_reads.loc[extending_reads['unmap_ext'] > 0, ['side','q_agree','valid_ext','max_ext','trim','unmap_ext','read_name','read_from','read_to','strand']].copy())
        if len(len_agree):
            len_agree.drop(columns=['q_ext_len','tq_agree','n_disagree','t_index'], inplace=True)
            len_agree.rename(columns={'qt_agree':'valid_ext'}, inplace=True)
            len_agree['valid_ext'] = len_agree['valid_ext'].astype(int)
            # Add max_ext
            len_agree['t_hap'] = 0
            len_agree = len_agree.merge(extending_reads[['scaf','side','hap','q_index']].rename(columns={'hap':'t_hap','q_index':'t_index'}), on=['scaf','side','t_hap'], how='left')
            len_agree['t_index'] = len_agree['t_index'].fillna(-1).astype(int)
            len_agree.drop(columns=['t_hap'], inplace=True)
            len_agree = len_agree.merge(hap_merger, on=['scaf','side','hap','q_index','t_index'], how='left')
            # Add mapping information
            len_agree = pd.concat([len_agree.reset_index(drop=True), mappings.loc[len_agree['q_index'].values, ['trim','unmap_ext','read_name','read_from','read_to','strand']].reset_index(drop=True)], axis=1)
            add_scaffolds.append(len_agree.loc[len_agree['unmap_ext'] > 0, ['side','q_agree','valid_ext','max_ext','trim','unmap_ext','read_name','read_from','read_to','strand']].copy())
        add_scaffolds = pd.concat(add_scaffolds, ignore_index=True)
        if len(add_scaffolds) == 0:
            gap_scaffolds = new_scaffolds
        else:
            add_scaffolds['new_scaf'] = np.minimum(add_scaffolds['unmap_ext'],add_scaffolds['q_agree'])+add_scaffolds['trim']
            add_scaffolds.loc[np.isnan(add_scaffolds['max_ext']) == False, 'new_scaf'] = np.minimum(add_scaffolds.loc[np.isnan(add_scaffolds['max_ext']) == False, 'new_scaf'], add_scaffolds.loc[np.isnan(add_scaffolds['max_ext']) == False, 'max_ext']).astype(int)
            add_scaffolds['new_scaf'] -= add_scaffolds['valid_ext']
            add_scaffolds = add_scaffolds[add_scaffolds['new_scaf'] >= min_scaf_len].copy()
            add_scaffolds['start0'] = np.where((add_scaffolds['strand'] == '+') == (add_scaffolds['side'] == 'r'), add_scaffolds['read_to'] + add_scaffolds['valid_ext'], add_scaffolds['read_from'] - add_scaffolds['valid_ext'] - add_scaffolds['new_scaf'])
            add_scaffolds['end0'] = np.where((add_scaffolds['strand'] == '+') == (add_scaffolds['side'] == 'r'), add_scaffolds['read_to'] + add_scaffolds['valid_ext'] + add_scaffolds['new_scaf'], add_scaffolds['read_from'] - add_scaffolds['valid_ext'])
            add_scaffolds = add_scaffolds[['read_name','start0','end0']].rename(columns={'read_name':'name0'})
            gap_scaffolds = pd.concat([new_scaffolds, add_scaffolds], ignore_index=True)

        # Trim the part that does not match the read independent of whether we can later extend the scaffold or not
        if len(extending_reads):
            # Trim left side (Do sides separately, because a contig can be both sides of a scaffold)
            scaffold_paths['side'] = np.where(0 == scaffold_paths['pos'], 'l', '')
            for h in range(ploidy):
                scaffold_paths = scaffold_paths.merge(extending_reads.loc[extending_reads['hap'] == h, ['scaf','side','trim']], on=['scaf','side'], how='left')
                scaffold_paths['trim'] = scaffold_paths['trim'].fillna(0).astype(int)
                scaffold_paths.loc[scaffold_paths[f'strand{h}'] == '+', f'start{h}'] += scaffold_paths.loc[scaffold_paths[f'strand{h}'] == '+', 'trim']
                scaffold_paths.loc[scaffold_paths[f'strand{h}'] == '-', f'end{h}'] -= scaffold_paths.loc[scaffold_paths[f'strand{h}'] == '-', 'trim']
                scaffold_paths.drop(columns=['trim'], inplace=True)
            # Trim right side
            scaffold_paths['side'] = np.where(scaffold_paths['scaf'] != scaffold_paths['scaf'].shift(-1), 'r', '')
            for h in range(ploidy):
                scaffold_paths = scaffold_paths.merge(extending_reads.loc[extending_reads['hap'] == h, ['scaf','side','trim']], on=['scaf','side'], how='left')
                scaffold_paths['trim'] = scaffold_paths['trim'].fillna(0).astype(int)
                scaffold_paths.loc[scaffold_paths[f'strand{h}'] == '+', f'end{h}'] -= scaffold_paths.loc[scaffold_paths[f'strand{h}'] == '+', 'trim']
                scaffold_paths.loc[scaffold_paths[f'strand{h}'] == '-', f'start{h}'] += scaffold_paths.loc[scaffold_paths[f'strand{h}'] == '-', 'trim']
                scaffold_paths.drop(columns=['trim'], inplace=True)
            # Get the last position in the scaffold (the right side extension gets last_pos+1)
            ext_pos = scaffold_paths.loc[scaffold_paths['side'] == 'r', ['scaf','pos']].copy()
            scaffold_paths.drop(columns=['side'], inplace=True)
        
        # Extend scaffolds
        extending_reads.loc[np.isnan(extending_reads['max_ext']) == False, 'valid_ext'] = np.minimum(extending_reads.loc[np.isnan(extending_reads['max_ext']) == False, 'valid_ext'], extending_reads.loc[np.isnan(extending_reads['max_ext']) == False, 'max_ext']).astype(int)
        extending_reads = extending_reads[extending_reads['valid_ext'] > 0].copy()
        if len(extending_reads):
            # Fill extension info
            extension_info['count'] = len(extending_reads)
            extension_info['left'] = len(extending_reads[extending_reads['side'] == 'l'])
            extension_info['right'] = len(extending_reads[extending_reads['side'] == 'r'])
            extension_info['mean'] = int(round(np.mean(extending_reads['valid_ext'])))
            extension_info['min'] = np.min(extending_reads['valid_ext'])
            extension_info['max'] = np.max(extending_reads['valid_ext'])
            
            # Get start and end of extending read
            extending_reads['start0'] = np.where((extending_reads['strand'] == '+') == (extending_reads['side'] == 'r'), extending_reads['read_to'], extending_reads['read_from']-extending_reads['valid_ext'])
            extending_reads['end0'] = np.where((extending_reads['strand'] == '+') == (extending_reads['side'] == 'r'), extending_reads['read_to']+extending_reads['valid_ext'], extending_reads['read_from'])

            # Change structure of extending_reads to the same as scaffold_paths
            extending_reads = extending_reads[['scaf','side','hap','read_name','start0','end0','strand']].rename(columns={'read_name':'name0','strand':'strand0'})
            for h in range(1,ploidy):
                extending_reads = extending_reads.merge(extending_reads.loc[extending_reads['hap'] == h, ['scaf','side','name0','start0','end0','strand0']].rename(columns={'name0':f'name{h}','start0':f'start{h}','end0':f'end{h}','strand0':f'strand{h}'}), on=['scaf','side'], how='left')
                extending_reads[[f'name{h}',f'strand{h}']] = extending_reads[[f'name{h}',f'strand{h}']].fillna('')
                extending_reads[[f'start{h}',f'end{h}']] = extending_reads[[f'start{h}',f'end{h}']].fillna(0).astype(int)
                extending_reads.loc[extending_reads['hap'] == h, ['name0','start0','end0','strand0']] = ['',0,0,'']
            extending_reads.sort_values(['scaf','side','hap'], inplace=True)
            extending_reads = extending_reads[(extending_reads['scaf'] != extending_reads['scaf'].shift(1)) | (extending_reads['side'] != extending_reads['side'].shift(1))].copy()
            extending_reads.drop(columns=['hap'], inplace=True)
            extending_reads = extending_reads.merge(ext_pos, on='scaf', how='left')
            extending_reads.loc[extending_reads['side'] == 'l', 'pos'] = 0
            extending_reads = extending_reads.merge(scaffold_paths[['scaf','pos']+[f'phase{h}' for h in range(ploidy)]+['sdist_left','sdist_right']], on=['scaf','pos'], how='left')
            for h in range(1,ploidy):
                extending_reads.loc[extending_reads[f'name{h}'] != '', f'phase{h}'] = np.abs(extending_reads.loc[extending_reads[f'name{h}'] != '', f'phase{h}'])
            extending_reads['pos'] += 1
            extending_reads.loc[extending_reads['side'] == 'l', 'pos'] = -1
            extending_reads['type'] = 'read'

            # Add extending_reads to scaffold_table, sort again and make sure that sdist_left and sdist_right are only set at the new ends
            scaffold_paths = scaffold_paths.append( extending_reads[['scaf','pos','type']+[f'{n}{h}' for h in range(ploidy) for n in ['phase','name','start','end','strand']]+['sdist_left','sdist_right']] )
            scaffold_paths.sort_values(['scaf','pos'], inplace=True)
            scaffold_paths['pos'] = scaffold_paths.groupby(['scaf'], sort=False).cumcount()
            scaffold_paths.loc[scaffold_paths['pos'] > 0, 'sdist_left'] = -1
            scaffold_paths.loc[scaffold_paths['scaf'] == scaffold_paths['scaf'].shift(-1), 'sdist_right'] = -1
            
        if len(gap_scaffolds):
            # Fill extension info
            gap_scaffolds['len'] = gap_scaffolds['end0'] - gap_scaffolds['start0']
            extension_info['new'] = len(gap_scaffolds)
            extension_info['new_mean'] = int(round(np.mean(gap_scaffolds['len'])))
            extension_info['new_min'] = np.min(gap_scaffolds['len'])
            extension_info['new_max'] = np.max(gap_scaffolds['len'])
            
            # Change structure of gap_scaffolds to the same as scaffold_paths
            gap_scaffolds['scaf'] = np.arange(len(gap_scaffolds)) + 1 + scaffold_paths['scaf'].max()
            gap_scaffolds['pos'] = 0
            gap_scaffolds['type'] = 'read'
            gap_scaffolds['phase0'] = np.arange(len(gap_scaffolds)) + 1 + scaffold_paths[[f'phase{h}' for h in range(ploidy)]].abs().max().max()
            gap_scaffolds['strand0'] = '+'
            for h in range(1,ploidy):
                gap_scaffolds[f'phase{h}'] = -gap_scaffolds['phase0']
                gap_scaffolds[[f'name{h}',f'strand{h}']] = ''
                gap_scaffolds[[f'start{h}',f'end{h}']] = 0
            gap_scaffolds[['sdist_left','sdist_right']] = -1
            scaffold_paths = scaffold_paths.append( gap_scaffolds[['scaf','pos','type']+[f'{n}{h}' for h in range(ploidy) for n in ['phase','name','start','end','strand']]+['sdist_left','sdist_right']] )

    if extension_info['count'] == 0:
        extension_info['left'] = 0
        extension_info['right'] = 0
        extension_info['mean'] = 0
        extension_info['min'] = 0
        extension_info['max'] = 0
        
    if extension_info['new'] == 0:
        extension_info['new_mean'] = 0
        extension_info['new_min'] = 0
        extension_info['new_max'] = 0
        
    return scaffold_paths, extension_info

def GaplessExtend(all_vs_all_mapping_file, prefix, min_length_contig_break):
    # Define parameters
    min_extension = 500    
    min_num_reads = 3
    max_mapping_uncertainty = 200
    min_scaf_len = 600
    
    # Remove output files, such that we do not accidentially use an old one after a crash
    for f in [f"{prefix}_extended_scaffold_paths.csv", f"{prefix}_used_reads.lst"]:
        if os.path.exists(f):
            os.remove(f)
    
    print( str(timedelta(seconds=process_time())), "Preparing data from files")
    scaffold_paths = pd.read_csv(prefix+"_scaffold_paths.csv").fillna('')
    ploidy = GetPloidyFromPaths(scaffold_paths)
    mappings = LoadExtensions(prefix, min_extension)
    extensions, hap_merger = LoadReads(all_vs_all_mapping_file, mappings, min_length_contig_break)
    
    print( str(timedelta(seconds=process_time())), "Searching for extensions")
    extensions, new_scaffolds = ClusterExtension(extensions, mappings, min_num_reads, min_scaf_len)
    scaffold_paths, extension_info = ExtendScaffolds(scaffold_paths, extensions, hap_merger, new_scaffolds, mappings, min_num_reads, max_mapping_uncertainty, min_scaf_len, ploidy)

    print( str(timedelta(seconds=process_time())), "Writing output")
    scaffold_paths.to_csv(f"{prefix}_extended_scaffold_paths.csv", index=False)
    np.savetxt(f"{prefix}_used_reads.lst", np.unique(pd.concat([scaffold_paths.loc[('read' == scaffold_paths['type']) & ('' != scaffold_paths[f'name{h}']), f'name{h}'] for h in range(ploidy)], ignore_index=True)), fmt='%s')
    
    print( str(timedelta(seconds=process_time())), "Finished")
    print( "Extended {} scaffolds (left: {}, right:{}).".format(extension_info['count'], extension_info['left'], extension_info['right']) )
    print( "The extensions ranged from {} to {} bases and had a mean length of {}.".format(extension_info['min'], extension_info['max'], extension_info['mean']) )
    print( "Added {} scaffolds in the gaps with a length ranging from {} to {} bases and a mean of {}.".format(extension_info['new'],extension_info['new_min'], extension_info['new_max'], extension_info['new_mean']) )

def SwitchBubbleWithMain(outpaths, hap, switch):
    swi = switch & (outpaths[f'phase{hap}'] >= 0)
    tmp = outpaths.loc[swi, ['name0','start0','end0','strand0']].values
    outpaths.loc[swi, ['name0','start0','end0','strand0']] = outpaths.loc[swi, [f'name{hap}',f'start{hap}',f'end{hap}',f'strand{hap}']].values
    outpaths.loc[swi, [f'name{hap}',f'start{hap}',f'end{hap}',f'strand{hap}']] = tmp
#
    return outpaths

def RemoveBubbleHaplotype(outpaths, hap, remove):
    rem = remove & (outpaths[f'phase{hap}'] >= 0)
    outpaths.loc[rem, [f'name{hap}',f'start{hap}',f'end{hap}',f'strand{hap}']] = ['',0,0,'']
    outpaths.loc[rem, f'phase{hap}'] = -outpaths.loc[rem, f'phase{hap}']
#
    return outpaths

def GaplessFinish(assembly_file, read_file, read_format, scaffold_file, output_files, haplotypes):
    min_length = 600 # Minimum length for a contig or alternative haplotype to be in the output
    skip_length = 50 # Haplotypes with a length shorter than this get lowest priority for being included in the main scaffold for mixed mode
    merge_dist = 1000 # Alternative haplotypes separated by at max merge_dist of common sequence are merged and output as a whole alternative contig in mixed mode
#
    if False == output_files[0]:
        if ".gz" == assembly_file[-3:len(assembly_file)]:
            output_files[0] = assembly_file.rsplit('.',2)[0]+"_gapless.fa"
        else:
            output_files[0] = assembly_file.rsplit('.',1)[0]+"_gapless.fa"
#
    # Remove output files, such that we do not accidentially use an old one after a crash
    for f in output_files:
        if os.path.exists(f):
            os.remove(f)
#
    print( str(timedelta(seconds=process_time())), "Loading scaffold info from: {}".format(scaffold_file))
    scaffold_paths = pd.read_csv(scaffold_file).fillna('')
    ploidy = GetPloidyFromPaths(scaffold_paths)
    for i in range(len(haplotypes)):
        if haplotypes[i] == -1:
            haplotypes[i] = -1 if ploidy > 1 else 0
        elif haplotypes[i] >= ploidy:
            print(f"The highest haplotype in scaffold file is {ploidy-1}, but {haplotypes[i]} was specified")
            sys.exit(1)
#
    print( str(timedelta(seconds=process_time())), "Loading assembly from: {}".format(assembly_file))
    contigs = {}
    with gzip.open(assembly_file, 'rt') if 'gz' == assembly_file.rsplit('.',1)[-1] else open(assembly_file, 'r') as fin:
        for record in SeqIO.parse(fin, "fasta"):
            contigs[ record.description.split(' ', 1)[0] ] = record.seq
#
    print( str(timedelta(seconds=process_time())), "Loading reads from: {}".format(read_file))
    reads = {}
    if read_format == False:
        lread_file = read_file.lower()
        if (lread_file[-3:] == ".fa") or (lread_file[-6:] == ".fasta") or (lread_file[-6:] == ".fa.gz") or (lread_file[-9:] == ".fasta.gz"):
            read_format = "fasta"
        else:
            read_format = "fastq"
    with gzip.open(read_file, 'rt') if 'gz' == read_file.rsplit('.',1)[-1] else open(read_file, 'r') as fin:
        for record in SeqIO.parse(fin, read_format):
            reads[ record.description.split(' ', 1)[0] ] = record.seq
    
    for outfile, hap in zip(output_files, haplotypes):
        outpaths = scaffold_paths.copy()
        outpaths['meta'] = ((outpaths['scaf'] != outpaths['scaf'].shift(1)) & (outpaths['sdist_left'] < 0)).cumsum()
        # Select the paths based on haplotype
        if hap < 0:
            print( str(timedelta(seconds=process_time())), "Writing mixed assembly to: {}".format(outfile))
            outpaths['bubble'] = ((outpaths[[f'phase{h}' for h in range(1,ploidy)]] < 0).all(axis=1) | (outpaths['scaf'] != outpaths['scaf'].shift(1))).cumsum()
            for h in range(ploidy):
                outpaths[f'len{h}'] = outpaths[f'end{h}']-outpaths[f'start{h}']
            # Merge bubbles if they are separeted by merge_dist or less
            bubbles = outpaths.groupby(['scaf','bubble'])[[f'len{h}' for h in range(ploidy)]].sum().reset_index()
            bubbles['mblock'] = bubbles['len0'].cumsum()
            bubbles = bubbles[(bubbles[[f'len{h}' for h in range(1,ploidy)]] > 0).any(axis=1)].copy()
            bubbles['mblock'] = bubbles['mblock'] - bubbles['mblock'].shift(1, fill_value=0) - bubbles['len0'] + bubbles[['bubble']].merge(outpaths[['bubble','len0']].groupby(['bubble']).first().reset_index(), on=['bubble'], how='left')['len0'].values # We also need to account for the first entry in every bubble that is identical to main (The definition of a bubble start)
            bubbles['new_bubble'] = bubbles['bubble'].shift(1, fill_value=-1)
            bubbles = bubbles[(bubbles['scaf'] == bubbles['scaf'].shift(1)) & (bubbles['mblock'] <= merge_dist)].copy()
            if len(bubbles):
                while True:
                    bubbles = bubbles[['bubble','new_bubble']].copy()
                    bubbles = bubbles.merge(bubbles.rename(columns={'bubble':'new_bubble','new_bubble':'min_bubble'}), on=['new_bubble'], how='left')
                    if np.sum(np.isnan(bubbles['min_bubble']) == False) == 0:
                        break
                    else:
                        bubbles.loc[np.isnan(bubbles['min_bubble']) == False, 'new_bubble'] = bubbles.loc[np.isnan(bubbles['min_bubble']) == False, 'min_bubble'].astype(int)
                        bubbles.drop(columns=['min_bubble'], inplace=True)
                outpaths = outpaths.merge(bubbles[['bubble','new_bubble']], on=['bubble'], how='left')
                outpaths.loc[np.isnan(outpaths['new_bubble']) == False, 'bubble'] = outpaths.loc[np.isnan(outpaths['new_bubble']) == False, 'new_bubble'].astype(int)
                outpaths.drop(columns=['new_bubble'], inplace=True)
            # Remove haplotypes that are inversions or identical, but shorter than alternatives (skip_length differences are tolerated to still count as identical)
            #!!!! Still need to handle the comparison between alternative haplotypes
            for h in range(1,ploidy):
                # find the haplotypes to remove
                outpaths['same'] = (outpaths[f'phase{h}'] < 0)
                outpaths['len0'] = outpaths['end0']-outpaths['start0']
                outpaths[f'len{h}'] = np.where(outpaths[f'phase{h}'] < 0, outpaths['len0'], outpaths[f'end{h}']-outpaths[f'start{h}'])
                outpaths['identical'] = (outpaths[f'phase{h}'] < 0) | (outpaths[f'len{h}'] <= skip_length) | ((outpaths[f'name{h}'] == outpaths['name0']) & (outpaths[f'start{h}'] >= outpaths['start0']-skip_length) & (outpaths[f'end{h}'] <= outpaths['end0']+skip_length)) # Inversions also count as identical here
                outpaths['identical2'] = (outpaths[f'phase{h}'] < 0) | (outpaths['len0'] <= skip_length) | ((outpaths[f'name{h}'] == outpaths['name0']) & (outpaths[f'start{h}']-skip_length <= outpaths['start0']) & (outpaths[f'end{h}']+skip_length >= outpaths['end0'])) # Inversions also count as identical here
                bubbles = outpaths.groupby(['bubble'])[['same','identical','identical2']].min().reset_index()
                bubbles = bubbles[(bubbles['same'] == False) & bubbles[['identical','identical2']].any(axis=1)].copy()
                bubbles = bubbles.merge(outpaths.groupby(['bubble'])[['len0',f'len{h}']].sum().reset_index(), on=['bubble'], how='left')
                bubbles.loc[bubbles['identical'] & (bubbles['len0'] >= bubbles[f'len{h}']), 'identical2'] = False
                bubbles.loc[bubbles['identical2'] & (bubbles['len0'] < bubbles[f'len{h}']), 'identical'] = False
                # Remove haplotypes
                outpaths = outpaths.drop(columns=['same','identical','identical2']).merge(bubbles[['bubble','identical','identical2']], on=['bubble'], how='left').fillna(False)
                outpaths = SwitchBubbleWithMain(outpaths, h, outpaths['identical2'])
                outpaths = RemoveBubbleHaplotype(outpaths, h, outpaths['identical'] | outpaths['identical2'])
            outpaths.drop(columns=['identical','identical2'], inplace=True)
            # Take the worst to map (shortest) haplotype as main, except if we are going to remove the alternative, because it is shorter min_length: than keep longest by putting it in main (bubble pathes shorter than skip_length are only kept by putting it into main if no longer path exist, because they can be easily recreated from reads)
            #for h in range(ploidy):
            #    outpaths[f'len{h}'] = np.where(outpaths[f'phase{h}'] < 0, outpaths['len0'], outpaths[f'end{h}']-outpaths[f'start{h}'])
            #bubbles = outpaths.groupby(['bubble'])[[f'len{h}' for h in range(ploidy)]].sum().reset_index()
            #bubbles = bubbles.merge(outpaths.groupby(['bubble'])[[f'phase{h}' for h in range(1,ploidy)]].max().reset_index(), on=['bubble'], how='left')
            #bubbles = bubbles.loc[(bubbles[[f'phase{h}' for h in range(1,ploidy)]] > 0).any(axis=1), ['bubble']+[f'len{h}' for h in range(ploidy)]].melt(id_vars='bubble',var_name='hap',value_name='len')
            #bubbles['hap'] = bubbles['hap'].str[3:].astype(int)
            #bubbles['prio'] = np.where(bubbles['len'] >= min_length, 2, np.where(bubbles['len'] < skip_length, 3, 1))
            #bubbles.loc[bubbles['len'] < min_length, 'len'] = -bubbles.loc[bubbles['len'] < min_length, 'len'] # For everything shorter than min_length we want the longest not the shortest
            #bubbles.sort_values(['bubble','prio','len','hap'], inplace=True) # Sort by hap to keep main if no reason to switch
            #bubbles = bubbles.groupby(['bubble'], sort=False).first().reset_index()
            #bubbles = bubbles[bubbles['hap'] > 0].copy()
            #outpaths = outpaths.merge(bubbles[['bubble','hap']].rename(columns={'hap':'switch'}), on=['bubble'], how='left')
            #for h in range(1,ploidy):
            #    outpaths = SwitchBubbleWithMain(outpaths, h, outpaths['switch']==h)
            #outpaths.drop(columns=['len0','len1','switch'], inplace=True)
            # Separate every alternative from a bubble into its own haplotype
            outpaths.rename(columns={'name0':'name','start0':'start','end0':'end','strand0':'strand'}, inplace=True)
            alternatives = []
            max_scaf = outpaths['scaf'].max()
            for h in range(1,ploidy):
                bubbles = outpaths.loc[outpaths[f'phase{h}'] >= 0, ['meta','bubble']].groupby(['meta','bubble']).size().reset_index(name='count').drop(columns='count')
                if len(bubbles):
                    bubbles['alt'] = bubbles.groupby(['meta']).cumcount() + 1
                    outpaths = outpaths.merge(bubbles[['bubble','alt']], on=['bubble'], how='left')
                    bubbles = outpaths.loc[np.isnan(outpaths['alt']) == False, ['meta','alt','pos','bubble','type',f'phase{h}',f'name{h}',f'start{h}',f'end{h}',f'strand{h}']].rename(columns={f'phase{h}':'phase',f'name{h}':'name',f'start{h}':'start',f'end{h}':'end',f'strand{h}':'strand'})
                    # Trim parts that are identical to main from both sides
                    while True:
                        old_len = len(bubbles)
                        bubbles = bubbles[(bubbles['phase'] >= 0) | ((bubbles['bubble'] == bubbles['bubble'].shift(1)) & (bubbles['bubble'] == bubbles['bubble'].shift(-1)))].copy()
                        if len(bubbles) == old_len:
                            break
                    outpaths.drop(columns=[f'phase{h}',f'name{h}',f'start{h}',f'end{h}',f'strand{h}','alt'], inplace=True)
                if len(bubbles):
                    # Prepare all necessary columns
                    bubbles['alt'] = bubbles['alt'].astype(int)
                    bubbles['scaf'] = (bubbles['bubble'] != bubbles['bubble'].shift(1)).cumsum() + max_scaf
                    max_scaf = bubbles['scaf'].max()
                    bubbles['pos'] = bubbles.groupby(['bubble']).cumcount()
                    bubbles['sdist_right'] = -1
                    bubbles['hap'] = h
                    alternatives.append(bubbles[['meta','hap','alt','scaf','pos','type','name','start','end','strand','sdist_right']])
            outpaths['alt'] = 0
            outpaths['hap'] = 0
            outpaths = pd.concat([outpaths[['meta','hap','alt','scaf','pos','type','name','start','end','strand','sdist_right']]] + alternatives, ignore_index=True).sort_values(['meta','alt','pos'])
            outpaths['meta'] = np.repeat("scaffold", len(outpaths)) + outpaths['meta'].astype(str) + np.where(outpaths['hap'] == 0, "", "_hap" + str(h) + "_alt" + outpaths['alt'].astype('str'))
        else:
            print( str(timedelta(seconds=process_time())), "Writing haplotype {} of assembly to: {}".format(hap,outfile))
            outpaths['meta'] = np.repeat("scaffold", len(outpaths)) + outpaths['meta'].astype(str)
            outpaths.rename(columns={'name0':'name','start0':'start','end0':'end','strand0':'strand'}, inplace=True)
            if hap > 0:
                outpaths.loc[outpaths[f'phase{hap}'] >= 0, ['name','start','end','strand']] = outpaths.loc[outpaths[f'phase{hap}'] >= 0, [f'name{hap}',f'start{hap}',f'end{hap}',f'strand{hap}']].values
        # Remove scaffolds that are shorter than min_length
        outpaths = outpaths.loc[outpaths['name'] != '', ['meta','scaf','pos','type','name','start','end','strand','sdist_right']].copy()
        outpaths['length'] = outpaths['end'] - outpaths['start']
        scaf_len = outpaths.groupby(['scaf'])['length'].agg(['sum','size'])
        outpaths = outpaths[np.repeat(scaf_len['sum'].values >= min_length, scaf_len['size'].values)].copy()
        outpaths.loc[(outpaths['meta'] != outpaths['meta'].shift(-1)), 'sdist_right'] = -1
        # Make sure sdist_right is int
        outpaths['sdist_right'] = outpaths['sdist_right'].astype(int)
        # Write to file
        try:
            with gzip.open(outfile, 'wb') if 'gz' == outfile.rsplit('.',1)[-1] else open(outfile, 'w') as fout:
                meta = ''
                seq = []
                for row in outpaths.itertuples(index=False):
                    # Check if scaffold changed
                    if meta != row.meta:
                        if meta != '':
                            # Write scaffold to disc
                            fout.write('>')
                            fout.write(meta)
                            fout.write('\n')
                            fout.write(''.join(seq))
                            fout.write('\n')
                        # Start new scaffold
                        meta = row.meta
                        seq = []
#
                    # Add sequences to scaffold
                    if 'contig' == row.type:
                        if row.strand == '-':
                            seq.append(str(contigs[row.name][row.start:row.end].reverse_complement()))
                        else:
                            seq.append(str(contigs[row.name][row.start:row.end]))
                    elif 'read' == row.type:
                        if row.strand == '-':
                            seq.append(str(reads[row.name][row.start:row.end].reverse_complement()))
                        else:
                            seq.append(str(reads[row.name][row.start:row.end]))
#
                    # Add N's to separate contigs
                    if 0 <= row.sdist_right:
                        seq.append( 'N' * row.sdist_right )
#
                # Write out last scaffold
                fout.write('>')
                fout.write(meta)
                fout.write('\n')
                fout.write(''.join(seq))
                fout.write('\n')
        except:
            # Make sure we do not leave an invalid output file, before crashing
            os.remove(outfile)
            raise
    print( str(timedelta(seconds=process_time())), "Finished" )

def GetMappingsInRegion(mapping_file, regions, min_mapq, min_mapping_length, keep_all_subreads, alignment_precision, min_length_contig_break, max_dist_contig_end):
    mappings = ReadPaf(mapping_file)
    
    # Filter mappings
    mappings = mappings[(min_mapq <= mappings['mapq']) & (min_mapping_length <= mappings['t_end'] - mappings['t_start'])].copy()
    
    # Remove all but the best subread
    if not keep_all_subreads:
        mappings = GetBestSubreads(mappings, alignment_precision)
    
    # Find mappings inside defined regions
    mappings['in_region'] = False
    for reg in regions:
         mappings.loc[ (reg['scaffold'] == mappings['t_name']) & (reg['start'] < mappings['t_end']) & (reg['end'] >= mappings['t_start']) , 'in_region'] = True
         
    # Only keep mappings that are in defined regions or are the previous or next mappings of a read in the region
    mappings.sort_values(['q_name','q_start','q_end'], inplace=True)
    mappings = mappings[ mappings['in_region'] | (mappings['in_region'].shift(-1, fill_value=False) & (mappings['q_name'] == mappings['q_name'].shift(-1, fill_value=''))) | (mappings['in_region'].shift(1, fill_value=False) & (mappings['q_name'] == mappings['q_name'].shift(1, fill_value=''))) ].copy()
    
    # Get left and right scaffold of read (first ignore strand and later switch next and prev if necessary)
    mappings['left_scaf'] = np.where(mappings['q_name'] != mappings['q_name'].shift(1, fill_value=''), '', mappings['t_name'].shift(1, fill_value=''))
    mappings['right_scaf'] = np.where(mappings['q_name'] != mappings['q_name'].shift(-1, fill_value=''), '', mappings['t_name'].shift(-1, fill_value=''))
    # Get distance to next mapping or distance to read end if no next mapping exists
    mappings['left_dist'] = np.where('' == mappings['left_scaf'], mappings['q_start'], mappings['q_start']-mappings['q_end'].shift(1, fill_value=0))
    mappings['right_dist'] = np.where('' == mappings['right_scaf'], mappings['q_len']-mappings['q_end'], mappings['q_start'].shift(-1, fill_value=0)-mappings['q_end'])
    # Get length of continued read after mapping end
    mappings['left_len'] = mappings['q_start']
    mappings['right_len'] = mappings['q_len']-mappings['q_end']

    # Switch left and right for negative strand
    tmp = mappings.loc['-' == mappings['strand'], 'left_scaf']
    mappings.loc['-' == mappings['strand'], 'left_scaf'] = mappings.loc['-' == mappings['strand'], 'right_scaf']
    mappings.loc['-' == mappings['strand'], 'right_scaf'] = tmp
    tmp = mappings.loc['-' == mappings['strand'], 'left_dist']
    mappings.loc['-' == mappings['strand'], 'left_dist'] = mappings.loc['-' == mappings['strand'], 'right_dist']
    mappings.loc['-' == mappings['strand'], 'right_dist'] = tmp
    tmp = mappings.loc['-' == mappings['strand'], 'left_len']
    mappings.loc['-' == mappings['strand'], 'left_len'] = mappings.loc['-' == mappings['strand'], 'right_len']
    mappings.loc['-' == mappings['strand'], 'right_len'] = tmp
    mappings.reset_index(drop=True, inplace=True)

    # Assign region the mapping belongs to, duplicate mappings if they belong to multiple regions
    if 1 == len(regions):
        # With only one region it is simple
        mappings['region'] = np.where(mappings['in_region'], 0, -1)
        mappings['start'] = regions[0]['start']
        mappings['end'] = regions[0]['end']
        mappings['scaffold'] = regions[0]['scaffold']
    else:
        region_order = []
        same_scaffold = []
        for i, reg in enumerate(regions):
            if len(same_scaffold) and reg['scaffold'] != regions[i-1]['scaffold']:
                region_order.append(same_scaffold)
                same_scaffold = []
                
            same_scaffold.append( (reg['start'], i) )

        region_order.append(same_scaffold)

        for i in range(len(region_order)):
            region_order[i].sort()

        inverted_order = [reg[1] for scaf in region_order for reg in scaf[::-1]]
        region_order = [reg[1] for scaf in region_order for reg in scaf]
    
        # Duplicate all entries and remove the incorrect ones
        mappings = mappings.loc[np.repeat(mappings.index, len(region_order))].copy()
        mappings['region'] = np.where(mappings['strand'] == '+', region_order*(int(len(mappings)/len(region_order))), inverted_order*(int(len(mappings)/len(region_order))))
        mappings = mappings[mappings['in_region'] | (mappings['region'] == 0)].copy() # The mappings not in any region do not need to be duplicated
        mappings.loc[mappings['in_region']==False, 'region'] = -1
        
        mappings = mappings.merge(pd.DataFrame.from_dict(regions).reset_index().rename(columns={'index':'region'}), on=['region'], how='left')
        mappings = mappings[(mappings['region'] == -1) | ((mappings['t_name'] == mappings['scaffold']) & (mappings['t_end'] > mappings['start']) & (mappings['t_start'] <= mappings['end']))].copy()
        
        
    # If the mapping reaches out of the region of interest, ignore left/right scaffold
    mappings['overrun_left'] = mappings['start'] > mappings['t_start']
    mappings.loc[mappings['overrun_left'], 'left_scaf'] = ''
    mappings.loc[mappings['overrun_left'], 'left_dist'] = 0
    mappings.loc[mappings['overrun_left'], 'left_len'] = 0
    mappings['overrun_right'] = mappings['end'] < mappings['t_end']-1
    mappings.loc[mappings['overrun_right'], 'right_scaf'] = ''
    mappings.loc[mappings['overrun_right'], 'right_dist'] = 0
    mappings.loc[mappings['overrun_right'], 'right_len'] = 0
    
    # Remove mappings that are less than min_mapping_length in the region, except if they belong to a read with multiple mappings
    mappings = mappings[ ((min_mapping_length <= mappings['t_end'] - mappings['start']) & (min_mapping_length <= mappings['end'] - mappings['t_start'])) | (mappings['q_name'] == mappings['q_name'].shift(1, fill_value="")) | (mappings['q_name'] == mappings['q_name'].shift(-1, fill_value="")) ].copy()
    
    mappings.drop(columns=['in_region','start','end','scaffold'], inplace=True)
        
    # Check if reads connect regions and if so in what way
    mappings['con_prev_reg'] = 'no'
    if 1 < len(regions):
        # Find indirect connections, where the previous region has a mapping somewhere before the current mapping
        for s in range(2,np.max(mappings.groupby(['q_name'], sort=False).size())+1):
            mappings.loc[('+' == mappings['strand']) & (mappings['q_name'] == mappings['q_name'].shift(s, fill_value="")) & (0 < mappings['region']) & (mappings['region']-1 == mappings['region'].shift(s, fill_value=-1)), 'con_prev_reg'] = 'indirect'
            mappings.loc[('-' == mappings['strand']) & (mappings['q_name'] == mappings['q_name'].shift(-s, fill_value="")) & (0 < mappings['region']) & (mappings['region']-1 == mappings['region'].shift(-s, fill_value=-1)), 'con_prev_reg'] = 'indirect'
        
        # Find direct connections
        mappings.loc[('+' == mappings['strand']) & (mappings['q_name'] == mappings['q_name'].shift(1, fill_value="")) & ('+' == mappings['strand'].shift(1, fill_value="")) &\
                     (0 < mappings['region']) & (mappings['region']-1 == mappings['region'].shift(1, fill_value=-1)), 'con_prev_reg'] = 'direct'
        mappings.loc[('-' == mappings['strand']) & (mappings['q_name'] == mappings['q_name'].shift(-1, fill_value="")) & ('-' == mappings['strand'].shift(-1, fill_value="")) &\
                 (0 < mappings['region']) & (mappings['region']-1 == mappings['region'].shift(-1, fill_value=-1)), 'con_prev_reg'] = 'direct'
        # Clear scaffold connection info for direct connections
        mappings.loc['direct' == mappings['con_prev_reg'], 'left_scaf'] = ''
        mappings.loc[('+' == mappings['strand']) & ('direct' == mappings['con_prev_reg'].shift(-1, fill_value='')), 'right_scaf'] = ''
        mappings.loc[('-' == mappings['strand']) & ('direct' == mappings['con_prev_reg'].shift(1, fill_value='')), 'right_scaf'] = ''
        
        # Find same mappings connecting reads (full connection)
        mappings.loc[('+' == mappings['strand']) & ('direct' == mappings['con_prev_reg']) &  (mappings['q_start'] == mappings['q_start'].shift(1, fill_value="")) &  (mappings['q_end'] == mappings['q_end'].shift(1, fill_value="")), 'con_prev_reg'] = 'full'
        mappings.loc[('-' == mappings['strand']) & ('direct' == mappings['con_prev_reg']) &  (mappings['q_start'] == mappings['q_start'].shift(-1, fill_value="")) &  (mappings['q_end'] == mappings['q_end'].shift(-1, fill_value="")), 'con_prev_reg'] = 'full'
        
    # Remove mappings that are not in a region
    mappings = mappings[0 <= mappings['region']].copy()
    
    # Assign read ids to the mappings sorted from left to right by appearance on screen to give position in drawing
    tmp = mappings.groupby(['q_name'], sort=False)['region'].agg(['size','min'])
    mappings['min_region'] = np.repeat(tmp['min'].values, tmp['size'].values)
    mappings['t_min_start'] = np.where(mappings['min_region'] == mappings['region'], mappings['t_start'], sys.maxsize)
    mappings['t_max_end'] = np.where(mappings['min_region'] == mappings['region'], mappings['t_end'], 0)
    tmp = mappings.groupby(['q_name'], sort=False)[['t_min_start','t_max_end']].agg({'t_min_start':['size','min'], 't_max_end':['max']})
    mappings['t_min_start'] = np.repeat(tmp[('t_min_start','min')].values, tmp[('t_min_start','size')].values)
    mappings['t_max_end'] = np.repeat(tmp[('t_max_end','max')].values, tmp[('t_min_start','size')].values)
    mappings.sort_values(['min_region','t_min_start','t_max_end','q_name','region','t_start','t_end'], ascending=[True,True,True,True,False,False,False], inplace=True)
    mappings['read_id'] = (mappings['q_name'] != mappings['q_name'].shift(1, fill_value="")).cumsum()
    mappings.drop(columns=['min_region','t_min_start','t_max_end'], inplace=True)
    
    # Check whether the reads support a contig break within the region (on left or right side of the read)
    mappings['left_break'] = ((mappings['t_start'] > max_dist_contig_end) &
                             ( (mappings['left_scaf'] != '') | (np.where('+' == mappings['strand'], mappings['q_start'], mappings['q_len']-mappings['q_end']) > min_length_contig_break) ) &
                             (mappings['overrun_left'] == False))

    mappings['right_break'] = ((mappings['t_len']-mappings['t_end'] > max_dist_contig_end) &
                              ( (mappings['right_scaf'] != '') | (np.where('+' == mappings['strand'], mappings['q_len']-mappings['q_end'], mappings['q_start']) > min_length_contig_break) ) &
                              (mappings['overrun_right'] == False))
    
    return mappings

def DrawRegions(draw, fnt, regions, total_x, region_part_height, read_section_height):
    region_bar_height = 10
    region_bar_read_separation = 5
    border_width_x = 50
    region_separation_width = 50
    pixels_separating_ticks = 100
    tick_width = 2
    tick_length = 2
    region_text_dist = 18

    region_bar_y_bottom = region_part_height - region_bar_read_separation
    region_bar_y_top = region_bar_y_bottom - region_bar_height
    length_to_pixels = (total_x - 2*border_width_x - (len(regions)-1)*region_separation_width) / sum( [reg['end']-reg['start'] for reg in regions] )
    
    cur_length = 0
    for i, reg in enumerate(regions):
        regions[i]['start_x'] = border_width_x + i*region_separation_width + int(round(cur_length * length_to_pixels))
        cur_length += reg['end']-reg['start']
        regions[i]['end_x'] = border_width_x + i*region_separation_width + int(round(cur_length * length_to_pixels))

        draw.rectangle((regions[i]['start_x'], region_bar_y_top, regions[i]['end_x'], region_bar_y_bottom), fill=(0, 0, 0), outline=(0, 0, 0))
        w, h = draw.textsize(regions[i]['scaffold'], font=fnt)
        draw.text(((regions[i]['start_x'] + regions[i]['end_x'] - w)/2, region_bar_y_top-2*region_text_dist), regions[i]['scaffold'], font=fnt, fill=(0,0,0))
        
        ticks = []
        ticks.append( (regions[i]['start_x'], str(regions[i]['start'])) )
        tick_num = max(0, int( (regions[i]['end_x']-regions[i]['start_x']) / pixels_separating_ticks ) - 1 )
        for n in range(1, tick_num+1):
            # First find ideal x (without regions[i]['start_x'] yet)
            x = int( (regions[i]['end_x']-regions[i]['start_x']) / (tick_num+1) * n )
            # Then calculate pos accordingly, round it to the next int and set x according to rounded pos
            pos = regions[i]['start']+x/length_to_pixels
            x = regions[i]['start_x'] + int(x + ( int( round(pos) ) - pos)*length_to_pixels )
            ticks.append( (x, str(int( round(pos) ))) )
        ticks.append( (regions[i]['end_x']-1, str(regions[i]['end'])) )
        
        for x, pos in ticks:
            draw.line(((x, region_bar_y_bottom+1), (x, region_bar_y_bottom+read_section_height)), fill=(175, 175, 175), width=tick_width)
            draw.line(((x, region_bar_y_top-tick_length), (x, region_bar_y_top)), fill=(0, 0, 0), width=tick_width)
            w, h = draw.textsize(pos, font=fnt)
            draw.text((x-w/2,region_bar_y_top-region_text_dist), pos, font=fnt, fill=(0,0,0))

    return regions, length_to_pixels

def GetMapQColor(mapq):
    if mapq < 10:
        return (255, 255, 255)
    else:
        return tuple([ int(round(255*c)) for c in sns.color_palette("Blues")[min(5,int(mapq/10)-1)] ])

def DrawArrow(draw, x, y, width, color, direction, head_len):
    if '+' == direction:
        draw.polygon(((x[1], y), (x[1]-head_len, y+width), (x[0], y+width), (x[0]+head_len, y), (x[0], y-width), (x[1]-head_len, y-width)), fill=color, outline=GetMapQColor(60))
    else:
        draw.polygon(((x[0], y), (x[0]+head_len, y-width), (x[1], y-width), (x[1]-head_len, y), (x[1], y+width), (x[0]+head_len, y+width)), fill=color, outline=GetMapQColor(60))
        
def DrawCross(draw, x, y, size, color):
    draw.line(((x-size, y-size), (x+size, y+size)), fill=color, width=4)
    draw.line(((x+size, y-size), (x-size, y+size)), fill=color, width=4)

def DrawMappings(draw, fnt, regions, mappings, region_part_height, pixel_per_read, length_to_pixels):
    arrow_width = 3
    arrow_head_len = 10
    last_read_id = -1
    for row in mappings.itertuples(index=False):
        # Draw line every 10 reads
        y = region_part_height + row.read_id*pixel_per_read
        if last_read_id != row.read_id and 0 == row.read_id % 10:
            for reg in regions:
                draw.line(((reg['start_x'], y), (reg['end_x'], y)), fill=(175, 175, 174), width=1)
        
        # Get positions
        x1 = max(regions[row.region]['start_x'] - (arrow_head_len if row.overrun_left else 0), regions[row.region]['start_x'] + (row.t_start-regions[row.region]['start'])*length_to_pixels)
        x2 = min(regions[row.region]['end_x'] + (arrow_head_len if row.overrun_right else 0), regions[row.region]['start_x'] + (row.t_end-regions[row.region]['start'])*length_to_pixels)
        y = y - int(pixel_per_read/2)
        x0 = max(regions[row.region]['start_x'] - arrow_head_len, x1 - row.left_len*length_to_pixels)
        x3 = min(regions[row.region]['end_x'] + arrow_head_len, x2 + row.right_len*length_to_pixels)

        # Draw read continuation
        if last_read_id != row.read_id and x2 != x3:
            draw.line(((x2 if row.strand == '+' else x2-arrow_head_len, y), (x3, y)), fill=(0, 0, 0), width=1)
        if x0 != x1:
            draw.line(((x0, y), (x1 if row.strand == '-' else x1+arrow_head_len, y)), fill=(0, 0, 0), width=1)

        # Draw mapping
        DrawArrow(draw, (x1, x2), y, arrow_width, GetMapQColor(row.mapq), row.strand, arrow_head_len)
        
        # Draw breaks
        if row.left_break:
            DrawCross(draw, x1, y, 6, (217,173,60))
        if row.right_break:
            DrawCross(draw, x2, y, 6, (217,173,60))
        
        last_read_id = row.read_id


def GaplessVisualize(region_defs, mapping_file, output, min_mapq, min_mapping_length, min_length_contig_break, keep_all_subreads):
    alignment_precision = 100
    max_dist_contig_end = 2000
    
    regions = []
    for reg in region_defs:
        split = reg.split(':')
        if 2 != len(split):
            print("Incorrect region definition: ", reg)
            sys.exit(1)
        else:
            scaf = split[0]
            split = split[1].split('-')
            if 2 != len(split):
                print("Incorrect region definition: ", reg)
                sys.exit(1)
            else:
                regions.append( {'scaffold':scaf, 'start':int(split[0]), 'end':int(split[1])} )
                
    for i, reg in enumerate(regions):
        for reg2 in regions[:i]:
            if reg['scaffold'] == reg2['scaffold'] and reg['start'] <= reg2['end'] and reg['end'] >= reg2['start']:
                print("Overlapping regions are not permitted: ", reg, " ", reg[2])
                sys.exit(1)
            
    # Prepare mappings
    mappings = GetMappingsInRegion(mapping_file, regions, min_mapq, min_mapping_length, keep_all_subreads, alignment_precision, min_length_contig_break, max_dist_contig_end)
    
    # Prepare drawing
    total_x = 1000
    region_part_height = 55
    pixel_per_read = 13
    
    if len(mappings):
        read_section_height = np.max(mappings['read_id'])*pixel_per_read
    else:
        read_section_height = 0
        
    img = Image.new('RGB', (total_x,region_part_height + read_section_height), (255, 255, 255))
    draw = ImageDraw.Draw(img)
    fnt = ImageFont.truetype("Pillow/Tests/fonts/FreeMono.ttf", 12)
    
    # Draw
    regions, length_to_pixels = DrawRegions(draw, fnt, regions, total_x, region_part_height, read_section_height)
    DrawMappings(draw, fnt, regions, mappings, region_part_height, pixel_per_read, length_to_pixels)

    # Save drawing
    img.save(output)

def TestFindBreakPoints():
    # Define test case
    max_dist_contig_end = 2000
    min_length_contig_break = 700
    max_break_point_distance = 200
    min_mapping_length = 500
    min_num_reads = 2
    merge_block_length = 10000
    prob_factor = 10
    prematurity_threshold = 0.05
    # str(mappings.merge(mappings.loc[mappings['t_id'] == 118, ['q_name']].drop_duplicates(), on=['q_name'], how='inner').reset_index(drop=True).to_dict())
    mappings = []
    mappings.append( pd.DataFrame( {'q_name': {0: 'm54120_180612_220526/16187705/17701_29105', 1: 'm54120_180612_220526/16187705/17701_29105', 2: 'm54120_180612_220526/25428675/5700_33036', 3: 'm54120_180612_220526/25428675/5700_33036', 4: 'm54120_180612_220526/25428675/5700_33036', 5: 'm54120_180612_220526/25428675/5700_33036', 6: 'm54120_180612_220526/27460573/0_13395', 7: 'm54120_180612_220526/27460573/0_13395', 8: 'm54120_180612_220526/27460573/0_13395', 9: 'm54120_180612_220526/67371818/0_18917', 10: 'm54120_180612_220526/67371818/0_18917', 11: 'm54120_180612_220526/67371818/0_18917', 12: 'm54120_180612_220526/67371818/0_18917', 13: 'm54120_180612_220526/67371818/0_18917', 14: 'm54120_180612_220526/8979240/0_9505', 15: 'm54120_180612_220526/8979240/0_9505', 16: 'm54120_180612_220526/8979240/0_9505', 17: 'm54120_180612_220526/8979240/0_9505', 18: 'm54120_180613_174920/11534735/0_12449', 19: 'm54120_180613_174920/11534735/0_12449', 20: 'm54120_180613_174920/46465314/5200_15221', 21: 'm54120_180613_174920/46465314/5200_15221', 22: 'm54120_180613_174920/46465314/5200_15221', 23: 'm54120_180613_174920/50593952/4543_34996', 24: 'm54120_180613_174920/50593952/4543_34996', 25: 'm54120_180613_174920/50593952/4543_34996', 26: 'm54120_180613_174920/50593952/4543_34996', 27: 'm54120_180613_174920/50593952/4543_34996', 28: 'm54120_180613_174920/50593952/4543_34996', 29: 'm54120_180613_174920/52822247/0_11810', 30: 'm54120_180613_174920/52822247/0_11810', 31: 'm54120_180613_174920/52822247/0_11810', 32: 'm54120_180613_174920/52822247/0_11810', 33: 'm54120_180613_174920/52822247/0_11810', 34: 'm54120_180613_174920/53281190/0_27428', 35: 'm54120_180613_174920/53281190/0_27428', 36: 'm54120_180613_174920/53281190/0_27428', 37: 'm54120_180613_174920/54657609/17230_29223', 38: 'm54120_180613_174920/54657609/17230_29223', 39: 'm54120_180613_174920/54657609/17230_29223', 40: 'm54120_180613_174920/55050982/0_1779', 41: 'm54120_180613_174920/61146050/0_6250', 42: 'm54120_180613_174920/65274152/42_10168', 43: 'm54120_180613_174920/65274152/42_10168', 44: 'm54120_180613_174920/65274152/42_10168', 45: 'm54120_180613_174920/65274152/42_10168', 46: 'm54120_180613_174920/6619329/55912_74859', 47: 'm54120_180613_174920/6619329/55912_74859', 48: 'm54120_180613_174920/68550867/141_18008', 49: 'm54120_180613_174920/68550867/141_18008', 50: 'm54120_180613_174920/71958901/8213_14879', 51: 'm54120_180613_174920/71958901/8213_14879', 52: 'm54120_180614_221556/22610210/0_31659', 53: 'm54120_180614_221556/22610210/0_31659', 54: 'm54120_180614_221556/22610210/0_31659', 55: 'm54120_180614_221556/22610210/0_31659', 56: 'm54120_180614_221556/22610210/0_31659', 57: 'm54120_180614_221556/30933921/26687_28412', 58: 'm54120_180614_221556/31654780/0_20228', 59: 'm54120_180614_221556/31654780/0_20228', 60: 'm54120_180614_221556/31654780/0_20228', 61: 'm54120_180614_221556/41354128/0_7297', 62: 'm54120_180614_221556/41354128/0_7297', 63: 'm54120_180614_221556/41354128/0_7297', 64: 'm54120_180614_221556/44106233/0_23620', 65: 'm54120_180614_221556/44106233/0_23620', 66: 'm54120_180614_221556/44106233/0_23620', 67: 'm54120_180614_221556/45744690/0_23358', 68: 'm54120_180614_221556/45744690/0_23358', 69: 'm54120_180614_221556/45744690/0_23358', 70: 'm54120_180614_221556/45744690/0_23358', 71: 'm54120_180614_221556/63439430/4166_15244', 72: 'm54120_180614_221556/63439430/4166_15244', 73: 'm54120_180614_221556/63439430/4166_15244', 74: 'm54120_180614_221556/66322803/9605_16481', 75: 'm54120_180614_221556/66322803/9605_16481', 76: 'm54120_180614_221556/68092430/12233_21670', 77: 'm54120_180614_221556/68616973/0_2444', 78: 'm54120_180615_083216/10158689/0_7957', 79: 'm54120_180615_083216/10158689/0_7957', 80: 'm54120_180615_083216/12648735/0_14207', 81: 'm54120_180615_083216/12648735/0_14207', 82: 'm54120_180615_083216/12648735/0_14207', 83: 'm54120_180615_083216/12648735/0_14207', 84: 'm54120_180615_083216/12648735/0_14207', 85: 'm54120_180615_083216/14483810/1870_28661', 86: 'm54120_180615_083216/14483810/1870_28661', 87: 'm54120_180615_083216/14483810/1870_28661', 88: 'm54120_180615_083216/23987141/0_12660', 89: 'm54120_180615_083216/23987141/0_12660', 90: 'm54120_180615_083216/23987141/0_12660', 91: 'm54120_180615_083216/24117500/0_16828', 92: 'm54120_180615_083216/24117500/0_16828', 93: 'm54120_180615_083216/24117500/0_16828', 94: 'm54120_180615_083216/24117500/0_16828', 95: 'm54120_180615_083216/24117500/0_16828', 96: 'm54120_180615_083216/32047878/0_20842', 97: 'm54120_180615_083216/32047878/0_20842', 98: 'm54120_180615_083216/53412536/46759_62935', 99: 'm54120_180615_083216/53412536/46759_62935', 100: 'm54120_180615_083216/53412536/46759_62935', 101: 'm54120_180615_083216/53412536/46759_62935', 102: 'm54120_180615_083216/59638731/0_7306', 103: 'm54120_180615_083216/59638731/0_7306', 104: 'm54120_180615_083216/59638731/0_7306', 105: 'm54120_180616_203619/26476629/40049_59589', 106: 'm54120_180616_203619/26476629/40049_59589', 107: 'm54120_180616_203619/26476629/40049_59589', 108: 'm54120_180616_203619/27067045/0_20021', 109: 'm54120_180616_203619/27067045/0_20021', 110: 'm54120_180616_203619/27067045/0_20021', 111: 'm54120_180616_203619/39387838/305_9268', 112: 'm54120_180616_203619/39387838/305_9268', 113: 'm54120_180616_203619/44826768/900_26938', 114: 'm54120_180616_203619/44826768/900_26938', 115: 'm54120_180616_203619/44826768/900_26938', 116: 'm54120_180616_203619/44826768/900_26938', 117: 'm54120_180616_203619/44826768/900_26938', 118: 'm54120_180616_203619/53216124/0_12683', 119: 'm54120_180616_203619/53216124/0_12683', 120: 'm54120_180616_203619/53216124/0_12683', 121: 'm54120_180616_203619/5440148/0_22728', 122: 'm54120_180616_203619/5440148/0_22728', 123: 'm54120_180616_203619/5440148/0_22728', 124: 'm54120_180616_203619/5440148/0_22728', 125: 'm54120_180616_203619/57868542/0_28377', 126: 'm54120_180616_203619/57868542/0_28377', 127: 'm54120_180616_203619/57868542/0_28377', 128: 'm54120_180616_203619/57868542/0_28377', 129: 'm54120_180616_203619/57868542/0_28377', 130: 'm54120_180616_203619/57868542/0_28377', 131: 'm54120_180616_203619/61407462/0_17958', 132: 'm54120_180616_203619/61407462/0_17958', 133: 'm54120_180616_203619/61407462/0_17958', 134: 'm54120_180616_203619/61407462/0_17958', 135: 'm54120_180616_203619/62128659/5450_12897', 136: 'm54120_180616_203619/62128659/5450_12897', 137: 'm54120_180616_203619/72155705/0_9775', 138: 'm54120_180616_203619/72155705/0_9775', 139: 'm54120_180616_203619/72155705/0_9775', 140: 'm54120_180616_203619/8192953/0_18125', 141: 'm54120_180616_203619/8192953/0_18125', 142: 'm54120_180616_203619/8192953/0_18125', 143: 'm54120_180617_065128/11731652/0_10791', 144: 'm54120_180617_065128/21627108/21423_32807', 145: 'm54120_180617_065128/21823602/13900_29136', 146: 'm54120_180617_065128/21823602/13900_29136', 147: 'm54120_180617_065128/27852928/7686_14155', 148: 'm54120_180617_065128/27852928/7686_14155', 149: 'm54120_180617_065128/27852928/7686_14155', 150: 'm54120_180617_065128/30540459/0_8501', 151: 'm54120_180617_065128/33095850/2723_11625', 152: 'm54120_180617_065128/33095850/2723_11625', 153: 'm54120_180617_065128/33095850/2723_11625', 154: 'm54120_180617_065128/33948061/8852_18083', 155: 'm54120_180617_065128/33948061/8852_18083', 156: 'm54120_180617_065128/38862991/0_8821', 157: 'm54120_180617_065128/38862991/0_8821', 158: 'm54120_180617_065128/38862991/0_8821', 159: 'm54120_180617_065128/5833441/0_24241', 160: 'm54120_180617_065128/5833441/0_24241', 161: 'm54120_180617_065128/5833441/0_24241', 162: 'm54120_180617_065128/5833441/0_24241', 163: 'm54120_180617_065128/61145270/14674_28516', 164: 'm54120_180617_065128/61145270/14674_28516', 165: 'm54120_180617_065128/61145270/14674_28516', 166: 'm54120_180617_065128/61145270/14674_28516', 167: 'm54120_180617_065128/62915376/0_20046', 168: 'm54120_180617_065128/62915376/0_20046', 169: 'm54120_180617_065128/62915376/0_20046', 170: 'm54120_180617_065128/65471199/0_7206', 171: 'm54120_180617_065128/65471199/0_7206', 172: 'm54120_180617_065128/65536164/1133_21100', 173: 'm54120_180617_065128/65536164/1133_21100', 174: 'm54120_180617_065128/65536164/1133_21100', 175: 'm54120_180617_065128/71696466/0_12493', 176: 'm54120_180617_065128/71696466/0_12493', 177: 'm54120_180617_065128/72679973/371_9853', 178: 'm54120_180617_065128/72679973/371_9853', 179: 'm54120_180617_065128/72679973/371_9853', 180: 'm54120_180617_065128/73401160/0_6065', 181: 'm54120_180617_065128/74449369/5015_16044', 182: 'm54120_180617_065128/74449369/5015_16044', 183: 'm54120_180617_065128/74449369/5015_16044', 184: 'm54120_180617_065128/74449369/5015_16044', 185: 'm54120_180623_025753/14942501/23987_38109', 186: 'm54120_180623_025753/14942501/23987_38109', 187: 'm54120_180623_025753/34209889/29885_39582', 188: 'm54120_180623_025753/34209889/29885_39582', 189: 'm54120_180623_025753/46268779/40538_48546', 190: 'm54120_180623_025753/46268779/40538_48546', 191: 'm54120_180623_025753/47972463/0_16430', 192: 'm54120_180623_025753/47972463/0_16430', 193: 'm54120_180623_025753/47972463/0_16430', 194: 'm54120_180623_025753/47972463/0_16430', 195: 'm54120_180623_025753/48693724/0_8026', 196: 'm54120_180623_025753/48693724/0_8026', 197: 'm54120_180623_025753/53084315/45115_59680', 198: 'm54120_180623_025753/60097146/33558_41081', 199: 'm54120_180623_025753/60097146/33558_41081', 200: 'm54120_180623_025753/60097146/33558_41081', 201: 'm54120_180623_025753/63898361/19400_25722', 202: 'm54120_180623_230400/16057269/10886_16567', 203: 'm54120_180623_230400/16057269/10886_16567', 204: 'm54120_180623_230400/17564356/22012_44198', 205: 'm54120_180623_230400/17564356/22012_44198', 206: 'm54120_180623_230400/17564356/22012_44198', 207: 'm54120_180623_230400/31981756/10362_16867', 208: 'm54120_180623_230400/31981756/10362_16867', 209: 'm54120_180623_230400/31981756/10362_16867', 210: 'm54120_180623_230400/34079670/16635_35578', 211: 'm54120_180623_230400/34079670/16635_35578', 212: 'm54120_180623_230400/34079670/16635_35578', 213: 'm54120_180623_230400/39650155/18382_31373', 214: 'm54120_180623_230400/39650155/18382_31373', 215: 'm54120_180623_230400/39650155/18382_31373', 216: 'm54120_180623_230400/39650155/18382_31373', 217: 'm54120_180623_230400/39650155/18382_31373', 218: 'm54120_180623_230400/40567632/20845_33922', 219: 'm54120_180623_230400/41484532/1250_10689', 220: 'm54120_180623_230400/41812491/4210_9515', 221: 'm54120_180623_230400/42337235/29064_38100', 222: 'm54120_180623_230400/42337235/29064_38100', 223: 'm54120_180623_230400/42337235/29064_38100', 224: 'm54120_180623_230400/42337235/29064_38100', 225: 'm54120_180623_230400/44368640/0_28393', 226: 'm54120_180623_230400/44368640/0_28393', 227: 'm54120_180623_230400/44368640/0_28393', 228: 'm54120_180623_230400/44368640/0_28393', 229: 'm54120_180623_230400/47579253/30409_45829', 230: 'm54120_180623_230400/47579253/30409_45829', 231: 'm54120_180623_230400/47579253/30409_45829', 232: 'm54120_180623_230400/47579253/30409_45829', 233: 'm54120_180623_230400/49283971/0_3678', 234: 'm54120_180623_230400/52232801/8950_20209', 235: 'm54120_180623_230400/52232801/8950_20209', 236: 'm54120_180623_230400/57541566/11508_26197', 237: 'm54120_180623_230400/57541566/11508_26197', 238: 'm54120_180623_230400/57541566/11508_26197', 239: 'm54120_180623_230400/57541566/11508_26197', 240: 'm54120_180623_230400/61800849/4897_30406', 241: 'm54120_180623_230400/61800849/4897_30406', 242: 'm54120_180623_230400/61800849/4897_30406', 243: 'm54120_180623_230400/61800849/4897_30406', 244: 'm54120_180623_230400/61800849/4897_30406', 245: 'm54120_180623_230400/70189230/0_9536', 246: 'm54120_180623_230400/70189230/0_9536', 247: 'm54120_180623_230400/70189230/0_9536', 248: 'm54120_180623_230400/70189230/0_9536', 249: 'm54120_180623_230400/9831369/0_6759', 250: 'm54120_180623_230400/9831369/0_6759', 251: 'm54120_180623_230400/9831369/0_6759', 252: 'm54120_180624_091622/10551664/0_10870', 253: 'm54120_180624_091622/10551664/0_10870', 254: 'm54120_180624_091622/10551664/0_10870', 255: 'm54120_180624_091622/11862124/34233_47980', 256: 'm54120_180624_091622/11862124/34233_47980', 257: 'm54120_180624_091622/14811838/2246_11357', 258: 'm54120_180624_091622/14811838/2246_11357', 259: 'm54120_180624_091622/23200586/16049_30562', 260: 'm54120_180624_091622/37552810/0_6563', 261: 'm54120_180624_091622/37552810/0_6563', 262: 'm54120_180624_091622/37552810/0_6563', 263: 'm54120_180624_091622/37618619/4194_6237', 264: 'm54120_180624_091622/42795380/10445_19915', 265: 'm54120_180624_091622/42795380/10445_19915', 266: 'm54120_180624_091622/47973084/16319_24118', 267: 'm54120_180624_091622/47973084/16319_24118', 268: 'm54120_180624_091622/54722748/12814_24390', 269: 'm54120_180624_091622/54722748/12814_24390', 270: 'm54120_180624_091622/54722748/12814_24390', 271: 'm54120_180624_091622/54722748/12814_24390', 272: 'm54120_180624_091622/54722748/12814_24390', 273: 'm54120_180624_091622/66126315/1525_45687', 274: 'm54120_180624_091622/66126315/1525_45687', 275: 'm54120_180624_091622/66126315/1525_45687', 276: 'm54120_180624_091622/66126315/1525_45687', 277: 'm54120_180624_091622/66126315/1525_45687', 278: 'm54120_180624_091622/66126315/1525_45687', 279: 'm54120_180624_091622/66126315/1525_45687', 280: 'm54120_180624_091622/6685573/0_30972', 281: 'm54120_180624_091622/6685573/0_30972', 282: 'm54120_180624_091622/6685573/0_30972', 283: 'm54120_180624_091622/69075607/0_18229', 284: 'm54120_180624_091622/69075607/0_18229', 285: 'm54120_180624_091622/69075607/0_18229', 286: 'm54120_180624_091622/69075607/0_18229', 287: 'm54120_180624_091622/7275003/23879_30932', 288: 'm54120_180624_091622/7275003/23879_30932', 289: 'm54120_180624_091622/7275003/23879_30932', 290: 'm54120_180624_091622/73007688/0_20260', 291: 'm54120_180624_091622/73007688/0_20260', 292: 'm54120_180624_091622/73007688/0_20260', 293: 'm54120_180624_091622/73204449/0_24344', 294: 'm54120_180624_091622/73204449/0_24344', 295: 'm54120_180624_091622/73204449/0_24344', 296: 'm54120_180624_193042/11665738/17408_33392', 297: 'm54120_180624_193042/12124452/13259_22255', 298: 'm54120_180624_193042/12124452/13259_22255', 299: 'm54120_180624_193042/12124452/13259_22255', 300: 'm54120_180624_193042/24248459/4862_28304', 301: 'm54120_180624_193042/34603219/33720_51855', 302: 'm54120_180624_193042/34603219/33720_51855', 303: 'm54120_180624_193042/35389919/15322_23251', 304: 'm54120_180624_193042/35389919/15322_23251', 305: 'm54120_180624_193042/36897686/24003_41675', 306: 'm54120_180624_193042/36897686/24003_41675', 307: 'm54120_180624_193042/45875466/4604_18561', 308: 'm54120_180624_193042/45875466/4604_18561', 309: 'm54120_180624_193042/46268648/7809_15222', 310: 'm54120_180624_193042/50659867/20109_35037', 311: 'm54120_180624_193042/5440035/1252_9274', 312: 'm54120_180624_193042/60424755/21193_30205', 313: 'm54120_180624_193042/61604207/8582_22570', 314: 'm54120_180624_193042/61604207/8582_22570', 315: 'm54120_180624_193042/61604207/8582_22570', 316: 'm54120_180624_193042/66912433/6739_23805', 317: 'm54120_180624_193042/66912433/6739_23805', 318: 'm54120_180624_193042/66912433/6739_23805', 319: 'm54120_180624_193042/66912433/6739_23805', 320: 'm54120_180624_193042/69469056/0_13826', 321: 'm54120_180624_193042/69469056/0_13826', 322: 'm54120_180624_193042/69469056/0_13826', 323: 'm54120_180624_193042/69469056/0_13826', 324: 'm54120_180624_193042/71500246/11679_18355', 325: 'm54120_180624_193042/71500246/11679_18355', 326: 'm54120_180624_193042/71500246/11679_18355', 327: 'm54120_180625_144344/30736472/0_1053', 328: 'm54120_180625_144344/35192921/0_9820', 329: 'm54120_180625_144344/35192921/0_9820', 330: 'm54120_180625_144344/59638609/24685_43849', 331: 'm54120_180625_144344/59638609/24685_43849', 332: 'm54120_180625_144344/59638609/24685_43849', 333: 'm54120_180625_144344/59638609/24685_43849', 334: 'm54120_180625_144344/67502330/6088_12728', 335: 'm54120_180625_144344/67502330/6088_12728', 336: 'm54120_180625_144344/70517447/45706_54040', 337: 'm54120_180628_193436/16777928/15979_27821', 338: 'm54120_180628_193436/16777928/15979_27821', 339: 'm54120_180628_193436/16777928/15979_27821', 340: 'm54120_180628_193436/16777928/15979_27821', 341: 'm54120_180628_193436/16777928/15979_27821', 342: 'm54120_180628_193436/17695126/0_10585', 343: 'm54120_180628_193436/17695126/0_10585', 344: 'm54120_180628_193436/23593322/2436_16978', 345: 'm54120_180628_193436/23593322/2436_16978', 346: 'm54120_180628_193436/23593322/2436_16978', 347: 'm54120_180628_193436/23593322/2436_16978', 348: 'm54120_180628_193436/23593322/2436_16978', 349: 'm54120_180628_193436/23593322/2436_16978', 350: 'm54120_180628_193436/23593322/2436_16978', 351: 'm54120_180628_193436/25756226/0_7178', 352: 'm54120_180628_193436/25756226/0_7178', 353: 'm54120_180628_193436/25756226/0_7178', 354: 'm54120_180628_193436/25756226/0_7178', 355: 'm54120_180628_193436/26869854/0_8874', 356: 'm54120_180628_193436/28508647/14468_27591', 357: 'm54120_180628_193436/28508647/14468_27591', 358: 'm54120_180628_193436/31065039/14008_33764', 359: 'm54120_180628_193436/31065039/14008_33764', 360: 'm54120_180628_193436/31065039/14008_33764', 361: 'm54120_180628_193436/39125779/6336_24798', 362: 'm54120_180628_193436/39125779/6336_24798', 363: 'm54120_180628_193436/39125779/6336_24798', 364: 'm54120_180628_193436/39125779/6336_24798', 365: 'm54120_180628_193436/60883928/11655_30879', 366: 'm54120_180628_193436/60883928/11655_30879', 367: 'm54120_180628_193436/60883928/11655_30879', 368: 'm54120_180628_193436/60883928/11655_30879', 369: 'm54120_180628_193436/60883928/11655_30879', 370: 'm54120_180628_193436/74449586/20823_29026'},
                                    'q_len': {0: 11404, 1: 11404, 2: 27336, 3: 27336, 4: 27336, 5: 27336, 6: 13395, 7: 13395, 8: 13395, 9: 18917, 10: 18917, 11: 18917, 12: 18917, 13: 18917, 14: 9505, 15: 9505, 16: 9505, 17: 9505, 18: 12449, 19: 12449, 20: 10021, 21: 10021, 22: 10021, 23: 30453, 24: 30453, 25: 30453, 26: 30453, 27: 30453, 28: 30453, 29: 11810, 30: 11810, 31: 11810, 32: 11810, 33: 11810, 34: 27428, 35: 27428, 36: 27428, 37: 11993, 38: 11993, 39: 11993, 40: 1779, 41: 6250, 42: 10126, 43: 10126, 44: 10126, 45: 10126, 46: 18947, 47: 18947, 48: 17867, 49: 17867, 50: 6666, 51: 6666, 52: 31659, 53: 31659, 54: 31659, 55: 31659, 56: 31659, 57: 1725, 58: 20228, 59: 20228, 60: 20228, 61: 7297, 62: 7297, 63: 7297, 64: 23620, 65: 23620, 66: 23620, 67: 23358, 68: 23358, 69: 23358, 70: 23358, 71: 11078, 72: 11078, 73: 11078, 74: 6876, 75: 6876, 76: 9437, 77: 2444, 78: 7957, 79: 7957, 80: 14207, 81: 14207, 82: 14207, 83: 14207, 84: 14207, 85: 26791, 86: 26791, 87: 26791, 88: 12660, 89: 12660, 90: 12660, 91: 16828, 92: 16828, 93: 16828, 94: 16828, 95: 16828, 96: 20842, 97: 20842, 98: 16176, 99: 16176, 100: 16176, 101: 16176, 102: 7306, 103: 7306, 104: 7306, 105: 19540, 106: 19540, 107: 19540, 108: 20021, 109: 20021, 110: 20021, 111: 8963, 112: 8963, 113: 26038, 114: 26038, 115: 26038, 116: 26038, 117: 26038, 118: 12683, 119: 12683, 120: 12683, 121: 22728, 122: 22728, 123: 22728, 124: 22728, 125: 28377, 126: 28377, 127: 28377, 128: 28377, 129: 28377, 130: 28377, 131: 17958, 132: 17958, 133: 17958, 134: 17958, 135: 7447, 136: 7447, 137: 9775, 138: 9775, 139: 9775, 140: 18125, 141: 18125, 142: 18125, 143: 10791, 144: 11384, 145: 15236, 146: 15236, 147: 6469, 148: 6469, 149: 6469, 150: 8501, 151: 8902, 152: 8902, 153: 8902, 154: 9231, 155: 9231, 156: 8821, 157: 8821, 158: 8821, 159: 24241, 160: 24241, 161: 24241, 162: 24241, 163: 13842, 164: 13842, 165: 13842, 166: 13842, 167: 20046, 168: 20046, 169: 20046, 170: 7206, 171: 7206, 172: 19967, 173: 19967, 174: 19967, 175: 12493, 176: 12493, 177: 9482, 178: 9482, 179: 9482, 180: 6065, 181: 11029, 182: 11029, 183: 11029, 184: 11029, 185: 14122, 186: 14122, 187: 9697, 188: 9697, 189: 8008, 190: 8008, 191: 16430, 192: 16430, 193: 16430, 194: 16430, 195: 8026, 196: 8026, 197: 14565, 198: 7523, 199: 7523, 200: 7523, 201: 6322, 202: 5681, 203: 5681, 204: 22186, 205: 22186, 206: 22186, 207: 6505, 208: 6505, 209: 6505, 210: 18943, 211: 18943, 212: 18943, 213: 12991, 214: 12991, 215: 12991, 216: 12991, 217: 12991, 218: 13077, 219: 9439, 220: 5305, 221: 9036, 222: 9036, 223: 9036, 224: 9036, 225: 28393, 226: 28393, 227: 28393, 228: 28393, 229: 15420, 230: 15420, 231: 15420, 232: 15420, 233: 3678, 234: 11259, 235: 11259, 236: 14689, 237: 14689, 238: 14689, 239: 14689, 240: 25509, 241: 25509, 242: 25509, 243: 25509, 244: 25509, 245: 9536, 246: 9536, 247: 9536, 248: 9536, 249: 6759, 250: 6759, 251: 6759, 252: 10870, 253: 10870, 254: 10870, 255: 13747, 256: 13747, 257: 9111, 258: 9111, 259: 14513, 260: 6563, 261: 6563, 262: 6563, 263: 2043, 264: 9470, 265: 9470, 266: 7799, 267: 7799, 268: 11576, 269: 11576, 270: 11576, 271: 11576, 272: 11576, 273: 44162, 274: 44162, 275: 44162, 276: 44162, 277: 44162, 278: 44162, 279: 44162, 280: 30972, 281: 30972, 282: 30972, 283: 18229, 284: 18229, 285: 18229, 286: 18229, 287: 7053, 288: 7053, 289: 7053, 290: 20260, 291: 20260, 292: 20260, 293: 24344, 294: 24344, 295: 24344, 296: 15984, 297: 8996, 298: 8996, 299: 8996, 300: 23442, 301: 18135, 302: 18135, 303: 7929, 304: 7929, 305: 17672, 306: 17672, 307: 13957, 308: 13957, 309: 7413, 310: 14928, 311: 8022, 312: 9012, 313: 13988, 314: 13988, 315: 13988, 316: 17066, 317: 17066, 318: 17066, 319: 17066, 320: 13826, 321: 13826, 322: 13826, 323: 13826, 324: 6676, 325: 6676, 326: 6676, 327: 1053, 328: 9820, 329: 9820, 330: 19164, 331: 19164, 332: 19164, 333: 19164, 334: 6640, 335: 6640, 336: 8334, 337: 11842, 338: 11842, 339: 11842, 340: 11842, 341: 11842, 342: 10585, 343: 10585, 344: 14542, 345: 14542, 346: 14542, 347: 14542, 348: 14542, 349: 14542, 350: 14542, 351: 7178, 352: 7178, 353: 7178, 354: 7178, 355: 8874, 356: 13123, 357: 13123, 358: 19756, 359: 19756, 360: 19756, 361: 18462, 362: 18462, 363: 18462, 364: 18462, 365: 19224, 366: 19224, 367: 19224, 368: 19224, 369: 19224, 370: 8203},
                                    'q_start': {0: 270, 1: 6281, 2: 3977, 3: 13432, 4: 22993, 5: 24884, 6: 20, 7: 3855, 8: 11577, 9: 877, 10: 6522, 11: 12167, 12: 13288, 13: 17033, 14: 12, 15: 700, 16: 3918, 17: 8627, 18: 6326, 19: 10072, 20: 2294, 21: 4448, 22: 8194, 23: 2186, 24: 3184, 25: 3908, 26: 7292, 27: 20502, 28: 26107, 29: 1, 30: 1077, 31: 3253, 32: 6183, 33: 9922, 34: 3747, 35: 11410, 36: 25595, 37: 0, 38: 810, 39: 10181, 40: 3, 41: 2678, 42: 0, 43: 2692, 44: 4544, 45: 8220, 46: 3, 47: 11817, 48: 0, 49: 12536, 50: 1, 51: 5328, 52: 8, 53: 20880, 54: 26565, 55: 29417, 56: 30034, 57: 0, 58: 0, 59: 772, 60: 7961, 61: 763, 62: 1618, 63: 5421, 64: 463, 65: 7325, 66: 18503, 67: 2636, 68: 9377, 69: 17713, 70: 21463, 71: 17, 72: 1251, 73: 5962, 74: 0, 75: 4016, 76: 4465, 77: 15, 78: 0, 79: 4019, 80: 333, 81: 1909, 82: 3957, 83: 4609, 84: 12246, 85: 1998, 86: 13059, 87: 17802, 88: 6959, 89: 10617, 90: 12453, 91: 544, 92: 3360, 93: 4720, 94: 8232, 95: 11489, 96: 0, 97: 8939, 98: 2901, 99: 6674, 100: 8402, 101: 14147, 102: 4, 103: 1788, 104: 5474, 105: 108, 106: 901, 107: 5271, 108: 408, 109: 963, 110: 13395, 111: 15, 112: 7966, 113: 202, 114: 9377, 115: 11997, 116: 17835, 117: 24219, 118: 0, 119: 5202, 120: 7259, 121: 0, 122: 720, 123: 16030, 124: 18340, 125: 5123, 126: 6320, 127: 17524, 128: 20762, 129: 21544, 130: 24821, 131: 2, 132: 1695, 133: 13700, 134: 17393, 135: 35, 136: 3736, 137: 44, 138: 917, 139: 6483, 140: 3724, 141: 9668, 142: 16300, 143: 3, 144: 9448, 145: 0, 146: 5063, 147: 0, 148: 891, 149: 4629, 150: 3337, 151: 10, 152: 745, 153: 6289, 154: 3, 155: 6352, 156: 890, 157: 3292, 158: 6996, 159: 0, 160: 13928, 161: 20228, 162: 22866, 163: 0, 164: 677, 165: 4968, 166: 7655, 167: 4, 168: 9322, 169: 13938, 170: 3, 171: 4331, 172: 11, 173: 6078, 174: 9028, 175: 6905, 176: 10640, 177: 0, 178: 836, 179: 4734, 180: 4074, 181: 7, 182: 780, 183: 5149, 184: 6371, 185: 719, 186: 11753, 187: 0, 188: 1243, 189: 6455, 190: 6893, 191: 0, 192: 811, 193: 5231, 194: 7829, 195: 5, 196: 2746, 197: 0, 198: 0, 199: 939, 200: 5280, 201: 4177, 202: 0, 203: 1799, 204: 7, 205: 8339, 206: 10960, 207: 39, 208: 418, 209: 4405, 210: 50, 211: 7535, 212: 14511, 213: 0, 214: 2068, 215: 4425, 216: 6378, 217: 10166, 218: 4, 219: 0, 220: 1999, 221: 0, 222: 744, 223: 4696, 224: 5781, 225: 0, 226: 5382, 227: 8719, 228: 27609, 229: 976, 230: 1704, 231: 5411, 232: 10729, 233: 116, 234: 12, 235: 4647, 236: 3, 237: 953, 238: 8172, 239: 14131, 240: 265, 241: 3522, 242: 15982, 243: 17127, 244: 20283, 245: 1, 246: 837, 247: 5369, 248: 6593, 249: 35, 250: 769, 251: 4804, 252: 245, 253: 3357, 254: 8939, 255: 6192, 256: 11864, 257: 45, 258: 2062, 259: 36, 260: 11, 261: 976, 262: 5934, 263: 667, 264: 0, 265: 6989, 266: 919, 267: 1940, 268: 352, 269: 1645, 270: 2543, 271: 5307, 272: 9462, 273: 22, 274: 4969, 275: 18394, 276: 20444, 277: 26558, 278: 28322, 279: 31745, 280: 12, 281: 7952, 282: 26072, 283: 222, 284: 5201, 285: 12562, 286: 16314, 287: 67, 288: 911, 289: 4902, 290: 6, 291: 1645, 292: 15110, 293: 0, 294: 11247, 295: 19448, 296: 1649, 297: 88, 298: 1427, 299: 7042, 300: 21, 301: 0, 302: 6633, 303: 0, 304: 5146, 305: 968, 306: 11719, 307: 880, 308: 8804, 309: 34, 310: 68, 311: 2929, 312: 6059, 313: 5292, 314: 8373, 315: 12095, 316: 8, 317: 2166, 318: 5877, 319: 11861, 320: 8, 321: 5223, 322: 6414, 323: 12939, 324: 107, 325: 1071, 326: 4857, 327: 240, 328: 0, 329: 6629, 330: 17, 331: 2850, 332: 11776, 333: 18284, 334: 886, 335: 4692, 336: 0, 337: 13, 338: 1555, 339: 2171, 340: 6111, 341: 9851, 342: 190, 343: 5615, 344: 148, 345: 1151, 346: 5820, 347: 7516, 348: 8621, 349: 10653, 350: 14053, 351: 16, 352: 442, 353: 1175, 354: 4544, 355: 13, 356: 233, 357: 8454, 358: 80, 359: 5099, 360: 8700, 361: 3369, 362: 5211, 363: 10825, 364: 12245, 365: 24, 366: 1975, 367: 6388, 368: 7110, 369: 14258, 370: 0},
                                    'q_end': {0: 3315, 1: 11401, 2: 10195, 3: 20469, 4: 26381, 5: 27331, 6: 2910, 7: 6923, 8: 13395, 9: 4002, 10: 9525, 11: 14267, 12: 18179, 13: 18900, 14: 1844, 15: 5664, 16: 8639, 17: 9499, 18: 11166, 19: 11873, 20: 4967, 21: 9289, 22: 10017, 23: 2932, 24: 5064, 25: 8821, 26: 11349, 27: 25366, 28: 30312, 29: 2447, 30: 4235, 31: 7887, 32: 10996, 33: 11810, 34: 5275, 35: 15552, 36: 27420, 37: 300, 38: 4902, 39: 11993, 40: 1776, 41: 5700, 42: 3881, 43: 5065, 44: 9309, 45: 10124, 46: 1788, 47: 14962, 48: 4669, 49: 17867, 50: 1982, 51: 6666, 52: 2180, 53: 28346, 54: 29440, 55: 31114, 56: 31655, 57: 1711, 58: 1893, 59: 5709, 60: 12177, 61: 1947, 62: 6552, 63: 7288, 64: 5431, 65: 14770, 66: 23604, 67: 6797, 68: 13947, 69: 22580, 70: 23354, 71: 2294, 72: 4264, 73: 11075, 74: 5763, 75: 6863, 76: 9436, 77: 2440, 78: 1964, 79: 7957, 80: 3294, 81: 5130, 82: 7030, 83: 9031, 84: 14205, 85: 4086, 86: 18153, 87: 22929, 88: 11718, 89: 12444, 90: 12660, 91: 3088, 92: 6427, 93: 7964, 94: 15285, 95: 16813, 96: 2763, 97: 14028, 98: 7167, 99: 10080, 100: 10969, 101: 16173, 102: 2313, 103: 6580, 104: 7306, 105: 2031, 106: 5824, 107: 7753, 108: 905, 109: 5697, 110: 20018, 111: 4881, 112: 8963, 113: 7699, 114: 12462, 115: 16766, 116: 21013, 117: 26038, 118: 1505, 119: 6566, 120: 11667, 121: 1822, 122: 5587, 123: 18801, 124: 22590, 125: 6871, 126: 11068, 127: 19829, 128: 21456, 129: 22811, 130: 25583, 131: 3416, 132: 15727, 133: 17593, 134: 17958, 135: 2168, 136: 7393, 137: 2022, 138: 5819, 139: 9684, 140: 7909, 141: 14290, 142: 18125, 143: 1765, 144: 11380, 145: 1901, 146: 7971, 147: 1844, 148: 5743, 149: 6456, 150: 8500, 151: 1859, 152: 5607, 153: 8897, 154: 8093, 155: 9228, 156: 3822, 157: 8078, 158: 8765, 159: 4850, 160: 18676, 161: 23687, 162: 24236, 163: 1771, 164: 5510, 165: 11264, 166: 13840, 167: 4808, 168: 10275, 169: 20046, 170: 6065, 171: 7206, 172: 7830, 173: 9030, 174: 19951, 175: 11754, 176: 12493, 177: 1963, 178: 5705, 179: 6820, 180: 6065, 181: 1917, 182: 5676, 183: 7659, 184: 10286, 185: 3800, 186: 14119, 187: 2964, 188: 9695, 189: 7431, 190: 8008, 191: 1923, 192: 5782, 193: 10121, 194: 14720, 195: 2727, 196: 4667, 197: 1785, 198: 2083, 199: 5820, 200: 7513, 201: 6312, 202: 3846, 203: 5681, 204: 9763, 205: 10961, 206: 13887, 207: 544, 208: 5634, 209: 6494, 210: 3401, 211: 13278, 212: 16514, 213: 2532, 214: 3753, 215: 6898, 216: 11291, 217: 12007, 218: 5336, 219: 2090, 220: 3197, 221: 1866, 222: 5690, 223: 6857, 224: 8913, 225: 2031, 226: 11851, 227: 16048, 228: 28379, 229: 2877, 230: 6781, 231: 9328, 232: 15395, 233: 3678, 234: 4987, 235: 5980, 236: 2084, 237: 5881, 238: 13071, 239: 14688, 240: 1643, 241: 10559, 242: 16972, 243: 23956, 244: 25501, 245: 1982, 246: 5922, 247: 7083, 248: 9507, 249: 1924, 250: 5824, 251: 6747, 252: 2987, 253: 5645, 254: 10865, 255: 8647, 256: 13744, 257: 3223, 258: 7261, 259: 5099, 260: 2137, 261: 6071, 262: 6286, 263: 1885, 264: 4760, 265: 9467, 266: 1907, 267: 3868, 268: 3598, 269: 4567, 270: 5329, 271: 10582, 272: 11576, 273: 4161, 274: 6513, 275: 24439, 276: 27807, 277: 29097, 278: 30169, 279: 33002, 280: 1957, 281: 12775, 282: 29308, 283: 8294, 284: 11880, 285: 17373, 286: 18186, 287: 2044, 288: 5901, 289: 7052, 290: 3427, 291: 15844, 292: 20258, 293: 3995, 294: 14650, 295: 24344, 296: 3870, 297: 2702, 298: 3889, 299: 8996, 300: 5422, 301: 1882, 302: 9496, 303: 6864, 304: 7927, 305: 8102, 306: 16925, 307: 3666, 308: 13951, 309: 5205, 310: 2252, 311: 8022, 312: 9012, 313: 10061, 314: 13198, 315: 13988, 316: 2051, 317: 5656, 318: 9410, 319: 17057, 320: 2017, 321: 7954, 322: 11102, 323: 13826, 324: 2177, 325: 5920, 326: 6658, 327: 1053, 328: 8402, 329: 9819, 330: 2520, 331: 6216, 332: 17104, 333: 19025, 334: 5818, 335: 6640, 336: 4864, 337: 1413, 338: 2352, 339: 5446, 340: 10985, 341: 11766, 342: 4474, 343: 10585, 344: 2269, 345: 6115, 346: 6582, 347: 10215, 348: 12036, 349: 13989, 350: 14540, 351: 446, 352: 2289, 353: 6071, 354: 6977, 355: 5165, 356: 6060, 357: 13122, 358: 2935, 359: 13109, 360: 15136, 361: 4099, 362: 9219, 363: 13427, 364: 17770, 365: 2245, 366: 6390, 367: 8223, 368: 11998, 369: 19130, 370: 1803},
                                    'strand': {0: '+', 1: '+', 2: '-', 3: '-', 4: '+', 5: '-', 6: '-', 7: '-', 8: '-', 9: '-', 10: '+', 11: '-', 12: '+', 13: '-', 14: '+', 15: '-', 16: '+', 17: '+', 18: '+', 19: '-', 20: '-', 21: '+', 22: '-', 23: '+', 24: '+', 25: '-', 26: '+', 27: '-', 28: '+', 29: '+', 30: '-', 31: '-', 32: '+', 33: '-', 34: '-', 35: '-', 36: '-', 37: '+', 38: '-', 39: '-', 40: '-', 41: '-', 42: '+', 43: '-', 44: '+', 45: '-', 46: '+', 47: '+', 48: '-', 49: '-', 50: '+', 51: '+', 52: '+', 53: '+', 54: '-', 55: '-', 56: '+', 57: '-', 58: '+', 59: '-', 60: '-', 61: '+', 62: '+', 63: '-', 64: '-', 65: '+', 66: '+', 67: '-', 68: '+', 69: '+', 70: '-', 71: '-', 72: '-', 73: '+', 74: '+', 75: '-', 76: '+', 77: '+', 78: '+', 79: '+', 80: '+', 81: '-', 82: '-', 83: '+', 84: '-', 85: '+', 86: '-', 87: '+', 88: '+', 89: '-', 90: '+', 91: '+', 92: '-', 93: '+', 94: '-', 95: '+', 96: '+', 97: '+', 98: '-', 99: '-', 100: '-', 101: '-', 102: '+', 103: '+', 104: '-', 105: '+', 106: '-', 107: '+', 108: '-', 109: '-', 110: '+', 111: '-', 112: '-', 113: '+', 114: '-', 115: '+', 116: '-', 117: '-', 118: '+', 119: '+', 120: '+', 121: '+', 122: '-', 123: '+', 124: '+', 125: '-', 126: '+', 127: '-', 128: '-', 129: '+', 130: '+', 131: '+', 132: '-', 133: '-', 134: '+', 135: '+', 136: '+', 137: '+', 138: '-', 139: '-', 140: '-', 141: '-', 142: '-', 143: '+', 144: '-', 145: '+', 146: '+', 147: '-', 148: '+', 149: '-', 150: '+', 151: '+', 152: '-', 153: '-', 154: '+', 155: '-', 156: '-', 157: '+', 158: '-', 159: '-', 160: '+', 161: '-', 162: '-', 163: '+', 164: '-', 165: '+', 166: '-', 167: '-', 168: '-', 169: '+', 170: '+', 171: '-', 172: '+', 173: '-', 174: '+', 175: '+', 176: '-', 177: '+', 178: '-', 179: '+', 180: '-', 181: '+', 182: '-', 183: '+', 184: '-', 185: '+', 186: '-', 187: '+', 188: '-', 189: '+', 190: '+', 191: '+', 192: '-', 193: '-', 194: '+', 195: '-', 196: '+', 197: '+', 198: '+', 199: '-', 200: '-', 201: '-', 202: '-', 203: '-', 204: '+', 205: '+', 206: '+', 207: '+', 208: '+', 209: '-', 210: '+', 211: '+', 212: '+', 213: '+', 214: '-', 215: '-', 216: '+', 217: '-', 218: '-', 219: '+', 220: '+', 221: '+', 222: '-', 223: '+', 224: '+', 225: '+', 226: '-', 227: '+', 228: '-', 229: '+', 230: '-', 231: '+', 232: '-', 233: '-', 234: '-', 235: '-', 236: '+', 237: '-', 238: '-', 239: '+', 240: '+', 241: '+', 242: '-', 243: '-', 244: '+', 245: '+', 246: '-', 247: '+', 248: '-', 249: '+', 250: '-', 251: '+', 252: '+', 253: '+', 254: '-', 255: '-', 256: '-', 257: '+', 258: '-', 259: '-', 260: '+', 261: '-', 262: '-', 263: '+', 264: '-', 265: '+', 266: '-', 267: '+', 268: '-', 269: '+', 270: '+', 271: '+', 272: '-', 273: '+', 274: '+', 275: '-', 276: '+', 277: '-', 278: '-', 279: '-', 280: '+', 281: '-', 282: '-', 283: '-', 284: '+', 285: '+', 286: '-', 287: '+', 288: '-', 289: '+', 290: '+', 291: '-', 292: '-', 293: '-', 294: '+', 295: '+', 296: '+', 297: '+', 298: '-', 299: '-', 300: '-', 301: '+', 302: '+', 303: '+', 304: '-', 305: '+', 306: '+', 307: '-', 308: '+', 309: '-', 310: '+', 311: '+', 312: '-', 313: '-', 314: '+', 315: '-', 316: '+', 317: '+', 318: '-', 319: '+', 320: '+', 321: '+', 322: '-', 323: '+', 324: '+', 325: '-', 326: '+', 327: '-', 328: '+', 329: '-', 330: '+', 331: '-', 332: '-', 333: '+', 334: '+', 335: '-', 336: '-', 337: '-', 338: '-', 339: '+', 340: '+', 341: '-', 342: '+', 343: '+', 344: '+', 345: '-', 346: '+', 347: '-', 348: '+', 349: '-', 350: '-', 351: '+', 352: '+', 353: '-', 354: '+', 355: '-', 356: '-', 357: '+', 358: '+', 359: '-', 360: '+', 361: '+', 362: '+', 363: '+', 364: '-', 365: '+', 366: '+', 367: '+', 368: '-', 369: '-', 370: '+'},
                                    't_name': {0: '113187', 1: '62', 2: '116409_chunk518-7476', 3: '62', 4: '126066', 5: '113187', 6: '193185', 7: '113187', 8: '62', 9: '193185', 10: '126066', 11: '46775', 12: '120644', 13: '62', 14: '62', 15: '120644', 16: '106509', 17: '90200', 18: '120644', 19: '62', 20: '113187', 21: '120644', 22: '62', 23: '100650', 24: '62', 25: '120644', 26: '136506_chunk1-19319', 27: '126066', 28: '88581', 29: '126066', 30: '116409_chunk518-7476', 31: '106509', 32: '120644', 33: '62', 34: '46775', 35: '88581', 36: '62', 37: '72362_chunk1-459', 38: '88581', 39: '62', 40: '62', 41: '62', 42: '126066', 43: '113187', 44: '120644', 45: '62', 46: '62', 47: '193185', 48: '62', 49: '116409_chunk518-7476', 50: '62', 51: '113187', 52: '62', 53: '141637', 54: '135645', 55: '52019', 56: '119590_chunk28005-46789', 57: '62', 58: '62', 59: '120644', 60: '126066', 61: '52019', 62: '120644', 63: '62', 64: '126066', 65: '141637', 66: '62', 67: '88581', 68: '126066', 69: '120644', 70: '62', 71: '73072_chunk755-4031', 72: '126066', 73: '62', 74: '110', 75: '62', 76: '62', 77: '62', 78: '62', 79: '106509', 80: '126066', 81: '113187', 82: '193185', 83: '126066', 84: '62', 85: '46775', 86: '126066', 87: '62', 88: '120644', 89: '62', 90: '72294_chunk1-460', 91: '113187', 92: '126066', 93: '116409_chunk518-7476', 94: '126066', 95: '62', 96: '113187', 97: '62', 98: '88581', 99: '113187', 100: '113187', 101: '62', 102: '126066', 103: '120644', 104: '62', 105: '62', 106: '120644', 107: '113187', 108: '129595_chunk26242-36910', 109: '62', 110: '126066', 111: '62', 112: '100650', 113: '128322', 114: '113187', 115: '126066', 116: '88581', 117: '62', 118: '116409_chunk518-7476', 119: '124280', 120: '62', 121: '62', 122: '120644', 123: '113187', 124: '88581', 125: '120644', 126: '116409_chunk518-7476', 127: '126066', 128: '47716', 129: '53942', 130: '62', 131: '62', 132: '110', 133: '151986_chunk1-3945', 134: '91532_chunk1-718', 135: '62', 136: '142717', 137: '62', 138: '120644', 139: '126066', 140: '88581', 141: '106509', 142: '62', 143: '62', 144: '62', 145: '62', 146: '113187', 147: '46775', 148: '120644', 149: '62', 150: '62', 151: '62', 152: '120644', 153: '126066', 154: '110', 155: '62', 156: '113187', 157: '120644', 158: '62', 159: '62', 160: '126066', 161: '113187', 162: '100305', 163: '62', 164: '120644', 165: '116409_chunk518-7476', 166: '127891_chunk752-7116', 167: '126066', 168: '85632', 169: '62', 170: '110', 171: '62', 172: '110', 173: '62', 174: '153478_chunk1-39114', 175: '120644', 176: '62', 177: '62', 178: '120644', 179: '46775', 180: '62', 181: '62', 182: '120644', 183: '113187', 184: '126066', 185: '126066', 186: '62', 187: '62', 188: '110', 189: '96815', 190: '62', 191: '62', 192: '120644', 193: '126066', 194: '149809', 195: '128124_chunk80425-175029', 196: '62', 197: '62', 198: '62', 199: '120644', 200: '126066', 201: '62', 202: '62', 203: '151836', 204: '110', 205: '87453', 206: '62', 207: '47573', 208: '120644', 209: '62', 210: '193185', 211: '62', 212: '125087', 213: '126066', 214: '46775', 215: '113187', 216: '120644', 217: '62', 218: '62', 219: '62', 220: '62', 221: '62', 222: '120644', 223: '46775', 224: '88581', 225: '62', 226: '126066', 227: '141637', 228: '85632', 229: '62', 230: '120644', 231: '113739', 232: '126066', 233: '62', 234: '62', 235: '113187', 236: '62', 237: '120644', 238: '126066', 239: '23078', 240: '62328', 241: '116409_chunk518-7476', 242: '63858', 243: '126066', 244: '62', 245: '62', 246: '120644', 247: '119590_chunk28005-46789', 248: '126066', 249: '62', 250: '120644', 251: '46775', 252: '126066', 253: '102159_chunk498-3471', 254: '62', 255: '113187', 256: '62', 257: '62', 258: '120644', 259: '62', 260: '62', 261: '120644', 262: '72378_chunk1-560', 263: '62', 264: '62', 265: '120644', 266: '50316', 267: '62', 268: '88581', 269: '126066', 270: '71219', 271: '120644', 272: '62', 273: '128723_chunk15414-46675', 274: '93811', 275: '62', 276: '126066', 277: '113187', 278: '62328', 279: '100650', 280: '62', 281: '126066', 282: '73072_chunk755-4031', 283: '141637', 284: '126066', 285: '120644', 286: '62', 287: '62', 288: '120644', 289: '46775', 290: '62', 291: '110', 292: '143837', 293: '126066', 294: '113187', 295: '62', 296: '62', 297: '126066', 298: '116409_chunk518-7476', 299: '62', 300: '62', 301: '62', 302: '113187', 303: '110', 304: '62', 305: '141637', 306: '62', 307: '126066', 308: '62', 309: '62', 310: '62', 311: '62', 312: '62', 313: '106509', 314: '120644', 315: '62', 316: '153717', 317: '131102_chunk1-7815', 318: '120644', 319: '62', 320: '62', 321: '113187', 322: '126066', 323: '62703', 324: '62', 325: '120644', 326: '137920_chunk1-2036', 327: '62', 328: '110', 329: '62', 330: '113187', 331: '126066', 332: '62', 333: '47677', 334: '120644', 335: '62', 336: '62', 337: '101995_chunk16643-56970', 338: '100650', 339: '126066', 340: '120644', 341: '62', 342: '88581', 343: '62', 344: '62', 345: '120644', 346: '100645', 347: '126066', 348: '116409_chunk518-7476', 349: '126066', 350: '90238', 351: '34445', 352: '62', 353: '120644', 354: '136506_chunk1-19319', 355: '62', 356: '62', 357: '120644', 358: '113187', 359: '126066', 360: '62', 361: '46775', 362: '124280', 363: '62', 364: '120644', 365: '113187', 366: '88581', 367: '62', 368: '120644', 369: '126066', 370: '62'},
                                    't_len': {0: 6607, 1: 5512, 2: 6959, 3: 5512, 4: 6855, 5: 6607, 6: 3190, 7: 6607, 8: 5512, 9: 3190, 10: 6855, 11: 2125, 12: 4954, 13: 5512, 14: 5512, 15: 4954, 16: 6347, 17: 1505, 18: 4954, 19: 5512, 20: 6607, 21: 4954, 22: 5512, 23: 1687, 24: 5512, 25: 4954, 26: 19319, 27: 6855, 28: 3996, 29: 6855, 30: 6959, 31: 6347, 32: 4954, 33: 5512, 34: 2125, 35: 3996, 36: 5512, 37: 459, 38: 3996, 39: 5512, 40: 5512, 41: 5512, 42: 6855, 43: 6607, 44: 4954, 45: 5512, 46: 5512, 47: 3190, 48: 5512, 49: 6959, 50: 5512, 51: 6607, 52: 5512, 53: 7421, 54: 2885, 55: 1647, 56: 18785, 57: 5512, 58: 5512, 59: 4954, 60: 6855, 61: 1647, 62: 4954, 63: 5512, 64: 6855, 65: 7421, 66: 5512, 67: 3996, 68: 6855, 69: 4954, 70: 5512, 71: 3277, 72: 6855, 73: 5512, 74: 13963, 75: 5512, 76: 5512, 77: 5512, 78: 5512, 79: 6347, 80: 6855, 81: 6607, 82: 3190, 83: 6855, 84: 5512, 85: 2125, 86: 6855, 87: 5512, 88: 4954, 89: 5512, 90: 460, 91: 6607, 92: 6855, 93: 6959, 94: 6855, 95: 5512, 96: 6607, 97: 5512, 98: 3996, 99: 6607, 100: 6607, 101: 5512, 102: 6855, 103: 4954, 104: 5512, 105: 5512, 106: 4954, 107: 6607, 108: 10669, 109: 5512, 110: 6855, 111: 5512, 112: 1687, 113: 47092, 114: 6607, 115: 6855, 116: 3996, 117: 5512, 118: 6959, 119: 5114, 120: 5512, 121: 5512, 122: 4954, 123: 6607, 124: 3996, 125: 4954, 126: 6959, 127: 6855, 128: 672, 129: 1291, 130: 5512, 131: 5512, 132: 13963, 133: 3945, 134: 718, 135: 5512, 136: 10469, 137: 5512, 138: 4954, 139: 6855, 140: 3996, 141: 6347, 142: 5512, 143: 5512, 144: 5512, 145: 5512, 146: 6607, 147: 2125, 148: 4954, 149: 5512, 150: 5512, 151: 5512, 152: 4954, 153: 6855, 154: 13963, 155: 5512, 156: 6607, 157: 4954, 158: 5512, 159: 5512, 160: 6855, 161: 6607, 162: 2136, 163: 5512, 164: 4954, 165: 6959, 166: 6365, 167: 6855, 168: 887, 169: 5512, 170: 13963, 171: 5512, 172: 13963, 173: 5512, 174: 39114, 175: 4954, 176: 5512, 177: 5512, 178: 4954, 179: 2125, 180: 5512, 181: 5512, 182: 4954, 183: 6607, 184: 6855, 185: 6855, 186: 5512, 187: 5512, 188: 13963, 189: 920, 190: 5512, 191: 5512, 192: 4954, 193: 6855, 194: 7027, 195: 94605, 196: 5512, 197: 5512, 198: 5512, 199: 4954, 200: 6855, 201: 5512, 202: 5512, 203: 7369, 204: 13963, 205: 2988, 206: 5512, 207: 902, 208: 4954, 209: 5512, 210: 3190, 211: 5512, 212: 46041, 213: 6855, 214: 2125, 215: 6607, 216: 4954, 217: 5512, 218: 5512, 219: 5512, 220: 5512, 221: 5512, 222: 4954, 223: 2125, 224: 3996, 225: 5512, 226: 6855, 227: 7421, 228: 887, 229: 5512, 230: 4954, 231: 9063, 232: 6855, 233: 5512, 234: 5512, 235: 6607, 236: 5512, 237: 4954, 238: 6855, 239: 635, 240: 1772, 241: 6959, 242: 1002, 243: 6855, 244: 5512, 245: 5512, 246: 4954, 247: 18785, 248: 6855, 249: 5512, 250: 4954, 251: 2125, 252: 6855, 253: 2974, 254: 5512, 255: 6607, 256: 5512, 257: 5512, 258: 4954, 259: 5512, 260: 5512, 261: 4954, 262: 560, 263: 5512, 264: 5512, 265: 4954, 266: 4425, 267: 5512, 268: 3996, 269: 6855, 270: 2707, 271: 4954, 272: 5512, 273: 31262, 274: 2572, 275: 5512, 276: 6855, 277: 6607, 278: 1772, 279: 1687, 280: 5512, 281: 6855, 282: 3277, 283: 7421, 284: 6855, 285: 4954, 286: 5512, 287: 5512, 288: 4954, 289: 2125, 290: 5512, 291: 13963, 292: 6269, 293: 6855, 294: 6607, 295: 5512, 296: 5512, 297: 6855, 298: 6959, 299: 5512, 300: 5512, 301: 5512, 302: 6607, 303: 13963, 304: 5512, 305: 7421, 306: 5512, 307: 6855, 308: 5512, 309: 5512, 310: 5512, 311: 5512, 312: 5512, 313: 6347, 314: 4954, 315: 5512, 316: 26558, 317: 7815, 318: 4954, 319: 5512, 320: 5512, 321: 6607, 322: 6855, 323: 1031, 324: 5512, 325: 4954, 326: 2036, 327: 5512, 328: 13963, 329: 5512, 330: 6607, 331: 6855, 332: 5512, 333: 1141, 334: 4954, 335: 5512, 336: 5512, 337: 40328, 338: 1687, 339: 6855, 340: 4954, 341: 5512, 342: 3996, 343: 5512, 344: 5512, 345: 4954, 346: 915, 347: 6855, 348: 6959, 349: 6855, 350: 717, 351: 872, 352: 5512, 353: 4954, 354: 19319, 355: 5512, 356: 5512, 357: 4954, 358: 6607, 359: 6855, 360: 5512, 361: 2125, 362: 5114, 363: 5512, 364: 4954, 365: 6607, 366: 3996, 367: 5512, 368: 4954, 369: 6855, 370: 5512},
                                    't_start': {0: 3361, 1: 1, 2: 604, 3: 12, 4: 4036, 5: 3922, 6: 0, 7: 3361, 8: 3670, 9: 0, 10: 3807, 11: 0, 12: 0, 13: 3640, 14: 3703, 15: 0, 16: 2, 17: 0, 18: 0, 19: 3648, 20: 3485, 21: 4, 22: 3619, 23: 942, 24: 3651, 25: 3, 26: 789, 27: 1722, 28: 1, 29: 4299, 30: 0, 31: 0, 32: 0, 33: 3539, 34: 516, 35: 0, 36: 3594, 37: 129, 38: 6, 39: 3623, 40: 1477, 41: 534, 42: 2797, 43: 3484, 44: 3, 45: 3527, 46: 3669, 47: 0, 48: 11, 49: 1458, 50: 3542, 51: 3473, 52: 3349, 53: 0, 54: 0, 55: 0, 56: 16062, 57: 82, 58: 3578, 59: 0, 60: 2467, 61: 436, 62: 3, 63: 3626, 64: 1722, 65: 4, 66: 1, 67: 0, 68: 2024, 69: 0, 70: 3578, 71: 0, 72: 3837, 73: 0, 74: 8098, 75: 2615, 76: 0, 77: 1775, 78: 3518, 79: 2, 80: 3774, 81: 2633, 82: 0, 83: 1736, 84: 3514, 85: 0, 86: 1567, 87: 7, 88: 2, 89: 3648, 90: 203, 91: 3431, 92: 3884, 93: 0, 94: 0, 95: 3, 96: 3749, 97: 2, 98: 32, 99: 2638, 100: 3484, 101: 3489, 102: 4127, 103: 3, 104: 3627, 105: 3570, 106: 0, 107: 3473, 108: 7364, 109: 1, 110: 0, 111: 3, 112: 709, 113: 270, 114: 2637, 115: 1801, 116: 950, 117: 3628, 118: 1224, 119: 11, 120: 1500, 121: 3643, 122: 0, 123: 3364, 124: 0, 125: 0, 126: 2181, 127: 4529, 128: 1, 129: 30, 130: 947, 131: 2038, 132: 0, 133: 1, 134: 0, 135: 3334, 136: 0, 137: 3494, 138: 3, 139: 3636, 140: 0, 141: 0, 142: 3666, 143: 3705, 144: 3553, 145: 3551, 146: 3473, 147: 4, 148: 0, 149: 3664, 150: 0, 151: 3634, 152: 10, 153: 4222, 154: 5665, 155: 2582, 156: 3473, 157: 1, 158: 3663, 159: 0, 160: 1765, 161: 2573, 162: 735, 163: 3656, 164: 0, 165: 605, 166: 90, 167: 6, 168: 16, 169: 26, 170: 7742, 171: 2578, 172: 6071, 173: 2545, 174: 18108, 175: 1, 176: 3606, 177: 3528, 178: 0, 179: 0, 180: 3497, 181: 3585, 182: 7, 183: 3483, 184: 2665, 185: 4017, 186: 3315, 187: 2508, 188: 5505, 189: 0, 190: 4, 191: 3566, 192: 3, 193: 0, 194: 13, 195: 54218, 196: 3604, 197: 3707, 198: 3420, 199: 2, 200: 4238, 201: 3475, 202: 2302, 203: 453, 204: 4072, 205: 0, 206: 2533, 207: 398, 208: 19, 209: 3564, 210: 0, 211: 0, 212: 33470, 213: 4359, 214: 445, 215: 3496, 216: 7, 217: 3648, 218: 5, 219: 3537, 220: 819, 221: 3615, 222: 6, 223: 0, 224: 2, 225: 3511, 226: 1, 227: 0, 228: 122, 229: 3651, 230: 0, 231: 0, 232: 2023, 233: 286, 234: 0, 235: 5351, 236: 3424, 237: 0, 238: 1746, 239: 0, 240: 396, 241: 0, 242: 8, 243: 0, 244: 2, 245: 3558, 246: 2, 247: 17055, 248: 3883, 249: 3629, 250: 0, 251: 5, 252: 4059, 253: 627, 254: 3571, 255: 3477, 256: 3628, 257: 2544, 258: 0, 259: 4, 260: 3543, 261: 0, 262: 190, 263: 21, 264: 4, 265: 0, 266: 3462, 267: 3535, 268: 0, 269: 3943, 270: 6, 271: 0, 272: 3452, 273: 19107, 274: 1105, 275: 20, 276: 1, 277: 3477, 278: 53, 279: 51, 280: 3473, 281: 1817, 282: 26, 283: 0, 284: 0, 285: 0, 286: 3783, 287: 3534, 288: 3, 289: 3, 290: 2146, 291: 2, 292: 1092, 293: 1729, 294: 2595, 295: 1, 296: 3243, 297: 4306, 298: 617, 299: 3545, 300: 17, 301: 3602, 302: 3421, 303: 6800, 304: 2627, 305: 0, 306: 6, 307: 3999, 308: 7, 309: 6, 310: 3535, 311: 0, 312: 2568, 313: 5, 314: 0, 315: 3560, 316: 24769, 317: 0, 318: 0, 319: 0, 320: 3430, 321: 3473, 322: 1988, 323: 3, 324: 3336, 325: 0, 326: 5, 327: 1234, 328: 5944, 329: 2327, 330: 3689, 331: 3716, 332: 1140, 333: 25, 334: 1, 335: 3582, 336: 0, 337: 7599, 338: 46, 339: 3513, 340: 3, 341: 3601, 342: 0, 343: 0, 344: 3394, 345: 3, 346: 30, 347: 4048, 348: 0, 349: 3883, 350: 242, 351: 356, 352: 3648, 353: 0, 354: 797, 355: 0, 356: 4, 357: 0, 358: 3429, 359: 8, 360: 1, 361: 1492, 362: 0, 363: 3402, 364: 5, 365: 3805, 366: 8, 367: 3651, 368: 1, 369: 1794, 370: 3659},
                                    't_end': {0: 6469, 1: 4993, 2: 6953, 3: 5500, 4: 6855, 5: 5944, 6: 2942, 7: 6469, 8: 5511, 9: 3153, 10: 6852, 11: 2122, 12: 4954, 13: 5510, 14: 5512, 15: 4953, 16: 5072, 17: 879, 18: 4954, 19: 5510, 20: 6281, 21: 4954, 22: 5512, 23: 1687, 24: 5512, 25: 4954, 26: 4969, 27: 6855, 28: 3996, 29: 6851, 30: 3273, 31: 5071, 32: 4949, 33: 5512, 34: 2125, 35: 3995, 36: 5512, 37: 452, 38: 3984, 39: 5509, 40: 3320, 41: 3578, 42: 6855, 43: 5940, 44: 4954, 45: 5508, 46: 5512, 47: 3185, 48: 4934, 49: 6953, 50: 5507, 51: 4807, 52: 5507, 53: 7421, 54: 2876, 55: 1647, 56: 17708, 57: 1748, 58: 5512, 59: 4954, 60: 6855, 61: 1647, 62: 4952, 63: 5512, 64: 6855, 65: 7421, 66: 5143, 67: 3985, 68: 6855, 69: 4950, 70: 5511, 71: 2272, 72: 6855, 73: 5163, 74: 13963, 75: 5509, 76: 4903, 77: 3958, 78: 5512, 79: 4063, 80: 6855, 81: 6148, 82: 3179, 83: 6490, 84: 5512, 85: 2125, 86: 6855, 87: 5152, 88: 4950, 89: 5512, 90: 425, 91: 6015, 92: 6855, 93: 3152, 94: 6855, 95: 5160, 96: 6523, 97: 5181, 98: 3996, 99: 6208, 100: 6032, 101: 5512, 102: 6507, 103: 4954, 104: 5512, 105: 5512, 106: 4951, 107: 5943, 108: 7861, 109: 4851, 110: 6795, 111: 4974, 112: 1687, 113: 7865, 114: 5939, 115: 6855, 116: 3985, 117: 5509, 118: 2710, 119: 1262, 120: 5506, 121: 5512, 122: 4952, 123: 6137, 124: 3996, 125: 1761, 126: 6959, 127: 6854, 128: 665, 129: 1280, 130: 1668, 131: 5497, 132: 13963, 133: 3902, 134: 576, 135: 5512, 136: 3877, 137: 5503, 138: 4953, 139: 6855, 140: 3984, 141: 5073, 142: 5512, 143: 5511, 144: 5512, 145: 5512, 146: 6451, 147: 1901, 148: 4947, 149: 5510, 150: 5173, 151: 5511, 152: 4947, 153: 6852, 154: 13963, 155: 5508, 156: 6523, 157: 4954, 158: 5512, 159: 4956, 160: 6836, 161: 6315, 162: 2118, 163: 5510, 164: 4954, 165: 6951, 166: 6365, 167: 4869, 168: 877, 169: 5321, 170: 13961, 171: 5512, 172: 13963, 173: 5512, 174: 28978, 175: 4951, 176: 5505, 177: 5505, 178: 4954, 179: 2115, 180: 5512, 181: 5512, 182: 4952, 183: 6021, 184: 6855, 185: 6843, 186: 5512, 187: 5512, 188: 13963, 189: 918, 190: 1041, 191: 5512, 192: 4954, 193: 4927, 194: 7024, 195: 56676, 196: 5512, 197: 5512, 198: 5512, 199: 4954, 200: 6507, 201: 5506, 202: 5234, 203: 3869, 204: 13958, 205: 2646, 206: 5512, 207: 896, 208: 4953, 209: 5512, 210: 3190, 211: 5225, 212: 35462, 213: 6854, 214: 2125, 215: 5955, 216: 4950, 217: 5509, 218: 5128, 219: 5512, 220: 2065, 221: 5512, 222: 4952, 223: 2125, 224: 3143, 225: 5508, 226: 6505, 227: 7421, 228: 887, 229: 5512, 230: 4951, 231: 4000, 232: 6851, 233: 3915, 234: 4347, 235: 6595, 236: 5505, 237: 4954, 238: 6855, 239: 540, 240: 1772, 241: 6959, 242: 1002, 243: 6832, 244: 5271, 245: 5504, 246: 4944, 247: 18782, 248: 6855, 249: 5503, 250: 4954, 251: 1876, 252: 6855, 253: 2972, 254: 5509, 255: 5979, 256: 5496, 257: 5485, 258: 4940, 259: 5158, 260: 5510, 261: 4952, 262: 535, 263: 950, 264: 4921, 265: 2572, 266: 4423, 267: 5487, 268: 3142, 269: 6820, 270: 2707, 271: 4946, 272: 5490, 273: 23216, 274: 2568, 275: 5512, 276: 6855, 277: 5907, 278: 1772, 279: 1239, 280: 5511, 281: 6848, 282: 3261, 283: 7371, 284: 6852, 285: 4865, 286: 5509, 287: 5508, 288: 4950, 289: 2113, 290: 5512, 291: 13963, 292: 6267, 293: 5933, 294: 6218, 295: 4987, 296: 5512, 297: 6855, 298: 3146, 299: 5512, 300: 5191, 301: 5512, 302: 6315, 303: 13958, 304: 5512, 305: 7421, 306: 5400, 307: 6855, 308: 5153, 309: 4953, 310: 5512, 311: 4958, 312: 5512, 313: 5037, 314: 4951, 315: 5507, 316: 26558, 317: 3261, 318: 3433, 319: 5188, 320: 5512, 321: 6247, 322: 6855, 323: 888, 324: 5512, 325: 4951, 326: 1815, 327: 2065, 328: 13963, 329: 5512, 330: 5983, 331: 6849, 332: 5507, 333: 706, 334: 4951, 335: 5512, 336: 4925, 337: 9008, 338: 879, 339: 6855, 340: 4951, 341: 5512, 342: 3996, 343: 4995, 344: 5512, 345: 4945, 346: 779, 347: 6557, 348: 3145, 349: 6818, 350: 682, 351: 783, 352: 5512, 353: 4937, 354: 3263, 355: 5125, 356: 5512, 357: 4544, 358: 5934, 359: 6855, 360: 5512, 361: 2120, 362: 3108, 363: 5510, 364: 4818, 365: 5955, 366: 3840, 367: 5512, 368: 4953, 369: 6855, 370: 5512},
                                    'matches': {0: 2669, 1: 4575, 2: 5637, 3: 5231, 4: 2590, 5: 1886, 6: 2491, 7: 2623, 8: 1690, 9: 2679, 10: 2592, 11: 1838, 12: 4460, 13: 1687, 14: 1658, 15: 4555, 16: 4049, 17: 781, 18: 4542, 19: 1695, 20: 2339, 21: 4502, 22: 1717, 23: 666, 24: 1730, 25: 4572, 26: 3435, 27: 4113, 28: 3430, 29: 2170, 30: 2732, 31: 3982, 32: 4465, 33: 1750, 34: 1329, 35: 3376, 36: 1715, 37: 289, 38: 3340, 39: 1670, 40: 1680, 41: 2567, 42: 3456, 43: 2120, 44: 4476, 45: 1800, 46: 1676, 47: 2745, 48: 4349, 49: 4778, 50: 1813, 51: 1161, 52: 1960, 53: 6865, 54: 2649, 55: 1494, 56: 1416, 57: 1516, 58: 1769, 59: 4565, 60: 3560, 61: 1056, 62: 4473, 63: 1704, 64: 4107, 65: 6833, 66: 4728, 67: 3423, 68: 3807, 69: 4538, 70: 1769, 71: 1946, 72: 2601, 73: 4655, 74: 5270, 75: 2593, 76: 4582, 77: 2047, 78: 1793, 79: 3341, 80: 2622, 81: 2786, 82: 2693, 83: 3646, 84: 1806, 85: 1899, 86: 4235, 87: 4643, 88: 4446, 89: 1706, 90: 196, 91: 2180, 92: 2563, 93: 2681, 94: 6057, 95: 4654, 96: 2351, 97: 4728, 98: 3403, 99: 2857, 100: 2224, 101: 1834, 102: 2031, 103: 4422, 104: 1689, 105: 1773, 106: 4542, 107: 2181, 108: 460, 109: 4396, 110: 5986, 111: 4520, 112: 844, 113: 6941, 114: 2638, 115: 3919, 116: 2510, 117: 1687, 118: 1248, 119: 1106, 120: 3369, 121: 1688, 122: 4460, 123: 2376, 124: 3450, 125: 1587, 126: 4318, 127: 2045, 128: 584, 129: 1084, 130: 662, 131: 3132, 132: 12793, 133: 3579, 134: 514, 135: 1954, 136: 3113, 137: 1834, 138: 4499, 139: 2777, 140: 3405, 141: 3940, 142: 1683, 143: 1655, 144: 1806, 145: 1799, 146: 2573, 147: 1661, 148: 4536, 149: 1694, 150: 4759, 151: 1711, 152: 4544, 153: 2307, 154: 7447, 155: 2626, 156: 2517, 157: 4413, 158: 1649, 159: 4497, 160: 3999, 161: 3006, 162: 1167, 163: 1681, 164: 4551, 165: 5654, 166: 5761, 167: 4080, 168: 811, 169: 5008, 170: 5567, 171: 2597, 172: 7144, 173: 2677, 174: 10033, 175: 4483, 176: 1699, 177: 1843, 178: 4550, 179: 1867, 180: 1834, 181: 1754, 182: 4524, 183: 2230, 184: 3356, 185: 2446, 186: 2012, 187: 2711, 188: 7647, 189: 830, 190: 925, 191: 1754, 192: 4484, 193: 4145, 194: 6316, 195: 2221, 196: 1607, 197: 1677, 198: 1890, 199: 4505, 200: 1984, 201: 1865, 202: 2730, 203: 3146, 204: 8909, 205: 2402, 206: 2714, 207: 431, 208: 4544, 209: 1797, 210: 2721, 211: 4774, 212: 1850, 213: 2155, 214: 1424, 215: 2152, 216: 4492, 217: 1698, 218: 4611, 219: 1766, 220: 897, 221: 1717, 222: 4504, 223: 1867, 224: 2709, 225: 1806, 226: 5823, 227: 6800, 228: 723, 229: 1725, 230: 4586, 231: 3304, 232: 3901, 233: 3270, 234: 3927, 235: 1092, 236: 1922, 237: 4563, 238: 4094, 239: 482, 240: 1187, 241: 6132, 242: 879, 243: 6017, 244: 4705, 245: 1728, 246: 4432, 247: 1472, 248: 2557, 249: 1676, 250: 4502, 251: 1653, 252: 2398, 253: 1957, 254: 1755, 255: 2172, 256: 1690, 257: 2665, 258: 4503, 259: 4632, 260: 1764, 261: 4499, 262: 302, 263: 848, 264: 4423, 265: 2320, 266: 895, 267: 1734, 268: 2681, 269: 2475, 270: 2305, 271: 4243, 272: 1824, 273: 3587, 274: 1219, 275: 4807, 276: 5842, 277: 2069, 278: 1448, 279: 1015, 280: 1786, 281: 3964, 282: 2801, 283: 6670, 284: 6034, 285: 4359, 286: 1563, 287: 1789, 288: 4507, 289: 1859, 290: 3027, 291: 12741, 292: 4735, 293: 3254, 294: 2909, 295: 4583, 296: 2013, 297: 2216, 298: 2144, 299: 1785, 300: 4678, 301: 1709, 302: 2494, 303: 6295, 304: 2542, 305: 6609, 306: 4829, 307: 2483, 308: 4657, 309: 4397, 310: 1808, 311: 4561, 312: 2671, 313: 4106, 314: 4460, 315: 1746, 316: 1647, 317: 3016, 318: 3121, 319: 4729, 320: 1875, 321: 2373, 322: 3880, 323: 780, 324: 1870, 325: 4445, 326: 1471, 327: 732, 328: 7117, 329: 2836, 330: 2016, 331: 2782, 332: 4020, 333: 572, 334: 4452, 335: 1742, 336: 4464, 337: 1265, 338: 691, 339: 2804, 340: 4454, 341: 1735, 342: 3422, 343: 4533, 344: 1911, 345: 4535, 346: 673, 347: 2191, 348: 2712, 349: 2579, 350: 390, 351: 376, 352: 1673, 353: 4471, 354: 2093, 355: 4658, 356: 4784, 357: 4083, 358: 2272, 359: 6321, 360: 5170, 361: 565, 362: 2825, 363: 1886, 364: 4417, 365: 1853, 366: 3274, 367: 1700, 368: 4509, 369: 4028, 370: 1649},
                                    'alignment_length': {0: 3201, 1: 5314, 2: 6514, 3: 7121, 4: 3428, 5: 2477, 6: 3051, 7: 3220, 8: 1891, 9: 3279, 10: 3156, 11: 2202, 12: 5154, 13: 1962, 14: 1899, 15: 5164, 16: 5255, 17: 911, 18: 5087, 19: 1907, 20: 2842, 21: 5085, 22: 1931, 23: 786, 24: 1953, 25: 5123, 26: 4395, 27: 5273, 28: 4385, 29: 2607, 30: 3380, 31: 5218, 32: 5065, 33: 2013, 34: 1641, 35: 4373, 36: 1953, 37: 325, 38: 4355, 39: 1943, 40: 1882, 41: 3198, 42: 4172, 43: 2520, 44: 5066, 45: 2014, 46: 1883, 47: 3316, 48: 5028, 49: 5677, 50: 2063, 51: 1396, 52: 2262, 53: 7763, 54: 2987, 55: 1774, 56: 1693, 57: 1786, 58: 1991, 59: 5153, 60: 4573, 61: 1253, 62: 5156, 63: 1962, 64: 5388, 65: 7718, 66: 5325, 67: 4357, 68: 5007, 69: 5092, 70: 1988, 71: 2395, 72: 3171, 73: 5388, 74: 6094, 75: 2999, 76: 5115, 77: 2463, 78: 2068, 79: 4282, 80: 3169, 81: 3610, 82: 3265, 83: 4901, 84: 2057, 85: 2182, 86: 5526, 87: 5359, 88: 5034, 89: 1911, 90: 224, 91: 2705, 92: 3199, 93: 3378, 94: 7332, 95: 5540, 96: 2892, 97: 5333, 98: 4452, 99: 3765, 100: 2700, 101: 2119, 102: 2453, 103: 5076, 104: 1931, 105: 2032, 106: 5142, 107: 2585, 108: 519, 109: 5013, 110: 6994, 111: 5099, 112: 1042, 113: 7877, 114: 3424, 115: 5214, 116: 3369, 117: 1929, 118: 1572, 119: 1414, 120: 4773, 121: 1916, 122: 5134, 123: 2895, 124: 4425, 125: 1822, 126: 4938, 127: 2421, 128: 721, 129: 1323, 130: 790, 131: 3573, 132: 14639, 133: 4060, 134: 596, 135: 2260, 136: 4015, 137: 2073, 138: 5145, 139: 3341, 140: 4396, 141: 5255, 142: 1911, 143: 1849, 144: 2023, 145: 1996, 146: 3031, 147: 1933, 148: 5088, 149: 1902, 150: 5375, 151: 1938, 152: 5074, 153: 2727, 154: 8557, 155: 3015, 156: 3143, 157: 5097, 158: 1892, 159: 5098, 160: 5198, 161: 3823, 162: 1449, 163: 1877, 164: 5075, 165: 6586, 166: 6464, 167: 5077, 168: 973, 169: 6225, 170: 6418, 171: 3029, 172: 8212, 173: 3075, 174: 11396, 175: 5117, 176: 1968, 177: 2034, 178: 5101, 179: 2173, 180: 2083, 181: 2009, 182: 5129, 183: 2636, 184: 4319, 185: 3176, 186: 2451, 187: 3106, 188: 8871, 189: 1016, 190: 1152, 191: 2018, 192: 5186, 193: 5135, 194: 7245, 195: 2842, 196: 2006, 197: 1855, 198: 2184, 199: 5114, 200: 2340, 201: 2210, 202: 3906, 203: 3973, 204: 10255, 205: 2760, 206: 3065, 207: 526, 208: 5381, 209: 2148, 210: 3471, 211: 5903, 212: 2071, 213: 2634, 214: 1771, 215: 2593, 216: 5135, 217: 1931, 218: 5536, 219: 2177, 220: 1305, 221: 1968, 222: 5177, 223: 2255, 224: 3280, 225: 2119, 226: 6756, 227: 7667, 228: 795, 229: 1972, 230: 5245, 231: 4235, 232: 5034, 233: 3761, 234: 5117, 235: 1386, 236: 2153, 237: 5119, 238: 5312, 239: 571, 240: 1440, 241: 7321, 242: 1033, 243: 7148, 244: 5488, 245: 2088, 246: 5351, 247: 1808, 248: 3064, 249: 1977, 250: 5262, 251: 2015, 252: 2902, 253: 2421, 254: 2006, 255: 2571, 256: 1981, 257: 3281, 258: 5381, 259: 5335, 260: 2221, 261: 5290, 262: 368, 263: 1240, 264: 5032, 265: 2627, 266: 1011, 267: 2043, 268: 3404, 269: 3063, 270: 2896, 271: 5609, 272: 2203, 273: 4392, 274: 1628, 275: 6320, 276: 7738, 277: 2669, 278: 1934, 279: 1326, 280: 2074, 281: 5273, 282: 3364, 283: 8374, 284: 7054, 285: 5063, 286: 1951, 287: 2057, 288: 5210, 289: 2251, 290: 3571, 291: 14816, 292: 5378, 293: 4386, 294: 3744, 295: 5137, 296: 2354, 297: 2743, 298: 2595, 299: 2053, 300: 5631, 301: 1979, 302: 2991, 303: 7351, 304: 2983, 305: 7595, 306: 5503, 307: 2945, 308: 5389, 309: 5419, 310: 2255, 311: 5271, 312: 3083, 313: 5197, 314: 5085, 315: 2002, 316: 2100, 317: 3591, 318: 3670, 319: 5418, 320: 2130, 321: 2874, 322: 5086, 323: 923, 324: 2252, 325: 5133, 326: 1901, 327: 864, 328: 8756, 329: 3335, 330: 2572, 331: 3476, 332: 5441, 333: 760, 334: 5180, 335: 2036, 336: 5123, 337: 1461, 338: 856, 339: 3467, 340: 5107, 341: 2000, 342: 4468, 343: 5220, 344: 2213, 345: 5153, 346: 789, 347: 2789, 348: 3507, 349: 3440, 350: 501, 351: 442, 352: 1939, 353: 5123, 354: 2565, 355: 5369, 356: 6161, 357: 4902, 358: 2914, 359: 8127, 360: 6540, 361: 749, 362: 4085, 363: 2687, 364: 5663, 365: 2325, 366: 4541, 367: 1925, 368: 5135, 369: 5291, 370: 1905},
                                    'mapq': {0: 60, 1: 60, 2: 60, 3: 60, 4: 55, 5: 49, 6: 60, 7: 60, 8: 60, 9: 29, 10: 60, 11: 36, 12: 56, 13: 60, 14: 60, 15: 22, 16: 60, 17: 47, 18: 60, 19: 60, 20: 60, 21: 32, 22: 53, 23: 31, 24: 40, 25: 26, 26: 60, 27: 41, 28: 60, 29: 60, 30: 60, 31: 60, 32: 27, 33: 60, 34: 40, 35: 20, 36: 50, 37: 21, 38: 60, 39: 60, 40: 29, 41: 60, 42: 32, 43: 51, 44: 31, 45: 49, 46: 60, 47: 60, 48: 60, 49: 60, 50: 60, 51: 49, 52: 60, 53: 42, 54: 60, 55: 20, 56: 36, 57: 26, 58: 60, 59: 26, 60: 48, 61: 60, 62: 36, 63: 40, 64: 35, 65: 32, 66: 60, 67: 20, 68: 60, 69: 36, 70: 60, 71: 60, 72: 60, 73: 60, 74: 60, 75: 60, 76: 60, 77: 21, 78: 41, 79: 60, 80: 60, 81: 22, 82: 51, 83: 43, 84: 48, 85: 51, 86: 60, 87: 60, 88: 22, 89: 59, 90: 33, 91: 60, 92: 60, 93: 55, 94: 25, 95: 60, 96: 50, 97: 60, 98: 55, 99: 43, 100: 33, 101: 44, 102: 60, 103: 60, 104: 35, 105: 33, 106: 31, 107: 60, 108: 60, 109: 60, 110: 60, 111: 60, 112: 60, 113: 60, 114: 46, 115: 48, 116: 41, 117: 44, 118: 60, 119: 60, 120: 60, 121: 31, 122: 28, 123: 33, 124: 42, 125: 60, 126: 60, 127: 60, 128: 25, 129: 48, 130: 24, 131: 60, 132: 60, 133: 60, 134: 52, 135: 60, 136: 40, 137: 60, 138: 20, 139: 60, 140: 55, 141: 60, 142: 60, 143: 56, 144: 60, 145: 56, 146: 60, 147: 60, 148: 38, 149: 37, 150: 60, 151: 60, 152: 43, 153: 58, 154: 60, 155: 60, 156: 60, 157: 23, 158: 60, 159: 60, 160: 60, 161: 60, 162: 45, 163: 60, 164: 43, 165: 60, 166: 60, 167: 60, 168: 37, 169: 60, 170: 30, 171: 60, 172: 60, 173: 60, 174: 60, 175: 43, 176: 60, 177: 58, 178: 22, 179: 59, 180: 60, 181: 44, 182: 51, 183: 54, 184: 60, 185: 60, 186: 60, 187: 60, 188: 60, 189: 41, 190: 23, 191: 35, 192: 45, 193: 60, 194: 54, 195: 60, 196: 60, 197: 60, 198: 60, 199: 34, 200: 60, 201: 60, 202: 26, 203: 33, 204: 60, 205: 60, 206: 44, 207: 60, 208: 30, 209: 60, 210: 60, 211: 60, 212: 60, 213: 60, 214: 53, 215: 43, 216: 60, 217: 44, 218: 60, 219: 60, 220: 25, 221: 36, 222: 49, 223: 42, 224: 60, 225: 60, 226: 60, 227: 21, 228: 35, 229: 56, 230: 22, 231: 60, 232: 60, 233: 25, 234: 60, 235: 60, 236: 36, 237: 22, 238: 60, 239: 60, 240: 53, 241: 60, 242: 60, 243: 60, 244: 60, 245: 52, 246: 60, 247: 60, 248: 60, 249: 60, 250: 44, 251: 60, 252: 60, 253: 22, 254: 57, 255: 42, 256: 58, 257: 41, 258: 20, 259: 60, 260: 60, 261: 50, 262: 29, 263: 42, 264: 60, 265: 60, 266: 60, 267: 48, 268: 60, 269: 60, 270: 60, 271: 44, 272: 49, 273: 60, 274: 45, 275: 30, 276: 60, 277: 60, 278: 60, 279: 60, 280: 49, 281: 40, 282: 23, 283: 60, 284: 31, 285: 60, 286: 60, 287: 34, 288: 43, 289: 60, 290: 49, 291: 60, 292: 60, 293: 60, 294: 60, 295: 60, 296: 59, 297: 60, 298: 25, 299: 60, 300: 60, 301: 50, 302: 41, 303: 60, 304: 60, 305: 43, 306: 36, 307: 60, 308: 60, 309: 60, 310: 39, 311: 60, 312: 60, 313: 60, 314: 30, 315: 60, 316: 60, 317: 60, 318: 47, 319: 60, 320: 59, 321: 21, 322: 46, 323: 30, 324: 60, 325: 45, 326: 27, 327: 49, 328: 60, 329: 54, 330: 60, 331: 60, 332: 60, 333: 28, 334: 53, 335: 60, 336: 60, 337: 60, 338: 60, 339: 60, 340: 22, 341: 60, 342: 60, 343: 60, 344: 60, 345: 48, 346: 60, 347: 60, 348: 60, 349: 60, 350: 60, 351: 54, 352: 28, 353: 24, 354: 60, 355: 60, 356: 60, 357: 40, 358: 60, 359: 60, 360: 60, 361: 46, 362: 60, 363: 53, 364: 60, 365: 40, 366: 60, 367: 60, 368: 21, 369: 22, 370: 60},
                                    't_id': {0: 117314, 1: 62, 2: 121439, 3: 62, 4: 134905, 5: 117314, 6: 219181, 7: 117314, 8: 62, 9: 219181, 10: 134905, 11: 46631, 12: 127151, 13: 62, 14: 62, 15: 127151, 16: 109187, 17: 91328, 18: 127151, 19: 62, 20: 117314, 21: 127151, 22: 62, 23: 102575, 24: 62, 25: 127151, 26: 150816, 27: 134905, 28: 89700, 29: 134905, 30: 121439, 31: 109187, 32: 127151, 33: 62, 34: 46631, 35: 89700, 36: 62, 37: 72579, 38: 89700, 39: 62, 40: 62, 41: 62, 42: 134905, 43: 117314, 44: 127151, 45: 62, 46: 62, 47: 219181, 48: 62, 49: 121439, 50: 62, 51: 117314, 52: 62, 53: 159248, 54: 149483, 55: 51875, 56: 125798, 57: 62, 58: 62, 59: 127151, 60: 134905, 61: 51875, 62: 127151, 63: 62, 64: 134905, 65: 159248, 66: 62, 67: 89700, 68: 134905, 69: 127151, 70: 62, 71: 74007, 72: 134905, 73: 62, 74: 145, 75: 62, 76: 62, 77: 62, 78: 62, 79: 109187, 80: 134905, 81: 117314, 82: 219181, 83: 134905, 84: 62, 85: 46631, 86: 134905, 87: 62, 88: 127151, 89: 62, 90: 72443, 91: 117314, 92: 134905, 93: 121439, 94: 134905, 95: 62, 96: 117314, 97: 62, 98: 89700, 99: 117314, 100: 117314, 101: 62, 102: 134905, 103: 127151, 104: 62, 105: 62, 106: 127151, 107: 117314, 108: 140134, 109: 62, 110: 134905, 111: 62, 112: 102575, 113: 138216, 114: 117314, 115: 134905, 116: 89700, 117: 62, 118: 121439, 119: 132399, 120: 62, 121: 62, 122: 127151, 123: 117314, 124: 89700, 125: 127151, 126: 121439, 127: 134905, 128: 47572, 129: 53798, 130: 62, 131: 62, 132: 145, 133: 176386, 134: 92880, 135: 62, 136: 161194, 137: 62, 138: 127151, 139: 134905, 140: 89700, 141: 109187, 142: 62, 143: 62, 144: 62, 145: 62, 146: 117314, 147: 46631, 148: 127151, 149: 62, 150: 62, 151: 62, 152: 127151, 153: 134905, 154: 145, 155: 62, 156: 117314, 157: 127151, 158: 62, 159: 62, 160: 134905, 161: 117314, 162: 102221, 163: 62, 164: 127151, 165: 121439, 166: 137568, 167: 134905, 168: 86745, 169: 62, 170: 145, 171: 62, 172: 145, 173: 62, 174: 178795, 175: 127151, 176: 62, 177: 62, 178: 127151, 179: 46631, 180: 62, 181: 62, 182: 127151, 183: 117314, 184: 134905, 185: 134905, 186: 62, 187: 62, 188: 145, 189: 98693, 190: 62, 191: 62, 192: 127151, 193: 134905, 194: 172850, 195: 137903, 196: 62, 197: 62, 198: 62, 199: 127151, 200: 134905, 201: 62, 202: 62, 203: 176139, 204: 145, 205: 88566, 206: 62, 207: 47429, 208: 127151, 209: 62, 210: 219181, 211: 62, 212: 133518, 213: 134905, 214: 46631, 215: 117314, 216: 127151, 217: 62, 218: 62, 219: 62, 220: 62, 221: 62, 222: 127151, 223: 46631, 224: 89700, 225: 62, 226: 134905, 227: 159248, 228: 86745, 229: 62, 230: 127151, 231: 118122, 232: 134905, 233: 62, 234: 62, 235: 117314, 236: 62, 237: 127151, 238: 134905, 239: 22934, 240: 62184, 241: 121439, 242: 63714, 243: 134905, 244: 62, 245: 62, 246: 127151, 247: 125798, 248: 134905, 249: 62, 250: 127151, 251: 46631, 252: 134905, 253: 104585, 254: 62, 255: 117314, 256: 62, 257: 62, 258: 127151, 259: 62, 260: 62, 261: 127151, 262: 72611, 263: 62, 264: 62, 265: 127151, 266: 50172, 267: 62, 268: 89700, 269: 134905, 270: 71075, 271: 127151, 272: 62, 273: 138819, 274: 95685, 275: 62, 276: 134905, 277: 117314, 278: 62184, 279: 102575, 280: 62, 281: 134905, 282: 74007, 283: 159248, 284: 134905, 285: 127151, 286: 62, 287: 62, 288: 127151, 289: 46631, 290: 62, 291: 145, 292: 163002, 293: 134905, 294: 117314, 295: 62, 296: 62, 297: 134905, 298: 121439, 299: 62, 300: 62, 301: 62, 302: 117314, 303: 145, 304: 62, 305: 159248, 306: 62, 307: 134905, 308: 62, 309: 62, 310: 62, 311: 62, 312: 62, 313: 109187, 314: 127151, 315: 62, 316: 179181, 317: 142406, 318: 127151, 319: 62, 320: 62, 321: 117314, 322: 134905, 323: 62559, 324: 62, 325: 127151, 326: 153129, 327: 62, 328: 145, 329: 62, 330: 117314, 331: 134905, 332: 62, 333: 47533, 334: 127151, 335: 62, 336: 62, 337: 104355, 338: 102575, 339: 134905, 340: 127151, 341: 62, 342: 89700, 343: 62, 344: 62, 345: 127151, 346: 102569, 347: 134905, 348: 121439, 349: 134905, 350: 91366, 351: 34301, 352: 62, 353: 127151, 354: 150816, 355: 62, 356: 62, 357: 127151, 358: 117314, 359: 134905, 360: 62, 361: 46631, 362: 132399, 363: 62, 364: 127151, 365: 117314, 366: 89700, 367: 62, 368: 127151, 369: 134905, 370: 62},
                                    'next_con': {0: 62, 1: -1, 2: 62, 3: 134905, 4: 117314, 5: -1, 6: 117314, 7: 62, 8: -1, 9: 134905, 10: 46631, 11: 127151, 12: 62, 13: -1, 14: 127151, 15: 109187, 16: 91328, 17: -1, 18: 62, 19: -1, 20: 127151, 21: 62, 22: -1, 23: 62, 24: 127151, 25: 150816, 26: 134905, 27: 89700, 28: -1, 29: 121439, 30: 109187, 31: 127151, 32: 62, 33: -1, 34: 89700, 35: 62, 36: -1, 37: 89700, 38: 62, 39: -1, 40: -1, 41: -1, 42: 117314, 43: 127151, 44: 62, 45: -1, 46: 219181, 47: -1, 48: 121439, 49: -1, 50: 117314, 51: -1, 52: 159248, 53: 149483, 54: 51875, 55: 125798, 56: -1, 57: -1, 58: 127151, 59: 134905, 60: -1, 61: 127151, 62: 62, 63: -1, 64: 159248, 65: 62, 66: -1, 67: 134905, 68: 127151, 69: 62, 70: -1, 71: 134905, 72: 62, 73: -1, 74: 62, 75: -1, 76: -1, 77: -1, 78: 109187, 79: -1, 80: 117314, 81: 219181, 82: 134905, 83: 62, 84: -1, 85: 134905, 86: 62, 87: -1, 88: 62, 89: 72443, 90: -1, 91: 134905, 92: 121439, 93: 134905, 94: 62, 95: -1, 96: 62, 97: -1, 98: 117314, 99: 117314, 100: 62, 101: -1, 102: 127151, 103: 62, 104: -1, 105: 127151, 106: 117314, 107: -1, 108: 62, 109: 134905, 110: -1, 111: 102575, 112: -1, 113: 117314, 114: 134905, 115: 89700, 116: 62, 117: -1, 118: 132399, 119: 62, 120: -1, 121: 127151, 122: 117314, 123: 89700, 124: -1, 125: 121439, 126: 134905, 127: 47572, 128: 53798, 129: 62, 130: -1, 131: 145, 132: 176386, 133: 92880, 134: -1, 135: 161194, 136: -1, 137: 127151, 138: 134905, 139: -1, 140: 109187, 141: 62, 142: -1, 143: -1, 144: -1, 145: 117314, 146: -1, 147: 127151, 148: 62, 149: -1, 150: -1, 151: 127151, 152: 134905, 153: -1, 154: 62, 155: -1, 156: 127151, 157: 62, 158: -1, 159: 134905, 160: 117314, 161: 102221, 162: -1, 163: 127151, 164: 121439, 165: 137568, 166: -1, 167: 86745, 168: 62, 169: -1, 170: 62, 171: -1, 172: 62, 173: 178795, 174: -1, 175: 62, 176: -1, 177: 127151, 178: 46631, 179: -1, 180: -1, 181: 127151, 182: 117314, 183: 134905, 184: -1, 185: 62, 186: -1, 187: 145, 188: -1, 189: 62, 190: -1, 191: 127151, 192: 134905, 193: 172850, 194: -1, 195: 62, 196: -1, 197: -1, 198: 127151, 199: 134905, 200: -1, 201: -1, 202: 176139, 203: -1, 204: 88566, 205: 62, 206: -1, 207: 127151, 208: 62, 209: -1, 210: 62, 211: 133518, 212: -1, 213: 46631, 214: 117314, 215: 127151, 216: 62, 217: -1, 218: -1, 219: -1, 220: -1, 221: 127151, 222: 46631, 223: 89700, 224: -1, 225: 134905, 226: 159248, 227: 86745, 228: -1, 229: 127151, 230: 118122, 231: 134905, 232: -1, 233: -1, 234: 117314, 235: -1, 236: 127151, 237: 134905, 238: 22934, 239: -1, 240: 121439, 241: 63714, 242: 134905, 243: 62, 244: -1, 245: 127151, 246: 125798, 247: 134905, 248: -1, 249: 127151, 250: 46631, 251: -1, 252: 104585, 253: 62, 254: -1, 255: 62, 256: -1, 257: 127151, 258: -1, 259: -1, 260: 127151, 261: 72611, 262: -1, 263: -1, 264: 127151, 265: -1, 266: 62, 267: -1, 268: 134905, 269: 71075, 270: 127151, 271: 62, 272: -1, 273: 95685, 274: 62, 275: 134905, 276: 117314, 277: 62184, 278: 102575, 279: -1, 280: 134905, 281: 74007, 282: -1, 283: 134905, 284: 127151, 285: 62, 286: -1, 287: 127151, 288: 46631, 289: -1, 290: 145, 291: 163002, 292: -1, 293: 117314, 294: 62, 295: -1, 296: -1, 297: 121439, 298: 62, 299: -1, 300: -1, 301: 117314, 302: -1, 303: 62, 304: -1, 305: 62, 306: -1, 307: 62, 308: -1, 309: -1, 310: -1, 311: -1, 312: -1, 313: 127151, 314: 62, 315: -1, 316: 142406, 317: 127151, 318: 62, 319: -1, 320: 117314, 321: 134905, 322: 62559, 323: -1, 324: 127151, 325: 153129, 326: -1, 327: -1, 328: 62, 329: -1, 330: 134905, 331: 62, 332: 47533, 333: -1, 334: 62, 335: -1, 336: -1, 337: 102575, 338: 134905, 339: 127151, 340: 62, 341: -1, 342: 62, 343: -1, 344: 127151, 345: 102569, 346: 134905, 347: 121439, 348: 134905, 349: 91366, 350: -1, 351: 62, 352: 127151, 353: 150816, 354: -1, 355: -1, 356: 127151, 357: -1, 358: 134905, 359: 62, 360: -1, 361: 132399, 362: 62, 363: 127151, 364: -1, 365: 89700, 366: 62, 367: 127151, 368: 134905, 369: -1, 370: -1},
                                    'next_strand': {0: '+', 1: '', 2: '-', 3: '+', 4: '-', 5: '', 6: '-', 7: '-', 8: '', 9: '+', 10: '-', 11: '+', 12: '-', 13: '', 14: '-', 15: '+', 16: '+', 17: '', 18: '-', 19: '', 20: '+', 21: '-', 22: '', 23: '+', 24: '-', 25: '+', 26: '-', 27: '+', 28: '', 29: '-', 30: '-', 31: '+', 32: '-', 33: '', 34: '-', 35: '-', 36: '', 37: '-', 38: '-', 39: '', 40: '', 41: '', 42: '-', 43: '+', 44: '-', 45: '', 46: '+', 47: '', 48: '-', 49: '', 50: '+', 51: '', 52: '+', 53: '-', 54: '-', 55: '+', 56: '', 57: '', 58: '-', 59: '-', 60: '', 61: '+', 62: '-', 63: '', 64: '+', 65: '+', 66: '', 67: '+', 68: '+', 69: '-', 70: '', 71: '-', 72: '+', 73: '', 74: '-', 75: '', 76: '', 77: '', 78: '+', 79: '', 80: '-', 81: '-', 82: '+', 83: '-', 84: '', 85: '-', 86: '+', 87: '', 88: '-', 89: '+', 90: '', 91: '-', 92: '+', 93: '-', 94: '+', 95: '', 96: '+', 97: '', 98: '-', 99: '-', 100: '-', 101: '', 102: '+', 103: '-', 104: '', 105: '-', 106: '+', 107: '', 108: '-', 109: '+', 110: '', 111: '-', 112: '', 113: '-', 114: '+', 115: '-', 116: '-', 117: '', 118: '+', 119: '+', 120: '', 121: '-', 122: '+', 123: '+', 124: '', 125: '+', 126: '-', 127: '-', 128: '+', 129: '+', 130: '', 131: '-', 132: '-', 133: '+', 134: '', 135: '+', 136: '', 137: '-', 138: '-', 139: '', 140: '-', 141: '-', 142: '', 143: '', 144: '', 145: '+', 146: '', 147: '+', 148: '-', 149: '', 150: '', 151: '-', 152: '-', 153: '', 154: '-', 155: '', 156: '+', 157: '-', 158: '', 159: '+', 160: '-', 161: '-', 162: '', 163: '-', 164: '+', 165: '-', 166: '', 167: '-', 168: '+', 169: '', 170: '-', 171: '', 172: '-', 173: '+', 174: '', 175: '-', 176: '', 177: '-', 178: '+', 179: '', 180: '', 181: '-', 182: '+', 183: '-', 184: '', 185: '-', 186: '', 187: '-', 188: '', 189: '+', 190: '', 191: '-', 192: '-', 193: '+', 194: '', 195: '+', 196: '', 197: '', 198: '-', 199: '-', 200: '', 201: '', 202: '-', 203: '', 204: '+', 205: '+', 206: '', 207: '+', 208: '-', 209: '', 210: '+', 211: '+', 212: '', 213: '-', 214: '-', 215: '+', 216: '-', 217: '', 218: '', 219: '', 220: '', 221: '-', 222: '+', 223: '+', 224: '', 225: '-', 226: '+', 227: '-', 228: '', 229: '-', 230: '+', 231: '-', 232: '', 233: '', 234: '-', 235: '', 236: '-', 237: '-', 238: '+', 239: '', 240: '+', 241: '-', 242: '-', 243: '+', 244: '', 245: '-', 246: '+', 247: '-', 248: '', 249: '-', 250: '+', 251: '', 252: '+', 253: '-', 254: '', 255: '-', 256: '', 257: '-', 258: '', 259: '', 260: '-', 261: '-', 262: '', 263: '', 264: '+', 265: '', 266: '+', 267: '', 268: '+', 269: '+', 270: '+', 271: '-', 272: '', 273: '+', 274: '-', 275: '+', 276: '-', 277: '-', 278: '-', 279: '', 280: '-', 281: '-', 282: '', 283: '+', 284: '+', 285: '-', 286: '', 287: '-', 288: '+', 289: '', 290: '-', 291: '-', 292: '', 293: '+', 294: '+', 295: '', 296: '', 297: '-', 298: '-', 299: '', 300: '', 301: '+', 302: '', 303: '-', 304: '', 305: '+', 306: '', 307: '+', 308: '', 309: '', 310: '', 311: '', 312: '', 313: '+', 314: '-', 315: '', 316: '+', 317: '-', 318: '+', 319: '', 320: '+', 321: '-', 322: '+', 323: '', 324: '-', 325: '+', 326: '', 327: '', 328: '-', 329: '', 330: '-', 331: '-', 332: '+', 333: '', 334: '-', 335: '', 336: '', 337: '-', 338: '+', 339: '+', 340: '-', 341: '', 342: '+', 343: '', 344: '-', 345: '+', 346: '-', 347: '+', 348: '-', 349: '-', 350: '', 351: '+', 352: '-', 353: '+', 354: '', 355: '', 356: '+', 357: '', 358: '-', 359: '+', 360: '', 361: '+', 362: '+', 363: '-', 364: '', 365: '+', 366: '+', 367: '-', 368: '-', 369: '', 370: ''},
                                    'prev_con': {0: -1, 1: 117314, 2: -1, 3: 121439, 4: 62, 5: 134905, 6: -1, 7: 219181, 8: 117314, 9: -1, 10: 219181, 11: 134905, 12: 46631, 13: 127151, 14: -1, 15: 62, 16: 127151, 17: 109187, 18: -1, 19: 127151, 20: -1, 21: 117314, 22: 127151, 23: -1, 24: 102575, 25: 62, 26: 127151, 27: 150816, 28: 134905, 29: -1, 30: 134905, 31: 121439, 32: 109187, 33: 127151, 34: -1, 35: 46631, 36: 89700, 37: -1, 38: 72579, 39: 89700, 40: -1, 41: -1, 42: -1, 43: 134905, 44: 117314, 45: 127151, 46: -1, 47: 62, 48: -1, 49: 62, 50: -1, 51: 62, 52: -1, 53: 62, 54: 159248, 55: 149483, 56: 51875, 57: -1, 58: -1, 59: 62, 60: 127151, 61: -1, 62: 51875, 63: 127151, 64: -1, 65: 134905, 66: 159248, 67: -1, 68: 89700, 69: 134905, 70: 127151, 71: -1, 72: 74007, 73: 134905, 74: -1, 75: 145, 76: -1, 77: -1, 78: -1, 79: 62, 80: -1, 81: 134905, 82: 117314, 83: 219181, 84: 134905, 85: -1, 86: 46631, 87: 134905, 88: -1, 89: 127151, 90: 62, 91: -1, 92: 117314, 93: 134905, 94: 121439, 95: 134905, 96: -1, 97: 117314, 98: -1, 99: 89700, 100: 117314, 101: 117314, 102: -1, 103: 134905, 104: 127151, 105: -1, 106: 62, 107: 127151, 108: -1, 109: 140134, 110: 62, 111: -1, 112: 62, 113: -1, 114: 138216, 115: 117314, 116: 134905, 117: 89700, 118: -1, 119: 121439, 120: 132399, 121: -1, 122: 62, 123: 127151, 124: 117314, 125: -1, 126: 127151, 127: 121439, 128: 134905, 129: 47572, 130: 53798, 131: -1, 132: 62, 133: 145, 134: 176386, 135: -1, 136: 62, 137: -1, 138: 62, 139: 127151, 140: -1, 141: 89700, 142: 109187, 143: -1, 144: -1, 145: -1, 146: 62, 147: -1, 148: 46631, 149: 127151, 150: -1, 151: -1, 152: 62, 153: 127151, 154: -1, 155: 145, 156: -1, 157: 117314, 158: 127151, 159: -1, 160: 62, 161: 134905, 162: 117314, 163: -1, 164: 62, 165: 127151, 166: 121439, 167: -1, 168: 134905, 169: 86745, 170: -1, 171: 145, 172: -1, 173: 145, 174: 62, 175: -1, 176: 127151, 177: -1, 178: 62, 179: 127151, 180: -1, 181: -1, 182: 62, 183: 127151, 184: 117314, 185: -1, 186: 134905, 187: -1, 188: 62, 189: -1, 190: 98693, 191: -1, 192: 62, 193: 127151, 194: 134905, 195: -1, 196: 137903, 197: -1, 198: -1, 199: 62, 200: 127151, 201: -1, 202: -1, 203: 62, 204: -1, 205: 145, 206: 88566, 207: -1, 208: 47429, 209: 127151, 210: -1, 211: 219181, 212: 62, 213: -1, 214: 134905, 215: 46631, 216: 117314, 217: 127151, 218: -1, 219: -1, 220: -1, 221: -1, 222: 62, 223: 127151, 224: 46631, 225: -1, 226: 62, 227: 134905, 228: 159248, 229: -1, 230: 62, 231: 127151, 232: 118122, 233: -1, 234: -1, 235: 62, 236: -1, 237: 62, 238: 127151, 239: 134905, 240: -1, 241: 62184, 242: 121439, 243: 63714, 244: 134905, 245: -1, 246: 62, 247: 127151, 248: 125798, 249: -1, 250: 62, 251: 127151, 252: -1, 253: 134905, 254: 104585, 255: -1, 256: 117314, 257: -1, 258: 62, 259: -1, 260: -1, 261: 62, 262: 127151, 263: -1, 264: -1, 265: 62, 266: -1, 267: 50172, 268: -1, 269: 89700, 270: 134905, 271: 71075, 272: 127151, 273: -1, 274: 138819, 275: 95685, 276: 62, 277: 134905, 278: 117314, 279: 62184, 280: -1, 281: 62, 282: 134905, 283: -1, 284: 159248, 285: 134905, 286: 127151, 287: -1, 288: 62, 289: 127151, 290: -1, 291: 62, 292: 145, 293: -1, 294: 134905, 295: 117314, 296: -1, 297: -1, 298: 134905, 299: 121439, 300: -1, 301: -1, 302: 62, 303: -1, 304: 145, 305: -1, 306: 159248, 307: -1, 308: 134905, 309: -1, 310: -1, 311: -1, 312: -1, 313: -1, 314: 109187, 315: 127151, 316: -1, 317: 179181, 318: 142406, 319: 127151, 320: -1, 321: 62, 322: 117314, 323: 134905, 324: -1, 325: 62, 326: 127151, 327: -1, 328: -1, 329: 145, 330: -1, 331: 117314, 332: 134905, 333: 62, 334: -1, 335: 127151, 336: -1, 337: -1, 338: 104355, 339: 102575, 340: 134905, 341: 127151, 342: -1, 343: 89700, 344: -1, 345: 62, 346: 127151, 347: 102569, 348: 134905, 349: 121439, 350: 134905, 351: -1, 352: 34301, 353: 62, 354: 127151, 355: -1, 356: -1, 357: 62, 358: -1, 359: 117314, 360: 134905, 361: -1, 362: 46631, 363: 132399, 364: 62, 365: -1, 366: 117314, 367: 89700, 368: 62, 369: 127151, 370: -1},
                                    'prev_strand': {0: '', 1: '+', 2: '', 3: '-', 4: '-', 5: '+', 6: '', 7: '-', 8: '-', 9: '', 10: '-', 11: '+', 12: '-', 13: '+', 14: '', 15: '+', 16: '-', 17: '+', 18: '', 19: '+', 20: '', 21: '-', 22: '+', 23: '', 24: '+', 25: '+', 26: '-', 27: '+', 28: '-', 29: '', 30: '+', 31: '-', 32: '-', 33: '+', 34: '', 35: '-', 36: '-', 37: '', 38: '+', 39: '-', 40: '', 41: '', 42: '', 43: '+', 44: '-', 45: '+', 46: '', 47: '+', 48: '', 49: '-', 50: '', 51: '+', 52: '', 53: '+', 54: '+', 55: '-', 56: '-', 57: '', 58: '', 59: '+', 60: '-', 61: '', 62: '+', 63: '+', 64: '', 65: '-', 66: '+', 67: '', 68: '-', 69: '+', 70: '+', 71: '', 72: '-', 73: '-', 74: '', 75: '+', 76: '', 77: '', 78: '', 79: '+', 80: '', 81: '+', 82: '-', 83: '-', 84: '+', 85: '', 86: '+', 87: '-', 88: '', 89: '+', 90: '-', 91: '', 92: '+', 93: '-', 94: '+', 95: '-', 96: '', 97: '+', 98: '', 99: '-', 100: '-', 101: '-', 102: '', 103: '+', 104: '+', 105: '', 106: '+', 107: '-', 108: '', 109: '-', 110: '-', 111: '', 112: '-', 113: '', 114: '+', 115: '-', 116: '+', 117: '-', 118: '', 119: '+', 120: '+', 121: '', 122: '+', 123: '-', 124: '+', 125: '', 126: '-', 127: '+', 128: '-', 129: '-', 130: '+', 131: '', 132: '+', 133: '-', 134: '-', 135: '', 136: '+', 137: '', 138: '+', 139: '-', 140: '', 141: '-', 142: '-', 143: '', 144: '', 145: '', 146: '+', 147: '', 148: '-', 149: '+', 150: '', 151: '', 152: '+', 153: '-', 154: '', 155: '+', 156: '', 157: '-', 158: '+', 159: '', 160: '-', 161: '+', 162: '-', 163: '', 164: '+', 165: '-', 166: '+', 167: '', 168: '-', 169: '-', 170: '', 171: '+', 172: '', 173: '+', 174: '-', 175: '', 176: '+', 177: '', 178: '+', 179: '-', 180: '', 181: '', 182: '+', 183: '-', 184: '+', 185: '', 186: '+', 187: '', 188: '+', 189: '', 190: '+', 191: '', 192: '+', 193: '-', 194: '-', 195: '', 196: '-', 197: '', 198: '', 199: '+', 200: '-', 201: '', 202: '', 203: '-', 204: '', 205: '+', 206: '+', 207: '', 208: '+', 209: '+', 210: '', 211: '+', 212: '+', 213: '', 214: '+', 215: '-', 216: '-', 217: '+', 218: '', 219: '', 220: '', 221: '', 222: '+', 223: '-', 224: '+', 225: '', 226: '+', 227: '-', 228: '+', 229: '', 230: '+', 231: '-', 232: '+', 233: '', 234: '', 235: '-', 236: '', 237: '+', 238: '-', 239: '-', 240: '', 241: '+', 242: '+', 243: '-', 244: '-', 245: '', 246: '+', 247: '-', 248: '+', 249: '', 250: '+', 251: '-', 252: '', 253: '+', 254: '+', 255: '', 256: '-', 257: '', 258: '+', 259: '', 260: '', 261: '+', 262: '-', 263: '', 264: '', 265: '-', 266: '', 267: '-', 268: '', 269: '-', 270: '+', 271: '+', 272: '+', 273: '', 274: '+', 275: '+', 276: '-', 277: '+', 278: '-', 279: '-', 280: '', 281: '+', 282: '-', 283: '', 284: '-', 285: '+', 286: '+', 287: '', 288: '+', 289: '-', 290: '', 291: '+', 292: '-', 293: '', 294: '-', 295: '+', 296: '', 297: '', 298: '+', 299: '-', 300: '', 301: '', 302: '+', 303: '', 304: '+', 305: '', 306: '+', 307: '', 308: '-', 309: '', 310: '', 311: '', 312: '', 313: '', 314: '-', 315: '+', 316: '', 317: '+', 318: '+', 319: '-', 320: '', 321: '+', 322: '+', 323: '-', 324: '', 325: '+', 326: '-', 327: '', 328: '', 329: '+', 330: '', 331: '+', 332: '-', 333: '-', 334: '', 335: '+', 336: '', 337: '', 338: '-', 339: '-', 340: '+', 341: '+', 342: '', 343: '+', 344: '', 345: '+', 346: '-', 347: '+', 348: '-', 349: '+', 350: '-', 351: '', 352: '+', 353: '+', 354: '-', 355: '', 356: '', 357: '-', 358: '', 359: '+', 360: '-', 361: '', 362: '+', 363: '+', 364: '+', 365: '', 366: '+', 367: '+', 368: '+', 369: '-', 370: ''},
                                    'read_start': {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0, 101: 0, 102: 0, 103: 0, 104: 0, 105: 0, 106: 0, 107: 0, 108: 0, 109: 0, 110: 0, 111: 0, 112: 0, 113: 0, 114: 0, 115: 0, 116: 0, 117: 0, 118: 0, 119: 0, 120: 0, 121: 0, 122: 0, 123: 0, 124: 0, 125: 0, 126: 0, 127: 0, 128: 0, 129: 0, 130: 0, 131: 0, 132: 0, 133: 0, 134: 0, 135: 0, 136: 0, 137: 0, 138: 0, 139: 0, 140: 0, 141: 0, 142: 0, 143: 0, 144: 0, 145: 0, 146: 0, 147: 0, 148: 0, 149: 0, 150: 0, 151: 0, 152: 0, 153: 0, 154: 0, 155: 0, 156: 0, 157: 0, 158: 0, 159: 0, 160: 0, 161: 0, 162: 0, 163: 0, 164: 0, 165: 0, 166: 0, 167: 0, 168: 0, 169: 0, 170: 0, 171: 0, 172: 0, 173: 0, 174: 0, 175: 0, 176: 0, 177: 0, 178: 0, 179: 0, 180: 0, 181: 0, 182: 0, 183: 0, 184: 0, 185: 0, 186: 0, 187: 0, 188: 0, 189: 0, 190: 0, 191: 0, 192: 0, 193: 0, 194: 0, 195: 0, 196: 0, 197: 0, 198: 0, 199: 0, 200: 0, 201: 0, 202: 0, 203: 0, 204: 0, 205: 0, 206: 0, 207: 0, 208: 0, 209: 0, 210: 0, 211: 0, 212: 0, 213: 0, 214: 0, 215: 0, 216: 0, 217: 0, 218: 0, 219: 0, 220: 0, 221: 0, 222: 0, 223: 0, 224: 0, 225: 0, 226: 0, 227: 0, 228: 0, 229: 0, 230: 0, 231: 0, 232: 0, 233: 0, 234: 0, 235: 0, 236: 0, 237: 0, 238: 0, 239: 0, 240: 0, 241: 0, 242: 0, 243: 0, 244: 0, 245: 0, 246: 0, 247: 0, 248: 0, 249: 0, 250: 0, 251: 0, 252: 0, 253: 0, 254: 0, 255: 0, 256: 0, 257: 0, 258: 0, 259: 0, 260: 0, 261: 0, 262: 0, 263: 0, 264: 0, 265: 0, 266: 0, 267: 0, 268: 0, 269: 0, 270: 0, 271: 0, 272: 0, 273: 0, 274: 0, 275: 0, 276: 0, 277: 0, 278: 0, 279: 0, 280: 0, 281: 0, 282: 0, 283: 0, 284: 0, 285: 0, 286: 0, 287: 0, 288: 0, 289: 0, 290: 0, 291: 0, 292: 0, 293: 0, 294: 0, 295: 0, 296: 0, 297: 0, 298: 0, 299: 0, 300: 0, 301: 0, 302: 0, 303: 0, 304: 0, 305: 0, 306: 0, 307: 0, 308: 0, 309: 0, 310: 0, 311: 0, 312: 0, 313: 0, 314: 0, 315: 0, 316: 0, 317: 0, 318: 0, 319: 0, 320: 0, 321: 0, 322: 0, 323: 0, 324: 0, 325: 0, 326: 0, 327: 0, 328: 0, 329: 0, 330: 0, 331: 0, 332: 0, 333: 0, 334: 0, 335: 0, 336: 0, 337: 0, 338: 0, 339: 0, 340: 0, 341: 0, 342: 0, 343: 0, 344: 0, 345: 0, 346: 0, 347: 0, 348: 0, 349: 0, 350: 0, 351: 0, 352: 0, 353: 0, 354: 0, 355: 0, 356: 0, 357: 0, 358: 0, 359: 0, 360: 0, 361: 0, 362: 0, 363: 0, 364: 0, 365: 0, 366: 0, 367: 0, 368: 0, 369: 0, 370: 0},
                                    'read_end': {0: 11404, 1: 11404, 2: 27336, 3: 27336, 4: 27336, 5: 27336, 6: 13395, 7: 13395, 8: 13395, 9: 18917, 10: 18917, 11: 18917, 12: 18917, 13: 18917, 14: 9505, 15: 9505, 16: 9505, 17: 9505, 18: 12449, 19: 12449, 20: 10021, 21: 10021, 22: 10021, 23: 30453, 24: 30453, 25: 30453, 26: 30453, 27: 30453, 28: 30453, 29: 11810, 30: 11810, 31: 11810, 32: 11810, 33: 11810, 34: 27428, 35: 27428, 36: 27428, 37: 11993, 38: 11993, 39: 11993, 40: 1779, 41: 6250, 42: 10126, 43: 10126, 44: 10126, 45: 10126, 46: 18947, 47: 18947, 48: 17867, 49: 17867, 50: 6666, 51: 6666, 52: 31659, 53: 31659, 54: 31659, 55: 31659, 56: 31659, 57: 1725, 58: 20228, 59: 20228, 60: 20228, 61: 7297, 62: 7297, 63: 7297, 64: 23620, 65: 23620, 66: 23620, 67: 23358, 68: 23358, 69: 23358, 70: 23358, 71: 11078, 72: 11078, 73: 11078, 74: 6876, 75: 6876, 76: 9437, 77: 2444, 78: 7957, 79: 7957, 80: 14207, 81: 14207, 82: 14207, 83: 14207, 84: 14207, 85: 26791, 86: 26791, 87: 26791, 88: 12660, 89: 12660, 90: 12660, 91: 16828, 92: 16828, 93: 16828, 94: 16828, 95: 16828, 96: 20842, 97: 20842, 98: 16176, 99: 16176, 100: 16176, 101: 16176, 102: 7306, 103: 7306, 104: 7306, 105: 19540, 106: 19540, 107: 19540, 108: 20021, 109: 20021, 110: 20021, 111: 8963, 112: 8963, 113: 26038, 114: 26038, 115: 26038, 116: 26038, 117: 26038, 118: 12683, 119: 12683, 120: 12683, 121: 22728, 122: 22728, 123: 22728, 124: 22728, 125: 28377, 126: 28377, 127: 28377, 128: 28377, 129: 28377, 130: 28377, 131: 17958, 132: 17958, 133: 17958, 134: 17958, 135: 7447, 136: 7447, 137: 9775, 138: 9775, 139: 9775, 140: 18125, 141: 18125, 142: 18125, 143: 10791, 144: 11384, 145: 15236, 146: 15236, 147: 6469, 148: 6469, 149: 6469, 150: 8501, 151: 8902, 152: 8902, 153: 8902, 154: 9231, 155: 9231, 156: 8821, 157: 8821, 158: 8821, 159: 24241, 160: 24241, 161: 24241, 162: 24241, 163: 13842, 164: 13842, 165: 13842, 166: 13842, 167: 20046, 168: 20046, 169: 20046, 170: 7206, 171: 7206, 172: 19967, 173: 19967, 174: 19967, 175: 12493, 176: 12493, 177: 9482, 178: 9482, 179: 9482, 180: 6065, 181: 11029, 182: 11029, 183: 11029, 184: 11029, 185: 14122, 186: 14122, 187: 9697, 188: 9697, 189: 8008, 190: 8008, 191: 16430, 192: 16430, 193: 16430, 194: 16430, 195: 8026, 196: 8026, 197: 14565, 198: 7523, 199: 7523, 200: 7523, 201: 6322, 202: 5681, 203: 5681, 204: 22186, 205: 22186, 206: 22186, 207: 6505, 208: 6505, 209: 6505, 210: 18943, 211: 18943, 212: 18943, 213: 12991, 214: 12991, 215: 12991, 216: 12991, 217: 12991, 218: 13077, 219: 9439, 220: 5305, 221: 9036, 222: 9036, 223: 9036, 224: 9036, 225: 28393, 226: 28393, 227: 28393, 228: 28393, 229: 15420, 230: 15420, 231: 15420, 232: 15420, 233: 3678, 234: 11259, 235: 11259, 236: 14689, 237: 14689, 238: 14689, 239: 14689, 240: 25509, 241: 25509, 242: 25509, 243: 25509, 244: 25509, 245: 9536, 246: 9536, 247: 9536, 248: 9536, 249: 6759, 250: 6759, 251: 6759, 252: 10870, 253: 10870, 254: 10870, 255: 13747, 256: 13747, 257: 9111, 258: 9111, 259: 14513, 260: 6563, 261: 6563, 262: 6563, 263: 2043, 264: 9470, 265: 9470, 266: 7799, 267: 7799, 268: 11576, 269: 11576, 270: 11576, 271: 11576, 272: 11576, 273: 44162, 274: 44162, 275: 44162, 276: 44162, 277: 44162, 278: 44162, 279: 44162, 280: 30972, 281: 30972, 282: 30972, 283: 18229, 284: 18229, 285: 18229, 286: 18229, 287: 7053, 288: 7053, 289: 7053, 290: 20260, 291: 20260, 292: 20260, 293: 24344, 294: 24344, 295: 24344, 296: 15984, 297: 8996, 298: 8996, 299: 8996, 300: 23442, 301: 18135, 302: 18135, 303: 7929, 304: 7929, 305: 17672, 306: 17672, 307: 13957, 308: 13957, 309: 7413, 310: 14928, 311: 8022, 312: 9012, 313: 13988, 314: 13988, 315: 13988, 316: 17066, 317: 17066, 318: 17066, 319: 17066, 320: 13826, 321: 13826, 322: 13826, 323: 13826, 324: 6676, 325: 6676, 326: 6676, 327: 1053, 328: 9820, 329: 9820, 330: 19164, 331: 19164, 332: 19164, 333: 19164, 334: 6640, 335: 6640, 336: 8334, 337: 11842, 338: 11842, 339: 11842, 340: 11842, 341: 11842, 342: 10585, 343: 10585, 344: 14542, 345: 14542, 346: 14542, 347: 14542, 348: 14542, 349: 14542, 350: 14542, 351: 7178, 352: 7178, 353: 7178, 354: 7178, 355: 8874, 356: 13123, 357: 13123, 358: 19756, 359: 19756, 360: 19756, 361: 18462, 362: 18462, 363: 18462, 364: 18462, 365: 19224, 366: 19224, 367: 19224, 368: 19224, 369: 19224, 370: 8203}} ) )
    mappings.append( pd.DataFrame( {'q_name': {0: 'm54120_180612_220526/21234554/0_9150', 1: 'm54120_180612_220526/21234554/0_9150', 2: 'm54120_180612_220526/21234554/0_9150', 3: 'm54120_180612_220526/21234554/0_9150', 4: 'm54120_180612_220526/21234554/0_9150', 5: 'm54120_180612_220526/21234554/0_9150', 6: 'm54120_180612_220526/21234554/0_9150', 7: 'm54120_180612_220526/21234554/0_9150', 8: 'm54120_180612_220526/26477162/0_8461', 9: 'm54120_180612_220526/26477162/0_8461', 10: 'm54120_180612_220526/26477162/0_8461', 11: 'm54120_180612_220526/29032580/0_24573', 12: 'm54120_180612_220526/29032580/0_24573', 13: 'm54120_180612_220526/29032580/0_24573', 14: 'm54120_180612_220526/29032580/0_24573', 15: 'm54120_180612_220526/29032580/0_24573', 16: 'm54120_180612_220526/29032580/0_24573', 17: 'm54120_180612_220526/29032580/0_24573', 18: 'm54120_180612_220526/29032580/0_24573', 19: 'm54120_180612_220526/29032580/0_24573', 20: 'm54120_180612_220526/29032580/0_24573', 21: 'm54120_180612_220526/30343801/14628_31461', 22: 'm54120_180612_220526/30343801/14628_31461', 23: 'm54120_180612_220526/30343801/14628_31461', 24: 'm54120_180612_220526/30343801/14628_31461', 25: 'm54120_180612_220526/30343801/14628_31461', 26: 'm54120_180612_220526/30343801/14628_31461', 27: 'm54120_180612_220526/38535977/507_20686', 28: 'm54120_180612_220526/38535977/507_20686', 29: 'm54120_180612_220526/38535977/507_20686', 30: 'm54120_180612_220526/38535977/507_20686', 31: 'm54120_180612_220526/38535977/507_20686', 32: 'm54120_180612_220526/38535977/507_20686', 33: 'm54120_180612_220526/38535977/507_20686', 34: 'm54120_180612_220526/38535977/507_20686', 35: 'm54120_180612_220526/38535977/507_20686', 36: 'm54120_180612_220526/38535977/507_20686', 37: 'm54120_180612_220526/38535977/507_20686', 38: 'm54120_180612_220526/42205779/1649_10465', 39: 'm54120_180612_220526/42205779/1649_10465', 40: 'm54120_180612_220526/48038273/21104_34417', 41: 'm54120_180612_220526/48038273/21104_34417', 42: 'm54120_180612_220526/48038273/21104_34417', 43: 'm54120_180612_220526/48038273/21104_34417', 44: 'm54120_180612_220526/48038273/21104_34417', 45: 'm54120_180612_220526/48038273/21104_34417', 46: 'm54120_180612_220526/49742281/17862_28926', 47: 'm54120_180612_220526/49742281/17862_28926', 48: 'm54120_180612_220526/49742281/28973_33992', 49: 'm54120_180612_220526/49742281/28973_33992', 50: 'm54120_180612_220526/49742281/28973_33992', 51: 'm54120_180612_220526/53478317/37981_39588', 52: 'm54120_180612_220526/53478317/44704_49197', 53: 'm54120_180612_220526/56033405/4001_12178', 54: 'm54120_180612_220526/56033405/4001_12178', 55: 'm54120_180612_220526/56033405/4001_12178', 56: 'm54120_180612_220526/56033405/4001_12178', 57: 'm54120_180612_220526/58851519/19433_23201', 58: 'm54120_180612_220526/59310373/0_16501', 59: 'm54120_180612_220526/59310373/0_16501', 60: 'm54120_180612_220526/59310373/0_16501', 61: 'm54120_180612_220526/59310373/0_16501', 62: 'm54120_180612_220526/59310373/0_16501', 63: 'm54120_180612_220526/59310373/0_16501', 64: 'm54120_180612_220526/59310373/0_16501', 65: 'm54120_180612_220526/59310373/0_16501', 66: 'm54120_180612_220526/59310373/0_16501', 67: 'm54120_180612_220526/61341973/31550_34491', 68: 'm54120_180612_220526/61341973/31550_34491', 69: 'm54120_180612_220526/61341973/31550_34491', 70: 'm54120_180612_220526/61604523/10093_15746', 71: 'm54120_180612_220526/61604523/10093_15746', 72: 'm54120_180612_220526/64880906/0_9319', 73: 'm54120_180612_220526/64880906/0_9319', 74: 'm54120_180613_174920/20381792/0_34620', 75: 'm54120_180613_174920/20381792/0_34620', 76: 'm54120_180613_174920/20381792/0_34620', 77: 'm54120_180613_174920/20381792/0_34620', 78: 'm54120_180613_174920/20381792/0_34620', 79: 'm54120_180613_174920/20381792/0_34620', 80: 'm54120_180613_174920/20381792/0_34620', 81: 'm54120_180613_174920/20381792/0_34620', 82: 'm54120_180613_174920/20381792/0_34620', 83: 'm54120_180613_174920/20381792/0_34620', 84: 'm54120_180613_174920/20381792/0_34620', 85: 'm54120_180613_174920/20578413/0_9522', 86: 'm54120_180613_174920/20578413/0_9522', 87: 'm54120_180613_174920/20578413/0_9522', 88: 'm54120_180613_174920/20578413/0_9522', 89: 'm54120_180613_174920/20578413/0_9522', 90: 'm54120_180613_174920/20644444/6592_17003', 91: 'm54120_180613_174920/20644444/6592_17003', 92: 'm54120_180613_174920/21495944/0_8230', 93: 'm54120_180613_174920/21495944/0_8230', 94: 'm54120_180613_174920/21495944/0_8230', 95: 'm54120_180613_174920/32243858/19733_29098', 96: 'm54120_180613_174920/32243858/19733_29098', 97: 'm54120_180613_174920/32243858/19733_29098', 98: 'm54120_180613_174920/32243858/19733_29098', 99: 'm54120_180613_174920/44172180/17268_33198', 100: 'm54120_180613_174920/44172180/17268_33198', 101: 'm54120_180613_174920/44172180/17268_33198', 102: 'm54120_180613_174920/44172180/17268_33198', 103: 'm54120_180613_174920/44172180/17268_33198', 104: 'm54120_180613_174920/44172180/17268_33198', 105: 'm54120_180613_174920/44172180/17268_33198', 106: 'm54120_180613_174920/44172180/17268_33198', 107: 'm54120_180613_174920/44172180/17268_33198', 108: 'm54120_180613_174920/44172180/17268_33198', 109: 'm54120_180613_174920/59769790/0_15314', 110: 'm54120_180613_174920/59769790/0_15314', 111: 'm54120_180613_174920/59769790/0_15314', 112: 'm54120_180613_174920/59769790/0_15314', 113: 'm54120_180613_174920/59769790/0_15314', 114: 'm54120_180613_174920/59769790/0_15314', 115: 'm54120_180613_174920/59769790/0_15314', 116: 'm54120_180613_174920/60687150/26966_43352', 117: 'm54120_180613_174920/60687150/26966_43352', 118: 'm54120_180613_174920/62325345/6156_23789', 119: 'm54120_180613_174920/62325345/6156_23789', 120: 'm54120_180613_174920/62325345/6156_23789', 121: 'm54120_180613_174920/62325345/6156_23789', 122: 'm54120_180613_174920/62325345/6156_23789', 123: 'm54120_180613_174920/62325345/6156_23789', 124: 'm54120_180613_174920/64488244/127_25671', 125: 'm54120_180613_174920/64488244/127_25671', 126: 'm54120_180613_174920/64488244/127_25671', 127: 'm54120_180613_174920/64488244/127_25671', 128: 'm54120_180613_174920/64488244/127_25671', 129: 'm54120_180613_174920/64488244/127_25671', 130: 'm54120_180613_174920/64488244/127_25671', 131: 'm54120_180613_174920/64488244/127_25671', 132: 'm54120_180613_174920/64488244/127_25671', 133: 'm54120_180613_174920/67961503/1202_6912', 134: 'm54120_180613_174920/67961503/1202_6912', 135: 'm54120_180613_174920/68747578/0_9038', 136: 'm54120_180613_174920/68747578/0_9038', 137: 'm54120_180613_174920/68747578/0_9038', 138: 'm54120_180613_174920/69993292/29_17563', 139: 'm54120_180613_174920/69993292/29_17563', 140: 'm54120_180613_174920/69993292/29_17563', 141: 'm54120_180613_174920/69993292/29_17563', 142: 'm54120_180613_174920/69993292/29_17563', 143: 'm54120_180613_174920/69993292/29_17563', 144: 'm54120_180613_174920/8520029/0_14944', 145: 'm54120_180613_174920/8520029/0_14944', 146: 'm54120_180613_174920/8520029/0_14944', 147: 'm54120_180613_174920/8520029/0_14944', 148: 'm54120_180614_221556/10944738/0_14959', 149: 'm54120_180614_221556/10944738/0_14959', 150: 'm54120_180614_221556/10944738/0_14959', 151: 'm54120_180614_221556/10944738/0_14959', 152: 'm54120_180614_221556/10944738/0_14959', 153: 'm54120_180614_221556/10944738/0_14959', 154: 'm54120_180614_221556/10944738/0_14959', 155: 'm54120_180614_221556/13173298/0_13733', 156: 'm54120_180614_221556/13173298/0_13733', 157: 'm54120_180614_221556/13173298/0_13733', 158: 'm54120_180614_221556/13173298/0_13733', 159: 'm54120_180614_221556/13173298/0_13733', 160: 'm54120_180614_221556/13173298/0_13733', 161: 'm54120_180614_221556/13173298/0_13733', 162: 'm54120_180614_221556/13173298/0_13733', 163: 'm54120_180614_221556/13173298/0_13733', 164: 'm54120_180614_221556/14418165/3485_14472', 165: 'm54120_180614_221556/14418165/3485_14472', 166: 'm54120_180614_221556/14418165/3485_14472', 167: 'm54120_180614_221556/14418165/3485_14472', 168: 'm54120_180614_221556/14745999/0_1407', 169: 'm54120_180614_221556/17040213/0_8679', 170: 'm54120_180614_221556/17040213/0_8679', 171: 'm54120_180614_221556/17040213/0_8679', 172: 'm54120_180614_221556/17040213/0_8679', 173: 'm54120_180614_221556/17040213/0_8679', 174: 'm54120_180614_221556/17040213/0_8679', 175: 'm54120_180614_221556/17040213/0_8679', 176: 'm54120_180614_221556/30081685/0_14354', 177: 'm54120_180614_221556/30081685/0_14354', 178: 'm54120_180614_221556/30081685/0_14354', 179: 'm54120_180614_221556/30081685/0_14354', 180: 'm54120_180614_221556/30081685/0_14354', 181: 'm54120_180614_221556/30081685/0_14354', 182: 'm54120_180614_221556/30081685/0_14354', 183: 'm54120_180614_221556/31326907/0_15720', 184: 'm54120_180614_221556/31326907/0_15720', 185: 'm54120_180614_221556/31326907/0_15720', 186: 'm54120_180614_221556/31326907/0_15720', 187: 'm54120_180614_221556/31326907/0_15720', 188: 'm54120_180614_221556/31326907/0_15720', 189: 'm54120_180614_221556/31326907/0_15720', 190: 'm54120_180614_221556/31326907/0_15720', 191: 'm54120_180614_221556/31327044/33485_52881', 192: 'm54120_180614_221556/31327044/33485_52881', 193: 'm54120_180614_221556/31327044/33485_52881', 194: 'm54120_180614_221556/31327044/33485_52881', 195: 'm54120_180614_221556/31327044/33485_52881', 196: 'm54120_180614_221556/31327044/33485_52881', 197: 'm54120_180614_221556/31327044/33485_52881', 198: 'm54120_180614_221556/31327044/33485_52881', 199: 'm54120_180614_221556/41353681/0_33316', 200: 'm54120_180614_221556/41353681/0_33316', 201: 'm54120_180614_221556/41353681/0_33316', 202: 'm54120_180614_221556/41353681/0_33316', 203: 'm54120_180614_221556/41353681/0_33316', 204: 'm54120_180614_221556/41353681/0_33316', 205: 'm54120_180614_221556/41353681/0_33316', 206: 'm54120_180614_221556/41353681/0_33316', 207: 'm54120_180614_221556/41353681/0_33316', 208: 'm54120_180614_221556/41353681/0_33316', 209: 'm54120_180614_221556/41353681/0_33316', 210: 'm54120_180614_221556/41747248/13228_27085', 211: 'm54120_180614_221556/41747248/13228_27085', 212: 'm54120_180614_221556/41747248/13228_27085', 213: 'm54120_180614_221556/41747248/13228_27085', 214: 'm54120_180614_221556/41747248/13228_27085', 215: 'm54120_180614_221556/42729696/0_7981', 216: 'm54120_180614_221556/42729696/0_7981', 217: 'm54120_180614_221556/42729696/0_7981', 218: 'm54120_180614_221556/42729696/0_7981', 219: 'm54120_180614_221556/42729696/0_7981', 220: 'm54120_180614_221556/50725818/1355_19884', 221: 'm54120_180614_221556/50725818/1355_19884', 222: 'm54120_180614_221556/50725818/1355_19884', 223: 'm54120_180614_221556/50725818/1355_19884', 224: 'm54120_180614_221556/50725818/1355_19884', 225: 'm54120_180614_221556/50725818/1355_19884', 226: 'm54120_180614_221556/50725818/1355_19884', 227: 'm54120_180614_221556/50725818/1355_19884', 228: 'm54120_180614_221556/50725818/1355_19884', 229: 'm54120_180614_221556/50725818/1355_19884', 230: 'm54120_180614_221556/52822563/0_4174', 231: 'm54120_180614_221556/5571322/0_27197', 232: 'm54120_180614_221556/5571322/0_27197', 233: 'm54120_180614_221556/5571322/0_27197', 234: 'm54120_180614_221556/5571322/0_27197', 235: 'm54120_180614_221556/57541100/0_13361', 236: 'm54120_180614_221556/57541100/0_13361', 237: 'm54120_180614_221556/57541100/0_13361', 238: 'm54120_180614_221556/57541100/0_13361', 239: 'm54120_180614_221556/57541100/0_13361', 240: 'm54120_180614_221556/62849942/23969_26279', 241: 'm54120_180614_221556/69403016/0_14531', 242: 'm54120_180614_221556/69403016/0_14531', 243: 'm54120_180614_221556/69403016/0_14531', 244: 'm54120_180614_221556/69403016/0_14531', 245: 'm54120_180614_221556/69403016/0_14531', 246: 'm54120_180614_221556/69403016/0_14531', 247: 'm54120_180614_221556/69403016/0_14531', 248: 'm54120_180615_083216/16712333/0_20463', 249: 'm54120_180615_083216/16712333/0_20463', 250: 'm54120_180615_083216/16712333/0_20463', 251: 'm54120_180615_083216/16712333/0_20463', 252: 'm54120_180615_083216/16712333/0_20463', 253: 'm54120_180615_083216/16712333/0_20463', 254: 'm54120_180615_083216/16712333/0_20463', 255: 'm54120_180615_083216/16712333/0_20463', 256: 'm54120_180615_083216/21299970/0_5045', 257: 'm54120_180615_083216/21299970/0_5045', 258: 'm54120_180615_083216/21299970/0_5045', 259: 'm54120_180615_083216/22806976/0_8994', 260: 'm54120_180615_083216/22806976/0_8994', 261: 'm54120_180615_083216/22806976/0_8994', 262: 'm54120_180615_083216/22806976/0_8994', 263: 'm54120_180615_083216/29557597/0_20695', 264: 'm54120_180615_083216/29557597/0_20695', 265: 'm54120_180615_083216/29557597/0_20695', 266: 'm54120_180615_083216/29557597/0_20695', 267: 'm54120_180615_083216/29557597/0_20695', 268: 'm54120_180615_083216/29557597/0_20695', 269: 'm54120_180615_083216/29557597/0_20695', 270: 'm54120_180615_083216/31326845/0_3179', 271: 'm54120_180615_083216/31326845/0_3179', 272: 'm54120_180615_083216/31326845/0_3179', 273: 'm54120_180615_083216/31457834/0_17932', 274: 'm54120_180615_083216/31457834/0_17932', 275: 'm54120_180615_083216/31457834/0_17932', 276: 'm54120_180615_083216/31457834/0_17932', 277: 'm54120_180615_083216/31457834/0_17932', 278: 'm54120_180615_083216/31457834/0_17932', 279: 'm54120_180615_083216/31457834/0_17932', 280: 'm54120_180615_083216/32702764/0_20975', 281: 'm54120_180615_083216/32702764/0_20975', 282: 'm54120_180615_083216/32702764/0_20975', 283: 'm54120_180615_083216/32702764/0_20975', 284: 'm54120_180615_083216/32702764/0_20975', 285: 'm54120_180615_083216/32702764/0_20975', 286: 'm54120_180615_083216/34472930/24818_34559', 287: 'm54120_180615_083216/34472930/24818_34559', 288: 'm54120_180615_083216/34865608/0_27567', 289: 'm54120_180615_083216/34865608/0_27567', 290: 'm54120_180615_083216/34865608/0_27567', 291: 'm54120_180615_083216/34865608/0_27567', 292: 'm54120_180615_083216/34865608/0_27567', 293: 'm54120_180615_083216/34865608/0_27567', 294: 'm54120_180615_083216/34865608/0_27567', 295: 'm54120_180615_083216/34865608/0_27567', 296: 'm54120_180615_083216/34865608/0_27567', 297: 'm54120_180615_083216/38732117/105_4282', 298: 'm54120_180615_083216/42402688/3488_27321', 299: 'm54120_180615_083216/42402688/3488_27321', 300: 'm54120_180615_083216/42402688/3488_27321', 301: 'm54120_180615_083216/42402688/3488_27321', 302: 'm54120_180615_083216/42402688/3488_27321', 303: 'm54120_180615_083216/42402688/3488_27321', 304: 'm54120_180615_083216/42402688/3488_27321', 305: 'm54120_180615_083216/45351645/15963_42433', 306: 'm54120_180615_083216/45351645/15963_42433', 307: 'm54120_180615_083216/45351645/15963_42433', 308: 'm54120_180615_083216/45351645/15963_42433', 309: 'm54120_180615_083216/45351645/15963_42433', 310: 'm54120_180615_083216/45351645/15963_42433', 311: 'm54120_180615_083216/45351645/15963_42433', 312: 'm54120_180615_083216/45351645/15963_42433', 313: 'm54120_180615_083216/47252379/0_8497', 314: 'm54120_180615_083216/47252379/0_8497', 315: 'm54120_180615_083216/47252379/0_8497', 316: 'm54120_180615_083216/51904905/1018_18996', 317: 'm54120_180615_083216/51904905/1018_18996', 318: 'm54120_180615_083216/51904905/1018_18996', 319: 'm54120_180615_083216/51904905/1018_18996', 320: 'm54120_180615_083216/51904905/1018_18996', 321: 'm54120_180615_083216/57279236/0_32503', 322: 'm54120_180615_083216/57279236/0_32503', 323: 'm54120_180615_083216/57279236/0_32503', 324: 'm54120_180615_083216/57279236/0_32503', 325: 'm54120_180615_083216/57279236/0_32503', 326: 'm54120_180615_083216/57279236/0_32503', 327: 'm54120_180615_083216/57279236/0_32503', 328: 'm54120_180615_083216/57279236/0_32503', 329: 'm54120_180615_083216/57279236/0_32503', 330: 'm54120_180615_083216/57279236/0_32503', 331: 'm54120_180615_083216/57279236/0_32503', 332: 'm54120_180615_083216/57279236/0_32503', 333: 'm54120_180615_083216/59638721/0_1445', 334: 'm54120_180615_083216/65209099/16432_42127', 335: 'm54120_180615_083216/65209099/16432_42127', 336: 'm54120_180615_083216/65209099/16432_42127', 337: 'm54120_180615_083216/65209099/16432_42127', 338: 'm54120_180615_083216/65209099/16432_42127', 339: 'm54120_180615_083216/65209099/16432_42127', 340: 'm54120_180615_083216/65209099/16432_42127', 341: 'm54120_180615_083216/65209099/16432_42127', 342: 'm54120_180615_083216/65209099/16432_42127', 343: 'm54120_180615_083216/65209099/16432_42127', 344: 'm54120_180615_083216/65209099/16432_42127', 345: 'm54120_180615_083216/65209099/16432_42127', 346: 'm54120_180615_083216/71434931/0_9788', 347: 'm54120_180615_083216/71434931/0_9788', 348: 'm54120_180615_083216/71434931/0_9788', 349: 'm54120_180615_083216/71828088/0_18777', 350: 'm54120_180615_083216/71828088/0_18777', 351: 'm54120_180615_083216/71828088/0_18777', 352: 'm54120_180615_083216/71828088/0_18777', 353: 'm54120_180615_083216/71828088/0_18777', 354: 'm54120_180615_083216/71828088/0_18777', 355: 'm54120_180615_083216/71828088/0_18777', 356: 'm54120_180615_083216/8323389/0_7884', 357: 'm54120_180615_083216/8323389/0_7884', 358: 'm54120_180615_083216/8323389/0_7884', 359: 'm54120_180615_083216/8323389/0_7884', 360: 'm54120_180615_083216/9569018/11583_24001', 361: 'm54120_180615_083216/9569018/11583_24001', 362: 'm54120_180615_083216/9569018/11583_24001', 363: 'm54120_180615_083216/9569018/11583_24001', 364: 'm54120_180615_083216/9569018/11583_24001', 365: 'm54120_180615_083216/9569018/11583_24001', 366: 'm54120_180616_203619/11403689/11696_16560', 367: 'm54120_180616_203619/22937966/0_20893', 368: 'm54120_180616_203619/22937966/0_20893', 369: 'm54120_180616_203619/22937966/0_20893', 370: 'm54120_180616_203619/22937966/0_20893', 371: 'm54120_180616_203619/22937966/0_20893', 372: 'm54120_180616_203619/22937966/0_20893', 373: 'm54120_180616_203619/22937966/0_20893', 374: 'm54120_180616_203619/22937966/0_20893', 375: 'm54120_180616_203619/22937966/0_20893', 376: 'm54120_180616_203619/25756602/21379_35784', 377: 'm54120_180616_203619/25756602/21379_35784', 378: 'm54120_180616_203619/25756602/21379_35784', 379: 'm54120_180616_203619/25756602/21379_35784', 380: 'm54120_180616_203619/25756602/21379_35784', 381: 'm54120_180616_203619/25756602/21379_35784', 382: 'm54120_180616_203619/26673796/5285_36172', 383: 'm54120_180616_203619/26673796/5285_36172', 384: 'm54120_180616_203619/26673796/5285_36172', 385: 'm54120_180616_203619/26673796/5285_36172', 386: 'm54120_180616_203619/26673796/5285_36172', 387: 'm54120_180616_203619/26673796/5285_36172', 388: 'm54120_180616_203619/26673796/5285_36172', 389: 'm54120_180616_203619/26673796/5285_36172', 390: 'm54120_180616_203619/26673796/5285_36172', 391: 'm54120_180616_203619/26673796/5285_36172', 392: 'm54120_180616_203619/26673796/5285_36172', 393: 'm54120_180616_203619/26673796/5285_36172', 394: 'm54120_180616_203619/26673796/5285_36172', 395: 'm54120_180616_203619/26673796/5285_36172', 396: 'm54120_180616_203619/26673796/5285_36172', 397: 'm54120_180616_203619/26673796/5285_36172', 398: 'm54120_180616_203619/26673796/5285_36172', 399: 'm54120_180616_203619/26935798/0_508', 400: 'm54120_180616_203619/29163799/0_11918', 401: 'm54120_180616_203619/29163799/0_11918', 402: 'm54120_180616_203619/29163799/0_11918', 403: 'm54120_180616_203619/32440603/504_33342', 404: 'm54120_180616_203619/32440603/504_33342', 405: 'm54120_180616_203619/32440603/504_33342', 406: 'm54120_180616_203619/32440603/504_33342', 407: 'm54120_180616_203619/32440603/504_33342', 408: 'm54120_180616_203619/32440603/504_33342', 409: 'm54120_180616_203619/32440603/504_33342', 410: 'm54120_180616_203619/32440603/504_33342', 411: 'm54120_180616_203619/32440603/504_33342', 412: 'm54120_180616_203619/32834533/0_21062', 413: 'm54120_180616_203619/32834533/0_21062', 414: 'm54120_180616_203619/32834533/0_21062', 415: 'm54120_180616_203619/32834533/0_21062', 416: 'm54120_180616_203619/32834533/0_21062', 417: 'm54120_180616_203619/32834533/0_21062', 418: 'm54120_180616_203619/32834533/0_21062', 419: 'm54120_180616_203619/32834533/0_21062', 420: 'm54120_180616_203619/33554799/1251_7925', 421: 'm54120_180616_203619/33554799/1251_7925', 422: 'm54120_180616_203619/33554799/1251_7925', 423: 'm54120_180616_203619/33554799/1251_7925', 424: 'm54120_180616_203619/33554799/1251_7925', 425: 'm54120_180616_203619/34144860/0_4694', 426: 'm54120_180616_203619/34144860/0_4694', 427: 'm54120_180616_203619/34144860/0_4694', 428: 'm54120_180616_203619/35390232/29061_42170', 429: 'm54120_180616_203619/35390232/29061_42170', 430: 'm54120_180616_203619/35390232/29061_42170', 431: 'm54120_180616_203619/35390232/29061_42170', 432: 'm54120_180616_203619/35390232/29061_42170', 433: 'm54120_180616_203619/35390232/29061_42170', 434: 'm54120_180616_203619/35390232/29061_42170', 435: 'm54120_180616_203619/37290575/0_1557', 436: 'm54120_180616_203619/37290575/0_1557', 437: 'm54120_180616_203619/37487167/0_12350', 438: 'm54120_180616_203619/37487167/0_12350', 439: 'm54120_180616_203619/37487167/0_12350', 440: 'm54120_180616_203619/37487167/0_12350', 441: 'm54120_180616_203619/37552518/0_18592', 442: 'm54120_180616_203619/37552518/0_18592', 443: 'm54120_180616_203619/37552518/0_18592', 444: 'm54120_180616_203619/37552518/0_18592', 445: 'm54120_180616_203619/37552518/0_18592', 446: 'm54120_180616_203619/37552518/0_18592', 447: 'm54120_180616_203619/37552518/0_18592', 448: 'm54120_180616_203619/37552518/0_18592', 449: 'm54120_180616_203619/37552518/0_18592', 450: 'm54120_180616_203619/40043503/0_22695', 451: 'm54120_180616_203619/40043503/0_22695', 452: 'm54120_180616_203619/40043503/0_22695', 453: 'm54120_180616_203619/40043503/0_22695', 454: 'm54120_180616_203619/40043503/0_22695', 455: 'm54120_180616_203619/40043503/0_22695', 456: 'm54120_180616_203619/40043503/0_22695', 457: 'm54120_180616_203619/40043503/0_22695', 458: 'm54120_180616_203619/40043503/0_22695', 459: 'm54120_180616_203619/40043503/0_22695', 460: 'm54120_180616_203619/41025731/0_2065', 461: 'm54120_180616_203619/42533000/0_9110', 462: 'm54120_180616_203619/42533000/0_9110', 463: 'm54120_180616_203619/42533000/0_9110', 464: 'm54120_180616_203619/43122782/0_7477', 465: 'm54120_180616_203619/43122782/0_7477', 466: 'm54120_180616_203619/43122782/0_7477', 467: 'm54120_180616_203619/43122782/0_7477', 468: 'm54120_180616_203619/46334756/0_15619', 469: 'm54120_180616_203619/46334756/0_15619', 470: 'm54120_180616_203619/46334756/0_15619', 471: 'm54120_180616_203619/46334756/0_15619', 472: 'm54120_180616_203619/46334756/0_15619', 473: 'm54120_180616_203619/46334756/0_15619', 474: 'm54120_180616_203619/46334756/0_15619', 475: 'm54120_180616_203619/46334756/0_15619', 476: 'm54120_180616_203619/49807437/2697_34746', 477: 'm54120_180616_203619/49807437/2697_34746', 478: 'm54120_180616_203619/49807437/2697_34746', 479: 'm54120_180616_203619/49807437/2697_34746', 480: 'm54120_180616_203619/49807437/2697_34746', 481: 'm54120_180616_203619/73728943/9377_23613', 482: 'm54120_180616_203619/73728943/9377_23613', 483: 'm54120_180616_203619/73728943/9377_23613', 484: 'm54120_180616_203619/73728943/9377_23613', 485: 'm54120_180616_203619/73728943/9377_23613', 486: 'm54120_180616_203619/73728943/9377_23613', 487: 'm54120_180616_203619/73728943/9377_23613', 488: 'm54120_180616_203619/73728943/9377_23613', 489: 'm54120_180616_203619/9044734/23133_29886', 490: 'm54120_180616_203619/9044734/23133_29886', 491: 'm54120_180617_065128/15597748/2417_9565', 492: 'm54120_180617_065128/15597748/2417_9565', 493: 'm54120_180617_065128/15597748/2417_9565', 494: 'm54120_180617_065128/16384237/18178_33002', 495: 'm54120_180617_065128/16384237/18178_33002', 496: 'm54120_180617_065128/16384237/18178_33002', 497: 'm54120_180617_065128/16384237/18178_33002', 498: 'm54120_180617_065128/16384237/18178_33002', 499: 'm54120_180617_065128/16384237/18178_33002', 500: 'm54120_180617_065128/18874779/0_7235', 501: 'm54120_180617_065128/26673929/8947_12108', 502: 'm54120_180617_065128/29753546/18955_25881', 503: 'm54120_180617_065128/30540249/0_12855', 504: 'm54120_180617_065128/30540249/0_12855', 505: 'm54120_180617_065128/30540249/0_12855', 506: 'm54120_180617_065128/30540249/0_12855', 507: 'm54120_180617_065128/30540249/0_12855', 508: 'm54120_180617_065128/32571701/0_6887', 509: 'm54120_180617_065128/32571701/0_6887', 510: 'm54120_180617_065128/32571701/0_6887', 511: 'm54120_180617_065128/32571701/0_6887', 512: 'm54120_180617_065128/34406856/1092_17728', 513: 'm54120_180617_065128/34406856/1092_17728', 514: 'm54120_180617_065128/34406856/1092_17728', 515: 'm54120_180617_065128/34406856/1092_17728', 516: 'm54120_180617_065128/34406856/1092_17728', 517: 'm54120_180617_065128/34406856/1092_17728', 518: 'm54120_180617_065128/42402547/0_26204', 519: 'm54120_180617_065128/42402547/0_26204', 520: 'm54120_180617_065128/42402547/0_26204', 521: 'm54120_180617_065128/42402547/0_26204', 522: 'm54120_180617_065128/61014398/9864_16828', 523: 'm54120_180617_065128/63374175/0_15086', 524: 'm54120_180617_065128/63374175/0_15086', 525: 'm54120_180617_065128/63374175/0_15086', 526: 'm54120_180617_065128/63374175/0_15086', 527: 'm54120_180617_065128/63374175/0_15086', 528: 'm54120_180617_065128/63374175/0_15086', 529: 'm54120_180617_065128/63374175/0_15086', 530: 'm54120_180617_065128/64029418/0_4134', 531: 'm54120_180617_065128/64029418/0_4134', 532: 'm54120_180617_065128/64029418/0_4134', 533: 'm54120_180617_065128/64029418/0_4134', 534: 'm54120_180617_065128/64291125/1875_13177', 535: 'm54120_180617_065128/64291125/1875_13177', 536: 'm54120_180617_065128/64291125/1875_13177', 537: 'm54120_180617_065128/64291125/1875_13177', 538: 'm54120_180617_065128/64291125/1875_13177', 539: 'm54120_180617_065128/73794211/20931_28104', 540: 'm54120_180617_065128/73794211/20931_28104', 541: 'm54120_180617_065128/73794211/20931_28104', 542: 'm54120_180617_065128/9372251/0_8028', 543: 'm54120_180617_065128/9372251/0_8028', 544: 'm54120_180617_065128/9372251/0_8028', 545: 'm54120_180623_025753/13042297/4163_14411', 546: 'm54120_180623_025753/13042297/4163_14411', 547: 'm54120_180623_025753/13042297/4163_14411', 548: 'm54120_180623_025753/13042297/4163_14411', 549: 'm54120_180623_025753/13042297/4163_14411', 550: 'm54120_180623_025753/13042297/4163_14411', 551: 'm54120_180623_025753/13042297/4163_14411', 552: 'm54120_180623_025753/13042297/4163_14411', 553: 'm54120_180623_025753/14745879/27891_32390', 554: 'm54120_180623_025753/14745879/27891_32390', 555: 'm54120_180623_025753/17367134/8004_9814', 556: 'm54120_180623_025753/17367134/8004_9814', 557: 'm54120_180623_025753/18743389/4768_12017', 558: 'm54120_180623_025753/18743389/4768_12017', 559: 'm54120_180623_025753/18743389/4768_12017', 560: 'm54120_180623_025753/18743389/4768_12017', 561: 'm54120_180623_025753/19726962/20876_26466', 562: 'm54120_180623_025753/19726962/20876_26466', 563: 'm54120_180623_025753/25166376/0_13371', 564: 'm54120_180623_025753/25166376/0_13371', 565: 'm54120_180623_025753/25166376/0_13371', 566: 'm54120_180623_025753/25166376/0_13371', 567: 'm54120_180623_025753/25166376/0_13371', 568: 'm54120_180623_025753/25166376/0_13371', 569: 'm54120_180623_025753/32571667/10170_12692', 570: 'm54120_180623_025753/40566992/9374_15123', 571: 'm54120_180623_025753/40566992/9374_15123', 572: 'm54120_180623_025753/40566992/9374_15123', 573: 'm54120_180623_025753/43975037/0_19358', 574: 'm54120_180623_025753/43975037/0_19358', 575: 'm54120_180623_025753/43975037/0_19358', 576: 'm54120_180623_025753/43975037/0_19358', 577: 'm54120_180623_025753/43975037/0_19358', 578: 'm54120_180623_025753/43975037/0_19358', 579: 'm54120_180623_025753/43975037/0_19358', 580: 'm54120_180623_025753/43975037/0_19358', 581: 'm54120_180623_025753/43975037/0_19358', 582: 'm54120_180623_025753/45940902/11627_28281', 583: 'm54120_180623_025753/45940902/11627_28281', 584: 'm54120_180623_025753/45940902/11627_28281', 585: 'm54120_180623_025753/45940902/11627_28281', 586: 'm54120_180623_025753/45940902/11627_28281', 587: 'm54120_180623_025753/45940902/11627_28281', 588: 'm54120_180623_025753/45940902/11627_28281', 589: 'm54120_180623_025753/46203409/20309_32124', 590: 'm54120_180623_025753/46203409/20309_32124', 591: 'm54120_180623_025753/46203409/20309_32124', 592: 'm54120_180623_025753/46203409/20309_32124', 593: 'm54120_180623_025753/46203409/20309_32124', 594: 'm54120_180623_025753/47252469/366_22469', 595: 'm54120_180623_025753/47252469/366_22469', 596: 'm54120_180623_025753/47252469/366_22469', 597: 'm54120_180623_025753/47252469/366_22469', 598: 'm54120_180623_025753/47252469/366_22469', 599: 'm54120_180623_025753/47252469/366_22469', 600: 'm54120_180623_025753/47252469/366_22469', 601: 'm54120_180623_025753/47252469/366_22469', 602: 'm54120_180623_025753/52495191/29086_31632', 603: 'm54120_180623_025753/52495191/29086_31632', 604: 'm54120_180623_025753/52495191/29086_31632', 605: 'm54120_180623_025753/53149967/11247_21049', 606: 'm54120_180623_025753/53149967/11247_21049', 607: 'm54120_180623_025753/53149967/11247_21049', 608: 'm54120_180623_025753/69731031/4431_11378', 609: 'm54120_180623_025753/69731031/4431_11378', 610: 'm54120_180623_025753/7340316/0_5988', 611: 'm54120_180623_025753/7340316/0_5988', 612: 'm54120_180623_025753/8388944/15910_30493', 613: 'm54120_180623_025753/8388944/15910_30493', 614: 'm54120_180623_025753/8388944/15910_30493', 615: 'm54120_180623_025753/8388944/15910_30493', 616: 'm54120_180623_230400/10289382/571_16794', 617: 'm54120_180623_230400/10289382/571_16794', 618: 'm54120_180623_230400/10289382/571_16794', 619: 'm54120_180623_230400/10289382/571_16794', 620: 'm54120_180623_230400/10289382/571_16794', 621: 'm54120_180623_230400/20709522/8659_16470', 622: 'm54120_180623_230400/20709522/8659_16470', 623: 'm54120_180623_230400/31982088/54440_66286', 624: 'm54120_180623_230400/31982088/54440_66286', 625: 'm54120_180623_230400/36766276/0_17351', 626: 'm54120_180623_230400/36766276/0_17351', 627: 'm54120_180623_230400/36766276/0_17351', 628: 'm54120_180623_230400/36766276/0_17351', 629: 'm54120_180623_230400/36766276/0_17351', 630: 'm54120_180623_230400/36962752/24825_29787', 631: 'm54120_180623_230400/36962752/24825_29787', 632: 'm54120_180623_230400/38208287/0_18363', 633: 'm54120_180623_230400/38208287/0_18363', 634: 'm54120_180623_230400/38208287/0_18363', 635: 'm54120_180623_230400/38208287/0_18363', 636: 'm54120_180623_230400/38208287/0_18363', 637: 'm54120_180623_230400/38208287/0_18363', 638: 'm54120_180623_230400/38208287/0_18363', 639: 'm54120_180623_230400/38208287/0_18363', 640: 'm54120_180623_230400/38208287/0_18363', 641: 'm54120_180623_230400/49939274/0_5058', 642: 'm54120_180623_230400/53936348/0_24065', 643: 'm54120_180623_230400/53936348/0_24065', 644: 'm54120_180623_230400/53936348/0_24065', 645: 'm54120_180623_230400/53936348/0_24065', 646: 'm54120_180623_230400/53936348/0_24065', 647: 'm54120_180623_230400/53936348/0_24065', 648: 'm54120_180623_230400/53936348/0_24065', 649: 'm54120_180623_230400/53936348/0_24065', 650: 'm54120_180623_230400/53936348/0_24065', 651: 'm54120_180623_230400/53936348/0_24065', 652: 'm54120_180623_230400/53936348/0_24065', 653: 'm54120_180623_230400/53936348/0_24065', 654: 'm54120_180623_230400/53936348/0_24065', 655: 'm54120_180623_230400/53936348/0_24065', 656: 'm54120_180623_230400/55182121/139_13059', 657: 'm54120_180623_230400/55182121/139_13059', 658: 'm54120_180623_230400/56885830/27035_35055', 659: 'm54120_180623_230400/59704270/0_6059', 660: 'm54120_180623_230400/59704270/0_6059', 661: 'm54120_180623_230400/59900601/0_7347', 662: 'm54120_180623_230400/59900601/0_7347', 663: 'm54120_180623_230400/59900601/0_7347', 664: 'm54120_180623_230400/65929496/0_316', 665: 'm54120_180623_230400/66715802/4298_10949', 666: 'm54120_180623_230400/8127460/15230_50458', 667: 'm54120_180623_230400/8127460/15230_50458', 668: 'm54120_180623_230400/8127460/15230_50458', 669: 'm54120_180623_230400/8127460/15230_50458', 670: 'm54120_180623_230400/9700033/41031_57100', 671: 'm54120_180623_230400/9700033/41031_57100', 672: 'm54120_180623_230400/9700033/41031_57100', 673: 'm54120_180623_230400/9700033/41031_57100', 674: 'm54120_180623_230400/9700033/41031_57100', 675: 'm54120_180623_230400/9700033/41031_57100', 676: 'm54120_180623_230400/9700033/41031_57100', 677: 'm54120_180623_230400/9700033/41031_57100', 678: 'm54120_180624_091622/17433236/394_20680', 679: 'm54120_180624_091622/17433236/394_20680', 680: 'm54120_180624_091622/17433236/394_20680', 681: 'm54120_180624_091622/17433236/394_20680', 682: 'm54120_180624_091622/17433236/394_20680', 683: 'm54120_180624_091622/22938285/21522_34278', 684: 'm54120_180624_091622/22938285/21522_34278', 685: 'm54120_180624_091622/24511105/14870_31092', 686: 'm54120_180624_091622/24511105/14870_31092', 687: 'm54120_180624_091622/24511105/14870_31092', 688: 'm54120_180624_091622/24511105/14870_31092', 689: 'm54120_180624_091622/24511105/14870_31092', 690: 'm54120_180624_091622/25362587/0_10300', 691: 'm54120_180624_091622/25362587/0_10300', 692: 'm54120_180624_091622/31785647/23457_42699', 693: 'm54120_180624_091622/31785647/23457_42699', 694: 'm54120_180624_091622/31785647/23457_42699', 695: 'm54120_180624_091622/31785647/23457_42699', 696: 'm54120_180624_091622/31785647/23457_42699', 697: 'm54120_180624_091622/31785647/23457_42699', 698: 'm54120_180624_091622/31785647/23457_42699', 699: 'm54120_180624_091622/31785647/23457_42699', 700: 'm54120_180624_091622/31785647/23457_42699', 701: 'm54120_180624_091622/32178741/0_20648', 702: 'm54120_180624_091622/32178741/0_20648', 703: 'm54120_180624_091622/32178741/0_20648', 704: 'm54120_180624_091622/32178741/0_20648', 705: 'm54120_180624_091622/32178741/0_20648', 706: 'm54120_180624_091622/32178741/0_20648', 707: 'm54120_180624_091622/32178741/0_20648', 708: 'm54120_180624_091622/32178741/0_20648', 709: 'm54120_180624_091622/32178741/0_20648', 710: 'm54120_180624_091622/35586629/20402_24210', 711: 'm54120_180624_091622/38798326/0_2591', 712: 'm54120_180624_091622/40501411/11845_27044', 713: 'm54120_180624_091622/40501411/11845_27044', 714: 'm54120_180624_091622/40501411/11845_27044', 715: 'm54120_180624_091622/40501411/11845_27044', 716: 'm54120_180624_091622/40501411/11845_27044', 717: 'm54120_180624_091622/47907251/28953_56451', 718: 'm54120_180624_091622/47907251/28953_56451', 719: 'm54120_180624_091622/49480509/3075_22050', 720: 'm54120_180624_091622/49480509/3075_22050', 721: 'm54120_180624_091622/49480509/3075_22050', 722: 'm54120_180624_091622/49480509/3075_22050', 723: 'm54120_180624_091622/49480509/3075_22050', 724: 'm54120_180624_091622/49480509/3075_22050', 725: 'm54120_180624_091622/49480509/3075_22050', 726: 'm54120_180624_091622/49480509/3075_22050', 727: 'm54120_180624_091622/49480509/3075_22050', 728: 'm54120_180624_091622/49807560/2233_24414', 729: 'm54120_180624_091622/49807560/2233_24414', 730: 'm54120_180624_091622/49807560/2233_24414', 731: 'm54120_180624_091622/49807560/2233_24414', 732: 'm54120_180624_091622/49807560/2233_24414', 733: 'm54120_180624_091622/49807560/2233_24414', 734: 'm54120_180624_091622/49807560/2233_24414', 735: 'm54120_180624_091622/49807560/2233_24414', 736: 'm54120_180624_091622/49808300/0_14154', 737: 'm54120_180624_091622/49808300/0_14154', 738: 'm54120_180624_091622/49808300/0_14154', 739: 'm54120_180624_091622/49808300/0_14154', 740: 'm54120_180624_091622/49808300/0_14154', 741: 'm54120_180624_091622/49808300/0_14154', 742: 'm54120_180624_091622/49808300/0_14154', 743: 'm54120_180624_091622/49808300/0_14154', 744: 'm54120_180624_091622/49808300/0_14154', 745: 'm54120_180624_091622/49938842/0_20042', 746: 'm54120_180624_091622/49938842/0_20042', 747: 'm54120_180624_091622/49938842/0_20042', 748: 'm54120_180624_091622/49938842/0_20042', 749: 'm54120_180624_091622/49938842/0_20042', 750: 'm54120_180624_091622/49938842/0_20042', 751: 'm54120_180624_091622/49938842/0_20042', 752: 'm54120_180624_091622/53543116/11389_22864', 753: 'm54120_180624_091622/53543116/11389_22864', 754: 'm54120_180624_091622/53543116/11389_22864', 755: 'm54120_180624_091622/53543116/11389_22864', 756: 'm54120_180624_091622/53543116/11389_22864', 757: 'm54120_180624_091622/6226698/0_8804', 758: 'm54120_180624_091622/6226698/0_8804', 759: 'm54120_180624_091622/6226698/0_8804', 760: 'm54120_180624_091622/6226698/0_8804', 761: 'm54120_180624_091622/6226698/0_8804', 762: 'm54120_180624_091622/6226698/0_8804', 763: 'm54120_180624_091622/6489049/0_10674', 764: 'm54120_180624_091622/6489049/0_10674', 765: 'm54120_180624_091622/6489049/0_10674', 766: 'm54120_180624_091622/6489049/0_10674', 767: 'm54120_180624_091622/70648557/0_30974', 768: 'm54120_180624_091622/70648557/0_30974', 769: 'm54120_180624_091622/70648557/0_30974', 770: 'm54120_180624_091622/70648557/0_30974', 771: 'm54120_180624_091622/70648557/0_30974', 772: 'm54120_180624_091622/70648557/0_30974', 773: 'm54120_180624_091622/73401082/0_14920', 774: 'm54120_180624_091622/73401082/0_14920', 775: 'm54120_180624_091622/73401082/0_14920', 776: 'm54120_180624_091622/73401082/0_14920', 777: 'm54120_180624_091622/73597845/0_1047', 778: 'm54120_180624_193042/10027136/303_1546', 779: 'm54120_180624_193042/24117847/19689_39651', 780: 'm54120_180624_193042/24117847/19689_39651', 781: 'm54120_180624_193042/24117847/19689_39651', 782: 'm54120_180624_193042/24117847/19689_39651', 783: 'm54120_180624_193042/24117847/19689_39651', 784: 'm54120_180624_193042/24117847/19689_39651', 785: 'm54120_180624_193042/24117847/19689_39651', 786: 'm54120_180624_193042/24117847/19689_39651', 787: 'm54120_180624_193042/24117847/19689_39651', 788: 'm54120_180624_193042/26149845/0_1921', 789: 'm54120_180624_193042/26149845/0_1921', 790: 'm54120_180624_193042/26149845/1974_18678', 791: 'm54120_180624_193042/26149845/1974_18678', 792: 'm54120_180624_193042/26149845/1974_18678', 793: 'm54120_180624_193042/26149845/1974_18678', 794: 'm54120_180624_193042/26149845/1974_18678', 795: 'm54120_180624_193042/26149845/1974_18678', 796: 'm54120_180624_193042/33227687/357_35254', 797: 'm54120_180624_193042/33227687/357_35254', 798: 'm54120_180624_193042/33227687/357_35254', 799: 'm54120_180624_193042/33227687/357_35254', 800: 'm54120_180624_193042/33227687/357_35254', 801: 'm54120_180624_193042/33227687/357_35254', 802: 'm54120_180624_193042/33227687/357_35254', 803: 'm54120_180624_193042/33227687/357_35254', 804: 'm54120_180624_193042/33227687/357_35254', 805: 'm54120_180624_193042/33227687/357_35254', 806: 'm54120_180624_193042/33227687/357_35254', 807: 'm54120_180624_193042/33685769/0_20868', 808: 'm54120_180624_193042/33685769/0_20868', 809: 'm54120_180624_193042/33685769/0_20868', 810: 'm54120_180624_193042/33685769/0_20868', 811: 'm54120_180624_193042/33685769/0_20868', 812: 'm54120_180624_193042/33685769/0_20868', 813: 'm54120_180624_193042/33685769/0_20868', 814: 'm54120_180624_193042/34865594/15046_17298', 815: 'm54120_180624_193042/37421280/0_8646', 816: 'm54120_180624_193042/37421280/0_8646', 817: 'm54120_180624_193042/37421280/0_8646', 818: 'm54120_180624_193042/37421280/0_8646', 819: 'm54120_180624_193042/37421280/0_8646', 820: 'm54120_180624_193042/40108390/13861_14888', 821: 'm54120_180624_193042/40698584/16055_24186', 822: 'm54120_180624_193042/40698584/16055_24186', 823: 'm54120_180624_193042/40698584/16055_24186', 824: 'm54120_180624_193042/44172164/26993_31862', 825: 'm54120_180624_193042/44172164/26993_31862', 826: 'm54120_180624_193042/44172164/26993_31862', 827: 'm54120_180624_193042/46138250/31103_38865', 828: 'm54120_180624_193042/51839448/5091_5737', 829: 'm54120_180624_193042/51905039/26160_31376', 830: 'm54120_180624_193042/51905039/26160_31376', 831: 'm54120_180624_193042/51905039/26160_31376', 832: 'm54120_180624_193042/54657820/84522_104925', 833: 'm54120_180624_193042/54657820/84522_104925', 834: 'm54120_180624_193042/54657820/84522_104925', 835: 'm54120_180624_193042/54657820/84522_104925', 836: 'm54120_180624_193042/54657820/84522_104925', 837: 'm54120_180624_193042/54657820/84522_104925', 838: 'm54120_180624_193042/54657820/84522_104925', 839: 'm54120_180624_193042/54657820/84522_104925', 840: 'm54120_180624_193042/54657820/84522_104925', 841: 'm54120_180624_193042/54657820/84522_104925', 842: 'm54120_180624_193042/54657820/84522_104925', 843: 'm54120_180624_193042/54657820/84522_104925', 844: 'm54120_180624_193042/55444397/0_12096', 845: 'm54120_180624_193042/55444397/0_12096', 846: 'm54120_180624_193042/55444397/0_12096', 847: 'm54120_180624_193042/55444397/0_12096', 848: 'm54120_180624_193042/55444397/0_12096', 849: 'm54120_180624_193042/58261932/0_9665', 850: 'm54120_180624_193042/58261932/0_9665', 851: 'm54120_180624_193042/58261932/0_9665', 852: 'm54120_180624_193042/58261932/0_9665', 853: 'm54120_180624_193042/58261932/0_9665', 854: 'm54120_180624_193042/58261932/0_9665', 855: 'm54120_180624_193042/58261932/0_9665', 856: 'm54120_180624_193042/58261932/0_9665', 857: 'm54120_180624_193042/60096777/44534_64434', 858: 'm54120_180624_193042/60096777/44534_64434', 859: 'm54120_180624_193042/60096777/44534_64434', 860: 'm54120_180624_193042/60096777/44534_64434', 861: 'm54120_180624_193042/60096777/44534_64434', 862: 'm54120_180624_193042/61931867/4611_22527', 863: 'm54120_180624_193042/61931867/4611_22527', 864: 'm54120_180624_193042/61931867/4611_22527', 865: 'm54120_180624_193042/61931867/4611_22527', 866: 'm54120_180624_193042/61931867/4611_22527', 867: 'm54120_180624_193042/61931867/4611_22527', 868: 'm54120_180624_193042/61931867/4611_22527', 869: 'm54120_180624_193042/72483417/0_2778', 870: 'm54120_180624_193042/73073470/7802_20379', 871: 'm54120_180624_193042/73073470/7802_20379', 872: 'm54120_180624_193042/73073470/7802_20379', 873: 'm54120_180624_193042/73073470/7802_20379', 874: 'm54120_180624_193042/7864829/0_2924', 875: 'm54120_180624_193042/8323372/0_5950', 876: 'm54120_180625_144344/13632463/0_998', 877: 'm54120_180625_144344/15401860/10629_28016', 878: 'm54120_180625_144344/15401860/10629_28016', 879: 'm54120_180625_144344/15401860/10629_28016', 880: 'm54120_180625_144344/15401860/10629_28016', 881: 'm54120_180625_144344/15401860/10629_28016', 882: 'm54120_180625_144344/18088560/23151_33490', 883: 'm54120_180625_144344/18088560/23151_33490', 884: 'm54120_180625_144344/18088560/23151_33490', 885: 'm54120_180625_144344/25232341/0_3638', 886: 'm54120_180625_144344/25232341/0_3638', 887: 'm54120_180625_144344/25232341/0_3638', 888: 'm54120_180625_144344/28640037/31522_39900', 889: 'm54120_180625_144344/28640037/31522_39900', 890: 'm54120_180625_144344/28640037/31522_39900', 891: 'm54120_180625_144344/39780908/5389_11968', 892: 'm54120_180625_144344/39780908/5389_11968', 893: 'm54120_180625_144344/44958403/8463_18938', 894: 'm54120_180625_144344/44958403/8463_18938', 895: 'm54120_180625_144344/44958403/8463_18938', 896: 'm54120_180625_144344/45744705/5337_17158', 897: 'm54120_180625_144344/45744705/5337_17158', 898: 'm54120_180625_144344/48104120/49389_58158', 899: 'm54120_180625_144344/51052806/10952_11988', 900: 'm54120_180625_144344/56427315/10103_12797', 901: 'm54120_180625_144344/57541517/25729_41904', 902: 'm54120_180625_144344/57541517/25729_41904', 903: 'm54120_180625_144344/57541517/25729_41904', 904: 'm54120_180625_144344/57541517/25729_41904', 905: 'm54120_180625_144344/57541517/25729_41904', 906: 'm54120_180625_144344/57541517/25729_41904', 907: 'm54120_180625_144344/57541517/25729_41904', 908: 'm54120_180625_144344/57541517/25729_41904', 909: 'm54120_180625_144344/64291699/26804_47231', 910: 'm54120_180625_144344/64291699/26804_47231', 911: 'm54120_180625_144344/64291699/26804_47231', 912: 'm54120_180625_144344/64291699/26804_47231', 913: 'm54120_180625_144344/64291699/26804_47231', 914: 'm54120_180625_144344/64291699/26804_47231', 915: 'm54120_180625_144344/64291699/26804_47231', 916: 'm54120_180625_144344/64291699/26804_47231', 917: 'm54120_180625_144344/66323206/35625_43337', 918: 'm54120_180625_144344/66323206/35625_43337', 919: 'm54120_180625_144344/66323206/35625_43337', 920: 'm54120_180625_144344/66323206/35625_43337', 921: 'm54120_180625_144344/67764642/44482_62119', 922: 'm54120_180625_144344/67764642/44482_62119', 923: 'm54120_180625_144344/67764642/44482_62119', 924: 'm54120_180625_144344/67764642/44482_62119', 925: 'm54120_180628_193436/14680574/0_12062', 926: 'm54120_180628_193436/14680574/0_12062', 927: 'm54120_180628_193436/14680574/0_12062', 928: 'm54120_180628_193436/15598334/13820_18562', 929: 'm54120_180628_193436/15598334/2642_13781', 930: 'm54120_180628_193436/15598334/2642_13781', 931: 'm54120_180628_193436/15598334/2642_13781', 932: 'm54120_180628_193436/15598334/2642_13781', 933: 'm54120_180628_193436/18940656/0_4940', 934: 'm54120_180628_193436/19267818/6241_17965', 935: 'm54120_180628_193436/19267818/6241_17965', 936: 'm54120_180628_193436/19267818/6241_17965', 937: 'm54120_180628_193436/19267818/6241_17965', 938: 'm54120_180628_193436/19267818/6241_17965', 939: 'm54120_180628_193436/25166488/21300_25877', 940: 'm54120_180628_193436/25166488/21300_25877', 941: 'm54120_180628_193436/25166488/21300_25877', 942: 'm54120_180628_193436/25166488/25927_43431', 943: 'm54120_180628_193436/25166488/25927_43431', 944: 'm54120_180628_193436/25166488/25927_43431', 945: 'm54120_180628_193436/25166488/25927_43431', 946: 'm54120_180628_193436/25166488/25927_43431', 947: 'm54120_180628_193436/25166488/25927_43431', 948: 'm54120_180628_193436/25166488/25927_43431', 949: 'm54120_180628_193436/25166488/25927_43431', 950: 'm54120_180628_193436/25166488/25927_43431', 951: 'm54120_180628_193436/29753870/3314_9201', 952: 'm54120_180628_193436/29753870/3314_9201', 953: 'm54120_180628_193436/29753870/3314_9201', 954: 'm54120_180628_193436/37421169/0_25238', 955: 'm54120_180628_193436/37421169/0_25238', 956: 'm54120_180628_193436/37421169/0_25238', 957: 'm54120_180628_193436/37421169/0_25238', 958: 'm54120_180628_193436/37421169/0_25238', 959: 'm54120_180628_193436/37421169/0_25238', 960: 'm54120_180628_193436/37421169/0_25238', 961: 'm54120_180628_193436/37421169/0_25238', 962: 'm54120_180628_193436/37421169/0_25238', 963: 'm54120_180628_193436/37421169/0_25238', 964: 'm54120_180628_193436/37421169/0_25238', 965: 'm54120_180628_193436/37421169/0_25238', 966: 'm54120_180628_193436/37421169/0_25238', 967: 'm54120_180628_193436/40567231/0_16036', 968: 'm54120_180628_193436/40567231/0_16036', 969: 'm54120_180628_193436/40567231/0_16036', 970: 'm54120_180628_193436/40567231/0_16036', 971: 'm54120_180628_193436/40567231/0_16036', 972: 'm54120_180628_193436/43123181/10989_22101', 973: 'm54120_180628_193436/43123181/10989_22101', 974: 'm54120_180628_193436/43123181/10989_22101', 975: 'm54120_180628_193436/43123181/10989_22101', 976: 'm54120_180628_193436/43123181/10989_22101', 977: 'm54120_180628_193436/43123181/10989_22101', 978: 'm54120_180628_193436/43123181/10989_22101', 979: 'm54120_180628_193436/47645307/9588_26426', 980: 'm54120_180628_193436/47645307/9588_26426', 981: 'm54120_180628_193436/47645307/9588_26426', 982: 'm54120_180628_193436/47645307/9588_26426', 983: 'm54120_180628_193436/53936552/4395_25355', 984: 'm54120_180628_193436/53936552/4395_25355', 985: 'm54120_180628_193436/53936552/4395_25355', 986: 'm54120_180628_193436/53936552/4395_25355', 987: 'm54120_180628_193436/53936552/4395_25355', 988: 'm54120_180628_193436/53936552/4395_25355', 989: 'm54120_180628_193436/53936552/4395_25355', 990: 'm54120_180628_193436/53936552/4395_25355', 991: 'm54120_180628_193436/53936552/4395_25355', 992: 'm54120_180628_193436/53936552/4395_25355', 993: 'm54120_180628_193436/55771856/23727_24505', 994: 'm54120_180628_193436/69272106/44843_47722', 995: 'm54120_180628_193436/69796675/289_3856', 996: 'm54120_180628_193436/70124174/23545_34430', 997: 'm54120_180628_193436/70124174/23545_34430', 998: 'm54120_180628_193436/70124174/23545_34430', 999: 'm54120_180628_193436/70124174/23545_34430', 1000: 'm54120_180628_193436/70124174/23545_34430', 1001: 'm54120_180628_193436/70124174/23545_34430', 1002: 'm54120_180628_193436/72155770/20888_30896', 1003: 'm54120_180628_193436/72155770/20888_30896', 1004: 'm54120_180628_193436/72155770/20888_30896', 1005: 'm54120_180628_193436/72155770/20888_30896', 1006: 'm54120_180628_193436/73073169/0_18409', 1007: 'm54120_180628_193436/73073169/0_18409', 1008: 'm54120_180628_193436/73073169/0_18409', 1009: 'm54120_180628_193436/73073169/0_18409', 1010: 'm54120_180628_193436/73073169/0_18409'},
                                    'q_len': {0: 9150, 1: 9150, 2: 9150, 3: 9150, 4: 9150, 5: 9150, 6: 9150, 7: 9150, 8: 8461, 9: 8461, 10: 8461, 11: 24573, 12: 24573, 13: 24573, 14: 24573, 15: 24573, 16: 24573, 17: 24573, 18: 24573, 19: 24573, 20: 24573, 21: 16833, 22: 16833, 23: 16833, 24: 16833, 25: 16833, 26: 16833, 27: 20179, 28: 20179, 29: 20179, 30: 20179, 31: 20179, 32: 20179, 33: 20179, 34: 20179, 35: 20179, 36: 20179, 37: 20179, 38: 8816, 39: 8816, 40: 13313, 41: 13313, 42: 13313, 43: 13313, 44: 13313, 45: 13313, 46: 11064, 47: 11064, 48: 5019, 49: 5019, 50: 5019, 51: 1607, 52: 4493, 53: 8177, 54: 8177, 55: 8177, 56: 8177, 57: 3768, 58: 16501, 59: 16501, 60: 16501, 61: 16501, 62: 16501, 63: 16501, 64: 16501, 65: 16501, 66: 16501, 67: 2941, 68: 2941, 69: 2941, 70: 5653, 71: 5653, 72: 9319, 73: 9319, 74: 34620, 75: 34620, 76: 34620, 77: 34620, 78: 34620, 79: 34620, 80: 34620, 81: 34620, 82: 34620, 83: 34620, 84: 34620, 85: 9522, 86: 9522, 87: 9522, 88: 9522, 89: 9522, 90: 10411, 91: 10411, 92: 8230, 93: 8230, 94: 8230, 95: 9365, 96: 9365, 97: 9365, 98: 9365, 99: 15930, 100: 15930, 101: 15930, 102: 15930, 103: 15930, 104: 15930, 105: 15930, 106: 15930, 107: 15930, 108: 15930, 109: 15314, 110: 15314, 111: 15314, 112: 15314, 113: 15314, 114: 15314, 115: 15314, 116: 16386, 117: 16386, 118: 17633, 119: 17633, 120: 17633, 121: 17633, 122: 17633, 123: 17633, 124: 25544, 125: 25544, 126: 25544, 127: 25544, 128: 25544, 129: 25544, 130: 25544, 131: 25544, 132: 25544, 133: 5710, 134: 5710, 135: 9038, 136: 9038, 137: 9038, 138: 17534, 139: 17534, 140: 17534, 141: 17534, 142: 17534, 143: 17534, 144: 14944, 145: 14944, 146: 14944, 147: 14944, 148: 14959, 149: 14959, 150: 14959, 151: 14959, 152: 14959, 153: 14959, 154: 14959, 155: 13733, 156: 13733, 157: 13733, 158: 13733, 159: 13733, 160: 13733, 161: 13733, 162: 13733, 163: 13733, 164: 10987, 165: 10987, 166: 10987, 167: 10987, 168: 1407, 169: 8679, 170: 8679, 171: 8679, 172: 8679, 173: 8679, 174: 8679, 175: 8679, 176: 14354, 177: 14354, 178: 14354, 179: 14354, 180: 14354, 181: 14354, 182: 14354, 183: 15720, 184: 15720, 185: 15720, 186: 15720, 187: 15720, 188: 15720, 189: 15720, 190: 15720, 191: 19396, 192: 19396, 193: 19396, 194: 19396, 195: 19396, 196: 19396, 197: 19396, 198: 19396, 199: 33316, 200: 33316, 201: 33316, 202: 33316, 203: 33316, 204: 33316, 205: 33316, 206: 33316, 207: 33316, 208: 33316, 209: 33316, 210: 13857, 211: 13857, 212: 13857, 213: 13857, 214: 13857, 215: 7981, 216: 7981, 217: 7981, 218: 7981, 219: 7981, 220: 18529, 221: 18529, 222: 18529, 223: 18529, 224: 18529, 225: 18529, 226: 18529, 227: 18529, 228: 18529, 229: 18529, 230: 4174, 231: 27197, 232: 27197, 233: 27197, 234: 27197, 235: 13361, 236: 13361, 237: 13361, 238: 13361, 239: 13361, 240: 2310, 241: 14531, 242: 14531, 243: 14531, 244: 14531, 245: 14531, 246: 14531, 247: 14531, 248: 20463, 249: 20463, 250: 20463, 251: 20463, 252: 20463, 253: 20463, 254: 20463, 255: 20463, 256: 5045, 257: 5045, 258: 5045, 259: 8994, 260: 8994, 261: 8994, 262: 8994, 263: 20695, 264: 20695, 265: 20695, 266: 20695, 267: 20695, 268: 20695, 269: 20695, 270: 3179, 271: 3179, 272: 3179, 273: 17932, 274: 17932, 275: 17932, 276: 17932, 277: 17932, 278: 17932, 279: 17932, 280: 20975, 281: 20975, 282: 20975, 283: 20975, 284: 20975, 285: 20975, 286: 9741, 287: 9741, 288: 27567, 289: 27567, 290: 27567, 291: 27567, 292: 27567, 293: 27567, 294: 27567, 295: 27567, 296: 27567, 297: 4177, 298: 23833, 299: 23833, 300: 23833, 301: 23833, 302: 23833, 303: 23833, 304: 23833, 305: 26470, 306: 26470, 307: 26470, 308: 26470, 309: 26470, 310: 26470, 311: 26470, 312: 26470, 313: 8497, 314: 8497, 315: 8497, 316: 17978, 317: 17978, 318: 17978, 319: 17978, 320: 17978, 321: 32503, 322: 32503, 323: 32503, 324: 32503, 325: 32503, 326: 32503, 327: 32503, 328: 32503, 329: 32503, 330: 32503, 331: 32503, 332: 32503, 333: 1445, 334: 25695, 335: 25695, 336: 25695, 337: 25695, 338: 25695, 339: 25695, 340: 25695, 341: 25695, 342: 25695, 343: 25695, 344: 25695, 345: 25695, 346: 9788, 347: 9788, 348: 9788, 349: 18777, 350: 18777, 351: 18777, 352: 18777, 353: 18777, 354: 18777, 355: 18777, 356: 7884, 357: 7884, 358: 7884, 359: 7884, 360: 12418, 361: 12418, 362: 12418, 363: 12418, 364: 12418, 365: 12418, 366: 4864, 367: 20893, 368: 20893, 369: 20893, 370: 20893, 371: 20893, 372: 20893, 373: 20893, 374: 20893, 375: 20893, 376: 14405, 377: 14405, 378: 14405, 379: 14405, 380: 14405, 381: 14405, 382: 30887, 383: 30887, 384: 30887, 385: 30887, 386: 30887, 387: 30887, 388: 30887, 389: 30887, 390: 30887, 391: 30887, 392: 30887, 393: 30887, 394: 30887, 395: 30887, 396: 30887, 397: 30887, 398: 30887, 399: 508, 400: 11918, 401: 11918, 402: 11918, 403: 32838, 404: 32838, 405: 32838, 406: 32838, 407: 32838, 408: 32838, 409: 32838, 410: 32838, 411: 32838, 412: 21062, 413: 21062, 414: 21062, 415: 21062, 416: 21062, 417: 21062, 418: 21062, 419: 21062, 420: 6674, 421: 6674, 422: 6674, 423: 6674, 424: 6674, 425: 4694, 426: 4694, 427: 4694, 428: 13109, 429: 13109, 430: 13109, 431: 13109, 432: 13109, 433: 13109, 434: 13109, 435: 1557, 436: 1557, 437: 12350, 438: 12350, 439: 12350, 440: 12350, 441: 18592, 442: 18592, 443: 18592, 444: 18592, 445: 18592, 446: 18592, 447: 18592, 448: 18592, 449: 18592, 450: 22695, 451: 22695, 452: 22695, 453: 22695, 454: 22695, 455: 22695, 456: 22695, 457: 22695, 458: 22695, 459: 22695, 460: 2065, 461: 9110, 462: 9110, 463: 9110, 464: 7477, 465: 7477, 466: 7477, 467: 7477, 468: 15619, 469: 15619, 470: 15619, 471: 15619, 472: 15619, 473: 15619, 474: 15619, 475: 15619, 476: 32049, 477: 32049, 478: 32049, 479: 32049, 480: 32049, 481: 14236, 482: 14236, 483: 14236, 484: 14236, 485: 14236, 486: 14236, 487: 14236, 488: 14236, 489: 6753, 490: 6753, 491: 7148, 492: 7148, 493: 7148, 494: 14824, 495: 14824, 496: 14824, 497: 14824, 498: 14824, 499: 14824, 500: 7235, 501: 3161, 502: 6926, 503: 12855, 504: 12855, 505: 12855, 506: 12855, 507: 12855, 508: 6887, 509: 6887, 510: 6887, 511: 6887, 512: 16636, 513: 16636, 514: 16636, 515: 16636, 516: 16636, 517: 16636, 518: 26204, 519: 26204, 520: 26204, 521: 26204, 522: 6964, 523: 15086, 524: 15086, 525: 15086, 526: 15086, 527: 15086, 528: 15086, 529: 15086, 530: 4134, 531: 4134, 532: 4134, 533: 4134, 534: 11302, 535: 11302, 536: 11302, 537: 11302, 538: 11302, 539: 7173, 540: 7173, 541: 7173, 542: 8028, 543: 8028, 544: 8028, 545: 10248, 546: 10248, 547: 10248, 548: 10248, 549: 10248, 550: 10248, 551: 10248, 552: 10248, 553: 4499, 554: 4499, 555: 1810, 556: 1810, 557: 7249, 558: 7249, 559: 7249, 560: 7249, 561: 5590, 562: 5590, 563: 13371, 564: 13371, 565: 13371, 566: 13371, 567: 13371, 568: 13371, 569: 2522, 570: 5749, 571: 5749, 572: 5749, 573: 19358, 574: 19358, 575: 19358, 576: 19358, 577: 19358, 578: 19358, 579: 19358, 580: 19358, 581: 19358, 582: 16654, 583: 16654, 584: 16654, 585: 16654, 586: 16654, 587: 16654, 588: 16654, 589: 11815, 590: 11815, 591: 11815, 592: 11815, 593: 11815, 594: 22103, 595: 22103, 596: 22103, 597: 22103, 598: 22103, 599: 22103, 600: 22103, 601: 22103, 602: 2546, 603: 2546, 604: 2546, 605: 9802, 606: 9802, 607: 9802, 608: 6947, 609: 6947, 610: 5988, 611: 5988, 612: 14583, 613: 14583, 614: 14583, 615: 14583, 616: 16223, 617: 16223, 618: 16223, 619: 16223, 620: 16223, 621: 7811, 622: 7811, 623: 11846, 624: 11846, 625: 17351, 626: 17351, 627: 17351, 628: 17351, 629: 17351, 630: 4962, 631: 4962, 632: 18363, 633: 18363, 634: 18363, 635: 18363, 636: 18363, 637: 18363, 638: 18363, 639: 18363, 640: 18363, 641: 5058, 642: 24065, 643: 24065, 644: 24065, 645: 24065, 646: 24065, 647: 24065, 648: 24065, 649: 24065, 650: 24065, 651: 24065, 652: 24065, 653: 24065, 654: 24065, 655: 24065, 656: 12920, 657: 12920, 658: 8020, 659: 6059, 660: 6059, 661: 7347, 662: 7347, 663: 7347, 664: 316, 665: 6651, 666: 35228, 667: 35228, 668: 35228, 669: 35228, 670: 16069, 671: 16069, 672: 16069, 673: 16069, 674: 16069, 675: 16069, 676: 16069, 677: 16069, 678: 20286, 679: 20286, 680: 20286, 681: 20286, 682: 20286, 683: 12756, 684: 12756, 685: 16222, 686: 16222, 687: 16222, 688: 16222, 689: 16222, 690: 10300, 691: 10300, 692: 19242, 693: 19242, 694: 19242, 695: 19242, 696: 19242, 697: 19242, 698: 19242, 699: 19242, 700: 19242, 701: 20648, 702: 20648, 703: 20648, 704: 20648, 705: 20648, 706: 20648, 707: 20648, 708: 20648, 709: 20648, 710: 3808, 711: 2591, 712: 15199, 713: 15199, 714: 15199, 715: 15199, 716: 15199, 717: 27498, 718: 27498, 719: 18975, 720: 18975, 721: 18975, 722: 18975, 723: 18975, 724: 18975, 725: 18975, 726: 18975, 727: 18975, 728: 22181, 729: 22181, 730: 22181, 731: 22181, 732: 22181, 733: 22181, 734: 22181, 735: 22181, 736: 14154, 737: 14154, 738: 14154, 739: 14154, 740: 14154, 741: 14154, 742: 14154, 743: 14154, 744: 14154, 745: 20042, 746: 20042, 747: 20042, 748: 20042, 749: 20042, 750: 20042, 751: 20042, 752: 11475, 753: 11475, 754: 11475, 755: 11475, 756: 11475, 757: 8804, 758: 8804, 759: 8804, 760: 8804, 761: 8804, 762: 8804, 763: 10674, 764: 10674, 765: 10674, 766: 10674, 767: 30974, 768: 30974, 769: 30974, 770: 30974, 771: 30974, 772: 30974, 773: 14920, 774: 14920, 775: 14920, 776: 14920, 777: 1047, 778: 1243, 779: 19962, 780: 19962, 781: 19962, 782: 19962, 783: 19962, 784: 19962, 785: 19962, 786: 19962, 787: 19962, 788: 1921, 789: 1921, 790: 16704, 791: 16704, 792: 16704, 793: 16704, 794: 16704, 795: 16704, 796: 34897, 797: 34897, 798: 34897, 799: 34897, 800: 34897, 801: 34897, 802: 34897, 803: 34897, 804: 34897, 805: 34897, 806: 34897, 807: 20868, 808: 20868, 809: 20868, 810: 20868, 811: 20868, 812: 20868, 813: 20868, 814: 2252, 815: 8646, 816: 8646, 817: 8646, 818: 8646, 819: 8646, 820: 1027, 821: 8131, 822: 8131, 823: 8131, 824: 4869, 825: 4869, 826: 4869, 827: 7762, 828: 646, 829: 5216, 830: 5216, 831: 5216, 832: 20403, 833: 20403, 834: 20403, 835: 20403, 836: 20403, 837: 20403, 838: 20403, 839: 20403, 840: 20403, 841: 20403, 842: 20403, 843: 20403, 844: 12096, 845: 12096, 846: 12096, 847: 12096, 848: 12096, 849: 9665, 850: 9665, 851: 9665, 852: 9665, 853: 9665, 854: 9665, 855: 9665, 856: 9665, 857: 19900, 858: 19900, 859: 19900, 860: 19900, 861: 19900, 862: 17916, 863: 17916, 864: 17916, 865: 17916, 866: 17916, 867: 17916, 868: 17916, 869: 2778, 870: 12577, 871: 12577, 872: 12577, 873: 12577, 874: 2924, 875: 5950, 876: 998, 877: 17387, 878: 17387, 879: 17387, 880: 17387, 881: 17387, 882: 10339, 883: 10339, 884: 10339, 885: 3638, 886: 3638, 887: 3638, 888: 8378, 889: 8378, 890: 8378, 891: 6579, 892: 6579, 893: 10475, 894: 10475, 895: 10475, 896: 11821, 897: 11821, 898: 8769, 899: 1036, 900: 2694, 901: 16175, 902: 16175, 903: 16175, 904: 16175, 905: 16175, 906: 16175, 907: 16175, 908: 16175, 909: 20427, 910: 20427, 911: 20427, 912: 20427, 913: 20427, 914: 20427, 915: 20427, 916: 20427, 917: 7712, 918: 7712, 919: 7712, 920: 7712, 921: 17637, 922: 17637, 923: 17637, 924: 17637, 925: 12062, 926: 12062, 927: 12062, 928: 4742, 929: 11139, 930: 11139, 931: 11139, 932: 11139, 933: 4940, 934: 11724, 935: 11724, 936: 11724, 937: 11724, 938: 11724, 939: 4577, 940: 4577, 941: 4577, 942: 17504, 943: 17504, 944: 17504, 945: 17504, 946: 17504, 947: 17504, 948: 17504, 949: 17504, 950: 17504, 951: 5887, 952: 5887, 953: 5887, 954: 25238, 955: 25238, 956: 25238, 957: 25238, 958: 25238, 959: 25238, 960: 25238, 961: 25238, 962: 25238, 963: 25238, 964: 25238, 965: 25238, 966: 25238, 967: 16036, 968: 16036, 969: 16036, 970: 16036, 971: 16036, 972: 11112, 973: 11112, 974: 11112, 975: 11112, 976: 11112, 977: 11112, 978: 11112, 979: 16838, 980: 16838, 981: 16838, 982: 16838, 983: 20960, 984: 20960, 985: 20960, 986: 20960, 987: 20960, 988: 20960, 989: 20960, 990: 20960, 991: 20960, 992: 20960, 993: 778, 994: 2879, 995: 3567, 996: 10885, 997: 10885, 998: 10885, 999: 10885, 1000: 10885, 1001: 10885, 1002: 10008, 1003: 10008, 1004: 10008, 1005: 10008, 1006: 18409, 1007: 18409, 1008: 18409, 1009: 18409, 1010: 18409},
                                    'q_start': {0: 0, 1: 576, 2: 2231, 3: 3005, 4: 5461, 5: 7523, 6: 8413, 7: 8844, 8: 838, 9: 2623, 10: 3441, 11: 9, 12: 6410, 13: 6558, 14: 7625, 15: 10241, 16: 13782, 17: 15072, 18: 19263, 19: 21207, 20: 21886, 21: 4505, 22: 5722, 23: 6282, 24: 7079, 25: 15432, 26: 16514, 27: 51, 28: 2917, 29: 3012, 30: 3247, 31: 4613, 32: 6894, 33: 10397, 34: 11647, 35: 15806, 36: 17740, 37: 18388, 38: 154, 39: 339, 40: 14, 41: 8206, 42: 9001, 43: 9517, 44: 10887, 45: 10929, 46: 74, 47: 6362, 48: 4, 49: 1656, 50: 3881, 51: 91, 52: 0, 53: 18, 54: 5159, 55: 6398, 56: 7680, 57: 0, 58: 0, 59: 435, 60: 1621, 61: 2921, 62: 3281, 63: 3655, 64: 5800, 65: 12481, 66: 14849, 67: 0, 68: 927, 69: 2238, 70: 242, 71: 1033, 72: 3581, 73: 5014, 74: 0, 75: 1466, 76: 2038, 77: 2390, 78: 3075, 79: 5216, 80: 13541, 81: 16936, 82: 25892, 83: 27531, 84: 28287, 85: 27, 86: 6589, 87: 7792, 88: 9058, 89: 9156, 90: 156, 91: 6912, 92: 4, 93: 6041, 94: 7230, 95: 749, 96: 1756, 97: 1809, 98: 2151, 99: 0, 100: 526, 101: 1021, 102: 2689, 103: 2913, 104: 3885, 105: 6284, 106: 7479, 107: 8874, 108: 12452, 109: 3698, 110: 4684, 111: 5060, 112: 10509, 113: 11021, 114: 12967, 115: 13967, 116: 4958, 117: 7784, 118: 4056, 119: 4335, 120: 6005, 121: 6746, 122: 14801, 123: 14946, 124: 2536, 125: 4246, 126: 5576, 127: 5872, 128: 6634, 129: 14681, 130: 15984, 131: 18258, 132: 21302, 133: 0, 134: 5453, 135: 0, 136: 5558, 137: 7034, 138: 3341, 139: 3690, 140: 5329, 141: 6103, 142: 14168, 143: 15284, 144: 2614, 145: 4191, 146: 4946, 147: 13026, 148: 0, 149: 1843, 150: 2567, 151: 3084, 152: 4413, 153: 4787, 154: 13902, 155: 0, 156: 1647, 157: 1726, 158: 1958, 159: 3020, 160: 3288, 161: 5576, 162: 9065, 163: 10323, 164: 958, 165: 1198, 166: 2940, 167: 3722, 168: 1, 169: 0, 170: 501, 171: 2336, 172: 3353, 173: 5837, 174: 7039, 175: 8474, 176: 1, 177: 822, 178: 1368, 179: 7309, 180: 12687, 181: 12764, 182: 13168, 183: 633, 184: 4406, 185: 6019, 186: 7459, 187: 11746, 188: 11973, 189: 12967, 190: 13332, 191: 864, 192: 1226, 193: 2383, 194: 2873, 195: 3632, 196: 12088, 197: 13189, 198: 15901, 199: 3, 200: 4406, 201: 6641, 202: 10435, 203: 12202, 204: 18507, 205: 19922, 206: 28481, 207: 29840, 208: 31223, 209: 31316, 210: 1, 211: 1160, 212: 2400, 213: 3806, 214: 7461, 215: 1209, 216: 1278, 217: 1572, 218: 3296, 219: 4151, 220: 304, 221: 2486, 222: 6185, 223: 7550, 224: 7786, 225: 9160, 226: 13393, 227: 13618, 228: 14679, 229: 14995, 230: 0, 231: 7992, 232: 14728, 233: 15973, 234: 20318, 235: 15, 236: 7217, 237: 8869, 238: 9647, 239: 11032, 240: 17, 241: 5, 242: 1375, 243: 2416, 244: 2749, 245: 10819, 246: 11594, 247: 12121, 248: 2, 249: 7549, 250: 7827, 251: 8181, 252: 9845, 253: 10600, 254: 18499, 255: 18744, 256: 2736, 257: 2958, 258: 4353, 259: 2037, 260: 2270, 261: 3349, 262: 3673, 263: 0, 264: 2466, 265: 15859, 266: 16351, 267: 17735, 268: 18047, 269: 18807, 270: 7, 271: 232, 272: 995, 273: 1, 274: 7986, 275: 8408, 276: 9447, 277: 12001, 278: 15505, 279: 16731, 280: 2084, 281: 3122, 282: 3453, 283: 11319, 284: 12560, 285: 17607, 286: 320, 287: 7517, 288: 0, 289: 1349, 290: 2073, 291: 2580, 292: 4691, 293: 7688, 294: 14363, 295: 16819, 296: 26043, 297: 2, 298: 375, 299: 8231, 300: 9497, 301: 15083, 302: 16262, 303: 20257, 304: 22749, 305: 244, 306: 4135, 307: 5544, 308: 9974, 309: 11358, 310: 19255, 311: 20487, 312: 25520, 313: 2, 314: 4278, 315: 5322, 316: 4362, 317: 6032, 318: 6776, 319: 15057, 320: 16384, 321: 1, 322: 1013, 323: 2620, 324: 4006, 325: 8440, 326: 9502, 327: 9816, 328: 17588, 329: 18773, 330: 20431, 331: 23746, 332: 30246, 333: 0, 334: 405, 335: 595, 336: 2574, 337: 6377, 338: 6589, 339: 7057, 340: 9187, 341: 12869, 342: 14639, 343: 20935, 344: 21156, 345: 22524, 346: 0, 347: 6757, 348: 8013, 349: 1497, 350: 3165, 351: 3954, 352: 12099, 353: 13111, 354: 13389, 355: 15633, 356: 0, 357: 2175, 358: 3383, 359: 5420, 360: 4, 361: 4200, 362: 5275, 363: 5527, 364: 8033, 365: 11571, 366: 0, 367: 4311, 368: 4532, 369: 5953, 370: 13913, 371: 15156, 372: 16311, 373: 16826, 374: 17296, 375: 18824, 376: 1, 377: 7548, 378: 8789, 379: 10200, 380: 10335, 381: 12978, 382: 691, 383: 1581, 384: 1804, 385: 3148, 386: 5412, 387: 8828, 388: 10034, 389: 14076, 390: 16328, 391: 16596, 392: 20020, 393: 21919, 394: 21982, 395: 22117, 396: 23070, 397: 23311, 398: 25523, 399: 82, 400: 2747, 401: 2971, 402: 4380, 403: 4082, 404: 5768, 405: 6537, 406: 14644, 407: 15676, 408: 15942, 409: 18277, 410: 21757, 411: 22993, 412: 91, 413: 581, 414: 1310, 415: 1797, 416: 3112, 417: 3163, 418: 13655, 419: 15946, 420: 1, 421: 3809, 422: 4053, 423: 5115, 424: 5389, 425: 1612, 426: 3259, 427: 4033, 428: 0, 429: 3175, 430: 4783, 431: 6143, 432: 10539, 433: 11591, 434: 11918, 435: 188, 436: 521, 437: 0, 438: 4540, 439: 5419, 440: 7852, 441: 0, 442: 1660, 443: 5066, 444: 13865, 445: 14814, 446: 15098, 447: 16453, 448: 16732, 449: 17508, 450: 3, 451: 2348, 452: 3736, 453: 3986, 454: 5370, 455: 5983, 456: 10279, 457: 11659, 458: 19581, 459: 20778, 460: 5, 461: 321, 462: 8495, 463: 8639, 464: 476, 465: 1803, 466: 2090, 467: 2843, 468: 2982, 469: 4673, 470: 11263, 471: 11653, 472: 12612, 473: 13325, 474: 13463, 475: 14468, 476: 15, 477: 21271, 478: 27215, 479: 27440, 480: 28820, 481: 1, 482: 47, 483: 434, 484: 8609, 485: 9716, 486: 9864, 487: 11218, 488: 11261, 489: 651, 490: 5050, 491: 2, 492: 6042, 493: 6181, 494: 3453, 495: 6302, 496: 7335, 497: 9882, 498: 13309, 499: 14518, 500: 679, 501: 188, 502: 38, 503: 0, 504: 863, 505: 5169, 506: 5387, 507: 6754, 508: 72, 509: 3609, 510: 3835, 511: 5155, 512: 212, 513: 4916, 514: 6013, 515: 8448, 516: 13548, 517: 14815, 518: 3, 519: 6504, 520: 7758, 521: 12918, 522: 0, 523: 8, 524: 5579, 525: 9864, 526: 11056, 527: 11226, 528: 11517, 529: 12295, 530: 0, 531: 945, 532: 1247, 533: 2023, 534: 3406, 535: 4620, 536: 4753, 537: 5063, 538: 5829, 539: 0, 540: 4174, 541: 5188, 542: 13, 543: 416, 544: 1661, 545: 12, 546: 734, 547: 1250, 548: 3052, 549: 4066, 550: 6519, 551: 7772, 552: 9196, 553: 457, 554: 4096, 555: 1, 556: 339, 557: 19, 558: 3365, 559: 4493, 560: 4891, 561: 0, 562: 447, 563: 1, 564: 3334, 565: 4563, 566: 5783, 567: 6696, 568: 9640, 569: 1, 570: 0, 571: 4325, 572: 5356, 573: 4, 574: 1608, 575: 2285, 576: 4472, 577: 8171, 578: 9792, 579: 11191, 580: 15579, 581: 16974, 582: 0, 583: 3433, 584: 5035, 585: 5802, 586: 14019, 587: 15071, 588: 15326, 589: 1570, 590: 3427, 591: 5020, 592: 5093, 593: 5856, 594: 50, 595: 4867, 596: 8424, 597: 11896, 598: 13130, 599: 17251, 600: 19078, 601: 19799, 602: 0, 603: 345, 604: 1576, 605: 0, 606: 7068, 607: 8273, 608: 218, 609: 5724, 610: 10, 611: 1802, 612: 2, 613: 5728, 614: 6998, 615: 14020, 616: 2474, 617: 3912, 618: 11924, 619: 13170, 620: 14719, 621: 70, 622: 6869, 623: 821, 624: 6813, 625: 6, 626: 1262, 627: 2484, 628: 4968, 629: 14431, 630: 208, 631: 2845, 632: 1769, 633: 2004, 634: 3093, 635: 3420, 636: 11324, 637: 12050, 638: 12560, 639: 13881, 640: 14257, 641: 5, 642: 2295, 643: 2793, 644: 3747, 645: 4402, 646: 5990, 647: 6064, 648: 6823, 649: 14799, 650: 14940, 651: 15983, 652: 16234, 653: 21819, 654: 21916, 655: 23175, 656: 4582, 657: 8183, 658: 447, 659: 160, 660: 5913, 661: 803, 662: 1028, 663: 2456, 664: 15, 665: 5844, 666: 7352, 667: 15130, 668: 16403, 669: 23729, 670: 3141, 671: 4328, 672: 4533, 673: 9982, 674: 10384, 675: 10480, 676: 12350, 677: 13371, 678: 661, 679: 6745, 680: 8198, 681: 16378, 682: 17644, 683: 2123, 684: 4706, 685: 4380, 686: 6083, 687: 6834, 688: 15042, 689: 16088, 690: 4803, 691: 6174, 692: 111, 693: 2529, 694: 3909, 695: 8077, 696: 8293, 697: 9332, 698: 9657, 699: 17546, 700: 18788, 701: 512, 702: 1524, 703: 1570, 704: 1911, 705: 10014, 706: 11239, 707: 12649, 708: 12785, 709: 16333, 710: 100, 711: 3, 712: 3, 713: 1076, 714: 1281, 715: 2040, 716: 10474, 717: 699, 718: 23861, 719: 3, 720: 1441, 721: 3530, 722: 4395, 723: 4838, 724: 10408, 725: 10633, 726: 14211, 727: 17886, 728: 350, 729: 5066, 730: 6730, 731: 7484, 732: 15511, 733: 15738, 734: 16797, 735: 19351, 736: 113, 737: 1261, 738: 3130, 739: 4155, 740: 6657, 741: 7901, 742: 9217, 743: 9309, 744: 13035, 745: 0, 746: 4203, 747: 5433, 748: 6949, 749: 7097, 750: 10515, 751: 17292, 752: 6, 753: 2307, 754: 3789, 755: 8553, 756: 9937, 757: 121, 758: 982, 759: 2478, 760: 3000, 761: 4809, 762: 5667, 763: 5, 764: 3111, 765: 4467, 766: 6714, 767: 2154, 768: 7728, 769: 14079, 770: 22605, 771: 24433, 772: 25211, 773: 3, 774: 7030, 775: 10603, 776: 13681, 777: 0, 778: 4, 779: 405, 780: 2221, 781: 2545, 782: 2901, 783: 5239, 784: 8867, 785: 15928, 786: 16134, 787: 17493, 788: 1, 789: 1132, 790: 0, 791: 6974, 792: 8199, 793: 9523, 794: 9568, 795: 13175, 796: 60, 797: 12529, 798: 14384, 799: 18067, 800: 18289, 801: 19291, 802: 19667, 803: 27544, 804: 28752, 805: 29924, 806: 30911, 807: 0, 808: 10957, 809: 12405, 810: 16824, 811: 17042, 812: 18052, 813: 18443, 814: 527, 815: 1, 816: 2500, 817: 2551, 818: 3760, 819: 7106, 820: 87, 821: 1952, 822: 3037, 823: 3360, 824: 110, 825: 1151, 826: 1487, 827: 3156, 828: 3, 829: 1199, 830: 2831, 831: 3550, 832: 3, 833: 282, 834: 534, 835: 8456, 836: 8546, 837: 8782, 838: 9802, 839: 10110, 840: 10832, 841: 12325, 842: 15831, 843: 17047, 844: 60, 845: 3748, 846: 4454, 847: 4954, 848: 9944, 849: 1029, 850: 2383, 851: 2683, 852: 3426, 853: 5862, 854: 7866, 855: 8710, 856: 9151, 857: 3643, 858: 12235, 859: 13869, 860: 15623, 861: 16437, 862: 5, 863: 6970, 864: 8339, 865: 9529, 866: 9682, 867: 11696, 868: 12682, 869: 7, 870: 927, 871: 1225, 872: 1979, 873: 11064, 874: 0, 875: 37, 876: 4, 877: 2135, 878: 3539, 879: 11593, 880: 12558, 881: 12855, 882: 272, 883: 8531, 884: 9560, 885: 3, 886: 2536, 887: 2683, 888: 5, 889: 338, 890: 1216, 891: 0, 892: 5058, 893: 367, 894: 4432, 895: 10152, 896: 0, 897: 3681, 898: 7626, 899: 247, 900: 0, 901: 112, 902: 3723, 903: 5276, 904: 7159, 905: 7302, 906: 9708, 907: 9869, 908: 11713, 909: 1, 910: 574, 911: 5923, 912: 6635, 913: 7145, 914: 8517, 915: 11332, 916: 18224, 917: 0, 918: 4132, 919: 6143, 920: 7180, 921: 0, 922: 8796, 923: 10535, 924: 11336, 925: 0, 926: 4608, 927: 5877, 928: 0, 929: 0, 930: 8410, 931: 9485, 932: 9818, 933: 16, 934: 683, 935: 917, 936: 2093, 937: 2433, 938: 10166, 939: 0, 940: 3651, 941: 4156, 942: 0, 943: 799, 944: 1316, 945: 2307, 946: 4696, 947: 5919, 948: 7181, 949: 7220, 950: 11339, 951: 6, 952: 1657, 953: 3054, 954: 19, 955: 3215, 956: 3382, 957: 4423, 958: 4718, 959: 6980, 960: 10446, 961: 11677, 962: 15739, 963: 18297, 964: 22002, 965: 24082, 966: 25034, 967: 5239, 968: 6636, 969: 10851, 970: 11063, 971: 12477, 972: 0, 973: 3042, 974: 4317, 975: 5995, 976: 6772, 977: 9202, 978: 10525, 979: 1266, 980: 3077, 981: 4039, 982: 10418, 983: 2088, 984: 3794, 985: 4533, 986: 6987, 987: 8794, 988: 9026, 989: 9789, 990: 9902, 991: 10319, 992: 16022, 993: 365, 994: 18, 995: 82, 996: 0, 997: 3394, 998: 3647, 999: 5027, 1000: 7270, 1001: 10370, 1002: 0, 1003: 1067, 1004: 5493, 1005: 6895, 1006: 7, 1007: 2906, 1008: 3660, 1009: 4175, 1010: 16212},
                                    'q_end': {0: 264, 1: 1773, 2: 2993, 3: 5424, 4: 6442, 5: 8303, 6: 8829, 7: 9150, 8: 2114, 9: 3430, 10: 8461, 11: 6272, 12: 6656, 13: 7673, 14: 7889, 15: 13457, 16: 15118, 17: 18895, 18: 21153, 19: 21634, 20: 24573, 21: 5752, 22: 5901, 23: 7073, 24: 15154, 25: 16557, 26: 16780, 27: 2963, 28: 3047, 29: 3290, 30: 4365, 31: 8682, 32: 10076, 33: 11696, 34: 15483, 35: 17680, 36: 18157, 37: 20179, 38: 367, 39: 8450, 40: 8183, 41: 9049, 42: 9355, 43: 10785, 44: 11041, 45: 11082, 46: 5378, 47: 6806, 48: 1624, 49: 2686, 50: 4680, 51: 1600, 52: 4487, 53: 5148, 54: 5940, 55: 7622, 56: 7763, 57: 3762, 58: 323, 59: 1133, 60: 2820, 61: 3067, 62: 4055, 63: 5296, 64: 12000, 65: 15533, 66: 16499, 67: 920, 68: 1695, 69: 2933, 70: 1026, 71: 5653, 72: 4727, 73: 9319, 74: 1312, 75: 2076, 76: 2288, 77: 2650, 78: 5262, 79: 14195, 80: 16473, 81: 22974, 82: 27078, 83: 28279, 84: 34617, 85: 6579, 86: 7333, 87: 8965, 88: 9210, 89: 9324, 90: 1947, 91: 10405, 92: 6030, 93: 6788, 94: 8230, 95: 1853, 96: 2006, 97: 2092, 98: 9359, 99: 518, 100: 920, 101: 1762, 102: 2819, 103: 3853, 104: 6276, 105: 7014, 106: 8696, 107: 9056, 108: 15922, 109: 4778, 110: 4922, 111: 10501, 112: 10920, 113: 11759, 114: 13934, 115: 15314, 116: 6272, 117: 16383, 118: 4208, 119: 5521, 120: 6734, 121: 14655, 122: 15037, 123: 16000, 124: 3781, 125: 5426, 126: 5920, 127: 6619, 128: 14396, 129: 15766, 130: 20003, 131: 21346, 132: 25529, 133: 5441, 134: 5663, 135: 5541, 136: 6545, 137: 8405, 138: 3515, 139: 4873, 140: 6088, 141: 13858, 142: 15330, 143: 15560, 144: 3740, 145: 4934, 146: 12746, 147: 14096, 148: 1830, 149: 2614, 150: 2916, 151: 4320, 152: 4567, 153: 6691, 154: 14955, 155: 1676, 156: 1853, 157: 2011, 158: 3063, 159: 3281, 160: 7367, 161: 8727, 162: 10368, 163: 13733, 164: 1103, 165: 2471, 166: 3715, 167: 10967, 168: 1407, 169: 404, 170: 1047, 171: 3321, 172: 5768, 173: 6574, 174: 8273, 175: 8650, 176: 525, 177: 1236, 178: 7351, 179: 12331, 180: 12857, 181: 13200, 182: 14354, 183: 4439, 184: 5688, 185: 9281, 186: 11683, 187: 12012, 188: 13063, 189: 13199, 190: 15674, 191: 1039, 192: 2420, 193: 2567, 194: 3618, 195: 11808, 196: 13230, 197: 13444, 198: 19170, 199: 3602, 200: 6322, 201: 10481, 202: 11822, 203: 15743, 204: 19632, 205: 28463, 206: 29328, 207: 31125, 208: 31379, 209: 31506, 210: 1147, 211: 1938, 212: 3614, 213: 3979, 214: 13857, 215: 1385, 216: 1434, 217: 2796, 218: 4137, 219: 7981, 220: 2175, 221: 6223, 222: 7455, 223: 7740, 224: 11025, 225: 13352, 226: 13664, 227: 14727, 228: 14950, 229: 18529, 230: 4024, 231: 14720, 232: 15485, 233: 17210, 234: 26631, 235: 4379, 236: 8414, 237: 9639, 238: 10368, 239: 13360, 240: 2304, 241: 1090, 242: 2465, 243: 2704, 244: 10810, 245: 11626, 246: 11910, 247: 13325, 248: 4418, 249: 7869, 250: 7997, 251: 9368, 252: 10592, 253: 18448, 254: 18788, 255: 19807, 256: 3013, 257: 4075, 258: 5045, 259: 2316, 260: 3393, 261: 3626, 262: 8994, 263: 3157, 264: 5525, 265: 16015, 266: 17579, 267: 18088, 268: 18797, 269: 20695, 270: 279, 271: 983, 272: 3174, 273: 7932, 274: 8135, 275: 9490, 276: 9709, 277: 15169, 278: 16779, 279: 17932, 280: 3167, 281: 3402, 282: 11309, 283: 12092, 284: 13745, 285: 20972, 286: 4978, 287: 8565, 288: 1336, 289: 2113, 290: 2415, 291: 3802, 292: 8288, 293: 13887, 294: 17505, 295: 26086, 296: 27548, 297: 4177, 298: 7929, 299: 9270, 300: 13467, 301: 16302, 302: 19947, 303: 22053, 304: 23831, 305: 4175, 306: 7409, 307: 9742, 308: 11078, 309: 19247, 310: 20026, 311: 21665, 312: 26304, 313: 4001, 314: 5362, 315: 5585, 316: 5583, 317: 6767, 318: 14777, 319: 16136, 320: 17977, 321: 1059, 322: 2281, 323: 5907, 324: 8213, 325: 9539, 326: 9771, 327: 17565, 328: 18322, 329: 19957, 330: 21589, 331: 29778, 332: 32503, 333: 1437, 334: 581, 335: 2618, 336: 6336, 337: 6617, 338: 7001, 339: 8864, 340: 12912, 341: 14136, 342: 17886, 343: 21204, 344: 22249, 345: 25695, 346: 6746, 347: 7546, 348: 9236, 349: 2714, 350: 3933, 351: 11805, 352: 13157, 353: 13377, 354: 17442, 355: 18776, 356: 2164, 357: 2933, 358: 4573, 359: 7735, 360: 3915, 361: 5320, 362: 5531, 363: 9899, 364: 11229, 365: 12418, 366: 4860, 367: 4583, 368: 5671, 369: 13903, 370: 14681, 371: 16356, 372: 16419, 373: 18362, 374: 18818, 375: 19638, 376: 7541, 377: 8312, 378: 10011, 379: 10376, 380: 10672, 381: 14382, 382: 1532, 383: 1850, 384: 2920, 385: 6716, 386: 8514, 387: 10078, 388: 13759, 389: 15885, 390: 16554, 391: 20059, 392: 21965, 393: 22028, 394: 22148, 395: 22754, 396: 23336, 397: 25570, 398: 30886, 399: 501, 400: 3015, 401: 4100, 402: 11804, 403: 5308, 404: 6528, 405: 14354, 406: 15717, 407: 15929, 408: 20076, 409: 21431, 410: 23040, 411: 24165, 412: 572, 413: 1355, 414: 1426, 415: 2973, 416: 3277, 417: 3344, 418: 16642, 419: 21062, 420: 3761, 421: 4097, 422: 5157, 423: 5379, 424: 6674, 425: 2797, 426: 4025, 427: 4694, 428: 3218, 429: 4449, 430: 8019, 431: 10307, 432: 11637, 433: 11873, 434: 13109, 435: 458, 436: 1550, 437: 1647, 438: 4954, 439: 6628, 440: 12350, 441: 2338, 442: 4601, 443: 11376, 444: 14650, 445: 14970, 446: 16299, 447: 16778, 448: 17488, 449: 18589, 450: 2392, 451: 3624, 452: 3939, 453: 7249, 454: 8276, 455: 10045, 456: 11380, 457: 19573, 458: 20323, 459: 21988, 460: 2006, 461: 8353, 462: 8731, 463: 9107, 464: 1649, 465: 2138, 466: 2835, 467: 7477, 468: 4212, 469: 5426, 470: 11556, 471: 12386, 472: 12863, 473: 13457, 474: 14441, 475: 15382, 476: 21313, 477: 24842, 478: 27484, 479: 28530, 480: 32046, 481: 146, 482: 293, 483: 8603, 484: 9396, 485: 9901, 486: 11077, 487: 11364, 488: 11439, 489: 4745, 490: 6207, 491: 5909, 492: 6283, 493: 7148, 494: 6004, 495: 7382, 496: 7600, 497: 12991, 498: 14564, 499: 14822, 500: 7234, 501: 3121, 502: 6924, 503: 2792, 504: 5163, 505: 5435, 506: 6481, 507: 12848, 508: 3558, 509: 3879, 510: 4925, 511: 6887, 512: 4616, 513: 6060, 514: 6280, 515: 11789, 516: 14859, 517: 16636, 518: 6494, 519: 7295, 520: 8988, 521: 19546, 522: 6964, 523: 6105, 524: 8878, 525: 11073, 526: 11214, 527: 11560, 528: 12283, 529: 15086, 530: 775, 531: 1294, 532: 2016, 533: 4134, 534: 4599, 535: 4764, 536: 5102, 537: 5822, 538: 11301, 539: 3888, 540: 5230, 541: 5443, 542: 407, 543: 1205, 544: 2878, 545: 726, 546: 1152, 547: 1976, 548: 4037, 549: 6511, 550: 7299, 551: 9000, 552: 9374, 553: 4047, 554: 4381, 555: 329, 556: 1642, 557: 3121, 558: 4601, 559: 4756, 560: 7249, 561: 422, 562: 5590, 563: 3326, 564: 4090, 565: 5736, 566: 5873, 567: 10216, 568: 13361, 569: 2291, 570: 4052, 571: 5395, 572: 5608, 573: 1548, 574: 1832, 575: 4151, 576: 8218, 577: 9445, 578: 13036, 579: 15373, 580: 16698, 581: 19346, 582: 579, 583: 4588, 584: 5795, 585: 13722, 586: 15112, 587: 15347, 588: 15741, 589: 2451, 590: 4618, 591: 5142, 592: 5845, 593: 11813, 594: 4599, 595: 5949, 596: 11557, 597: 13173, 598: 16935, 599: 19077, 600: 19427, 601: 22100, 602: 334, 603: 1118, 604: 2546, 605: 7056, 606: 7821, 607: 9450, 608: 5430, 609: 6799, 610: 1807, 611: 5988, 612: 5714, 613: 6517, 614: 8218, 615: 14568, 616: 3617, 617: 11916, 618: 12709, 619: 14387, 620: 14881, 621: 6001, 622: 7811, 623: 6485, 624: 7928, 625: 1252, 626: 2000, 627: 3667, 628: 8173, 629: 17343, 630: 2290, 631: 4962, 632: 2046, 633: 3133, 634: 3371, 635: 11314, 636: 12097, 637: 12393, 638: 13773, 639: 14035, 640: 14840, 641: 5054, 642: 2436, 643: 3931, 644: 4072, 645: 5610, 646: 6108, 647: 6809, 648: 14650, 649: 15024, 650: 16029, 651: 16244, 652: 20240, 653: 21967, 654: 23221, 655: 24033, 656: 5927, 657: 12920, 658: 8013, 659: 5641, 660: 6059, 661: 1080, 662: 2175, 663: 7347, 664: 311, 665: 6639, 666: 15114, 667: 15921, 668: 17604, 669: 29342, 670: 4263, 671: 4486, 672: 9975, 673: 10386, 674: 10598, 675: 11272, 676: 13331, 677: 15652, 678: 4013, 679: 7904, 680: 16362, 681: 17176, 682: 18885, 683: 3353, 684: 12660, 685: 5604, 686: 6824, 687: 14754, 688: 16120, 689: 16222, 690: 5901, 691: 10296, 692: 2574, 693: 5759, 694: 8056, 695: 8337, 696: 9376, 697: 9608, 698: 17538, 699: 18331, 700: 19242, 701: 1612, 702: 1762, 703: 1858, 704: 10006, 705: 10779, 706: 12456, 707: 12820, 708: 13133, 709: 20648, 710: 3808, 711: 2589, 712: 776, 713: 1234, 714: 2030, 715: 10179, 716: 11527, 717: 6764, 718: 27440, 719: 1408, 720: 2447, 721: 4298, 722: 4845, 723: 10361, 724: 10677, 725: 11727, 726: 17268, 727: 18959, 728: 1135, 729: 6266, 730: 7476, 731: 15466, 732: 15773, 733: 16845, 734: 17065, 735: 22181, 736: 865, 737: 1480, 738: 4118, 739: 6646, 740: 7432, 741: 9118, 742: 9354, 743: 9473, 744: 14146, 745: 4190, 746: 4952, 747: 6624, 748: 7285, 749: 8197, 750: 16748, 751: 20040, 752: 2353, 753: 5663, 754: 8068, 755: 9663, 756: 11472, 757: 1345, 758: 2468, 759: 2654, 760: 4311, 761: 5660, 762: 6812, 763: 2830, 764: 4205, 765: 8497, 766: 9835, 767: 8389, 768: 10469, 769: 18664, 770: 23859, 771: 25192, 772: 30969, 773: 6742, 774: 8123, 775: 13725, 776: 14920, 777: 1047, 778: 1240, 779: 2182, 780: 2441, 781: 2897, 782: 4915, 783: 8902, 784: 10111, 785: 16185, 786: 17218, 787: 19962, 788: 858, 789: 1921, 790: 6965, 791: 7732, 792: 9382, 793: 9680, 794: 9750, 795: 16704, 796: 12573, 797: 15717, 798: 18016, 799: 18338, 800: 19391, 801: 19527, 802: 27530, 803: 28280, 804: 29969, 805: 30020, 806: 33028, 807: 11001, 808: 14447, 809: 16820, 810: 17093, 811: 18151, 812: 18305, 813: 20867, 814: 2252, 815: 2544, 816: 2598, 817: 3314, 818: 4961, 819: 7244, 820: 1026, 821: 3087, 822: 3315, 823: 8127, 824: 1198, 825: 1434, 826: 4869, 827: 7760, 828: 642, 829: 2382, 830: 3539, 831: 5212, 832: 279, 833: 521, 834: 8501, 835: 8581, 836: 8825, 837: 9845, 838: 10058, 839: 11255, 840: 14138, 841: 15494, 842: 17093, 843: 20403, 844: 3731, 845: 4487, 846: 4795, 847: 6141, 848: 11538, 849: 2216, 850: 2723, 851: 3409, 852: 5824, 853: 6815, 854: 8612, 855: 9142, 856: 9665, 857: 10021, 858: 13365, 859: 15122, 860: 16418, 861: 19899, 862: 6958, 863: 7844, 864: 9574, 865: 9638, 866: 9830, 867: 11849, 868: 17916, 869: 2769, 870: 1268, 871: 1971, 872: 10750, 873: 12344, 874: 2775, 875: 5950, 876: 998, 877: 3245, 878: 11585, 879: 12377, 880: 12770, 881: 14038, 882: 8242, 883: 9605, 884: 9829, 885: 2398, 886: 2775, 887: 3638, 888: 384, 889: 1207, 890: 8378, 891: 4737, 892: 6212, 893: 3464, 894: 10138, 895: 10473, 896: 3665, 897: 4598, 898: 8730, 899: 1035, 900: 2266, 901: 3706, 902: 4650, 903: 6817, 904: 7349, 905: 7482, 906: 9860, 907: 9984, 908: 13553, 909: 513, 910: 5915, 911: 6678, 912: 6976, 913: 8333, 914: 8697, 915: 17708, 916: 20424, 917: 3706, 918: 4316, 919: 7150, 920: 7709, 921: 4806, 922: 10048, 923: 11320, 924: 17634, 925: 4314, 926: 5697, 927: 6116, 928: 4742, 929: 3023, 930: 9526, 931: 9774, 932: 11139, 933: 4927, 934: 966, 935: 2139, 936: 2375, 937: 9408, 938: 10996, 939: 3644, 940: 4061, 941: 4572, 942: 267, 943: 964, 944: 2277, 945: 4686, 946: 5462, 947: 7124, 948: 7266, 949: 7374, 950: 17494, 951: 1420, 952: 2757, 953: 5886, 954: 3077, 955: 3474, 956: 4458, 957: 4671, 958: 8764, 959: 10129, 960: 11719, 961: 15424, 962: 17566, 963: 22035, 964: 23930, 965: 24715, 966: 25235, 967: 8480, 968: 10785, 969: 11109, 970: 12195, 971: 16036, 972: 584, 973: 3212, 974: 5489, 975: 6738, 976: 9174, 977: 10166, 978: 10693, 979: 2573, 980: 4000, 981: 9689, 982: 13952, 983: 3318, 984: 4525, 985: 6955, 986: 7962, 987: 8893, 988: 9769, 989: 9904, 990: 10312, 991: 15741, 992: 17128, 993: 770, 994: 2384, 995: 3220, 996: 3341, 997: 3691, 998: 4746, 999: 9039, 1000: 10414, 1001: 10867, 1002: 2894, 1003: 5248, 1004: 6625, 1005: 9832, 1006: 2891, 1007: 3704, 1008: 4009, 1009: 5390, 1010: 18396},
                                    'strand': {0: '-', 1: '-', 2: '-', 3: '-', 4: '-', 5: '-', 6: '-', 7: '-', 8: '-', 9: '-', 10: '-', 11: '-', 12: '+', 13: '+', 14: '-', 15: '-', 16: '+', 17: '+', 18: '+', 19: '+', 20: '-', 21: '-', 22: '+', 23: '-', 24: '-', 25: '+', 26: '-', 27: '-', 28: '+', 29: '+', 30: '+', 31: '-', 32: '-', 33: '+', 34: '+', 35: '+', 36: '+', 37: '-', 38: '-', 39: '+', 40: '+', 41: '+', 42: '-', 43: '+', 44: '-', 45: '-', 46: '+', 47: '+', 48: '-', 49: '-', 50: '-', 51: '-', 52: '-', 53: '+', 54: '+', 55: '+', 56: '-', 57: '+', 58: '+', 59: '+', 60: '+', 61: '-', 62: '-', 63: '+', 64: '-', 65: '+', 66: '-', 67: '+', 68: '+', 69: '+', 70: '-', 71: '-', 72: '-', 73: '+', 74: '-', 75: '-', 76: '+', 77: '+', 78: '+', 79: '+', 80: '-', 81: '+', 82: '-', 83: '-', 84: '-', 85: '+', 86: '+', 87: '+', 88: '-', 89: '+', 90: '-', 91: '+', 92: '+', 93: '+', 94: '+', 95: '-', 96: '-', 97: '-', 98: '+', 99: '+', 100: '+', 101: '+', 102: '+', 103: '+', 104: '+', 105: '+', 106: '+', 107: '+', 108: '-', 109: '-', 110: '-', 111: '+', 112: '+', 113: '+', 114: '+', 115: '+', 116: '-', 117: '-', 118: '+', 119: '-', 120: '-', 121: '-', 122: '+', 123: '+', 124: '-', 125: '-', 126: '+', 127: '-', 128: '-', 129: '+', 130: '+', 131: '-', 132: '-', 133: '+', 134: '+', 135: '+', 136: '+', 137: '+', 138: '-', 139: '-', 140: '-', 141: '-', 142: '+', 143: '-', 144: '-', 145: '-', 146: '-', 147: '+', 148: '+', 149: '+', 150: '-', 151: '+', 152: '-', 153: '+', 154: '+', 155: '-', 156: '+', 157: '+', 158: '+', 159: '-', 160: '-', 161: '-', 162: '+', 163: '+', 164: '+', 165: '-', 166: '-', 167: '-', 168: '-', 169: '+', 170: '+', 171: '+', 172: '+', 173: '+', 174: '+', 175: '+', 176: '-', 177: '+', 178: '+', 179: '+', 180: '-', 181: '-', 182: '+', 183: '-', 184: '-', 185: '+', 186: '-', 187: '+', 188: '-', 189: '-', 190: '+', 191: '-', 192: '-', 193: '+', 194: '-', 195: '-', 196: '+', 197: '-', 198: '-', 199: '+', 200: '-', 201: '-', 202: '-', 203: '+', 204: '-', 205: '+', 206: '+', 207: '+', 208: '-', 209: '+', 210: '+', 211: '+', 212: '+', 213: '+', 214: '-', 215: '-', 216: '+', 217: '-', 218: '-', 219: '-', 220: '-', 221: '-', 222: '-', 223: '-', 224: '+', 225: '+', 226: '+', 227: '-', 228: '-', 229: '+', 230: '-', 231: '+', 232: '+', 233: '+', 234: '-', 235: '+', 236: '-', 237: '-', 238: '-', 239: '-', 240: '+', 241: '+', 242: '-', 243: '-', 244: '+', 245: '+', 246: '-', 247: '+', 248: '+', 249: '-', 250: '-', 251: '-', 252: '-', 253: '-', 254: '+', 255: '+', 256: '+', 257: '-', 258: '+', 259: '+', 260: '-', 261: '-', 262: '+', 263: '+', 264: '-', 265: '+', 266: '-', 267: '+', 268: '-', 269: '-', 270: '+', 271: '-', 272: '-', 273: '-', 274: '+', 275: '+', 276: '-', 277: '-', 278: '+', 279: '+', 280: '-', 281: '-', 282: '+', 283: '+', 284: '+', 285: '-', 286: '+', 287: '+', 288: '+', 289: '+', 290: '-', 291: '+', 292: '+', 293: '-', 294: '+', 295: '-', 296: '-', 297: '+', 298: '-', 299: '+', 300: '+', 301: '+', 302: '+', 303: '+', 304: '-', 305: '+', 306: '+', 307: '-', 308: '-', 309: '+', 310: '+', 311: '+', 312: '+', 313: '-', 314: '+', 315: '-', 316: '-', 317: '-', 318: '-', 319: '+', 320: '-', 321: '-', 322: '-', 323: '+', 324: '-', 325: '-', 326: '-', 327: '+', 328: '+', 329: '+', 330: '-', 331: '-', 332: '+', 333: '-', 334: '+', 335: '+', 336: '+', 337: '-', 338: '-', 339: '-', 340: '-', 341: '-', 342: '+', 343: '+', 344: '-', 345: '+', 346: '+', 347: '+', 348: '+', 349: '-', 350: '-', 351: '-', 352: '+', 353: '-', 354: '-', 355: '-', 356: '+', 357: '+', 358: '+', 359: '+', 360: '-', 361: '+', 362: '-', 363: '+', 364: '-', 365: '+', 366: '+', 367: '+', 368: '-', 369: '+', 370: '+', 371: '+', 372: '-', 373: '-', 374: '+', 375: '+', 376: '+', 377: '+', 378: '+', 379: '+', 380: '+', 381: '-', 382: '-', 383: '+', 384: '+', 385: '+', 386: '-', 387: '+', 388: '+', 389: '+', 390: '+', 391: '-', 392: '-', 393: '-', 394: '-', 395: '-', 396: '+', 397: '+', 398: '+', 399: '+', 400: '+', 401: '-', 402: '+', 403: '-', 404: '-', 405: '-', 406: '+', 407: '-', 408: '+', 409: '-', 410: '+', 411: '+', 412: '+', 413: '+', 414: '-', 415: '+', 416: '-', 417: '+', 418: '+', 419: '-', 420: '-', 421: '+', 422: '+', 423: '-', 424: '-', 425: '-', 426: '-', 427: '-', 428: '-', 429: '-', 430: '+', 431: '-', 432: '-', 433: '-', 434: '+', 435: '-', 436: '+', 437: '+', 438: '-', 439: '-', 440: '-', 441: '+', 442: '-', 443: '+', 444: '-', 445: '+', 446: '-', 447: '+', 448: '-', 449: '-', 450: '-', 451: '-', 452: '-', 453: '+', 454: '+', 455: '+', 456: '-', 457: '+', 458: '+', 459: '+', 460: '+', 461: '-', 462: '+', 463: '+', 464: '-', 465: '+', 466: '-', 467: '-', 468: '-', 469: '-', 470: '+', 471: '+', 472: '+', 473: '+', 474: '+', 475: '+', 476: '+', 477: '+', 478: '+', 479: '-', 480: '+', 481: '-', 482: '-', 483: '+', 484: '+', 485: '-', 486: '+', 487: '-', 488: '+', 489: '-', 490: '+', 491: '-', 492: '+', 493: '+', 494: '-', 495: '+', 496: '-', 497: '-', 498: '+', 499: '+', 500: '-', 501: '-', 502: '+', 503: '+', 504: '+', 505: '+', 506: '-', 507: '+', 508: '-', 509: '+', 510: '+', 511: '-', 512: '-', 513: '+', 514: '-', 515: '-', 516: '+', 517: '+', 518: '+', 519: '+', 520: '+', 521: '-', 522: '-', 523: '+', 524: '-', 525: '-', 526: '-', 527: '+', 528: '-', 529: '-', 530: '-', 531: '+', 532: '-', 533: '-', 534: '-', 535: '-', 536: '+', 537: '-', 538: '-', 539: '-', 540: '+', 541: '-', 542: '+', 543: '+', 544: '+', 545: '+', 546: '+', 547: '+', 548: '+', 549: '+', 550: '+', 551: '+', 552: '+', 553: '-', 554: '+', 555: '-', 556: '-', 557: '+', 558: '-', 559: '-', 560: '+', 561: '-', 562: '-', 563: '+', 564: '+', 565: '+', 566: '-', 567: '+', 568: '-', 569: '+', 570: '-', 571: '+', 572: '-', 573: '+', 574: '-', 575: '-', 576: '-', 577: '-', 578: '+', 579: '+', 580: '-', 581: '+', 582: '-', 583: '-', 584: '-', 585: '-', 586: '+', 587: '-', 588: '+', 589: '+', 590: '-', 591: '+', 592: '-', 593: '-', 594: '-', 595: '+', 596: '-', 597: '+', 598: '+', 599: '+', 600: '+', 601: '-', 602: '+', 603: '+', 604: '+', 605: '+', 606: '+', 607: '+', 608: '-', 609: '+', 610: '-', 611: '+', 612: '+', 613: '+', 614: '+', 615: '+', 616: '-', 617: '+', 618: '+', 619: '+', 620: '-', 621: '+', 622: '+', 623: '-', 624: '+', 625: '+', 626: '+', 627: '+', 628: '+', 629: '+', 630: '+', 631: '-', 632: '+', 633: '-', 634: '-', 635: '+', 636: '+', 637: '-', 638: '+', 639: '-', 640: '-', 641: '-', 642: '-', 643: '+', 644: '-', 645: '-', 646: '+', 647: '-', 648: '-', 649: '+', 650: '+', 651: '-', 652: '+', 653: '+', 654: '+', 655: '+', 656: '-', 657: '-', 658: '+', 659: '-', 660: '+', 661: '+', 662: '-', 663: '+', 664: '+', 665: '+', 666: '+', 667: '+', 668: '+', 669: '-', 670: '-', 671: '-', 672: '+', 673: '+', 674: '-', 675: '+', 676: '+', 677: '+', 678: '+', 679: '-', 680: '+', 681: '+', 682: '+', 683: '-', 684: '-', 685: '-', 686: '-', 687: '-', 688: '+', 689: '-', 690: '-', 691: '+', 692: '+', 693: '+', 694: '-', 695: '+', 696: '-', 697: '-', 698: '+', 699: '+', 700: '+', 701: '-', 702: '-', 703: '-', 704: '+', 705: '+', 706: '+', 707: '+', 708: '+', 709: '-', 710: '+', 711: '+', 712: '-', 713: '+', 714: '-', 715: '-', 716: '+', 717: '-', 718: '+', 719: '-', 720: '-', 721: '-', 722: '-', 723: '-', 724: '+', 725: '+', 726: '-', 727: '+', 728: '-', 729: '-', 730: '-', 731: '-', 732: '+', 733: '+', 734: '-', 735: '-', 736: '+', 737: '-', 738: '+', 739: '+', 740: '+', 741: '+', 742: '-', 743: '+', 744: '-', 745: '+', 746: '+', 747: '+', 748: '+', 749: '+', 750: '-', 751: '+', 752: '+', 753: '+', 754: '+', 755: '-', 756: '+', 757: '+', 758: '-', 759: '+', 760: '-', 761: '-', 762: '-', 763: '-', 764: '+', 765: '-', 766: '-', 767: '+', 768: '-', 769: '+', 770: '-', 771: '-', 772: '-', 773: '-', 774: '+', 775: '-', 776: '-', 777: '+', 778: '+', 779: '+', 780: '-', 781: '-', 782: '-', 783: '-', 784: '-', 785: '+', 786: '-', 787: '+', 788: '-', 789: '+', 790: '+', 791: '+', 792: '+', 793: '-', 794: '+', 795: '-', 796: '+', 797: '+', 798: '+', 799: '+', 800: '-', 801: '-', 802: '+', 803: '+', 804: '+', 805: '+', 806: '+', 807: '+', 808: '+', 809: '+', 810: '+', 811: '-', 812: '-', 813: '+', 814: '+', 815: '+', 816: '-', 817: '+', 818: '+', 819: '+', 820: '-', 821: '-', 822: '-', 823: '+', 824: '-', 825: '-', 826: '+', 827: '+', 828: '-', 829: '-', 830: '-', 831: '-', 832: '-', 833: '-', 834: '-', 835: '+', 836: '+', 837: '+', 838: '-', 839: '+', 840: '+', 841: '-', 842: '+', 843: '+', 844: '+', 845: '+', 846: '-', 847: '+', 848: '-', 849: '-', 850: '+', 851: '-', 852: '-', 853: '-', 854: '-', 855: '-', 856: '-', 857: '+', 858: '-', 859: '-', 860: '-', 861: '-', 862: '+', 863: '+', 864: '+', 865: '-', 866: '-', 867: '+', 868: '-', 869: '+', 870: '+', 871: '-', 872: '-', 873: '+', 874: '-', 875: '-', 876: '+', 877: '-', 878: '+', 879: '+', 880: '-', 881: '+', 882: '-', 883: '+', 884: '-', 885: '-', 886: '+', 887: '+', 888: '+', 889: '-', 890: '-', 891: '-', 892: '+', 893: '+', 894: '+', 895: '+', 896: '+', 897: '+', 898: '-', 899: '-', 900: '+', 901: '+', 902: '+', 903: '+', 904: '+', 905: '-', 906: '+', 907: '+', 908: '-', 909: '+', 910: '+', 911: '+', 912: '-', 913: '+', 914: '+', 915: '-', 916: '+', 917: '+', 918: '-', 919: '+', 920: '+', 921: '+', 922: '-', 923: '-', 924: '-', 925: '-', 926: '+', 927: '-', 928: '+', 929: '+', 930: '-', 931: '-', 932: '+', 933: '-', 934: '+', 935: '-', 936: '-', 937: '+', 938: '+', 939: '+', 940: '+', 941: '+', 942: '+', 943: '+', 944: '+', 945: '+', 946: '+', 947: '+', 948: '-', 949: '-', 950: '-', 951: '+', 952: '-', 953: '+', 954: '-', 955: '+', 956: '+', 957: '-', 958: '+', 959: '-', 960: '+', 961: '+', 962: '+', 963: '-', 964: '-', 965: '-', 966: '+', 967: '+', 968: '-', 969: '+', 970: '-', 971: '+', 972: '-', 973: '-', 974: '-', 975: '-', 976: '-', 977: '-', 978: '-', 979: '-', 980: '-', 981: '-', 982: '-', 983: '-', 984: '-', 985: '-', 986: '-', 987: '-', 988: '-', 989: '-', 990: '-', 991: '-', 992: '+', 993: '+', 994: '-', 995: '-', 996: '-', 997: '+', 998: '+', 999: '-', 1000: '-', 1001: '-', 1002: '+', 1003: '-', 1004: '-', 1005: '+', 1006: '+', 1007: '+', 1008: '-', 1009: '+', 1010: '+'},
                                    't_name': {0: '66469', 1: '136697', 2: '123807', 3: '91', 4: '132834', 5: '132834', 6: '132834', 7: '91', 8: '136697', 9: '123807', 10: '91', 11: '91', 12: '19083', 13: '141476', 14: '119527', 15: '96486', 16: '141412', 17: '160_chunk1-3837', 18: '160_chunk3848-5723', 19: '35105', 20: '186', 21: '136697', 22: '54509', 23: '123807', 24: '91', 25: '141476', 26: '119527', 27: '91', 28: '83584', 29: '51922', 30: '141476', 31: '122608', 32: '96486', 33: '141412', 34: '160_chunk1-3837', 35: '160_chunk3848-5723', 36: '35105', 37: '186', 38: '33933', 39: '91', 40: '91', 41: '123807', 42: '54584', 43: '136697', 44: '25256', 45: '24660', 46: '91', 47: '132834', 48: '91', 49: '132834', 50: '132834', 51: '91', 52: '91', 53: '91', 54: '123807', 55: '136697', 56: '32185', 57: '91', 58: '91', 59: '123807', 60: '136697', 61: '25256', 62: '124436', 63: '147712_chunk6903-56151', 64: '111062', 65: '117059', 66: '95_chunk1-9398', 67: '91', 68: '123807', 69: '136697', 70: '123807', 71: '91', 72: '141476', 73: '91', 74: '144178', 75: '102502', 76: '85580', 77: '65797', 78: '146610', 79: '95_chunk1-9398', 80: '117059', 81: '111062', 82: '136697', 83: '123807', 84: '91', 85: '91', 86: '123807', 87: '136697', 88: '25256', 89: '118704', 90: '91', 91: '91', 92: '91', 93: '123807', 94: '136697', 95: '141476', 96: '19083', 97: '51922', 98: '91', 99: '91', 100: '132834', 101: '132834', 102: '132834', 103: '132834', 104: '91', 105: '123807', 106: '136697', 107: '118704', 108: '111062', 109: '141476', 110: '19083', 111: '91', 112: '132834', 113: '132834', 114: '132834', 115: '91', 116: '136697', 117: '91', 118: '24660', 119: '136697', 120: '123807', 121: '91', 122: '19083', 123: '141476', 124: '146634', 125: '136697', 126: '54584', 127: '123807', 128: '91', 129: '141476', 130: '130214', 131: '96486', 132: '134485_chunk708-26276', 133: '91', 134: '123807', 135: '91', 136: '123807', 137: '136697', 138: '118704', 139: '136697', 140: '123807', 141: '91', 142: '141476', 143: '119527', 144: '136697', 145: '123807', 146: '91', 147: '141476', 148: '91', 149: '123807', 150: '54584', 151: '136697', 152: '25256', 153: '131084', 154: '117059', 155: '91', 156: '33933', 157: '51922', 158: '141476', 159: '119527', 160: '139555', 161: '96486', 162: '141412', 163: '160_chunk1-3837', 164: '25256', 165: '136697', 166: '123807', 167: '91', 168: '91', 169: '132834', 170: '132834', 171: '132834', 172: '91', 173: '123807', 174: '136697', 175: '118704', 176: '141476', 177: '91', 178: '128194_chunk1-35063', 179: '128194_chunk35074-40172', 180: '10178', 181: '78330', 182: '151105', 183: '160_chunk1-3837', 184: '141412', 185: '96486', 186: '89958', 187: '119527', 188: '141476', 189: '19083', 190: '91', 191: '118704', 192: '136697', 193: '54509', 194: '123807', 195: '91', 196: '141476', 197: '119527', 198: '96486', 199: '186', 200: '160_chunk3848-5723', 201: '160_chunk1-3837', 202: '141412', 203: '96486', 204: '141476', 205: '91', 206: '123807', 207: '136697', 208: '25256', 209: '118704', 210: '91', 211: '123807', 212: '136697', 213: '118704', 214: '111062', 215: '118704', 216: '24660', 217: '136697', 218: '123807', 219: '91', 220: '160_chunk3848-5723', 221: '160_chunk1-3837', 222: '141412', 223: '54557', 224: '96486', 225: '150800', 226: '119527', 227: '141476', 228: '51922', 229: '91', 230: '91', 231: '91', 232: '123807', 233: '136697', 234: '111062', 235: '111062', 236: '136697', 237: '123807', 238: '91', 239: '91', 240: '91', 241: '119703', 242: '141476', 243: '51922', 244: '91', 245: '123807', 246: '54584', 247: '136697', 248: '111062', 249: '66469', 250: '118704', 251: '136697', 252: '123807', 253: '91', 254: '51922', 255: '141476', 256: '119527', 257: '141476', 258: '91', 259: '119527', 260: '141476', 261: '51922', 262: '91', 263: '95_chunk1-9398', 264: '117059', 265: '42964', 266: '136697', 267: '54584', 268: '123807', 269: '91', 270: '54584', 271: '123807', 272: '91', 273: '91', 274: '51922', 275: '141476', 276: '119527', 277: '96486', 278: '141412', 279: '160_chunk1-3837', 280: '141476', 281: '51922', 282: '91', 283: '123807', 284: '136697', 285: '111062', 286: '91', 287: '132834', 288: '91', 289: '123807', 290: '54584', 291: '136697', 292: '121668_chunk211-31434', 293: '111062', 294: '117059', 295: '95_chunk1-9398', 296: '146610', 297: '91', 298: '91', 299: '141476', 300: '140666', 301: '141412', 302: '160_chunk1-3837', 303: '160_chunk3848-5723', 304: '186', 305: '134485_chunk708-26276', 306: '96486', 307: '148121_chunk1-81802', 308: '141476', 309: '91', 310: '123807', 311: '136697', 312: '136338', 313: '91', 314: '141476', 315: '119527', 316: '136697', 317: '123807', 318: '91', 319: '141476', 320: '146454', 321: '160_chunk1-3837', 322: '141412', 323: '96486', 324: '121656', 325: '141476', 326: '51922', 327: '91', 328: '123807', 329: '136697', 330: '138144', 331: '111062', 332: '117059', 333: '91', 334: '81562', 335: '144178', 336: '186', 337: '85233', 338: '35105', 339: '160_chunk3848-5723', 340: '160_chunk1-3837', 341: '141412', 342: '96486', 343: '119527', 344: '141476', 345: '91', 346: '91', 347: '123807', 348: '136697', 349: '136697', 350: '123807', 351: '91', 352: '141476', 353: '119527', 354: '138951', 355: '96486', 356: '91', 357: '123807', 358: '136697', 359: '138324_chunk1-43965', 360: '91', 361: '141476', 362: '119527', 363: '146343_chunk1-20189', 364: '96486', 365: '141412', 366: '91', 367: '119527', 368: '141476', 369: '91', 370: '123807', 371: '136697', 372: '33260', 373: '136080', 374: '102059_chunk1-4400', 375: '136308', 376: '91', 377: '123807', 378: '136697', 379: '118704', 380: '66469', 381: '111062', 382: '91', 383: '51922', 384: '141476', 385: '138586', 386: '96486', 387: '141412', 388: '160_chunk1-3837', 389: '160_chunk3848-5723', 390: '85233', 391: '186', 392: '144178', 393: '20354', 394: '81562', 395: '102502', 396: '20889', 397: '146610', 398: '95_chunk1-9398', 399: '91', 400: '119527', 401: '141476', 402: '91', 403: '136697', 404: '123807', 405: '91', 406: '141476', 407: '119527', 408: '130605', 409: '96486', 410: '141412', 411: '160_chunk1-3837', 412: '91', 413: '123807', 414: '21535', 415: '136697', 416: '24660', 417: '118704', 418: '117059', 419: '95_chunk1-9398', 420: '91', 421: '51922', 422: '141476', 423: '119527', 424: '130774', 425: '136697', 426: '123807', 427: '91', 428: '160_chunk1-3837', 429: '141412', 430: '96486', 431: '132823', 432: '141476', 433: '51922', 434: '91', 435: '51922', 436: '91', 437: '111062', 438: '148155_chunk1-33996', 439: '136697', 440: '91', 441: '95_chunk1-9398', 442: '117059', 443: '111062', 444: '141571', 445: '24660', 446: '136697', 447: '54584', 448: '123807', 449: '91', 450: '160_chunk1-3837', 451: '141412', 452: '54557', 453: '96486', 454: '110893_chunk1-18912', 455: '146184_chunk1806-36442', 456: '141476', 457: '91', 458: '123807', 459: '136697', 460: '91', 461: '91', 462: '19083', 463: '141476', 464: '136697', 465: '54584', 466: '123807', 467: '91', 468: '136697', 469: '123807', 470: '132834', 471: '132834', 472: '132834', 473: '132834', 474: '132834', 475: '91', 476: '134485_chunk708-26276', 477: '96486', 478: '119527', 479: '141476', 480: '91', 481: '141476', 482: '19083', 483: '91', 484: '123807', 485: '54509', 486: '136697', 487: '24660', 488: '118704', 489: '91', 490: '141476', 491: '91', 492: '19083', 493: '141476', 494: '91', 495: '141476', 496: '119527', 497: '96486', 498: '141412', 499: '160_chunk1-3837', 500: '91', 501: '91', 502: '91', 503: '96486', 504: '110882', 505: '119527', 506: '141476', 507: '91', 508: '91', 509: '51922', 510: '141476', 511: '151904', 512: '91', 513: '141476', 514: '119527', 515: '118112_chunk19048-49051', 516: '141412', 517: '160_chunk1-3837', 518: '91', 519: '123807', 520: '136697', 521: '111062', 522: '91', 523: '111062', 524: '121162', 525: '136697', 526: '54605', 527: '54584', 528: '123807', 529: '91', 530: '136697', 531: '54584', 532: '123807', 533: '91', 534: '136697', 535: '42706', 536: '54584', 537: '123807', 538: '91', 539: '91', 540: '141476', 541: '119527', 542: '91', 543: '123807', 544: '136697', 545: '91', 546: '132834', 547: '132834', 548: '132834', 549: '91', 550: '123807', 551: '136697', 552: '118704', 553: '91', 554: '51922', 555: '132834', 556: '91', 557: '126023_chunk1-32832', 558: '141476', 559: '19083', 560: '91', 561: '123807', 562: '91', 563: '91', 564: '123807', 565: '136697', 566: '32317', 567: '144589', 568: '111062', 569: '91', 570: '91', 571: '141476', 572: '119527', 573: '186', 574: '85233', 575: '160_chunk3848-5723', 576: '160_chunk1-3837', 577: '141412', 578: '96486', 579: '146014_chunk69135-115488', 580: '141476', 581: '91', 582: '136338', 583: '136697', 584: '123807', 585: '91', 586: '141476', 587: '119527', 588: '47458', 589: '150686_chunk151589-188775', 590: '136697', 591: '21535', 592: '123807', 593: '91', 594: '91', 595: '141476', 596: '96486', 597: '141412', 598: '160_chunk1-3837', 599: '160_chunk3848-5723', 600: '82488', 601: '186', 602: '91', 603: '123807', 604: '136697', 605: '91', 606: '123807', 607: '136697', 608: '91', 609: '141476', 610: '91', 611: '128434_chunk40450-64064', 612: '91', 613: '123807', 614: '136697', 615: '119852_chunk4670-42761', 616: '141476', 617: '91', 618: '123807', 619: '136697', 620: '42964', 621: '134186_chunk14531-60580', 622: '91', 623: '91', 624: '141476', 625: '91', 626: '123807', 627: '136697', 628: '141285_chunk22496-37551', 629: '117059', 630: '91', 631: '91', 632: '119527', 633: '141476', 634: '51922', 635: '91', 636: '123807', 637: '54584', 638: '136697', 639: '25256', 640: '148698', 641: '91', 642: '18283', 643: '129591', 644: '66469', 645: '136697', 646: '21535', 647: '123807', 648: '91', 649: '19083', 650: '141476', 651: '119527', 652: '143862_chunk1-39748', 653: '43232', 654: '141412', 655: '160_chunk1-3837', 656: '136697', 657: '91', 658: '91', 659: '91', 660: '141476', 661: '119527', 662: '141476', 663: '91', 664: '91', 665: '91', 666: '91', 667: '123807', 668: '136697', 669: '95_chunk10626-16032', 670: '141476', 671: '34439', 672: '91', 673: '132834', 674: '143070_chunk58456-137491', 675: '132834', 676: '132834', 677: '91', 678: '96486', 679: '141476', 680: '91', 681: '123807', 682: '136697', 683: '136697', 684: '91', 685: '136697', 686: '123807', 687: '91', 688: '141476', 689: '119527', 690: '141476', 691: '91', 692: '134485_chunk708-26276', 693: '96486', 694: '148896_chunk93053-172245', 695: '119527', 696: '141476', 697: '51922', 698: '91', 699: '123807', 700: '136697', 701: '141476', 702: '19083', 703: '51922', 704: '91', 705: '123807', 706: '136697', 707: '118704', 708: '66469', 709: '111062', 710: '91', 711: '91', 712: '136697', 713: '33943', 714: '123807', 715: '91', 716: '141476', 717: '91', 718: '160_chunk1-3837', 719: '91', 720: '132834', 721: '132834', 722: '132834', 723: '91', 724: '51922', 725: '141476', 726: '96486', 727: '141412', 728: '136338', 729: '136697', 730: '123807', 731: '91', 732: '51922', 733: '141476', 734: '119527', 735: '96486', 736: '91', 737: '143070_chunk58456-137491', 738: '132834', 739: '91', 740: '123807', 741: '136697', 742: '45881', 743: '118704', 744: '111062', 745: '91', 746: '123807', 747: '136697', 748: '66469', 749: '128315_chunk1-31669', 750: '111062', 751: '117059', 752: '134485_chunk708-26276', 753: '96486', 754: '121435', 755: '141476', 756: '91', 757: '55556', 758: '122547', 759: '42964', 760: '136697', 761: '123807', 762: '91', 763: '91', 764: '141476', 765: '151904', 766: '96486', 767: '95_chunk1-9398', 768: '117059', 769: '111062', 770: '136697', 771: '123807', 772: '91', 773: '91', 774: '141476', 775: '96486', 776: '134485_chunk708-26276', 777: '91', 778: '91', 779: '186', 780: '85233', 781: '82488', 782: '160_chunk3848-5723', 783: '160_chunk1-3837', 784: '141412', 785: '119527', 786: '141476', 787: '91', 788: '141476', 789: '91', 790: '91', 791: '123807', 792: '136697', 793: '24660', 794: '118704', 795: '111062', 796: '134485_chunk708-26276', 797: '96486', 798: '121842', 799: '119527', 800: '141476', 801: '19083', 802: '91', 803: '123807', 804: '136697', 805: '27577', 806: '123590', 807: '134485_chunk708-26276', 808: '96486', 809: '146014_chunk69135-115488', 810: '119527', 811: '141476', 812: '19083', 813: '91', 814: '91', 815: '91', 816: '21078', 817: '123807', 818: '136697', 819: '19185', 820: '91', 821: '141476', 822: '51922', 823: '91', 824: '141476', 825: '51922', 826: '91', 827: '91', 828: '91', 829: '136697', 830: '123807', 831: '91', 832: '149861_chunk961-42801', 833: '123807', 834: '91', 835: '83584', 836: '51922', 837: '141476', 838: '119527', 839: '108001_chunk10028-25877', 840: '145343', 841: '96486', 842: '141412', 843: '160_chunk1-3837', 844: '91', 845: '123807', 846: '54584', 847: '136697', 848: '111062', 849: '136697', 850: '54584', 851: '123807', 852: '91', 853: '132834', 854: '132834', 855: '132834', 856: '91', 857: '111062', 858: '150624_chunk1-28922', 859: '136697', 860: '123807', 861: '91', 862: '91', 863: '123807', 864: '136697', 865: '33260', 866: '25256', 867: '18283', 868: '111062', 869: '91', 870: '54584', 871: '123807', 872: '91', 873: '141476', 874: '91', 875: '91', 876: '91', 877: '141476', 878: '91', 879: '123807', 880: '121380', 881: '136697', 882: '91', 883: '141476', 884: '119527', 885: '91', 886: '19083', 887: '141476', 888: '54584', 889: '123807', 890: '91', 891: '91', 892: '141476', 893: '91', 894: '91', 895: '123807', 896: '91', 897: '123807', 898: '91', 899: '91', 900: '91', 901: '91', 902: '123807', 903: '136697', 904: '118704', 905: '42964', 906: '18283', 907: '19185', 908: '111062', 909: '93632', 910: '91', 911: '123807', 912: '54584', 913: '136697', 914: '118704', 915: '111062', 916: '117059', 917: '91', 918: '143070_chunk58456-137491', 919: '132834', 920: '91', 921: '111062', 922: '136697', 923: '123807', 924: '91', 925: '91', 926: '141476', 927: '64000', 928: '91', 929: '152091', 930: '141476', 931: '51922', 932: '91', 933: '91', 934: '119527', 935: '141476', 936: '51922', 937: '91', 938: '132834', 939: '91', 940: '132834', 941: '132834', 942: '132834', 943: '132834', 944: '132834', 945: '91', 946: '123807', 947: '136697', 948: '32185', 949: '25256', 950: '111062', 951: '133567', 952: '141476', 953: '91', 954: '91', 955: '19083', 956: '141476', 957: '119527', 958: '123058', 959: '96486', 960: '141412', 961: '160_chunk1-3837', 962: '160_chunk3848-5723', 963: '186', 964: '144178', 965: '102502', 966: '65797', 967: '96486', 968: '144663_chunk118420-167606', 969: '119527', 970: '141476', 971: '91', 972: '136338', 973: '88890', 974: '136697', 975: '123807', 976: '91', 977: '132834', 978: '132834', 979: '136697', 980: '123807', 981: '91', 982: '91', 983: '136697', 984: '123807', 985: '91', 986: '132834', 987: '132834', 988: '132834', 989: '132834', 990: '132834', 991: '91', 992: '141476', 993: '91', 994: '91', 995: '91', 996: '91', 997: '51922', 998: '141476', 999: '115677', 1000: '96486', 1001: '134485_chunk708-26276', 1002: '96486', 1003: '123481', 1004: '141476', 1005: '91', 1006: '91', 1007: '123807', 1008: '54584', 1009: '136697', 1010: '117059'},
                                    't_len': {0: 338, 1: 1223, 2: 778, 3: 8010, 4: 101058, 5: 101058, 6: 101058, 7: 8010, 8: 1223, 9: 778, 10: 8010, 11: 8010, 12: 246, 13: 1112, 14: 272, 15: 3255, 16: 1310, 17: 3837, 18: 1876, 19: 440, 20: 3810, 21: 1223, 22: 188, 23: 778, 24: 8010, 25: 1112, 26: 272, 27: 8010, 28: 129, 29: 283, 30: 1112, 31: 50428, 32: 3255, 33: 1310, 34: 3837, 35: 1876, 36: 440, 37: 3810, 38: 219, 39: 8010, 40: 8010, 41: 778, 42: 352, 43: 1223, 44: 155, 45: 159, 46: 8010, 47: 101058, 48: 8010, 49: 101058, 50: 101058, 51: 8010, 52: 8010, 53: 8010, 54: 778, 55: 1223, 56: 82, 57: 8010, 58: 8010, 59: 778, 60: 1223, 61: 155, 62: 19122, 63: 49249, 64: 6265, 65: 3038, 66: 9398, 67: 8010, 68: 778, 69: 1223, 70: 778, 71: 8010, 72: 1112, 73: 8010, 74: 2033, 75: 642, 76: 269, 77: 261, 78: 2272, 79: 9398, 80: 3038, 81: 6265, 82: 1223, 83: 778, 84: 8010, 85: 8010, 86: 778, 87: 1223, 88: 155, 89: 182, 90: 8010, 91: 8010, 92: 8010, 93: 778, 94: 1223, 95: 1112, 96: 246, 97: 283, 98: 8010, 99: 8010, 100: 101058, 101: 101058, 102: 101058, 103: 101058, 104: 8010, 105: 778, 106: 1223, 107: 182, 108: 6265, 109: 1112, 110: 246, 111: 8010, 112: 101058, 113: 101058, 114: 101058, 115: 8010, 116: 1223, 117: 8010, 118: 159, 119: 1223, 120: 778, 121: 8010, 122: 246, 123: 1112, 124: 51518, 125: 1223, 126: 352, 127: 778, 128: 8010, 129: 1112, 130: 43278, 131: 3255, 132: 25569, 133: 8010, 134: 778, 135: 8010, 136: 778, 137: 1223, 138: 182, 139: 1223, 140: 778, 141: 8010, 142: 1112, 143: 272, 144: 1223, 145: 778, 146: 8010, 147: 1112, 148: 8010, 149: 778, 150: 352, 151: 1223, 152: 155, 153: 46424, 154: 3038, 155: 8010, 156: 219, 157: 283, 158: 1112, 159: 272, 160: 24056, 161: 3255, 162: 1310, 163: 3837, 164: 155, 165: 1223, 166: 778, 167: 8010, 168: 8010, 169: 101058, 170: 101058, 171: 101058, 172: 8010, 173: 778, 174: 1223, 175: 182, 176: 1112, 177: 8010, 178: 35063, 179: 5099, 180: 170, 181: 459, 182: 86602, 183: 3837, 184: 1310, 185: 3255, 186: 33038, 187: 272, 188: 1112, 189: 246, 190: 8010, 191: 182, 192: 1223, 193: 188, 194: 778, 195: 8010, 196: 1112, 197: 272, 198: 3255, 199: 3810, 200: 1876, 201: 3837, 202: 1310, 203: 3255, 204: 1112, 205: 8010, 206: 778, 207: 1223, 208: 155, 209: 182, 210: 8010, 211: 778, 212: 1223, 213: 182, 214: 6265, 215: 182, 216: 159, 217: 1223, 218: 778, 219: 8010, 220: 1876, 221: 3837, 222: 1310, 223: 193, 224: 3255, 225: 97815, 226: 272, 227: 1112, 228: 283, 229: 8010, 230: 8010, 231: 8010, 232: 778, 233: 1223, 234: 6265, 235: 6265, 236: 1223, 237: 778, 238: 8010, 239: 8010, 240: 8010, 241: 42492, 242: 1112, 243: 283, 244: 8010, 245: 778, 246: 352, 247: 1223, 248: 6265, 249: 338, 250: 182, 251: 1223, 252: 778, 253: 8010, 254: 283, 255: 1112, 256: 272, 257: 1112, 258: 8010, 259: 272, 260: 1112, 261: 283, 262: 8010, 263: 9398, 264: 3038, 265: 168, 266: 1223, 267: 352, 268: 778, 269: 8010, 270: 352, 271: 778, 272: 8010, 273: 8010, 274: 283, 275: 1112, 276: 272, 277: 3255, 278: 1310, 279: 3837, 280: 1112, 281: 283, 282: 8010, 283: 778, 284: 1223, 285: 6265, 286: 8010, 287: 101058, 288: 8010, 289: 778, 290: 352, 291: 1223, 292: 31224, 293: 6265, 294: 3038, 295: 9398, 296: 2272, 297: 8010, 298: 8010, 299: 1112, 300: 47299, 301: 1310, 302: 3837, 303: 1876, 304: 3810, 305: 25569, 306: 3255, 307: 81802, 308: 1112, 309: 8010, 310: 778, 311: 1223, 312: 851, 313: 8010, 314: 1112, 315: 272, 316: 1223, 317: 778, 318: 8010, 319: 1112, 320: 39526, 321: 3837, 322: 1310, 323: 3255, 324: 40678, 325: 1112, 326: 283, 327: 8010, 328: 778, 329: 1223, 330: 85730, 331: 6265, 332: 3038, 333: 8010, 334: 171, 335: 2033, 336: 3810, 337: 237, 338: 440, 339: 1876, 340: 3837, 341: 1310, 342: 3255, 343: 272, 344: 1112, 345: 8010, 346: 8010, 347: 778, 348: 1223, 349: 1223, 350: 778, 351: 8010, 352: 1112, 353: 272, 354: 172220, 355: 3255, 356: 8010, 357: 778, 358: 1223, 359: 43965, 360: 8010, 361: 1112, 362: 272, 363: 20189, 364: 3255, 365: 1310, 366: 8010, 367: 272, 368: 1112, 369: 8010, 370: 778, 371: 1223, 372: 104, 373: 45253, 374: 4400, 375: 86803, 376: 8010, 377: 778, 378: 1223, 379: 182, 380: 338, 381: 6265, 382: 8010, 383: 283, 384: 1112, 385: 40159, 386: 3255, 387: 1310, 388: 3837, 389: 1876, 390: 237, 391: 3810, 392: 2033, 393: 110, 394: 171, 395: 642, 396: 293, 397: 2272, 398: 9398, 399: 8010, 400: 272, 401: 1112, 402: 8010, 403: 1223, 404: 778, 405: 8010, 406: 1112, 407: 272, 408: 40213, 409: 3255, 410: 1310, 411: 3837, 412: 8010, 413: 778, 414: 122, 415: 1223, 416: 159, 417: 182, 418: 3038, 419: 9398, 420: 8010, 421: 283, 422: 1112, 423: 272, 424: 49465, 425: 1223, 426: 778, 427: 8010, 428: 3837, 429: 1310, 430: 3255, 431: 46788, 432: 1112, 433: 283, 434: 8010, 435: 283, 436: 8010, 437: 6265, 438: 33996, 439: 1223, 440: 8010, 441: 9398, 442: 3038, 443: 6265, 444: 71514, 445: 159, 446: 1223, 447: 352, 448: 778, 449: 8010, 450: 3837, 451: 1310, 452: 193, 453: 3255, 454: 18912, 455: 34637, 456: 1112, 457: 8010, 458: 778, 459: 1223, 460: 8010, 461: 8010, 462: 246, 463: 1112, 464: 1223, 465: 352, 466: 778, 467: 8010, 468: 1223, 469: 778, 470: 101058, 471: 101058, 472: 101058, 473: 101058, 474: 101058, 475: 8010, 476: 25569, 477: 3255, 478: 272, 479: 1112, 480: 8010, 481: 1112, 482: 246, 483: 8010, 484: 778, 485: 188, 486: 1223, 487: 159, 488: 182, 489: 8010, 490: 1112, 491: 8010, 492: 246, 493: 1112, 494: 8010, 495: 1112, 496: 272, 497: 3255, 498: 1310, 499: 3837, 500: 8010, 501: 8010, 502: 8010, 503: 3255, 504: 13205, 505: 272, 506: 1112, 507: 8010, 508: 8010, 509: 283, 510: 1112, 511: 40126, 512: 8010, 513: 1112, 514: 272, 515: 30004, 516: 1310, 517: 3837, 518: 8010, 519: 778, 520: 1223, 521: 6265, 522: 8010, 523: 6265, 524: 89842, 525: 1223, 526: 162, 527: 352, 528: 778, 529: 8010, 530: 1223, 531: 352, 532: 778, 533: 8010, 534: 1223, 535: 147, 536: 352, 537: 778, 538: 8010, 539: 8010, 540: 1112, 541: 272, 542: 8010, 543: 778, 544: 1223, 545: 8010, 546: 101058, 547: 101058, 548: 101058, 549: 8010, 550: 778, 551: 1223, 552: 182, 553: 8010, 554: 283, 555: 101058, 556: 8010, 557: 32832, 558: 1112, 559: 246, 560: 8010, 561: 778, 562: 8010, 563: 8010, 564: 778, 565: 1223, 566: 88, 567: 48713, 568: 6265, 569: 8010, 570: 8010, 571: 1112, 572: 272, 573: 3810, 574: 237, 575: 1876, 576: 3837, 577: 1310, 578: 3255, 579: 46354, 580: 1112, 581: 8010, 582: 851, 583: 1223, 584: 778, 585: 8010, 586: 1112, 587: 272, 588: 360, 589: 37187, 590: 1223, 591: 122, 592: 778, 593: 8010, 594: 8010, 595: 1112, 596: 3255, 597: 1310, 598: 3837, 599: 1876, 600: 348, 601: 3810, 602: 8010, 603: 778, 604: 1223, 605: 8010, 606: 778, 607: 1223, 608: 8010, 609: 1112, 610: 8010, 611: 23615, 612: 8010, 613: 778, 614: 1223, 615: 38092, 616: 1112, 617: 8010, 618: 778, 619: 1223, 620: 168, 621: 46050, 622: 8010, 623: 8010, 624: 1112, 625: 8010, 626: 778, 627: 1223, 628: 15056, 629: 3038, 630: 8010, 631: 8010, 632: 272, 633: 1112, 634: 283, 635: 8010, 636: 778, 637: 352, 638: 1223, 639: 155, 640: 67396, 641: 8010, 642: 148, 643: 60688, 644: 338, 645: 1223, 646: 122, 647: 778, 648: 8010, 649: 246, 650: 1112, 651: 272, 652: 39748, 653: 150, 654: 1310, 655: 3837, 656: 1223, 657: 8010, 658: 8010, 659: 8010, 660: 1112, 661: 272, 662: 1112, 663: 8010, 664: 8010, 665: 8010, 666: 8010, 667: 778, 668: 1223, 669: 5407, 670: 1112, 671: 178, 672: 8010, 673: 101058, 674: 79036, 675: 101058, 676: 101058, 677: 8010, 678: 3255, 679: 1112, 680: 8010, 681: 778, 682: 1223, 683: 1223, 684: 8010, 685: 1223, 686: 778, 687: 8010, 688: 1112, 689: 272, 690: 1112, 691: 8010, 692: 25569, 693: 3255, 694: 79193, 695: 272, 696: 1112, 697: 283, 698: 8010, 699: 778, 700: 1223, 701: 1112, 702: 246, 703: 283, 704: 8010, 705: 778, 706: 1223, 707: 182, 708: 338, 709: 6265, 710: 8010, 711: 8010, 712: 1223, 713: 163, 714: 778, 715: 8010, 716: 1112, 717: 8010, 718: 3837, 719: 8010, 720: 101058, 721: 101058, 722: 101058, 723: 8010, 724: 283, 725: 1112, 726: 3255, 727: 1310, 728: 851, 729: 1223, 730: 778, 731: 8010, 732: 283, 733: 1112, 734: 272, 735: 3255, 736: 8010, 737: 79036, 738: 101058, 739: 8010, 740: 778, 741: 1223, 742: 139, 743: 182, 744: 6265, 745: 8010, 746: 778, 747: 1223, 748: 338, 749: 31669, 750: 6265, 751: 3038, 752: 25569, 753: 3255, 754: 55105, 755: 1112, 756: 8010, 757: 4449, 758: 40894, 759: 168, 760: 1223, 761: 778, 762: 8010, 763: 8010, 764: 1112, 765: 40126, 766: 3255, 767: 9398, 768: 3038, 769: 6265, 770: 1223, 771: 778, 772: 8010, 773: 8010, 774: 1112, 775: 3255, 776: 25569, 777: 8010, 778: 8010, 779: 3810, 780: 237, 781: 348, 782: 1876, 783: 3837, 784: 1310, 785: 272, 786: 1112, 787: 8010, 788: 1112, 789: 8010, 790: 8010, 791: 778, 792: 1223, 793: 159, 794: 182, 795: 6265, 796: 25569, 797: 3255, 798: 44494, 799: 272, 800: 1112, 801: 246, 802: 8010, 803: 778, 804: 1223, 805: 102, 806: 27482, 807: 25569, 808: 3255, 809: 46354, 810: 272, 811: 1112, 812: 246, 813: 8010, 814: 8010, 815: 8010, 816: 102, 817: 778, 818: 1223, 819: 128, 820: 8010, 821: 1112, 822: 283, 823: 8010, 824: 1112, 825: 283, 826: 8010, 827: 8010, 828: 8010, 829: 1223, 830: 778, 831: 8010, 832: 41841, 833: 778, 834: 8010, 835: 129, 836: 283, 837: 1112, 838: 272, 839: 15850, 840: 40017, 841: 3255, 842: 1310, 843: 3837, 844: 8010, 845: 778, 846: 352, 847: 1223, 848: 6265, 849: 1223, 850: 352, 851: 778, 852: 8010, 853: 101058, 854: 101058, 855: 101058, 856: 8010, 857: 6265, 858: 28922, 859: 1223, 860: 778, 861: 8010, 862: 8010, 863: 778, 864: 1223, 865: 104, 866: 155, 867: 148, 868: 6265, 869: 8010, 870: 352, 871: 778, 872: 8010, 873: 1112, 874: 8010, 875: 8010, 876: 8010, 877: 1112, 878: 8010, 879: 778, 880: 54853, 881: 1223, 882: 8010, 883: 1112, 884: 272, 885: 8010, 886: 246, 887: 1112, 888: 352, 889: 778, 890: 8010, 891: 8010, 892: 1112, 893: 8010, 894: 8010, 895: 778, 896: 8010, 897: 778, 898: 8010, 899: 8010, 900: 8010, 901: 8010, 902: 778, 903: 1223, 904: 182, 905: 168, 906: 148, 907: 128, 908: 6265, 909: 7083, 910: 8010, 911: 778, 912: 352, 913: 1223, 914: 182, 915: 6265, 916: 3038, 917: 8010, 918: 79036, 919: 101058, 920: 8010, 921: 6265, 922: 1223, 923: 778, 924: 8010, 925: 8010, 926: 1112, 927: 257, 928: 8010, 929: 65492, 930: 1112, 931: 283, 932: 8010, 933: 8010, 934: 272, 935: 1112, 936: 283, 937: 8010, 938: 101058, 939: 8010, 940: 101058, 941: 101058, 942: 101058, 943: 101058, 944: 101058, 945: 8010, 946: 778, 947: 1223, 948: 82, 949: 155, 950: 6265, 951: 40311, 952: 1112, 953: 8010, 954: 8010, 955: 246, 956: 1112, 957: 272, 958: 51458, 959: 3255, 960: 1310, 961: 3837, 962: 1876, 963: 3810, 964: 2033, 965: 642, 966: 261, 967: 3255, 968: 49187, 969: 272, 970: 1112, 971: 8010, 972: 851, 973: 188, 974: 1223, 975: 778, 976: 8010, 977: 101058, 978: 101058, 979: 1223, 980: 778, 981: 8010, 982: 8010, 983: 1223, 984: 778, 985: 8010, 986: 101058, 987: 101058, 988: 101058, 989: 101058, 990: 101058, 991: 8010, 992: 1112, 993: 8010, 994: 8010, 995: 8010, 996: 8010, 997: 283, 998: 1112, 999: 55385, 1000: 3255, 1001: 25569, 1002: 3255, 1003: 18523, 1004: 1112, 1005: 8010, 1006: 8010, 1007: 778, 1008: 352, 1009: 1223, 1010: 3038},
                                    't_start': {0: 0, 1: 3, 2: 4, 3: 5561, 4: 37941, 5: 14922, 6: 4236, 7: 5244, 8: 2, 9: 0, 10: 3139, 11: 7, 12: 0, 13: 0, 14: 0, 15: 3, 16: 0, 17: 0, 18: 0, 19: 0, 20: 1083, 21: 8, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 2, 29: 0, 30: 3, 31: 391, 32: 0, 33: 0, 34: 0, 35: 0, 36: 3, 37: 2016, 38: 0, 39: 2, 40: 520, 41: 14, 42: 1, 43: 3, 44: 2, 45: 8, 46: 506, 47: 15094, 48: 5561, 49: 37944, 50: 14920, 51: 1081, 52: 812, 53: 2881, 54: 3, 55: 0, 56: 0, 57: 2883, 58: 7681, 59: 59, 60: 0, 61: 0, 62: 24, 63: 22747, 64: 0, 65: 6, 66: 7724, 67: 7055, 68: 0, 69: 0, 70: 0, 71: 3340, 72: 0, 73: 0, 74: 0, 75: 6, 76: 0, 77: 1, 78: 16, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 1454, 85: 1280, 86: 1, 87: 0, 88: 3, 89: 8, 90: 0, 91: 1760, 92: 1798, 93: 4, 94: 3, 95: 3, 96: 0, 97: 10, 98: 1, 99: 5018, 100: 4236, 101: 14922, 102: 36291, 103: 37953, 104: 5562, 105: 0, 106: 0, 107: 0, 108: 2634, 109: 0, 110: 1, 111: 0, 112: 4236, 113: 14920, 114: 37941, 115: 5559, 116: 0, 117: 448, 118: 0, 119: 0, 120: 4, 121: 0, 122: 0, 123: 1, 124: 2208, 125: 0, 126: 0, 127: 9, 128: 12, 129: 0, 130: 38663, 131: 0, 132: 21273, 133: 2524, 134: 4, 135: 2289, 136: 3, 137: 0, 138: 3, 139: 0, 140: 1, 141: 0, 142: 0, 143: 0, 144: 0, 145: 4, 146: 0, 147: 0, 148: 6651, 149: 2, 150: 2, 151: 0, 152: 0, 153: 40099, 154: 0, 155: 1, 156: 1, 157: 0, 158: 0, 159: 4, 160: 274, 161: 6, 162: 0, 163: 0, 164: 0, 165: 0, 166: 0, 167: 1007, 168: 3216, 169: 4246, 170: 14918, 171: 37941, 172: 5561, 173: 22, 174: 0, 175: 1, 176: 8, 177: 8, 178: 28937, 179: 2, 180: 3, 181: 15, 182: 0, 183: 1, 184: 4, 185: 2, 186: 14626, 187: 1, 188: 0, 189: 4, 190: 0, 191: 0, 192: 3, 193: 0, 194: 6, 195: 0, 196: 1, 197: 2, 198: 30, 199: 254, 200: 0, 201: 0, 202: 0, 203: 1, 204: 0, 205: 0, 206: 3, 207: 5, 208: 0, 209: 0, 210: 6847, 211: 3, 212: 3, 213: 1, 214: 364, 215: 0, 216: 2, 217: 0, 218: 5, 219: 4483, 220: 0, 221: 1, 222: 2, 223: 0, 224: 1, 225: 87633, 226: 0, 227: 0, 228: 0, 229: 4, 230: 4059, 231: 1403, 232: 0, 233: 0, 234: 0, 235: 1785, 236: 0, 237: 0, 238: 7274, 239: 5125, 240: 3485, 241: 37548, 242: 0, 243: 0, 244: 0, 245: 0, 246: 43, 247: 0, 248: 1699, 249: 2, 250: 2, 251: 0, 252: 0, 253: 6, 254: 0, 255: 0, 256: 2, 257: 0, 258: 0, 259: 1, 260: 0, 261: 2, 262: 0, 263: 6139, 264: 0, 265: 4, 266: 0, 267: 0, 268: 0, 269: 6130, 270: 58, 271: 5, 272: 5753, 273: 0, 274: 3, 275: 6, 276: 0, 277: 0, 278: 4, 279: 0, 280: 3, 281: 5, 282: 3, 283: 0, 284: 0, 285: 2797, 286: 1114, 287: 37941, 288: 6668, 289: 2, 290: 2, 291: 0, 292: 2693, 293: 0, 294: 0, 295: 0, 296: 747, 297: 3800, 298: 4, 299: 22, 300: 43023, 301: 1, 302: 0, 303: 3, 304: 2693, 305: 21575, 306: 2, 307: 13862, 308: 0, 309: 0, 310: 0, 311: 0, 312: 60, 313: 0, 314: 4, 315: 0, 316: 0, 317: 2, 318: 2, 319: 0, 320: 11852, 321: 0, 322: 0, 323: 1, 324: 35880, 325: 3, 326: 2, 327: 0, 328: 19, 329: 3, 330: 52025, 331: 0, 332: 0, 333: 3478, 334: 0, 335: 0, 336: 0, 337: 3, 338: 0, 339: 0, 340: 0, 341: 17, 342: 12, 343: 0, 344: 0, 345: 1, 346: 1165, 347: 3, 348: 3, 349: 2, 350: 0, 351: 0, 352: 7, 353: 0, 354: 65940, 355: 4, 356: 5804, 357: 3, 358: 3, 359: 3215, 360: 0, 361: 0, 362: 8, 363: 15035, 364: 6, 365: 0, 366: 712, 367: 1, 368: 1, 369: 3, 370: 2, 371: 6, 372: 0, 373: 43235, 374: 2496, 375: 76372, 376: 376, 377: 0, 378: 0, 379: 0, 380: 0, 381: 4832, 382: 1, 383: 0, 384: 0, 385: 36248, 386: 4, 387: 2, 388: 0, 389: 3, 390: 3, 391: 0, 392: 0, 393: 0, 394: 0, 395: 0, 396: 0, 397: 0, 398: 0, 399: 2093, 400: 0, 401: 0, 402: 0, 403: 0, 404: 0, 405: 0, 406: 3, 407: 0, 408: 22835, 409: 3, 410: 0, 411: 0, 412: 7532, 413: 2, 414: 5, 415: 0, 416: 0, 417: 1, 418: 0, 419: 4245, 420: 0, 421: 1, 422: 3, 423: 1, 424: 44797, 425: 0, 426: 0, 427: 7342, 428: 1, 429: 3, 430: 0, 431: 41876, 432: 0, 433: 2, 434: 0, 435: 3, 436: 5, 437: 4511, 438: 9702, 439: 0, 440: 3389, 441: 7009, 442: 0, 443: 1, 444: 780, 445: 0, 446: 0, 447: 0, 448: 4, 449: 6903, 450: 1, 451: 11, 452: 0, 453: 0, 454: 398, 455: 0, 456: 0, 457: 0, 458: 2, 459: 0, 460: 3199, 461: 0, 462: 0, 463: 0, 464: 0, 465: 7, 466: 0, 467: 3259, 468: 0, 469: 0, 470: 4338, 471: 14920, 472: 23110, 473: 36290, 474: 37949, 475: 5561, 476: 4605, 477: 0, 478: 1, 479: 0, 480: 2, 481: 0, 482: 3, 483: 2, 484: 0, 485: 0, 486: 0, 487: 4, 488: 1, 489: 6, 490: 5, 491: 0, 492: 0, 493: 0, 494: 0, 495: 0, 496: 0, 497: 10, 498: 0, 499: 0, 500: 1414, 501: 2864, 502: 572, 503: 562, 504: 390, 505: 0, 506: 1, 507: 2, 508: 0, 509: 1, 510: 0, 511: 23757, 512: 0, 513: 0, 514: 0, 515: 26346, 516: 0, 517: 0, 518: 1708, 519: 1, 520: 0, 521: 89, 522: 114, 523: 257, 524: 35559, 525: 0, 526: 0, 527: 0, 528: 5, 529: 5146, 530: 3, 531: 7, 532: 0, 533: 5885, 534: 0, 535: 0, 536: 1, 537: 0, 538: 2392, 539: 0, 540: 7, 541: 0, 542: 7607, 543: 0, 544: 7, 545: 4841, 546: 4236, 547: 14920, 548: 37941, 549: 5559, 550: 0, 551: 2, 552: 3, 553: 1, 554: 0, 555: 4239, 556: 4171, 557: 2954, 558: 0, 559: 0, 560: 0, 561: 3, 562: 3429, 563: 4626, 564: 0, 565: 5, 566: 0, 567: 35319, 568: 2696, 569: 5481, 570: 4, 571: 0, 572: 9, 573: 2248, 574: 0, 575: 0, 576: 1, 577: 0, 578: 1, 579: 1810, 580: 0, 581: 2, 582: 62, 583: 0, 584: 0, 585: 1, 586: 2, 587: 0, 588: 0, 589: 22909, 590: 7, 591: 0, 592: 2, 593: 1828, 594: 0, 595: 1, 596: 0, 597: 9, 598: 0, 599: 3, 600: 0, 601: 1235, 602: 7704, 603: 4, 604: 1, 605: 662, 606: 3, 607: 1, 608: 0, 609: 0, 610: 1351, 611: 14083, 612: 2342, 613: 3, 614: 0, 615: 23463, 616: 1, 617: 3, 618: 0, 619: 3, 620: 2, 621: 30834, 622: 238, 623: 1, 624: 4, 625: 6813, 626: 2, 627: 3, 628: 9605, 629: 2, 630: 1653, 631: 2343, 632: 0, 633: 9, 634: 2, 635: 0, 636: 1, 637: 0, 638: 5, 639: 2, 640: 8628, 641: 1655, 642: 0, 643: 22272, 644: 8, 645: 0, 646: 0, 647: 4, 648: 0, 649: 8, 650: 4, 651: 0, 652: 34338, 653: 1, 654: 0, 655: 0, 656: 2, 657: 3555, 658: 0, 659: 0, 660: 4, 661: 0, 662: 0, 663: 2, 664: 1216, 665: 5502, 666: 26, 667: 3, 668: 0, 669: 28, 670: 0, 671: 2, 672: 0, 673: 4236, 674: 204, 675: 14920, 676: 37944, 677: 5561, 678: 0, 679: 0, 680: 5, 681: 1, 682: 0, 683: 3, 684: 3354, 685: 4, 686: 4, 687: 1, 688: 0, 689: 123, 690: 0, 691: 1, 692: 23013, 693: 0, 694: 4290, 695: 0, 696: 0, 697: 0, 698: 5, 699: 0, 700: 0, 701: 3, 702: 0, 703: 0, 704: 5, 705: 0, 706: 0, 707: 0, 708: 3, 709: 1882, 710: 3962, 711: 1555, 712: 14, 713: 5, 714: 2, 715: 4, 716: 17, 717: 2493, 718: 0, 719: 5561, 720: 37944, 721: 14920, 722: 4225, 723: 0, 724: 0, 725: 0, 726: 134, 727: 4, 728: 63, 729: 11, 730: 0, 731: 0, 732: 2, 733: 2, 734: 0, 735: 359, 736: 4765, 737: 201, 738: 37941, 739: 5566, 740: 4, 741: 3, 742: 0, 743: 0, 744: 5067, 745: 3853, 746: 3, 747: 0, 748: 0, 749: 27739, 750: 11, 751: 0, 752: 23377, 753: 0, 754: 670, 755: 3, 756: 0, 757: 770, 758: 28910, 759: 8, 760: 0, 761: 0, 762: 6988, 763: 6, 764: 1, 765: 21353, 766: 4, 767: 3530, 768: 290, 769: 2017, 770: 6, 771: 1, 772: 2607, 773: 6, 774: 1, 775: 0, 776: 24302, 777: 926, 778: 5267, 779: 1996, 780: 9, 781: 0, 782: 3, 783: 11, 784: 4, 785: 1, 786: 1, 787: 0, 788: 0, 789: 2, 790: 863, 791: 0, 792: 1, 793: 0, 794: 0, 795: 2633, 796: 12763, 797: 0, 798: 4, 799: 0, 800: 0, 801: 0, 802: 0, 803: 3, 804: 0, 805: 0, 806: 13970, 807: 14821, 808: 3, 809: 1810, 810: 0, 811: 0, 812: 0, 813: 0, 814: 4915, 815: 5405, 816: 0, 817: 0, 818: 0, 819: 0, 820: 3925, 821: 0, 822: 0, 823: 0, 824: 0, 825: 2, 826: 1, 827: 0, 828: 2255, 829: 6, 830: 5, 831: 6226, 832: 8004, 833: 1, 834: 0, 835: 0, 836: 0, 837: 3, 838: 4, 839: 14574, 840: 33368, 841: 0, 842: 10, 843: 0, 844: 4279, 845: 6, 846: 10, 847: 1, 848: 4561, 849: 0, 850: 7, 851: 4, 852: 5561, 853: 37941, 854: 14918, 855: 4236, 856: 5057, 857: 0, 858: 2813, 859: 7, 860: 6, 861: 4708, 862: 1734, 863: 3, 864: 0, 865: 0, 866: 0, 867: 0, 868: 1280, 869: 1992, 870: 0, 871: 0, 872: 0, 873: 0, 874: 3611, 875: 2008, 876: 4592, 877: 2, 878: 0, 879: 0, 880: 14517, 881: 6, 882: 0, 883: 2, 884: 1, 885: 0, 886: 0, 887: 2, 888: 0, 889: 0, 890: 1596, 891: 7, 892: 0, 893: 3, 894: 3246, 895: 3, 896: 4753, 897: 3, 898: 6861, 899: 1307, 900: 5907, 901: 4782, 902: 3, 903: 3, 904: 2, 905: 0, 906: 0, 907: 11, 908: 4454, 909: 1873, 910: 2608, 911: 2, 912: 7, 913: 3, 914: 0, 915: 0, 916: 0, 917: 1730, 918: 245, 919: 37941, 920: 5561, 921: 1670, 922: 0, 923: 4, 924: 2061, 925: 0, 926: 0, 927: 0, 928: 1342, 929: 33902, 930: 8, 931: 0, 932: 2, 933: 1873, 934: 2, 935: 0, 936: 10, 937: 0, 938: 14924, 939: 1732, 940: 4236, 941: 14920, 942: 15531, 943: 30439, 944: 37941, 945: 5561, 946: 0, 947: 0, 948: 0, 949: 0, 950: 876, 951: 1959, 952: 0, 953: 3, 954: 0, 955: 3, 956: 5, 957: 5, 958: 46850, 959: 0, 960: 0, 961: 2, 962: 2, 963: 0, 964: 1, 965: 0, 966: 0, 967: 0, 968: 13483, 969: 0, 970: 0, 971: 3, 972: 60, 973: 9, 974: 8, 975: 12, 976: 5561, 977: 37944, 978: 30437, 979: 6, 980: 11, 981: 3345, 982: 0, 983: 4, 984: 0, 985: 5561, 986: 37941, 987: 21511, 988: 14933, 989: 14450, 990: 4236, 991: 0, 992: 0, 993: 4167, 994: 9, 995: 3, 996: 3, 997: 0, 998: 1, 999: 13323, 1000: 0, 1001: 25073, 1002: 389, 1003: 13785, 1004: 0, 1005: 0, 1006: 5105, 1007: 3, 1008: 0, 1009: 2, 1010: 2},
                                    't_end': {0: 269, 1: 1212, 2: 778, 3: 8010, 4: 38929, 5: 15794, 6: 4645, 7: 5557, 8: 1223, 9: 773, 10: 8008, 11: 6289, 12: 246, 13: 1112, 14: 272, 15: 3255, 16: 1310, 17: 3837, 18: 1876, 19: 422, 20: 3809, 21: 1223, 22: 179, 23: 776, 24: 8010, 25: 1112, 26: 270, 27: 2879, 28: 129, 29: 283, 30: 1112, 31: 4611, 32: 3255, 33: 1310, 34: 3837, 35: 1872, 36: 425, 37: 3810, 38: 212, 39: 8010, 40: 8010, 41: 778, 42: 352, 43: 1220, 44: 153, 45: 159, 46: 5564, 47: 15643, 48: 6999, 49: 38930, 50: 15798, 51: 2568, 52: 5336, 53: 8010, 54: 778, 55: 1223, 56: 82, 57: 6568, 58: 8010, 59: 778, 60: 1223, 61: 149, 62: 779, 63: 24353, 64: 6205, 65: 3038, 66: 9398, 67: 8010, 68: 778, 69: 721, 70: 772, 71: 8010, 72: 1112, 73: 4182, 74: 1359, 75: 642, 76: 267, 77: 261, 78: 2272, 79: 9395, 80: 3038, 81: 6200, 82: 1223, 83: 778, 84: 8010, 85: 8007, 86: 760, 87: 1219, 88: 153, 89: 182, 90: 1759, 91: 5204, 92: 8010, 93: 776, 94: 1043, 95: 1108, 96: 246, 97: 283, 98: 7127, 99: 5564, 100: 4645, 101: 15802, 102: 36426, 103: 38930, 104: 8010, 105: 778, 106: 1223, 107: 182, 108: 6209, 109: 1108, 110: 245, 111: 5566, 112: 4645, 113: 15802, 114: 38930, 115: 6914, 116: 1216, 117: 7996, 118: 159, 119: 1223, 120: 778, 121: 8010, 122: 239, 123: 1111, 124: 3535, 125: 1222, 126: 352, 127: 778, 128: 8006, 129: 1112, 130: 42987, 131: 3234, 132: 25567, 133: 8009, 134: 181, 135: 8005, 136: 777, 137: 1223, 138: 181, 139: 1223, 140: 778, 141: 8010, 142: 1112, 143: 272, 144: 1204, 145: 778, 146: 8010, 147: 1111, 148: 8010, 149: 778, 150: 350, 151: 1223, 152: 155, 153: 42011, 154: 1062, 155: 1731, 156: 210, 157: 283, 158: 1112, 159: 269, 160: 4513, 161: 3255, 162: 1310, 163: 3409, 164: 152, 165: 1223, 166: 773, 167: 8010, 168: 4597, 169: 4645, 170: 15615, 171: 38928, 172: 8005, 173: 778, 174: 1219, 175: 176, 176: 549, 177: 438, 178: 35063, 179: 5097, 180: 170, 181: 456, 182: 1190, 183: 3828, 184: 1294, 185: 3208, 186: 18812, 187: 264, 188: 1112, 189: 246, 190: 2336, 191: 182, 192: 1218, 193: 183, 194: 776, 195: 8010, 196: 1112, 197: 266, 198: 3249, 199: 3810, 200: 1873, 201: 3837, 202: 1310, 203: 3255, 204: 1107, 205: 8010, 206: 778, 207: 1223, 208: 155, 209: 177, 210: 8010, 211: 778, 212: 1222, 213: 182, 214: 6205, 215: 182, 216: 159, 217: 1223, 218: 778, 219: 8007, 220: 1873, 221: 3836, 222: 1306, 223: 193, 224: 3255, 225: 91914, 226: 270, 227: 1112, 228: 283, 229: 3558, 230: 7981, 231: 8010, 232: 776, 233: 1217, 234: 6204, 235: 6205, 236: 1223, 237: 775, 238: 8010, 239: 7292, 240: 5780, 241: 38700, 242: 1112, 243: 283, 244: 8010, 245: 767, 246: 349, 247: 1223, 248: 6209, 249: 334, 250: 182, 251: 1221, 252: 775, 253: 8010, 254: 281, 255: 1112, 256: 272, 257: 1112, 258: 705, 259: 272, 260: 1112, 261: 278, 262: 5272, 263: 9397, 264: 3035, 265: 161, 266: 1218, 267: 351, 268: 772, 269: 8010, 270: 351, 271: 778, 272: 8010, 273: 7951, 274: 155, 275: 1112, 276: 269, 277: 3251, 278: 1310, 279: 1208, 280: 1112, 281: 283, 282: 8007, 283: 776, 284: 1223, 285: 6201, 286: 5541, 287: 38930, 288: 8010, 289: 776, 290: 349, 291: 1223, 292: 6265, 293: 6205, 294: 3036, 295: 9398, 296: 2268, 297: 7937, 298: 7826, 299: 1112, 300: 47256, 301: 1310, 302: 3836, 303: 1876, 304: 3810, 305: 25565, 306: 3255, 307: 18155, 308: 1112, 309: 8010, 310: 778, 311: 1220, 312: 850, 313: 4088, 314: 1112, 315: 265, 316: 1223, 317: 777, 318: 8010, 319: 1112, 320: 13568, 321: 1105, 322: 1310, 323: 3246, 324: 40207, 325: 1112, 326: 277, 327: 8010, 328: 778, 329: 1221, 330: 53190, 331: 6205, 332: 2349, 333: 4947, 334: 171, 335: 2030, 336: 3810, 337: 237, 338: 421, 339: 1874, 340: 3835, 341: 1310, 342: 3255, 343: 272, 344: 1112, 345: 3053, 346: 8010, 347: 778, 348: 1223, 349: 1223, 350: 778, 351: 7996, 352: 1112, 353: 271, 354: 70274, 355: 3255, 356: 8010, 357: 778, 358: 1219, 359: 5528, 360: 3832, 361: 1112, 362: 270, 363: 19354, 364: 3255, 365: 856, 366: 5734, 367: 272, 368: 1112, 369: 8010, 370: 774, 371: 1223, 372: 103, 373: 44779, 374: 4023, 375: 77183, 376: 8010, 377: 771, 378: 1222, 379: 176, 380: 338, 381: 6205, 382: 828, 383: 283, 384: 1112, 385: 40158, 386: 3250, 387: 1310, 388: 3837, 389: 1876, 390: 235, 391: 3810, 392: 2025, 393: 110, 394: 171, 395: 642, 396: 289, 397: 2272, 398: 5426, 399: 2521, 400: 268, 401: 1112, 402: 7503, 403: 1223, 404: 778, 405: 8008, 406: 1112, 407: 269, 408: 27168, 409: 3255, 410: 1310, 411: 1166, 412: 8010, 413: 778, 414: 122, 415: 1222, 416: 159, 417: 182, 418: 3038, 419: 9398, 420: 3782, 421: 283, 422: 1112, 423: 271, 424: 46112, 425: 1223, 426: 776, 427: 8010, 428: 3291, 429: 1310, 430: 3255, 431: 46151, 432: 1112, 433: 283, 434: 1200, 435: 283, 436: 1049, 437: 6205, 438: 10122, 439: 1220, 440: 8010, 441: 9397, 442: 3038, 443: 6203, 444: 1585, 445: 159, 446: 1223, 447: 352, 448: 777, 449: 8010, 450: 2454, 451: 1310, 452: 193, 453: 3255, 454: 3239, 455: 3674, 456: 1112, 457: 8010, 458: 775, 459: 1219, 460: 5304, 461: 8010, 462: 242, 463: 469, 464: 1223, 465: 352, 466: 778, 467: 8010, 468: 1222, 469: 775, 470: 4645, 471: 15802, 472: 23429, 473: 36426, 474: 38930, 475: 6499, 476: 25564, 477: 3252, 478: 269, 479: 1112, 480: 3272, 481: 146, 482: 245, 483: 8010, 484: 778, 485: 188, 486: 1223, 487: 151, 488: 181, 489: 4013, 490: 1110, 491: 5916, 492: 246, 493: 976, 494: 2499, 495: 1112, 496: 270, 497: 3253, 498: 1310, 499: 308, 500: 7989, 501: 5566, 502: 7559, 503: 3252, 504: 4685, 505: 272, 506: 1112, 507: 6093, 508: 3572, 509: 283, 510: 1112, 511: 25582, 512: 3850, 513: 1112, 514: 272, 515: 29728, 516: 1308, 517: 1783, 518: 8010, 519: 778, 520: 1223, 521: 6209, 522: 6701, 523: 6209, 524: 38893, 525: 1223, 526: 162, 527: 349, 528: 778, 529: 8010, 530: 772, 531: 352, 532: 775, 533: 8010, 534: 1221, 535: 147, 536: 349, 537: 775, 538: 8010, 539: 3830, 540: 1112, 541: 272, 542: 8010, 543: 778, 544: 1223, 545: 5564, 546: 4644, 547: 15785, 548: 38930, 549: 8010, 550: 775, 551: 1220, 552: 182, 553: 3448, 554: 283, 555: 4577, 556: 5566, 557: 5858, 558: 1109, 559: 246, 560: 2218, 561: 386, 562: 8010, 563: 8010, 564: 762, 565: 1223, 566: 88, 567: 39067, 568: 6205, 569: 7470, 570: 3762, 571: 1109, 572: 272, 573: 3810, 574: 232, 575: 1876, 576: 3837, 577: 1310, 578: 3255, 579: 6054, 580: 1112, 581: 2399, 582: 670, 583: 1217, 584: 778, 585: 8010, 586: 1112, 587: 269, 588: 360, 589: 23850, 590: 1223, 591: 122, 592: 778, 593: 8010, 594: 4683, 595: 1112, 596: 3255, 597: 1307, 598: 3834, 599: 1876, 600: 348, 601: 3810, 602: 8010, 603: 774, 604: 1009, 605: 8006, 606: 778, 607: 1223, 608: 5244, 609: 1112, 610: 3191, 611: 18308, 612: 8009, 613: 778, 614: 1223, 615: 24002, 616: 1112, 617: 8010, 618: 778, 619: 1219, 620: 168, 621: 36203, 622: 1157, 623: 5459, 624: 1112, 625: 8007, 626: 753, 627: 1223, 628: 12770, 629: 2941, 630: 3614, 631: 4368, 632: 272, 633: 1107, 634: 283, 635: 8007, 636: 778, 637: 352, 638: 1223, 639: 155, 640: 9198, 641: 6750, 642: 148, 643: 23453, 644: 338, 645: 1223, 646: 119, 647: 778, 648: 8009, 649: 238, 650: 1112, 651: 270, 652: 38631, 653: 150, 654: 1310, 655: 776, 656: 1221, 657: 7501, 658: 7470, 659: 5547, 660: 164, 661: 272, 662: 1112, 663: 4993, 664: 1490, 665: 6306, 666: 8007, 667: 778, 668: 1223, 669: 5406, 670: 1112, 671: 167, 672: 5566, 673: 4645, 674: 433, 675: 15792, 676: 38930, 677: 7889, 678: 3255, 679: 1112, 680: 8004, 681: 777, 682: 1221, 683: 1221, 684: 8010, 685: 1223, 686: 772, 687: 8010, 688: 1101, 689: 270, 690: 1112, 691: 4130, 692: 25566, 693: 3255, 694: 8604, 695: 272, 696: 1112, 697: 283, 698: 8010, 699: 778, 700: 477, 701: 1107, 702: 243, 703: 283, 704: 8010, 705: 778, 706: 1223, 707: 173, 708: 338, 709: 6203, 710: 7614, 711: 4214, 712: 806, 713: 159, 714: 770, 715: 8010, 716: 1106, 717: 8009, 718: 3075, 719: 6978, 720: 38930, 721: 15802, 722: 4645, 723: 5566, 724: 283, 725: 1112, 726: 3255, 727: 990, 728: 839, 729: 1223, 730: 771, 731: 8008, 732: 274, 733: 1112, 734: 272, 735: 3253, 736: 5564, 737: 433, 738: 38930, 739: 8010, 740: 778, 741: 1214, 742: 139, 743: 170, 744: 6203, 745: 8008, 746: 778, 747: 1218, 748: 334, 749: 28844, 750: 6196, 751: 2491, 752: 25566, 753: 3247, 754: 4897, 755: 1056, 756: 1392, 757: 1991, 758: 30369, 759: 166, 760: 1221, 761: 778, 762: 8010, 763: 2915, 764: 1109, 765: 25556, 766: 3251, 767: 9391, 768: 3036, 769: 6205, 770: 1222, 771: 768, 772: 8002, 773: 6870, 774: 1111, 775: 3255, 776: 25569, 777: 2027, 778: 6457, 779: 3810, 780: 237, 781: 346, 782: 1872, 783: 3837, 784: 1310, 785: 271, 786: 1112, 787: 2580, 788: 875, 789: 815, 790: 8010, 791: 778, 792: 1219, 793: 155, 794: 181, 795: 6205, 796: 25569, 797: 3255, 798: 3507, 799: 272, 800: 1112, 801: 246, 802: 8010, 803: 768, 804: 1223, 805: 99, 806: 16090, 807: 25568, 808: 3253, 809: 6034, 810: 272, 811: 1112, 812: 246, 813: 2425, 814: 6279, 815: 8010, 816: 102, 817: 778, 818: 1223, 819: 128, 820: 4891, 821: 1112, 822: 283, 823: 4855, 824: 1112, 825: 283, 826: 3447, 827: 4368, 828: 2849, 829: 1212, 830: 773, 831: 8010, 832: 8282, 833: 250, 834: 8010, 835: 129, 836: 283, 837: 1112, 838: 272, 839: 15832, 840: 36809, 841: 3255, 842: 1310, 843: 3437, 844: 8007, 845: 767, 846: 350, 847: 1223, 848: 6204, 849: 1223, 850: 346, 851: 775, 852: 8007, 853: 38923, 854: 15799, 855: 4645, 856: 5566, 857: 6203, 858: 3974, 859: 1223, 860: 778, 861: 8007, 862: 8010, 863: 770, 864: 1223, 865: 104, 866: 150, 867: 148, 868: 6204, 869: 4893, 870: 352, 871: 776, 872: 8010, 873: 1112, 874: 6427, 875: 8010, 876: 5589, 877: 1112, 878: 8010, 879: 778, 880: 14732, 881: 1223, 882: 7998, 883: 1112, 884: 272, 885: 2443, 886: 246, 887: 982, 888: 352, 889: 775, 890: 8010, 891: 4418, 892: 1112, 893: 2705, 894: 8010, 895: 280, 896: 8008, 897: 777, 898: 7899, 899: 2070, 900: 8010, 901: 8008, 902: 770, 903: 1194, 904: 182, 905: 168, 906: 140, 907: 127, 908: 6209, 909: 2364, 910: 8010, 911: 778, 912: 350, 913: 1222, 914: 182, 915: 6204, 916: 1982, 917: 5551, 918: 433, 919: 38930, 920: 6098, 921: 6209, 922: 1206, 923: 771, 924: 8007, 925: 4316, 926: 1109, 927: 248, 928: 6064, 929: 36760, 930: 1108, 931: 280, 932: 1304, 933: 6880, 934: 272, 935: 1112, 936: 281, 937: 5557, 938: 15795, 939: 5566, 940: 4645, 941: 15471, 942: 15801, 943: 30607, 944: 38930, 945: 8008, 946: 778, 947: 1223, 948: 82, 949: 155, 950: 6205, 951: 3389, 952: 1112, 953: 2888, 954: 2906, 955: 246, 956: 1112, 957: 262, 958: 51206, 959: 3250, 960: 1310, 961: 3837, 962: 1876, 963: 3810, 964: 2019, 965: 642, 966: 201, 967: 3255, 968: 17729, 969: 272, 970: 1112, 971: 3587, 972: 671, 973: 183, 974: 1216, 975: 771, 976: 8010, 977: 38930, 978: 30602, 979: 1215, 980: 775, 981: 8009, 982: 2918, 983: 1223, 984: 771, 985: 8010, 986: 38930, 987: 21610, 988: 15801, 989: 14556, 990: 4645, 991: 5566, 992: 1112, 993: 4574, 994: 2279, 995: 3276, 996: 3399, 997: 283, 998: 1112, 999: 17502, 1000: 3243, 1001: 25565, 1002: 3255, 1003: 18027, 1004: 1112, 1005: 2719, 1006: 8008, 1007: 777, 1008: 352, 1009: 1223, 1010: 2174},
                                    'matches': {0: 243, 1: 1084, 2: 688, 3: 2202, 4: 919, 5: 706, 6: 377, 7: 285, 8: 1104, 9: 708, 10: 4453, 11: 5781, 12: 226, 13: 1027, 14: 248, 15: 2913, 16: 1212, 17: 3547, 18: 1751, 19: 390, 20: 2318, 21: 1140, 22: 166, 23: 725, 24: 7439, 25: 1036, 26: 250, 27: 2627, 28: 120, 29: 267, 30: 1019, 31: 3496, 32: 2921, 33: 1217, 34: 3559, 35: 1738, 36: 390, 37: 1653, 38: 193, 39: 7277, 40: 6862, 41: 718, 42: 316, 43: 1132, 44: 142, 45: 144, 46: 4511, 47: 393, 48: 1315, 49: 904, 50: 701, 51: 1362, 52: 4161, 53: 4642, 54: 709, 55: 1130, 56: 78, 57: 3442, 58: 306, 59: 658, 60: 1094, 61: 140, 62: 639, 63: 1288, 64: 5645, 65: 2753, 66: 1529, 67: 870, 68: 722, 69: 653, 70: 726, 71: 4284, 72: 1020, 73: 3694, 74: 1194, 75: 564, 76: 236, 77: 239, 78: 2024, 79: 8341, 80: 2694, 81: 5509, 82: 1087, 83: 697, 84: 5906, 85: 5978, 86: 684, 87: 1080, 88: 140, 89: 156, 90: 1546, 91: 2963, 92: 5510, 93: 688, 94: 916, 95: 994, 96: 231, 97: 252, 98: 6336, 99: 483, 100: 367, 101: 685, 102: 127, 103: 868, 104: 2137, 105: 689, 106: 1089, 107: 169, 108: 3156, 109: 1024, 110: 225, 111: 5018, 112: 375, 113: 694, 114: 898, 115: 1229, 116: 1121, 117: 6892, 118: 147, 119: 1124, 120: 692, 121: 7281, 122: 222, 123: 1000, 124: 1048, 125: 1095, 126: 316, 127: 692, 128: 7181, 129: 1010, 130: 3501, 131: 2850, 132: 3910, 133: 4994, 134: 167, 135: 5086, 136: 720, 137: 1119, 138: 160, 139: 1103, 140: 706, 141: 7219, 142: 1015, 143: 247, 144: 1060, 145: 694, 146: 7247, 147: 1007, 148: 1264, 149: 698, 150: 316, 151: 1098, 152: 148, 153: 1527, 154: 956, 155: 1545, 156: 193, 157: 265, 158: 1026, 159: 241, 160: 3523, 161: 2864, 162: 1212, 163: 3125, 164: 136, 165: 1105, 166: 706, 167: 6370, 168: 1257, 169: 358, 170: 515, 171: 901, 172: 2205, 173: 684, 174: 1123, 175: 164, 176: 485, 177: 384, 178: 5577, 179: 4656, 180: 157, 181: 410, 182: 1085, 183: 3474, 184: 1177, 185: 2887, 186: 3429, 187: 252, 188: 1008, 189: 213, 190: 2125, 191: 168, 192: 1104, 193: 172, 194: 698, 195: 7305, 196: 1020, 197: 240, 198: 2855, 199: 3228, 200: 1693, 201: 3463, 202: 1195, 203: 2988, 204: 1007, 205: 7357, 206: 721, 207: 1116, 208: 148, 209: 162, 210: 1060, 211: 713, 212: 1127, 213: 167, 214: 5340, 215: 165, 216: 144, 217: 1109, 218: 714, 219: 3254, 220: 1713, 221: 3466, 222: 1193, 223: 179, 224: 2936, 225: 3602, 226: 257, 227: 1022, 228: 261, 229: 3217, 230: 3583, 231: 5764, 232: 698, 233: 1093, 234: 5442, 235: 3952, 236: 1105, 237: 700, 238: 649, 239: 2033, 240: 2101, 241: 909, 242: 996, 243: 250, 244: 7193, 245: 712, 246: 283, 247: 1101, 248: 4054, 249: 294, 250: 164, 251: 1097, 252: 703, 253: 7251, 254: 259, 255: 998, 256: 255, 257: 1039, 258: 640, 259: 257, 260: 1044, 261: 255, 262: 4889, 263: 2880, 264: 2723, 265: 149, 266: 1115, 267: 325, 268: 697, 269: 1716, 270: 257, 271: 702, 272: 1976, 273: 7254, 274: 145, 275: 994, 276: 253, 277: 2884, 278: 1194, 279: 1120, 280: 998, 281: 255, 282: 7194, 283: 697, 284: 1114, 285: 3056, 286: 3874, 287: 886, 288: 1193, 289: 709, 290: 318, 291: 1108, 292: 2705, 293: 5658, 294: 2755, 295: 8575, 296: 1408, 297: 3794, 298: 6700, 299: 946, 300: 3351, 301: 1125, 302: 3370, 303: 1628, 304: 984, 305: 3573, 306: 2984, 307: 3624, 308: 1031, 309: 7260, 310: 703, 311: 1102, 312: 719, 313: 3686, 314: 1008, 315: 249, 316: 1120, 317: 698, 318: 7224, 319: 1019, 320: 1363, 321: 958, 322: 1149, 323: 2940, 324: 3587, 325: 982, 326: 246, 327: 7144, 328: 693, 329: 1098, 330: 935, 331: 5509, 332: 2097, 333: 1319, 334: 161, 335: 1846, 336: 3453, 337: 220, 338: 376, 339: 1687, 340: 3502, 341: 1183, 342: 2963, 343: 256, 344: 1031, 345: 2794, 346: 6053, 347: 720, 348: 1056, 349: 1103, 350: 698, 351: 7235, 352: 985, 353: 250, 354: 3504, 355: 2877, 356: 2009, 357: 690, 358: 1090, 359: 1782, 360: 3435, 361: 1003, 362: 230, 363: 3520, 364: 2852, 365: 783, 366: 4537, 367: 255, 368: 1027, 369: 7295, 370: 714, 371: 1105, 372: 99, 373: 1262, 374: 1223, 375: 659, 376: 6906, 377: 708, 378: 1106, 379: 168, 380: 311, 381: 1227, 382: 755, 383: 254, 384: 1026, 385: 3151, 386: 2870, 387: 1174, 388: 3460, 389: 1691, 390: 211, 391: 3223, 392: 1814, 393: 105, 394: 155, 395: 591, 396: 259, 397: 2075, 398: 4900, 399: 375, 400: 249, 401: 1026, 402: 6827, 403: 1117, 404: 696, 405: 7205, 406: 1008, 407: 238, 408: 3550, 409: 2879, 410: 1193, 411: 1073, 412: 449, 413: 716, 414: 111, 415: 1096, 416: 150, 417: 167, 418: 2745, 419: 4701, 420: 3464, 421: 263, 422: 1021, 423: 252, 424: 1062, 425: 1094, 426: 700, 427: 598, 428: 2989, 429: 1191, 430: 2964, 431: 3591, 432: 1023, 433: 257, 434: 1082, 435: 258, 436: 932, 437: 1519, 438: 359, 439: 1116, 440: 4151, 441: 2155, 442: 2713, 443: 5585, 444: 654, 445: 152, 446: 1116, 447: 303, 448: 708, 449: 1004, 450: 2204, 451: 1192, 452: 184, 453: 2960, 454: 2469, 455: 3092, 456: 1011, 457: 7290, 458: 699, 459: 1118, 460: 1785, 461: 7246, 462: 216, 463: 423, 464: 1096, 465: 315, 466: 690, 467: 4294, 468: 1102, 469: 699, 470: 272, 471: 684, 472: 224, 473: 128, 474: 909, 475: 812, 476: 19049, 477: 3005, 478: 253, 479: 1002, 480: 3003, 481: 136, 482: 225, 483: 7347, 484: 713, 485: 173, 486: 1119, 487: 138, 488: 169, 489: 3610, 490: 991, 491: 5442, 492: 230, 493: 904, 494: 2278, 495: 1018, 496: 248, 497: 2874, 498: 1193, 499: 290, 500: 5998, 501: 2458, 502: 6352, 503: 2488, 504: 3598, 505: 253, 506: 1040, 507: 5563, 508: 3247, 509: 249, 510: 1026, 511: 1511, 512: 3533, 513: 1015, 514: 238, 515: 2835, 516: 1202, 517: 1613, 518: 5713, 519: 711, 520: 1105, 521: 5473, 522: 6118, 523: 5363, 524: 2611, 525: 1125, 526: 148, 527: 315, 528: 709, 529: 2590, 530: 695, 531: 313, 532: 710, 533: 1927, 534: 1094, 535: 139, 536: 322, 537: 691, 538: 5060, 539: 3508, 540: 1003, 541: 245, 542: 364, 543: 724, 544: 1097, 545: 658, 546: 372, 547: 667, 548: 904, 549: 2211, 550: 719, 551: 1107, 552: 167, 553: 3047, 554: 253, 555: 308, 556: 1202, 557: 2398, 558: 1030, 559: 231, 560: 2013, 561: 350, 562: 4207, 563: 3039, 564: 693, 565: 1072, 566: 83, 567: 2691, 568: 3129, 569: 1811, 570: 3391, 571: 998, 572: 233, 573: 1419, 574: 206, 575: 1664, 576: 3472, 577: 1177, 578: 2944, 579: 3638, 580: 1037, 581: 2171, 582: 537, 583: 1092, 584: 709, 585: 7171, 586: 998, 587: 250, 588: 328, 589: 722, 590: 1080, 591: 116, 592: 696, 593: 5491, 594: 4174, 595: 998, 596: 2847, 597: 1172, 598: 3465, 599: 1690, 600: 320, 601: 2128, 602: 283, 603: 715, 604: 905, 605: 6637, 606: 703, 607: 1099, 608: 4761, 609: 1002, 610: 1649, 611: 3829, 612: 5119, 613: 707, 614: 1089, 615: 497, 616: 1032, 617: 7297, 618: 719, 619: 1113, 620: 152, 621: 4861, 622: 839, 623: 5050, 624: 1037, 625: 1082, 626: 667, 627: 1109, 628: 2482, 629: 2648, 630: 1650, 631: 1853, 632: 253, 633: 1001, 634: 259, 635: 7258, 636: 714, 637: 320, 638: 1129, 639: 147, 640: 494, 641: 4600, 642: 133, 643: 943, 644: 292, 645: 1129, 646: 114, 647: 712, 648: 7241, 649: 211, 650: 1001, 651: 250, 652: 3562, 653: 139, 654: 1202, 655: 696, 656: 1083, 657: 3538, 658: 6785, 659: 5042, 660: 140, 661: 252, 662: 1023, 663: 4478, 664: 256, 665: 719, 666: 6267, 667: 691, 668: 1039, 669: 4666, 670: 1006, 671: 152, 672: 4920, 673: 363, 674: 197, 675: 662, 676: 900, 677: 2069, 678: 2941, 679: 1033, 680: 7285, 681: 716, 682: 1122, 683: 1121, 684: 4090, 685: 1122, 686: 687, 687: 7248, 688: 1005, 689: 129, 690: 1004, 691: 3754, 692: 2276, 693: 2977, 694: 3639, 695: 242, 696: 1008, 697: 254, 698: 7223, 699: 695, 700: 431, 701: 1011, 702: 222, 703: 254, 704: 7270, 705: 703, 706: 1109, 707: 163, 708: 315, 709: 3905, 710: 3291, 711: 2341, 712: 714, 713: 144, 714: 690, 715: 7186, 716: 976, 717: 5007, 718: 2876, 719: 1276, 720: 903, 721: 705, 722: 389, 723: 5013, 724: 252, 725: 1028, 726: 2740, 727: 883, 728: 690, 729: 1107, 730: 691, 731: 7278, 732: 247, 733: 1015, 734: 245, 735: 2555, 736: 686, 737: 194, 738: 881, 739: 2166, 740: 697, 741: 1077, 742: 125, 743: 153, 744: 986, 745: 3749, 746: 683, 747: 1117, 748: 308, 749: 925, 750: 5636, 751: 2345, 752: 2005, 753: 2990, 754: 3593, 755: 968, 756: 1276, 757: 909, 758: 1173, 759: 154, 760: 1120, 761: 721, 762: 920, 763: 2523, 764: 982, 765: 3451, 766: 2807, 767: 5164, 768: 2470, 769: 3633, 770: 1080, 771: 683, 772: 4776, 773: 6148, 774: 1012, 775: 2859, 776: 1135, 777: 965, 778: 1076, 779: 1614, 780: 207, 781: 318, 782: 1676, 783: 3405, 784: 1150, 785: 242, 786: 1002, 787: 2276, 788: 802, 789: 726, 790: 6351, 791: 692, 792: 1088, 793: 146, 794: 170, 795: 3215, 796: 11556, 797: 2931, 798: 2968, 799: 252, 800: 1014, 801: 224, 802: 7240, 803: 679, 804: 1116, 805: 93, 806: 1634, 807: 9968, 808: 3043, 809: 3674, 810: 254, 811: 1036, 812: 227, 813: 2238, 814: 1244, 815: 2317, 816: 95, 817: 698, 818: 1105, 819: 121, 820: 868, 821: 1029, 822: 258, 823: 4375, 824: 1022, 825: 255, 826: 3130, 827: 3893, 828: 531, 829: 1091, 830: 661, 831: 1524, 832: 255, 833: 224, 834: 7356, 835: 123, 836: 265, 837: 1002, 838: 244, 839: 951, 840: 2935, 841: 2929, 842: 1185, 843: 3139, 844: 3308, 845: 683, 846: 306, 847: 1074, 848: 1427, 849: 1088, 850: 311, 851: 678, 852: 2159, 853: 890, 854: 696, 855: 385, 856: 469, 857: 5496, 858: 892, 859: 1121, 860: 690, 861: 2962, 862: 5703, 863: 714, 864: 1115, 865: 102, 866: 141, 867: 140, 868: 4467, 869: 2566, 870: 312, 871: 695, 872: 7273, 873: 1036, 874: 2541, 875: 5433, 876: 877, 877: 1010, 878: 7244, 879: 717, 880: 188, 881: 1100, 882: 7187, 883: 997, 884: 250, 885: 2157, 886: 216, 887: 889, 888: 311, 889: 715, 890: 5872, 891: 3945, 892: 973, 893: 2460, 894: 4354, 895: 264, 896: 2926, 897: 714, 898: 955, 899: 701, 900: 1919, 901: 2936, 902: 716, 903: 1102, 904: 171, 905: 163, 906: 132, 907: 108, 908: 1616, 909: 451, 910: 4897, 911: 708, 912: 316, 913: 1084, 914: 169, 915: 5649, 916: 1815, 917: 3424, 918: 166, 919: 910, 920: 476, 921: 4094, 922: 1095, 923: 708, 924: 5437, 925: 3892, 926: 985, 927: 222, 928: 4232, 929: 2551, 930: 1002, 931: 261, 932: 1162, 933: 4470, 934: 250, 935: 1014, 936: 251, 937: 5030, 938: 681, 939: 3378, 940: 380, 941: 391, 942: 239, 943: 158, 944: 896, 945: 2191, 946: 710, 947: 1116, 948: 77, 949: 145, 950: 4882, 951: 1165, 952: 1003, 953: 2604, 954: 2572, 955: 217, 956: 986, 957: 233, 958: 3485, 959: 2871, 960: 1179, 961: 3496, 962: 1696, 963: 3409, 964: 1815, 965: 598, 966: 188, 967: 2975, 968: 3562, 969: 236, 970: 1004, 971: 3253, 972: 539, 973: 156, 974: 1087, 975: 668, 976: 2177, 977: 887, 978: 158, 979: 1092, 980: 688, 981: 4243, 982: 2632, 983: 1090, 984: 682, 985: 2185, 986: 895, 987: 94, 988: 688, 989: 104, 990: 373, 991: 5003, 992: 1033, 993: 349, 994: 2034, 995: 2597, 996: 3073, 997: 257, 998: 1009, 999: 3487, 1000: 2901, 1001: 455, 1002: 2581, 1003: 3510, 1004: 1028, 1005: 2473, 1006: 2590, 1007: 708, 1008: 316, 1009: 1097, 1010: 1990},
                                    'alignment_length': {0: 281, 1: 1258, 2: 803, 3: 2549, 4: 1024, 5: 918, 6: 431, 7: 325, 8: 1320, 9: 832, 10: 5213, 11: 6534, 12: 253, 13: 1149, 14: 280, 15: 3361, 16: 1379, 17: 3985, 18: 1966, 19: 445, 20: 2991, 21: 1292, 22: 185, 23: 808, 24: 8361, 25: 1164, 26: 273, 27: 3041, 28: 132, 29: 289, 30: 1158, 31: 4405, 32: 3351, 33: 1351, 34: 4000, 35: 1949, 36: 431, 37: 1862, 38: 220, 39: 8479, 40: 8438, 41: 869, 42: 373, 43: 1305, 44: 158, 45: 157, 46: 5550, 47: 575, 48: 1658, 49: 1081, 50: 945, 51: 1571, 52: 4681, 53: 5371, 54: 812, 55: 1275, 56: 85, 57: 3884, 58: 338, 59: 736, 60: 1265, 61: 153, 62: 812, 63: 1740, 64: 6452, 65: 3194, 66: 1730, 67: 973, 68: 798, 69: 730, 70: 806, 71: 4839, 72: 1194, 73: 4496, 74: 1413, 75: 659, 76: 275, 77: 269, 78: 2319, 79: 9600, 80: 3126, 81: 6408, 82: 1257, 83: 794, 84: 6712, 85: 6979, 86: 783, 87: 1249, 88: 156, 89: 178, 90: 1879, 91: 3720, 92: 6423, 93: 806, 94: 1075, 95: 1152, 96: 257, 97: 291, 98: 7587, 99: 554, 100: 417, 101: 900, 102: 135, 103: 1012, 104: 2573, 105: 791, 106: 1279, 107: 189, 108: 3697, 109: 1130, 110: 250, 111: 5757, 112: 422, 113: 895, 114: 1024, 115: 1400, 116: 1358, 117: 8827, 118: 161, 119: 1250, 120: 783, 121: 8303, 122: 243, 123: 1130, 124: 1370, 125: 1253, 126: 364, 127: 789, 128: 8223, 129: 1138, 130: 4474, 131: 3311, 132: 4469, 133: 5741, 134: 218, 135: 5926, 136: 1005, 137: 1419, 138: 182, 139: 1253, 140: 812, 141: 8206, 142: 1209, 143: 288, 144: 1214, 145: 788, 146: 8207, 147: 1131, 148: 1884, 149: 809, 150: 363, 151: 1295, 152: 158, 153: 2016, 154: 1110, 155: 1782, 156: 213, 157: 296, 158: 1144, 159: 270, 160: 4374, 161: 3328, 162: 1348, 163: 3552, 164: 155, 165: 1322, 166: 811, 167: 7544, 168: 1463, 169: 417, 170: 708, 171: 1028, 172: 2524, 173: 773, 174: 1273, 175: 186, 176: 556, 177: 441, 178: 6303, 179: 5291, 180: 174, 181: 451, 182: 1243, 183: 3983, 184: 1334, 185: 3400, 186: 4556, 187: 273, 188: 1144, 189: 247, 190: 2436, 191: 183, 192: 1262, 193: 191, 194: 787, 195: 8539, 196: 1184, 197: 271, 198: 3437, 199: 3764, 200: 2006, 201: 4026, 202: 1438, 203: 3629, 204: 1173, 205: 8829, 206: 865, 207: 1321, 208: 161, 209: 199, 210: 1210, 211: 817, 212: 1270, 213: 181, 214: 6606, 215: 185, 216: 162, 217: 1285, 218: 870, 219: 3937, 220: 1944, 221: 3932, 222: 1338, 223: 199, 224: 3362, 225: 4496, 226: 277, 227: 1161, 228: 286, 229: 3723, 230: 4190, 231: 7175, 232: 802, 233: 1300, 234: 6623, 235: 4594, 236: 1265, 237: 801, 238: 762, 239: 2378, 240: 2401, 241: 1224, 242: 1145, 243: 298, 244: 8441, 245: 830, 246: 327, 247: 1263, 248: 4645, 249: 337, 250: 182, 251: 1257, 252: 792, 253: 8275, 254: 296, 255: 1133, 256: 286, 257: 1163, 258: 730, 259: 287, 260: 1155, 261: 288, 262: 5502, 263: 3361, 264: 3192, 265: 161, 266: 1278, 267: 368, 268: 790, 269: 1962, 270: 298, 271: 793, 272: 2330, 273: 8288, 274: 154, 275: 1144, 276: 273, 277: 3328, 278: 1332, 279: 1252, 280: 1145, 281: 293, 282: 8289, 283: 816, 284: 1251, 285: 3559, 286: 4928, 287: 1095, 288: 1407, 289: 799, 290: 358, 291: 1273, 292: 3819, 293: 6458, 294: 3290, 295: 9705, 296: 1567, 297: 4342, 298: 8176, 299: 1128, 300: 4447, 301: 1347, 302: 3991, 303: 1953, 304: 1153, 305: 4144, 306: 3408, 307: 4538, 308: 1149, 309: 8268, 310: 814, 311: 1249, 312: 818, 313: 4242, 314: 1135, 315: 274, 316: 1271, 317: 783, 318: 8431, 319: 1132, 320: 1775, 321: 1139, 322: 1357, 323: 3428, 324: 4612, 325: 1171, 326: 284, 327: 8260, 328: 773, 329: 1250, 330: 1227, 331: 6408, 332: 2399, 333: 1527, 334: 180, 335: 2121, 336: 3937, 337: 247, 338: 433, 339: 1912, 340: 3931, 341: 1328, 342: 3374, 343: 279, 344: 1142, 345: 3315, 346: 7155, 347: 826, 348: 1300, 349: 1275, 350: 811, 351: 8234, 352: 1124, 353: 277, 354: 4477, 355: 3334, 356: 2277, 357: 815, 358: 1257, 359: 2454, 360: 4090, 361: 1172, 362: 273, 363: 4758, 364: 3390, 365: 886, 366: 5151, 367: 281, 368: 1170, 369: 8310, 370: 799, 371: 1249, 372: 111, 373: 1618, 374: 1604, 375: 861, 376: 7931, 377: 797, 378: 1277, 379: 181, 380: 350, 381: 1462, 382: 879, 383: 287, 384: 1164, 385: 4013, 386: 3305, 387: 1334, 388: 3941, 389: 1912, 390: 237, 391: 3892, 392: 2071, 393: 112, 394: 174, 395: 665, 396: 290, 397: 2373, 398: 5662, 399: 438, 400: 276, 401: 1174, 402: 7795, 403: 1279, 404: 798, 405: 8256, 406: 1131, 407: 269, 408: 4490, 409: 3337, 410: 1349, 411: 1214, 412: 500, 413: 812, 414: 119, 415: 1247, 416: 169, 417: 189, 418: 3145, 419: 5369, 420: 3917, 421: 298, 422: 1156, 423: 275, 424: 1396, 425: 1260, 426: 808, 427: 698, 428: 3389, 429: 1344, 430: 3381, 431: 4485, 432: 1144, 433: 294, 434: 1249, 435: 284, 436: 1086, 437: 1747, 438: 434, 439: 1253, 440: 4747, 441: 2464, 442: 3123, 443: 6645, 444: 844, 445: 160, 446: 1257, 447: 357, 448: 798, 449: 1147, 450: 2531, 451: 1347, 452: 208, 453: 3403, 454: 3022, 455: 4340, 456: 1157, 457: 8296, 458: 792, 459: 1262, 460: 2175, 461: 8421, 462: 248, 463: 491, 464: 1253, 465: 355, 466: 804, 467: 4879, 468: 1287, 469: 796, 470: 316, 471: 889, 472: 339, 473: 140, 474: 1026, 475: 978, 476: 22270, 477: 3686, 478: 281, 479: 1151, 480: 3377, 481: 150, 482: 252, 483: 8478, 484: 813, 485: 192, 486: 1269, 487: 152, 488: 185, 489: 4247, 490: 1198, 491: 6164, 492: 249, 493: 1007, 494: 2661, 495: 1131, 496: 278, 497: 3302, 498: 1332, 499: 314, 500: 6879, 501: 3080, 502: 7211, 503: 2883, 504: 4596, 505: 278, 506: 1134, 507: 6386, 508: 3674, 509: 291, 510: 1141, 511: 1904, 512: 4504, 513: 1189, 514: 279, 515: 3574, 516: 1364, 517: 1898, 518: 6742, 519: 826, 520: 1278, 521: 6856, 522: 7178, 523: 6432, 524: 3546, 525: 1260, 526: 165, 527: 356, 528: 801, 529: 2940, 530: 804, 531: 363, 532: 806, 533: 2221, 534: 1267, 535: 148, 536: 362, 537: 801, 538: 5797, 539: 4089, 540: 1127, 541: 277, 542: 417, 543: 820, 544: 1276, 545: 749, 546: 433, 547: 892, 548: 1031, 549: 2559, 550: 810, 551: 1278, 552: 187, 553: 3725, 554: 299, 555: 347, 556: 1438, 557: 3304, 558: 1262, 559: 268, 560: 2457, 561: 430, 562: 5299, 563: 3499, 564: 796, 565: 1254, 566: 94, 567: 4056, 568: 3909, 569: 2351, 570: 4186, 571: 1129, 572: 269, 573: 1615, 574: 238, 575: 1978, 576: 3950, 577: 1346, 578: 3382, 579: 4452, 580: 1157, 581: 2493, 582: 620, 583: 1243, 584: 796, 585: 8353, 586: 1149, 587: 288, 588: 430, 589: 973, 590: 1254, 591: 124, 592: 798, 593: 6346, 594: 4811, 595: 1148, 596: 3336, 597: 1335, 598: 4012, 599: 1932, 600: 366, 601: 2642, 602: 348, 603: 798, 604: 1030, 605: 7497, 606: 796, 607: 1242, 608: 5446, 609: 1147, 610: 1899, 611: 4389, 612: 5978, 613: 831, 614: 1289, 615: 570, 616: 1178, 617: 8352, 618: 819, 619: 1265, 620: 171, 621: 6107, 622: 987, 623: 5874, 624: 1154, 625: 1299, 626: 783, 627: 1256, 628: 3382, 629: 3074, 630: 2169, 631: 2194, 632: 283, 633: 1189, 634: 292, 635: 8307, 636: 811, 637: 364, 638: 1266, 639: 157, 640: 608, 641: 5286, 642: 151, 643: 1219, 644: 346, 645: 1256, 646: 121, 647: 788, 648: 8250, 649: 234, 650: 1145, 651: 276, 652: 4414, 653: 157, 654: 1370, 655: 884, 656: 1392, 657: 4851, 658: 7902, 659: 5751, 660: 162, 661: 285, 662: 1187, 663: 5192, 664: 302, 665: 829, 666: 8356, 667: 836, 668: 1289, 669: 5902, 670: 1174, 671: 169, 672: 5808, 673: 423, 674: 232, 675: 952, 676: 1023, 677: 2442, 678: 3472, 679: 1188, 680: 8512, 681: 826, 682: 1279, 683: 1277, 684: 8244, 685: 1277, 686: 783, 687: 8317, 688: 1126, 689: 148, 690: 1145, 691: 4318, 692: 2605, 693: 3372, 694: 4490, 695: 279, 696: 1141, 697: 292, 698: 8285, 699: 836, 700: 483, 701: 1146, 702: 250, 703: 298, 704: 8440, 705: 808, 706: 1268, 707: 176, 708: 358, 709: 4522, 710: 3894, 711: 2753, 712: 812, 713: 161, 714: 795, 715: 8521, 716: 1110, 717: 6253, 718: 3648, 719: 1474, 720: 1045, 721: 908, 722: 461, 723: 5836, 724: 288, 725: 1136, 726: 3232, 727: 1121, 728: 820, 729: 1252, 730: 786, 731: 8375, 732: 278, 733: 1150, 734: 283, 735: 2971, 736: 815, 737: 238, 738: 1062, 739: 2629, 740: 812, 741: 1278, 742: 146, 743: 175, 744: 1185, 745: 4377, 746: 808, 747: 1256, 748: 348, 749: 1154, 750: 6505, 751: 2782, 752: 2421, 753: 3477, 754: 4569, 755: 1158, 756: 1595, 757: 1312, 758: 1569, 759: 178, 760: 1355, 761: 867, 762: 1175, 763: 3005, 764: 1153, 765: 4359, 766: 3336, 767: 6501, 768: 2877, 769: 4775, 770: 1314, 771: 810, 772: 5995, 773: 7102, 774: 1149, 775: 3324, 776: 1306, 777: 1124, 778: 1297, 779: 1893, 780: 231, 781: 367, 782: 2111, 783: 3897, 784: 1334, 785: 276, 786: 1147, 787: 2641, 788: 896, 789: 836, 790: 7387, 791: 806, 792: 1260, 793: 161, 794: 187, 795: 3704, 796: 13168, 797: 3354, 798: 3874, 799: 286, 800: 1143, 801: 254, 802: 8255, 803: 791, 804: 1275, 805: 100, 806: 2272, 807: 11413, 808: 3580, 809: 4665, 810: 281, 811: 1149, 812: 263, 813: 2513, 814: 1767, 815: 2697, 816: 103, 817: 800, 818: 1258, 819: 142, 820: 992, 821: 1174, 822: 290, 823: 5015, 824: 1148, 825: 292, 826: 3566, 827: 4817, 828: 665, 829: 1247, 830: 778, 831: 1808, 832: 284, 833: 255, 834: 8321, 835: 129, 836: 289, 837: 1124, 838: 274, 839: 1315, 840: 3550, 841: 3330, 842: 1324, 843: 3536, 844: 3870, 845: 792, 846: 353, 847: 1254, 848: 1698, 849: 1265, 850: 352, 851: 784, 852: 2538, 853: 1004, 854: 902, 855: 444, 856: 537, 857: 6679, 858: 1260, 859: 1289, 860: 828, 861: 3594, 862: 7187, 863: 904, 864: 1284, 865: 111, 866: 154, 867: 158, 868: 5418, 869: 2960, 870: 363, 871: 790, 872: 9101, 873: 1320, 874: 2931, 875: 6213, 876: 1058, 877: 1164, 878: 8442, 879: 816, 880: 222, 881: 1249, 882: 8352, 883: 1132, 884: 283, 885: 2534, 886: 253, 887: 1000, 888: 396, 889: 885, 890: 7345, 891: 4918, 892: 1212, 893: 3183, 894: 5843, 895: 323, 896: 3789, 897: 929, 898: 1129, 899: 813, 900: 2328, 901: 3678, 902: 940, 903: 1561, 904: 195, 905: 183, 906: 156, 907: 120, 908: 1897, 909: 530, 910: 5607, 911: 797, 912: 355, 913: 1263, 914: 188, 915: 6622, 916: 2268, 917: 3949, 918: 193, 919: 1045, 920: 559, 921: 4986, 922: 1312, 923: 807, 924: 6503, 925: 4503, 926: 1143, 927: 251, 928: 4968, 929: 3129, 930: 1169, 931: 297, 932: 1386, 933: 5196, 934: 290, 935: 1261, 936: 290, 937: 7175, 938: 976, 939: 3911, 940: 423, 941: 560, 942: 283, 943: 171, 944: 1014, 945: 2517, 946: 804, 947: 1261, 948: 87, 949: 159, 950: 6309, 951: 1545, 952: 1153, 953: 2991, 954: 3191, 955: 270, 956: 1140, 957: 266, 958: 4532, 959: 3331, 960: 1344, 961: 3938, 962: 1936, 963: 3959, 964: 2040, 965: 654, 966: 210, 967: 3375, 968: 4494, 969: 281, 970: 1185, 971: 3716, 972: 627, 973: 180, 974: 1239, 975: 791, 976: 2546, 977: 1021, 978: 170, 979: 1351, 980: 948, 981: 5771, 982: 3633, 983: 1288, 984: 789, 985: 2563, 986: 1022, 987: 100, 988: 893, 989: 117, 990: 424, 991: 5758, 992: 1152, 993: 428, 994: 2457, 995: 3379, 996: 3530, 997: 305, 998: 1155, 999: 4353, 1000: 3321, 1001: 513, 1002: 3026, 1003: 4538, 1004: 1175, 1005: 3037, 1006: 3020, 1007: 827, 1008: 368, 1009: 1279, 1010: 2273},
                                    'mapq': {0: 29, 1: 60, 2: 60, 3: 60, 4: 60, 5: 60, 6: 37, 7: 28, 8: 60, 9: 60, 10: 60, 11: 60, 12: 60, 13: 60, 14: 51, 15: 60, 16: 60, 17: 60, 18: 60, 19: 52, 20: 60, 21: 60, 22: 60, 23: 60, 24: 60, 25: 60, 26: 60, 27: 60, 28: 22, 29: 49, 30: 60, 31: 60, 32: 60, 33: 60, 34: 60, 35: 60, 36: 60, 37: 60, 38: 48, 39: 60, 40: 60, 41: 60, 42: 37, 43: 60, 44: 60, 45: 60, 46: 60, 47: 23, 48: 51, 49: 60, 50: 55, 51: 60, 52: 60, 53: 60, 54: 55, 55: 60, 56: 23, 57: 60, 58: 60, 59: 60, 60: 60, 61: 42, 62: 33, 63: 24, 64: 60, 65: 60, 66: 60, 67: 60, 68: 60, 69: 60, 70: 60, 71: 60, 72: 60, 73: 60, 74: 60, 75: 60, 76: 56, 77: 22, 78: 60, 79: 60, 80: 60, 81: 39, 82: 60, 83: 60, 84: 60, 85: 60, 86: 60, 87: 60, 88: 27, 89: 30, 90: 60, 91: 60, 92: 60, 93: 60, 94: 60, 95: 60, 96: 60, 97: 44, 98: 60, 99: 60, 100: 60, 101: 60, 102: 24, 103: 60, 104: 60, 105: 60, 106: 60, 107: 28, 108: 60, 109: 60, 110: 39, 111: 60, 112: 20, 113: 60, 114: 60, 115: 60, 116: 37, 117: 60, 118: 23, 119: 60, 120: 60, 121: 60, 122: 60, 123: 60, 124: 39, 125: 60, 126: 60, 127: 60, 128: 60, 129: 60, 130: 28, 131: 60, 132: 60, 133: 60, 134: 28, 135: 60, 136: 60, 137: 60, 138: 33, 139: 60, 140: 60, 141: 60, 142: 60, 143: 60, 144: 60, 145: 60, 146: 60, 147: 60, 148: 60, 149: 60, 150: 51, 151: 60, 152: 47, 153: 21, 154: 60, 155: 60, 156: 60, 157: 60, 158: 60, 159: 52, 160: 57, 161: 60, 162: 60, 163: 60, 164: 22, 165: 60, 166: 60, 167: 60, 168: 60, 169: 59, 170: 31, 171: 60, 172: 60, 173: 60, 174: 60, 175: 54, 176: 38, 177: 60, 178: 60, 179: 60, 180: 46, 181: 60, 182: 60, 183: 60, 184: 60, 185: 60, 186: 47, 187: 60, 188: 60, 189: 30, 190: 60, 191: 60, 192: 60, 193: 60, 194: 60, 195: 60, 196: 60, 197: 25, 198: 60, 199: 60, 200: 60, 201: 60, 202: 60, 203: 60, 204: 60, 205: 60, 206: 60, 207: 60, 208: 39, 209: 25, 210: 60, 211: 60, 212: 60, 213: 32, 214: 60, 215: 23, 216: 43, 217: 60, 218: 55, 219: 60, 220: 60, 221: 60, 222: 60, 223: 29, 224: 60, 225: 34, 226: 60, 227: 60, 228: 51, 229: 60, 230: 60, 231: 60, 232: 60, 233: 60, 234: 58, 235: 60, 236: 60, 237: 60, 238: 52, 239: 60, 240: 60, 241: 41, 242: 60, 243: 37, 244: 60, 245: 60, 246: 60, 247: 60, 248: 60, 249: 30, 250: 21, 251: 60, 252: 60, 253: 60, 254: 20, 255: 60, 256: 39, 257: 60, 258: 60, 259: 60, 260: 60, 261: 51, 262: 60, 263: 60, 264: 60, 265: 31, 266: 60, 267: 60, 268: 60, 269: 60, 270: 51, 271: 60, 272: 60, 273: 60, 274: 27, 275: 60, 276: 60, 277: 60, 278: 60, 279: 60, 280: 60, 281: 39, 282: 60, 283: 60, 284: 60, 285: 60, 286: 60, 287: 41, 288: 60, 289: 60, 290: 32, 291: 60, 292: 31, 293: 60, 294: 60, 295: 60, 296: 60, 297: 60, 298: 60, 299: 60, 300: 23, 301: 60, 302: 60, 303: 60, 304: 60, 305: 60, 306: 60, 307: 36, 308: 60, 309: 60, 310: 60, 311: 60, 312: 46, 313: 60, 314: 60, 315: 60, 316: 60, 317: 60, 318: 60, 319: 60, 320: 48, 321: 60, 322: 60, 323: 60, 324: 60, 325: 60, 326: 60, 327: 60, 328: 60, 329: 60, 330: 46, 331: 41, 332: 60, 333: 60, 334: 25, 335: 60, 336: 60, 337: 60, 338: 33, 339: 60, 340: 60, 341: 60, 342: 60, 343: 60, 344: 60, 345: 60, 346: 60, 347: 60, 348: 60, 349: 60, 350: 60, 351: 60, 352: 60, 353: 24, 354: 60, 355: 60, 356: 60, 357: 60, 358: 60, 359: 44, 360: 60, 361: 60, 362: 57, 363: 38, 364: 60, 365: 60, 366: 60, 367: 60, 368: 60, 369: 60, 370: 60, 371: 60, 372: 20, 373: 26, 374: 45, 375: 22, 376: 60, 377: 60, 378: 60, 379: 45, 380: 47, 381: 39, 382: 60, 383: 21, 384: 60, 385: 51, 386: 60, 387: 60, 388: 60, 389: 60, 390: 41, 391: 60, 392: 60, 393: 23, 394: 39, 395: 60, 396: 56, 397: 60, 398: 60, 399: 27, 400: 60, 401: 60, 402: 60, 403: 60, 404: 60, 405: 60, 406: 60, 407: 46, 408: 60, 409: 60, 410: 60, 411: 60, 412: 60, 413: 60, 414: 30, 415: 60, 416: 43, 417: 60, 418: 60, 419: 60, 420: 60, 421: 59, 422: 60, 423: 60, 424: 60, 425: 60, 426: 60, 427: 36, 428: 60, 429: 60, 430: 60, 431: 60, 432: 60, 433: 43, 434: 60, 435: 60, 436: 60, 437: 60, 438: 43, 439: 60, 440: 60, 441: 60, 442: 60, 443: 60, 444: 38, 445: 36, 446: 60, 447: 60, 448: 60, 449: 60, 450: 60, 451: 60, 452: 45, 453: 60, 454: 60, 455: 21, 456: 60, 457: 60, 458: 60, 459: 60, 460: 60, 461: 60, 462: 36, 463: 32, 464: 60, 465: 60, 466: 48, 467: 60, 468: 60, 469: 60, 470: 24, 471: 60, 472: 40, 473: 22, 474: 60, 475: 60, 476: 60, 477: 56, 478: 60, 479: 60, 480: 60, 481: 57, 482: 53, 483: 60, 484: 60, 485: 21, 486: 60, 487: 45, 488: 60, 489: 60, 490: 60, 491: 60, 492: 60, 493: 60, 494: 60, 495: 60, 496: 60, 497: 60, 498: 60, 499: 45, 500: 60, 501: 60, 502: 60, 503: 60, 504: 60, 505: 60, 506: 60, 507: 60, 508: 60, 509: 24, 510: 60, 511: 20, 512: 60, 513: 60, 514: 20, 515: 47, 516: 60, 517: 60, 518: 60, 519: 60, 520: 60, 521: 54, 522: 60, 523: 47, 524: 26, 525: 60, 526: 34, 527: 35, 528: 60, 529: 60, 530: 60, 531: 60, 532: 60, 533: 60, 534: 60, 535: 30, 536: 60, 537: 60, 538: 60, 539: 60, 540: 60, 541: 20, 542: 57, 543: 60, 544: 60, 545: 60, 546: 25, 547: 60, 548: 60, 549: 60, 550: 60, 551: 60, 552: 54, 553: 60, 554: 23, 555: 60, 556: 60, 557: 21, 558: 60, 559: 45, 560: 60, 561: 50, 562: 60, 563: 60, 564: 60, 565: 60, 566: 22, 567: 30, 568: 60, 569: 21, 570: 60, 571: 60, 572: 29, 573: 60, 574: 21, 575: 60, 576: 60, 577: 60, 578: 60, 579: 60, 580: 60, 581: 60, 582: 34, 583: 60, 584: 60, 585: 60, 586: 60, 587: 60, 588: 21, 589: 45, 590: 60, 591: 23, 592: 60, 593: 60, 594: 60, 595: 60, 596: 60, 597: 60, 598: 60, 599: 60, 600: 39, 601: 60, 602: 25, 603: 60, 604: 60, 605: 60, 606: 60, 607: 60, 608: 60, 609: 60, 610: 60, 611: 60, 612: 60, 613: 60, 614: 60, 615: 43, 616: 60, 617: 60, 618: 60, 619: 60, 620: 24, 621: 60, 622: 60, 623: 60, 624: 60, 625: 60, 626: 60, 627: 60, 628: 33, 629: 60, 630: 60, 631: 60, 632: 60, 633: 60, 634: 21, 635: 60, 636: 60, 637: 53, 638: 60, 639: 40, 640: 40, 641: 60, 642: 37, 643: 30, 644: 37, 645: 60, 646: 33, 647: 60, 648: 60, 649: 24, 650: 60, 651: 60, 652: 60, 653: 35, 654: 60, 655: 50, 656: 60, 657: 25, 658: 60, 659: 60, 660: 38, 661: 41, 662: 60, 663: 60, 664: 25, 665: 60, 666: 60, 667: 60, 668: 60, 669: 46, 670: 60, 671: 22, 672: 60, 673: 36, 674: 24, 675: 35, 676: 60, 677: 60, 678: 60, 679: 60, 680: 60, 681: 60, 682: 60, 683: 60, 684: 60, 685: 60, 686: 60, 687: 60, 688: 60, 689: 25, 690: 60, 691: 60, 692: 60, 693: 60, 694: 60, 695: 29, 696: 60, 697: 44, 698: 60, 699: 60, 700: 60, 701: 60, 702: 60, 703: 22, 704: 60, 705: 60, 706: 60, 707: 54, 708: 28, 709: 60, 710: 60, 711: 60, 712: 60, 713: 26, 714: 60, 715: 60, 716: 60, 717: 60, 718: 60, 719: 60, 720: 60, 721: 55, 722: 60, 723: 60, 724: 25, 725: 60, 726: 60, 727: 37, 728: 27, 729: 60, 730: 60, 731: 60, 732: 43, 733: 60, 734: 60, 735: 60, 736: 60, 737: 20, 738: 60, 739: 60, 740: 60, 741: 60, 742: 30, 743: 45, 744: 60, 745: 60, 746: 60, 747: 60, 748: 21, 749: 39, 750: 60, 751: 60, 752: 60, 753: 60, 754: 60, 755: 60, 756: 60, 757: 25, 758: 24, 759: 55, 760: 60, 761: 60, 762: 38, 763: 60, 764: 60, 765: 25, 766: 21, 767: 60, 768: 60, 769: 60, 770: 60, 771: 60, 772: 60, 773: 60, 774: 60, 775: 60, 776: 60, 777: 29, 778: 60, 779: 60, 780: 42, 781: 25, 782: 60, 783: 60, 784: 60, 785: 60, 786: 60, 787: 60, 788: 60, 789: 60, 790: 60, 791: 60, 792: 60, 793: 60, 794: 43, 795: 60, 796: 60, 797: 60, 798: 60, 799: 24, 800: 60, 801: 43, 802: 60, 803: 60, 804: 60, 805: 20, 806: 21, 807: 60, 808: 60, 809: 60, 810: 44, 811: 60, 812: 43, 813: 60, 814: 60, 815: 60, 816: 21, 817: 60, 818: 60, 819: 46, 820: 60, 821: 60, 822: 25, 823: 60, 824: 60, 825: 60, 826: 60, 827: 60, 828: 25, 829: 60, 830: 60, 831: 60, 832: 30, 833: 24, 834: 60, 835: 21, 836: 30, 837: 60, 838: 46, 839: 29, 840: 43, 841: 60, 842: 60, 843: 60, 844: 60, 845: 60, 846: 60, 847: 60, 848: 60, 849: 60, 850: 60, 851: 60, 852: 60, 853: 60, 854: 60, 855: 60, 856: 60, 857: 60, 858: 27, 859: 60, 860: 60, 861: 60, 862: 60, 863: 60, 864: 60, 865: 35, 866: 60, 867: 28, 868: 60, 869: 60, 870: 30, 871: 60, 872: 60, 873: 60, 874: 60, 875: 60, 876: 60, 877: 60, 878: 60, 879: 60, 880: 36, 881: 60, 882: 60, 883: 60, 884: 60, 885: 60, 886: 60, 887: 60, 888: 23, 889: 24, 890: 60, 891: 60, 892: 24, 893: 36, 894: 60, 895: 38, 896: 60, 897: 60, 898: 60, 899: 60, 900: 60, 901: 60, 902: 32, 903: 25, 904: 56, 905: 43, 906: 33, 907: 22, 908: 60, 909: 60, 910: 60, 911: 60, 912: 33, 913: 60, 914: 58, 915: 60, 916: 60, 917: 60, 918: 26, 919: 60, 920: 22, 921: 60, 922: 22, 923: 60, 924: 60, 925: 60, 926: 60, 927: 39, 928: 60, 929: 60, 930: 60, 931: 49, 932: 60, 933: 60, 934: 35, 935: 60, 936: 20, 937: 60, 938: 60, 939: 60, 940: 42, 941: 60, 942: 24, 943: 55, 944: 60, 945: 60, 946: 60, 947: 60, 948: 28, 949: 31, 950: 29, 951: 39, 952: 60, 953: 60, 954: 60, 955: 25, 956: 60, 957: 33, 958: 23, 959: 60, 960: 60, 961: 60, 962: 60, 963: 60, 964: 60, 965: 60, 966: 23, 967: 60, 968: 60, 969: 41, 970: 60, 971: 60, 972: 60, 973: 25, 974: 60, 975: 60, 976: 60, 977: 60, 978: 60, 979: 60, 980: 34, 981: 60, 982: 55, 983: 60, 984: 60, 985: 60, 986: 60, 987: 27, 988: 60, 989: 23, 990: 51, 991: 60, 992: 60, 993: 33, 994: 60, 995: 60, 996: 60, 997: 23, 998: 60, 999: 60, 1000: 60, 1001: 60, 1002: 60, 1003: 60, 1004: 60, 1005: 60, 1006: 60, 1007: 60, 1008: 22, 1009: 60, 1010: 60},
                                    't_id': {0: 66325, 1: 151140, 2: 131695, 3: 118, 4: 145106, 5: 145106, 6: 145106, 7: 118, 8: 151140, 9: 131695, 10: 118, 11: 118, 12: 18939, 13: 158997, 14: 125700, 15: 98361, 16: 158877, 17: 229, 18: 230, 19: 34961, 20: 267, 21: 151140, 22: 54365, 23: 131695, 24: 118, 25: 158997, 26: 125700, 27: 118, 28: 84697, 29: 51778, 30: 158997, 31: 129953, 32: 98361, 33: 158877, 34: 229, 35: 230, 36: 34961, 37: 267, 38: 33789, 39: 118, 40: 118, 41: 131695, 42: 54440, 43: 151140, 44: 25112, 45: 24516, 46: 118, 47: 145106, 48: 118, 49: 145106, 50: 145106, 51: 118, 52: 118, 53: 118, 54: 131695, 55: 151140, 56: 32041, 57: 118, 58: 118, 59: 131695, 60: 151140, 61: 25112, 62: 132623, 63: 169477, 64: 114749, 65: 122377, 66: 122, 67: 118, 68: 131695, 69: 151140, 70: 131695, 71: 118, 72: 158997, 73: 118, 74: 163573, 75: 105008, 76: 86693, 77: 65653, 78: 167755, 79: 122, 80: 122377, 81: 114749, 82: 151140, 83: 131695, 84: 118, 85: 118, 86: 131695, 87: 151140, 88: 25112, 89: 124554, 90: 118, 91: 118, 92: 118, 93: 131695, 94: 151140, 95: 158997, 96: 18939, 97: 51778, 98: 118, 99: 118, 100: 145106, 101: 145106, 102: 145106, 103: 145106, 104: 118, 105: 131695, 106: 151140, 107: 124554, 108: 114749, 109: 158997, 110: 18939, 111: 118, 112: 145106, 113: 145106, 114: 145106, 115: 118, 116: 151140, 117: 118, 118: 24516, 119: 151140, 120: 131695, 121: 118, 122: 18939, 123: 158997, 124: 167804, 125: 151140, 126: 54440, 127: 131695, 128: 118, 129: 158997, 130: 141077, 131: 98361, 132: 147715, 133: 118, 134: 131695, 135: 118, 136: 131695, 137: 151140, 138: 124554, 139: 151140, 140: 131695, 141: 118, 142: 158997, 143: 125700, 144: 151140, 145: 131695, 146: 118, 147: 158997, 148: 118, 149: 131695, 150: 54440, 151: 151140, 152: 25112, 153: 142381, 154: 122377, 155: 118, 156: 33789, 157: 51778, 158: 158997, 159: 125700, 160: 155864, 161: 98361, 162: 158877, 163: 229, 164: 25112, 165: 151140, 166: 131695, 167: 118, 168: 118, 169: 145106, 170: 145106, 171: 145106, 172: 118, 173: 131695, 174: 151140, 175: 124554, 176: 158997, 177: 118, 178: 138012, 179: 138013, 180: 10034, 181: 79436, 182: 174958, 183: 229, 184: 158877, 185: 98361, 186: 91080, 187: 125700, 188: 158997, 189: 18939, 190: 118, 191: 124554, 192: 151140, 193: 54365, 194: 131695, 195: 118, 196: 158997, 197: 125700, 198: 98361, 199: 267, 200: 230, 201: 229, 202: 158877, 203: 98361, 204: 158997, 205: 118, 206: 131695, 207: 151140, 208: 25112, 209: 124554, 210: 118, 211: 131695, 212: 151140, 213: 124554, 214: 114749, 215: 124554, 216: 24516, 217: 151140, 218: 131695, 219: 118, 220: 230, 221: 229, 222: 158877, 223: 54413, 224: 98361, 225: 174486, 226: 125700, 227: 158997, 228: 51778, 229: 118, 230: 118, 231: 118, 232: 131695, 233: 151140, 234: 114749, 235: 114749, 236: 151140, 237: 131695, 238: 118, 239: 118, 240: 118, 241: 125948, 242: 158997, 243: 51778, 244: 118, 245: 131695, 246: 54440, 247: 151140, 248: 114749, 249: 66325, 250: 124554, 251: 151140, 252: 131695, 253: 118, 254: 51778, 255: 158997, 256: 125700, 257: 158997, 258: 118, 259: 125700, 260: 158997, 261: 51778, 262: 118, 263: 122, 264: 122377, 265: 42820, 266: 151140, 267: 54440, 268: 131695, 269: 118, 270: 54440, 271: 131695, 272: 118, 273: 118, 274: 51778, 275: 158997, 276: 125700, 277: 98361, 278: 158877, 279: 229, 280: 158997, 281: 51778, 282: 118, 283: 131695, 284: 151140, 285: 114749, 286: 118, 287: 145106, 288: 118, 289: 131695, 290: 54440, 291: 151140, 292: 128586, 293: 114749, 294: 122377, 295: 122, 296: 167755, 297: 118, 298: 118, 299: 158997, 300: 157626, 301: 158877, 302: 229, 303: 230, 304: 267, 305: 147715, 306: 98361, 307: 170148, 308: 158997, 309: 118, 310: 131695, 311: 151140, 312: 150550, 313: 118, 314: 158997, 315: 125700, 316: 151140, 317: 131695, 318: 118, 319: 158997, 320: 167466, 321: 229, 322: 158877, 323: 98361, 324: 128565, 325: 158997, 326: 51778, 327: 118, 328: 131695, 329: 151140, 330: 153497, 331: 114749, 332: 122377, 333: 118, 334: 82672, 335: 163573, 336: 267, 337: 86346, 338: 34961, 339: 230, 340: 229, 341: 158877, 342: 98361, 343: 125700, 344: 158997, 345: 118, 346: 118, 347: 131695, 348: 151140, 349: 151140, 350: 131695, 351: 118, 352: 158997, 353: 125700, 354: 154857, 355: 98361, 356: 118, 357: 131695, 358: 151140, 359: 153806, 360: 118, 361: 158997, 362: 125700, 363: 167253, 364: 98361, 365: 158877, 366: 118, 367: 125700, 368: 158997, 369: 118, 370: 131695, 371: 151140, 372: 33116, 373: 150138, 374: 104442, 375: 150500, 376: 118, 377: 131695, 378: 151140, 379: 124554, 380: 66325, 381: 114749, 382: 118, 383: 51778, 384: 158997, 385: 154254, 386: 98361, 387: 158877, 388: 229, 389: 230, 390: 86346, 391: 267, 392: 163573, 393: 20210, 394: 82672, 395: 105008, 396: 20745, 397: 167755, 398: 122, 399: 118, 400: 125700, 401: 158997, 402: 118, 403: 151140, 404: 131695, 405: 118, 406: 158997, 407: 125700, 408: 141660, 409: 98361, 410: 158877, 411: 229, 412: 118, 413: 131695, 414: 21391, 415: 151140, 416: 24516, 417: 124554, 418: 122377, 419: 122, 420: 118, 421: 51778, 422: 158997, 423: 125700, 424: 141913, 425: 151140, 426: 131695, 427: 118, 428: 229, 429: 158877, 430: 98361, 431: 145088, 432: 158997, 433: 51778, 434: 118, 435: 51778, 436: 118, 437: 114749, 438: 170196, 439: 151140, 440: 118, 441: 122, 442: 122377, 443: 114749, 444: 159149, 445: 24516, 446: 151140, 447: 54440, 448: 131695, 449: 118, 450: 229, 451: 158877, 452: 54413, 453: 98361, 454: 114497, 455: 166956, 456: 158997, 457: 118, 458: 131695, 459: 151140, 460: 118, 461: 118, 462: 18939, 463: 158997, 464: 151140, 465: 54440, 466: 131695, 467: 118, 468: 151140, 469: 131695, 470: 145106, 471: 145106, 472: 145106, 473: 145106, 474: 145106, 475: 118, 476: 147715, 477: 98361, 478: 125700, 479: 158997, 480: 118, 481: 158997, 482: 18939, 483: 118, 484: 131695, 485: 54365, 486: 151140, 487: 24516, 488: 124554, 489: 118, 490: 158997, 491: 118, 492: 18939, 493: 158997, 494: 118, 495: 158997, 496: 125700, 497: 98361, 498: 158877, 499: 229, 500: 118, 501: 118, 502: 118, 503: 98361, 504: 114477, 505: 125700, 506: 158997, 507: 118, 508: 118, 509: 51778, 510: 158997, 511: 176226, 512: 118, 513: 158997, 514: 125700, 515: 123790, 516: 158877, 517: 229, 518: 118, 519: 131695, 520: 151140, 521: 114749, 522: 118, 523: 114749, 524: 127906, 525: 151140, 526: 54461, 527: 54440, 528: 131695, 529: 118, 530: 151140, 531: 54440, 532: 131695, 533: 118, 534: 151140, 535: 42562, 536: 54440, 537: 131695, 538: 118, 539: 118, 540: 158997, 541: 125700, 542: 118, 543: 131695, 544: 151140, 545: 118, 546: 145106, 547: 145106, 548: 145106, 549: 118, 550: 131695, 551: 151140, 552: 124554, 553: 118, 554: 51778, 555: 145106, 556: 118, 557: 134846, 558: 158997, 559: 18939, 560: 118, 561: 131695, 562: 118, 563: 118, 564: 131695, 565: 151140, 566: 32173, 567: 164355, 568: 114749, 569: 118, 570: 118, 571: 158997, 572: 125700, 573: 267, 574: 86346, 575: 230, 576: 229, 577: 158877, 578: 98361, 579: 166651, 580: 158997, 581: 118, 582: 150550, 583: 151140, 584: 131695, 585: 118, 586: 158997, 587: 125700, 588: 47314, 589: 174263, 590: 151140, 591: 21391, 592: 131695, 593: 118, 594: 118, 595: 158997, 596: 98361, 597: 158877, 598: 229, 599: 230, 600: 83601, 601: 267, 602: 118, 603: 131695, 604: 151140, 605: 118, 606: 131695, 607: 151140, 608: 118, 609: 158997, 610: 118, 611: 138391, 612: 118, 613: 131695, 614: 151140, 615: 126130, 616: 158997, 617: 118, 618: 131695, 619: 151140, 620: 42820, 621: 147289, 622: 118, 623: 118, 624: 158997, 625: 118, 626: 131695, 627: 151140, 628: 158660, 629: 122377, 630: 118, 631: 118, 632: 125700, 633: 158997, 634: 51778, 635: 118, 636: 131695, 637: 54440, 638: 151140, 639: 25112, 640: 171018, 641: 118, 642: 18139, 643: 140128, 644: 66325, 645: 151140, 646: 21391, 647: 131695, 648: 118, 649: 18939, 650: 158997, 651: 125700, 652: 163044, 653: 43088, 654: 158877, 655: 229, 656: 151140, 657: 118, 658: 118, 659: 118, 660: 158997, 661: 125700, 662: 158997, 663: 118, 664: 118, 665: 118, 666: 118, 667: 131695, 668: 151140, 669: 124, 670: 158997, 671: 34295, 672: 118, 673: 145106, 674: 161745, 675: 145106, 676: 145106, 677: 118, 678: 98361, 679: 158997, 680: 118, 681: 131695, 682: 151140, 683: 151140, 684: 118, 685: 151140, 686: 131695, 687: 118, 688: 158997, 689: 125700, 690: 158997, 691: 118, 692: 147715, 693: 98361, 694: 171365, 695: 125700, 696: 158997, 697: 51778, 698: 118, 699: 131695, 700: 151140, 701: 158997, 702: 18939, 703: 51778, 704: 118, 705: 131695, 706: 151140, 707: 124554, 708: 66325, 709: 114749, 710: 118, 711: 118, 712: 151140, 713: 33799, 714: 131695, 715: 118, 716: 158997, 717: 118, 718: 229, 719: 118, 720: 145106, 721: 145106, 722: 145106, 723: 118, 724: 51778, 725: 158997, 726: 98361, 727: 158877, 728: 150550, 729: 151140, 730: 131695, 731: 118, 732: 51778, 733: 158997, 734: 125700, 735: 98361, 736: 118, 737: 161745, 738: 145106, 739: 118, 740: 131695, 741: 151140, 742: 45737, 743: 124554, 744: 114749, 745: 118, 746: 131695, 747: 151140, 748: 66325, 749: 138205, 750: 114749, 751: 122377, 752: 147715, 753: 98361, 754: 128239, 755: 158997, 756: 118, 757: 55412, 758: 129870, 759: 42820, 760: 151140, 761: 131695, 762: 118, 763: 118, 764: 158997, 765: 176226, 766: 98361, 767: 122, 768: 122377, 769: 114749, 770: 151140, 771: 131695, 772: 118, 773: 118, 774: 158997, 775: 98361, 776: 147715, 777: 118, 778: 118, 779: 267, 780: 86346, 781: 83601, 782: 230, 783: 229, 784: 158877, 785: 125700, 786: 158997, 787: 118, 788: 158997, 789: 118, 790: 118, 791: 131695, 792: 151140, 793: 24516, 794: 124554, 795: 114749, 796: 147715, 797: 98361, 798: 128841, 799: 125700, 800: 158997, 801: 18939, 802: 118, 803: 131695, 804: 151140, 805: 27433, 806: 131403, 807: 147715, 808: 98361, 809: 166651, 810: 125700, 811: 158997, 812: 18939, 813: 118, 814: 118, 815: 118, 816: 20934, 817: 131695, 818: 151140, 819: 19041, 820: 118, 821: 158997, 822: 51778, 823: 118, 824: 158997, 825: 51778, 826: 118, 827: 118, 828: 118, 829: 151140, 830: 131695, 831: 118, 832: 172920, 833: 131695, 834: 118, 835: 84697, 836: 51778, 837: 158997, 838: 125700, 839: 111219, 840: 165576, 841: 98361, 842: 158877, 843: 229, 844: 118, 845: 131695, 846: 54440, 847: 151140, 848: 114749, 849: 151140, 850: 54440, 851: 131695, 852: 118, 853: 145106, 854: 145106, 855: 145106, 856: 118, 857: 114749, 858: 174138, 859: 151140, 860: 131695, 861: 118, 862: 118, 863: 131695, 864: 151140, 865: 33116, 866: 25112, 867: 18139, 868: 114749, 869: 118, 870: 54440, 871: 131695, 872: 118, 873: 158997, 874: 118, 875: 118, 876: 118, 877: 158997, 878: 118, 879: 131695, 880: 128176, 881: 151140, 882: 118, 883: 158997, 884: 125700, 885: 118, 886: 18939, 887: 158997, 888: 54440, 889: 131695, 890: 118, 891: 118, 892: 158997, 893: 118, 894: 118, 895: 131695, 896: 118, 897: 131695, 898: 118, 899: 118, 900: 118, 901: 118, 902: 131695, 903: 151140, 904: 124554, 905: 42820, 906: 18139, 907: 19041, 908: 114749, 909: 95506, 910: 118, 911: 131695, 912: 54440, 913: 151140, 914: 124554, 915: 114749, 916: 122377, 917: 118, 918: 161745, 919: 145106, 920: 118, 921: 114749, 922: 151140, 923: 131695, 924: 118, 925: 118, 926: 158997, 927: 63856, 928: 118, 929: 176571, 930: 158997, 931: 51778, 932: 118, 933: 118, 934: 125700, 935: 158997, 936: 51778, 937: 118, 938: 145106, 939: 118, 940: 145106, 941: 145106, 942: 145106, 943: 145106, 944: 145106, 945: 118, 946: 131695, 947: 151140, 948: 32041, 949: 25112, 950: 114749, 951: 146259, 952: 158997, 953: 118, 954: 118, 955: 18939, 956: 158997, 957: 125700, 958: 130619, 959: 98361, 960: 158877, 961: 229, 962: 230, 963: 267, 964: 163573, 965: 105008, 966: 65653, 967: 98361, 968: 164479, 969: 125700, 970: 158997, 971: 118, 972: 150550, 973: 90009, 974: 151140, 975: 131695, 976: 118, 977: 145106, 978: 145106, 979: 151140, 980: 131695, 981: 118, 982: 118, 983: 151140, 984: 131695, 985: 118, 986: 145106, 987: 145106, 988: 145106, 989: 145106, 990: 145106, 991: 118, 992: 158997, 993: 118, 994: 118, 995: 118, 996: 118, 997: 51778, 998: 158997, 999: 120543, 1000: 98361, 1001: 147715, 1002: 98361, 1003: 131253, 1004: 158997, 1005: 118, 1006: 118, 1007: 131695, 1008: 54440, 1009: 151140, 1010: 122377},
                                    'next_con': {0: 151140, 1: 131695, 2: 118, 3: 145106, 4: 145106, 5: 145106, 6: 118, 7: -1, 8: 131695, 9: 118, 10: -1, 11: 18939, 12: 158997, 13: 125700, 14: 98361, 15: 158877, 16: 229, 17: 230, 18: 34961, 19: 267, 20: -1, 21: 54365, 22: 131695, 23: 118, 24: 158997, 25: 125700, 26: -1, 27: 84697, 28: 51778, 29: 158997, 30: 129953, 31: 98361, 32: 158877, 33: 229, 34: 230, 35: 34961, 36: 267, 37: -1, 38: 118, 39: -1, 40: 131695, 41: 54440, 42: 151140, 43: 25112, 44: 24516, 45: -1, 46: 145106, 47: -1, 48: 145106, 49: 145106, 50: -1, 51: -1, 52: -1, 53: 131695, 54: 151140, 55: 32041, 56: -1, 57: -1, 58: 131695, 59: 151140, 60: 25112, 61: 132623, 62: 169477, 63: 114749, 64: 122377, 65: 122, 66: -1, 67: 131695, 68: 151140, 69: -1, 70: 118, 71: -1, 72: 118, 73: -1, 74: 105008, 75: 86693, 76: 65653, 77: 167755, 78: 122, 79: 122377, 80: 114749, 81: 151140, 82: 131695, 83: 118, 84: -1, 85: 131695, 86: 151140, 87: 25112, 88: 124554, 89: -1, 90: 118, 91: -1, 92: 131695, 93: 151140, 94: -1, 95: 18939, 96: 51778, 97: 118, 98: -1, 99: 145106, 100: 145106, 101: 145106, 102: 145106, 103: 118, 104: 131695, 105: 151140, 106: 124554, 107: 114749, 108: -1, 109: 18939, 110: 118, 111: 145106, 112: 145106, 113: 145106, 114: 118, 115: -1, 116: 118, 117: -1, 118: 151140, 119: 131695, 120: 118, 121: 18939, 122: 158997, 123: -1, 124: 151140, 125: 54440, 126: 131695, 127: 118, 128: 158997, 129: 141077, 130: 98361, 131: 147715, 132: -1, 133: 131695, 134: -1, 135: 131695, 136: 151140, 137: -1, 138: 151140, 139: 131695, 140: 118, 141: 158997, 142: 125700, 143: -1, 144: 131695, 145: 118, 146: 158997, 147: -1, 148: 131695, 149: 54440, 150: 151140, 151: 25112, 152: 142381, 153: 122377, 154: -1, 155: 33789, 156: 51778, 157: 158997, 158: 125700, 159: 155864, 160: 98361, 161: 158877, 162: 229, 163: -1, 164: 151140, 165: 131695, 166: 118, 167: -1, 168: -1, 169: 145106, 170: 145106, 171: 118, 172: 131695, 173: 151140, 174: 124554, 175: -1, 176: 118, 177: 138012, 178: 138013, 179: 10034, 180: 79436, 181: 174958, 182: -1, 183: 158877, 184: 98361, 185: 91080, 186: 125700, 187: 158997, 188: 18939, 189: 118, 190: -1, 191: 151140, 192: 54365, 193: 131695, 194: 118, 195: 158997, 196: 125700, 197: 98361, 198: -1, 199: 230, 200: 229, 201: 158877, 202: 98361, 203: 158997, 204: 118, 205: 131695, 206: 151140, 207: 25112, 208: 124554, 209: -1, 210: 131695, 211: 151140, 212: 124554, 213: 114749, 214: -1, 215: 24516, 216: 151140, 217: 131695, 218: 118, 219: -1, 220: 229, 221: 158877, 222: 54413, 223: 98361, 224: 174486, 225: 125700, 226: 158997, 227: 51778, 228: 118, 229: -1, 230: -1, 231: 131695, 232: 151140, 233: 114749, 234: -1, 235: 151140, 236: 131695, 237: 118, 238: 118, 239: -1, 240: -1, 241: 158997, 242: 51778, 243: 118, 244: 131695, 245: 54440, 246: 151140, 247: -1, 248: 66325, 249: 124554, 250: 151140, 251: 131695, 252: 118, 253: 51778, 254: 158997, 255: -1, 256: 158997, 257: 118, 258: -1, 259: 158997, 260: 51778, 261: 118, 262: -1, 263: 122377, 264: 42820, 265: 151140, 266: 54440, 267: 131695, 268: 118, 269: -1, 270: 131695, 271: 118, 272: -1, 273: 51778, 274: 158997, 275: 125700, 276: 98361, 277: 158877, 278: 229, 279: -1, 280: 51778, 281: 118, 282: 131695, 283: 151140, 284: 114749, 285: -1, 286: 145106, 287: -1, 288: 131695, 289: 54440, 290: 151140, 291: 128586, 292: 114749, 293: 122377, 294: 122, 295: 167755, 296: -1, 297: -1, 298: 158997, 299: 157626, 300: 158877, 301: 229, 302: 230, 303: 267, 304: -1, 305: 98361, 306: 170148, 307: 158997, 308: 118, 309: 131695, 310: 151140, 311: 150550, 312: -1, 313: 158997, 314: 125700, 315: -1, 316: 131695, 317: 118, 318: 158997, 319: 167466, 320: -1, 321: 158877, 322: 98361, 323: 128565, 324: 158997, 325: 51778, 326: 118, 327: 131695, 328: 151140, 329: 153497, 330: 114749, 331: 122377, 332: -1, 333: -1, 334: 163573, 335: 267, 336: 86346, 337: 34961, 338: 230, 339: 229, 340: 158877, 341: 98361, 342: 125700, 343: 158997, 344: 118, 345: -1, 346: 131695, 347: 151140, 348: -1, 349: 131695, 350: 118, 351: 158997, 352: 125700, 353: 154857, 354: 98361, 355: -1, 356: 131695, 357: 151140, 358: 153806, 359: -1, 360: 158997, 361: 125700, 362: 167253, 363: 98361, 364: 158877, 365: -1, 366: -1, 367: 158997, 368: 118, 369: 131695, 370: 151140, 371: 33116, 372: 150138, 373: 104442, 374: 150500, 375: -1, 376: 131695, 377: 151140, 378: 124554, 379: 66325, 380: 114749, 381: -1, 382: 51778, 383: 158997, 384: 154254, 385: 98361, 386: 158877, 387: 229, 388: 230, 389: 86346, 390: 267, 391: 163573, 392: 20210, 393: 82672, 394: 105008, 395: 20745, 396: 167755, 397: 122, 398: -1, 399: -1, 400: 158997, 401: 118, 402: -1, 403: 131695, 404: 118, 405: 158997, 406: 125700, 407: 141660, 408: 98361, 409: 158877, 410: 229, 411: -1, 412: 131695, 413: 21391, 414: 151140, 415: 24516, 416: 124554, 417: 122377, 418: 122, 419: -1, 420: 51778, 421: 158997, 422: 125700, 423: 141913, 424: -1, 425: 131695, 426: 118, 427: -1, 428: 158877, 429: 98361, 430: 145088, 431: 158997, 432: 51778, 433: 118, 434: -1, 435: 118, 436: -1, 437: 170196, 438: 151140, 439: 118, 440: -1, 441: 122377, 442: 114749, 443: 159149, 444: 24516, 445: 151140, 446: 54440, 447: 131695, 448: 118, 449: -1, 450: 158877, 451: 54413, 452: 98361, 453: 114497, 454: 166956, 455: 158997, 456: 118, 457: 131695, 458: 151140, 459: -1, 460: -1, 461: 18939, 462: 158997, 463: -1, 464: 54440, 465: 131695, 466: 118, 467: -1, 468: 131695, 469: 145106, 470: 145106, 471: 145106, 472: 145106, 473: 145106, 474: 118, 475: -1, 476: 98361, 477: 125700, 478: 158997, 479: 118, 480: -1, 481: 18939, 482: 118, 483: 131695, 484: 54365, 485: 151140, 486: 24516, 487: 124554, 488: -1, 489: 158997, 490: -1, 491: 18939, 492: 158997, 493: -1, 494: 158997, 495: 125700, 496: 98361, 497: 158877, 498: 229, 499: -1, 500: -1, 501: -1, 502: -1, 503: 114477, 504: 125700, 505: 158997, 506: 118, 507: -1, 508: 51778, 509: 158997, 510: 176226, 511: -1, 512: 158997, 513: 125700, 514: 123790, 515: 158877, 516: 229, 517: -1, 518: 131695, 519: 151140, 520: 114749, 521: -1, 522: -1, 523: 127906, 524: 151140, 525: 54461, 526: 54440, 527: 131695, 528: 118, 529: -1, 530: 54440, 531: 131695, 532: 118, 533: -1, 534: 42562, 535: 54440, 536: 131695, 537: 118, 538: -1, 539: 158997, 540: 125700, 541: -1, 542: 131695, 543: 151140, 544: -1, 545: 145106, 546: 145106, 547: 145106, 548: 118, 549: 131695, 550: 151140, 551: 124554, 552: -1, 553: 51778, 554: -1, 555: 118, 556: -1, 557: 158997, 558: 18939, 559: 118, 560: -1, 561: 118, 562: -1, 563: 131695, 564: 151140, 565: 32173, 566: 164355, 567: 114749, 568: -1, 569: -1, 570: 158997, 571: 125700, 572: -1, 573: 86346, 574: 230, 575: 229, 576: 158877, 577: 98361, 578: 166651, 579: 158997, 580: 118, 581: -1, 582: 151140, 583: 131695, 584: 118, 585: 158997, 586: 125700, 587: 47314, 588: -1, 589: 151140, 590: 21391, 591: 131695, 592: 118, 593: -1, 594: 158997, 595: 98361, 596: 158877, 597: 229, 598: 230, 599: 83601, 600: 267, 601: -1, 602: 131695, 603: 151140, 604: -1, 605: 131695, 606: 151140, 607: -1, 608: 158997, 609: -1, 610: 138391, 611: -1, 612: 131695, 613: 151140, 614: 126130, 615: -1, 616: 118, 617: 131695, 618: 151140, 619: 42820, 620: -1, 621: 118, 622: -1, 623: 158997, 624: -1, 625: 131695, 626: 151140, 627: 158660, 628: 122377, 629: -1, 630: 118, 631: -1, 632: 158997, 633: 51778, 634: 118, 635: 131695, 636: 54440, 637: 151140, 638: 25112, 639: 171018, 640: -1, 641: -1, 642: 140128, 643: 66325, 644: 151140, 645: 21391, 646: 131695, 647: 118, 648: 18939, 649: 158997, 650: 125700, 651: 163044, 652: 43088, 653: 158877, 654: 229, 655: -1, 656: 118, 657: -1, 658: -1, 659: 158997, 660: -1, 661: 158997, 662: 118, 663: -1, 664: -1, 665: -1, 666: 131695, 667: 151140, 668: 124, 669: -1, 670: 34295, 671: 118, 672: 145106, 673: 161745, 674: 145106, 675: 145106, 676: 118, 677: -1, 678: 158997, 679: 118, 680: 131695, 681: 151140, 682: -1, 683: 118, 684: -1, 685: 131695, 686: 118, 687: 158997, 688: 125700, 689: -1, 690: 118, 691: -1, 692: 98361, 693: 171365, 694: 125700, 695: 158997, 696: 51778, 697: 118, 698: 131695, 699: 151140, 700: -1, 701: 18939, 702: 51778, 703: 118, 704: 131695, 705: 151140, 706: 124554, 707: 66325, 708: 114749, 709: -1, 710: -1, 711: -1, 712: 33799, 713: 131695, 714: 118, 715: 158997, 716: -1, 717: 229, 718: -1, 719: 145106, 720: 145106, 721: 145106, 722: 118, 723: 51778, 724: 158997, 725: 98361, 726: 158877, 727: -1, 728: 151140, 729: 131695, 730: 118, 731: 51778, 732: 158997, 733: 125700, 734: 98361, 735: -1, 736: 161745, 737: 145106, 738: 118, 739: 131695, 740: 151140, 741: 45737, 742: 124554, 743: 114749, 744: -1, 745: 131695, 746: 151140, 747: 66325, 748: 138205, 749: 114749, 750: 122377, 751: -1, 752: 98361, 753: 128239, 754: 158997, 755: 118, 756: -1, 757: 129870, 758: 42820, 759: 151140, 760: 131695, 761: 118, 762: -1, 763: 158997, 764: 176226, 765: 98361, 766: -1, 767: 122377, 768: 114749, 769: 151140, 770: 131695, 771: 118, 772: -1, 773: 158997, 774: 98361, 775: 147715, 776: -1, 777: -1, 778: -1, 779: 86346, 780: 83601, 781: 230, 782: 229, 783: 158877, 784: 125700, 785: 158997, 786: 118, 787: -1, 788: 118, 789: -1, 790: 131695, 791: 151140, 792: 24516, 793: 124554, 794: 114749, 795: -1, 796: 98361, 797: 128841, 798: 125700, 799: 158997, 800: 18939, 801: 118, 802: 131695, 803: 151140, 804: 27433, 805: 131403, 806: -1, 807: 98361, 808: 166651, 809: 125700, 810: 158997, 811: 18939, 812: 118, 813: -1, 814: -1, 815: 20934, 816: 131695, 817: 151140, 818: 19041, 819: -1, 820: -1, 821: 51778, 822: 118, 823: -1, 824: 51778, 825: 118, 826: -1, 827: -1, 828: -1, 829: 131695, 830: 118, 831: -1, 832: 131695, 833: 118, 834: 84697, 835: 51778, 836: 158997, 837: 125700, 838: 111219, 839: 165576, 840: 98361, 841: 158877, 842: 229, 843: -1, 844: 131695, 845: 54440, 846: 151140, 847: 114749, 848: -1, 849: 54440, 850: 131695, 851: 118, 852: 145106, 853: 145106, 854: 145106, 855: 118, 856: -1, 857: 174138, 858: 151140, 859: 131695, 860: 118, 861: -1, 862: 131695, 863: 151140, 864: 33116, 865: 25112, 866: 18139, 867: 114749, 868: -1, 869: -1, 870: 131695, 871: 118, 872: 158997, 873: -1, 874: -1, 875: -1, 876: -1, 877: 118, 878: 131695, 879: 128176, 880: 151140, 881: -1, 882: 158997, 883: 125700, 884: -1, 885: 18939, 886: 158997, 887: -1, 888: 131695, 889: 118, 890: -1, 891: 158997, 892: -1, 893: 118, 894: 131695, 895: -1, 896: 131695, 897: -1, 898: -1, 899: -1, 900: -1, 901: 131695, 902: 151140, 903: 124554, 904: 42820, 905: 18139, 906: 19041, 907: 114749, 908: -1, 909: 118, 910: 131695, 911: 54440, 912: 151140, 913: 124554, 914: 114749, 915: 122377, 916: -1, 917: 161745, 918: 145106, 919: 118, 920: -1, 921: 151140, 922: 131695, 923: 118, 924: -1, 925: 158997, 926: 63856, 927: -1, 928: -1, 929: 158997, 930: 51778, 931: 118, 932: -1, 933: -1, 934: 158997, 935: 51778, 936: 118, 937: 145106, 938: -1, 939: 145106, 940: 145106, 941: -1, 942: 145106, 943: 145106, 944: 118, 945: 131695, 946: 151140, 947: 32041, 948: 25112, 949: 114749, 950: -1, 951: 158997, 952: 118, 953: -1, 954: 18939, 955: 158997, 956: 125700, 957: 130619, 958: 98361, 959: 158877, 960: 229, 961: 230, 962: 267, 963: 163573, 964: 105008, 965: 65653, 966: -1, 967: 164479, 968: 125700, 969: 158997, 970: 118, 971: -1, 972: 90009, 973: 151140, 974: 131695, 975: 118, 976: 145106, 977: 145106, 978: -1, 979: 131695, 980: 118, 981: 118, 982: -1, 983: 131695, 984: 118, 985: 145106, 986: 145106, 987: 145106, 988: 145106, 989: 145106, 990: 118, 991: 158997, 992: -1, 993: -1, 994: -1, 995: -1, 996: 51778, 997: 158997, 998: 120543, 999: 98361, 1000: 147715, 1001: -1, 1002: 131253, 1003: 158997, 1004: 118, 1005: -1, 1006: 131695, 1007: 54440, 1008: 151140, 1009: 122377, 1010: -1},
                                    'next_strand': {0: '-', 1: '-', 2: '-', 3: '-', 4: '-', 5: '-', 6: '-', 7: '', 8: '-', 9: '-', 10: '', 11: '+', 12: '+', 13: '-', 14: '-', 15: '+', 16: '+', 17: '+', 18: '+', 19: '-', 20: '', 21: '+', 22: '-', 23: '-', 24: '+', 25: '-', 26: '', 27: '+', 28: '+', 29: '+', 30: '-', 31: '-', 32: '+', 33: '+', 34: '+', 35: '+', 36: '-', 37: '', 38: '+', 39: '', 40: '+', 41: '-', 42: '+', 43: '-', 44: '-', 45: '', 46: '+', 47: '', 48: '-', 49: '-', 50: '', 51: '', 52: '', 53: '+', 54: '+', 55: '-', 56: '', 57: '', 58: '+', 59: '+', 60: '-', 61: '-', 62: '+', 63: '-', 64: '+', 65: '-', 66: '', 67: '+', 68: '+', 69: '', 70: '-', 71: '', 72: '+', 73: '', 74: '-', 75: '+', 76: '+', 77: '+', 78: '+', 79: '-', 80: '+', 81: '-', 82: '-', 83: '-', 84: '', 85: '+', 86: '+', 87: '-', 88: '+', 89: '', 90: '+', 91: '', 92: '+', 93: '+', 94: '', 95: '-', 96: '-', 97: '+', 98: '', 99: '+', 100: '+', 101: '+', 102: '+', 103: '+', 104: '+', 105: '+', 106: '+', 107: '-', 108: '', 109: '-', 110: '+', 111: '+', 112: '+', 113: '+', 114: '+', 115: '', 116: '-', 117: '', 118: '-', 119: '-', 120: '-', 121: '+', 122: '+', 123: '', 124: '-', 125: '+', 126: '-', 127: '-', 128: '+', 129: '+', 130: '-', 131: '-', 132: '', 133: '+', 134: '', 135: '+', 136: '+', 137: '', 138: '-', 139: '-', 140: '-', 141: '+', 142: '-', 143: '', 144: '-', 145: '-', 146: '+', 147: '', 148: '+', 149: '-', 150: '+', 151: '-', 152: '+', 153: '+', 154: '', 155: '+', 156: '+', 157: '+', 158: '-', 159: '-', 160: '-', 161: '+', 162: '+', 163: '', 164: '-', 165: '-', 166: '-', 167: '', 168: '', 169: '+', 170: '+', 171: '+', 172: '+', 173: '+', 174: '+', 175: '', 176: '+', 177: '+', 178: '+', 179: '-', 180: '-', 181: '+', 182: '', 183: '-', 184: '+', 185: '-', 186: '+', 187: '-', 188: '-', 189: '+', 190: '', 191: '-', 192: '+', 193: '-', 194: '-', 195: '+', 196: '-', 197: '-', 198: '', 199: '-', 200: '-', 201: '-', 202: '+', 203: '-', 204: '+', 205: '+', 206: '+', 207: '-', 208: '+', 209: '', 210: '+', 211: '+', 212: '+', 213: '-', 214: '', 215: '+', 216: '-', 217: '-', 218: '-', 219: '', 220: '-', 221: '-', 222: '-', 223: '+', 224: '+', 225: '+', 226: '-', 227: '-', 228: '+', 229: '', 230: '', 231: '+', 232: '+', 233: '-', 234: '', 235: '-', 236: '-', 237: '-', 238: '-', 239: '', 240: '', 241: '-', 242: '-', 243: '+', 244: '+', 245: '-', 246: '+', 247: '', 248: '-', 249: '-', 250: '-', 251: '-', 252: '-', 253: '+', 254: '+', 255: '', 256: '-', 257: '+', 258: '', 259: '-', 260: '-', 261: '+', 262: '', 263: '-', 264: '+', 265: '-', 266: '+', 267: '-', 268: '-', 269: '', 270: '-', 271: '-', 272: '', 273: '+', 274: '+', 275: '-', 276: '-', 277: '+', 278: '+', 279: '', 280: '-', 281: '+', 282: '+', 283: '+', 284: '-', 285: '', 286: '+', 287: '', 288: '+', 289: '-', 290: '+', 291: '+', 292: '-', 293: '+', 294: '-', 295: '-', 296: '', 297: '', 298: '+', 299: '+', 300: '+', 301: '+', 302: '+', 303: '-', 304: '', 305: '+', 306: '-', 307: '-', 308: '+', 309: '+', 310: '+', 311: '+', 312: '', 313: '+', 314: '-', 315: '', 316: '-', 317: '-', 318: '+', 319: '-', 320: '', 321: '-', 322: '+', 323: '-', 324: '-', 325: '-', 326: '+', 327: '+', 328: '+', 329: '-', 330: '-', 331: '+', 332: '', 333: '', 334: '+', 335: '+', 336: '-', 337: '-', 338: '-', 339: '-', 340: '-', 341: '+', 342: '+', 343: '-', 344: '+', 345: '', 346: '+', 347: '+', 348: '', 349: '-', 350: '-', 351: '+', 352: '-', 353: '-', 354: '-', 355: '', 356: '+', 357: '+', 358: '+', 359: '', 360: '+', 361: '-', 362: '+', 363: '-', 364: '+', 365: '', 366: '', 367: '-', 368: '+', 369: '+', 370: '+', 371: '-', 372: '-', 373: '+', 374: '+', 375: '', 376: '+', 377: '+', 378: '+', 379: '+', 380: '-', 381: '', 382: '+', 383: '+', 384: '+', 385: '-', 386: '+', 387: '+', 388: '+', 389: '+', 390: '-', 391: '-', 392: '-', 393: '-', 394: '-', 395: '+', 396: '+', 397: '+', 398: '', 399: '', 400: '-', 401: '+', 402: '', 403: '-', 404: '-', 405: '+', 406: '-', 407: '+', 408: '-', 409: '+', 410: '+', 411: '', 412: '+', 413: '-', 414: '+', 415: '-', 416: '+', 417: '+', 418: '-', 419: '', 420: '+', 421: '+', 422: '-', 423: '-', 424: '', 425: '-', 426: '-', 427: '', 428: '-', 429: '+', 430: '-', 431: '-', 432: '-', 433: '+', 434: '', 435: '+', 436: '', 437: '-', 438: '-', 439: '-', 440: '', 441: '-', 442: '+', 443: '-', 444: '+', 445: '-', 446: '+', 447: '-', 448: '-', 449: '', 450: '-', 451: '-', 452: '+', 453: '+', 454: '+', 455: '-', 456: '+', 457: '+', 458: '+', 459: '', 460: '', 461: '+', 462: '+', 463: '', 464: '+', 465: '-', 466: '-', 467: '', 468: '-', 469: '+', 470: '+', 471: '+', 472: '+', 473: '+', 474: '+', 475: '', 476: '+', 477: '+', 478: '-', 479: '+', 480: '', 481: '-', 482: '+', 483: '+', 484: '-', 485: '+', 486: '-', 487: '+', 488: '', 489: '+', 490: '', 491: '+', 492: '+', 493: '', 494: '+', 495: '-', 496: '-', 497: '+', 498: '+', 499: '', 500: '', 501: '', 502: '', 503: '+', 504: '+', 505: '-', 506: '+', 507: '', 508: '+', 509: '+', 510: '-', 511: '', 512: '+', 513: '-', 514: '-', 515: '+', 516: '+', 517: '', 518: '+', 519: '+', 520: '-', 521: '', 522: '', 523: '-', 524: '-', 525: '-', 526: '+', 527: '-', 528: '-', 529: '', 530: '+', 531: '-', 532: '-', 533: '', 534: '-', 535: '+', 536: '-', 537: '-', 538: '', 539: '+', 540: '-', 541: '', 542: '+', 543: '+', 544: '', 545: '+', 546: '+', 547: '+', 548: '+', 549: '+', 550: '+', 551: '+', 552: '', 553: '+', 554: '', 555: '-', 556: '', 557: '-', 558: '-', 559: '+', 560: '', 561: '-', 562: '', 563: '+', 564: '+', 565: '-', 566: '+', 567: '-', 568: '', 569: '', 570: '+', 571: '-', 572: '', 573: '-', 574: '-', 575: '-', 576: '-', 577: '+', 578: '+', 579: '-', 580: '+', 581: '', 582: '-', 583: '-', 584: '-', 585: '+', 586: '-', 587: '+', 588: '', 589: '-', 590: '+', 591: '-', 592: '-', 593: '', 594: '+', 595: '-', 596: '+', 597: '+', 598: '+', 599: '+', 600: '-', 601: '', 602: '+', 603: '+', 604: '', 605: '+', 606: '+', 607: '', 608: '+', 609: '', 610: '+', 611: '', 612: '+', 613: '+', 614: '+', 615: '', 616: '+', 617: '+', 618: '+', 619: '-', 620: '', 621: '+', 622: '', 623: '+', 624: '', 625: '+', 626: '+', 627: '+', 628: '+', 629: '', 630: '-', 631: '', 632: '-', 633: '-', 634: '+', 635: '+', 636: '-', 637: '+', 638: '-', 639: '-', 640: '', 641: '', 642: '+', 643: '-', 644: '-', 645: '+', 646: '-', 647: '-', 648: '+', 649: '+', 650: '-', 651: '+', 652: '+', 653: '+', 654: '+', 655: '', 656: '-', 657: '', 658: '', 659: '+', 660: '', 661: '-', 662: '+', 663: '', 664: '', 665: '', 666: '+', 667: '+', 668: '-', 669: '', 670: '-', 671: '+', 672: '+', 673: '-', 674: '+', 675: '+', 676: '+', 677: '', 678: '-', 679: '+', 680: '+', 681: '+', 682: '', 683: '-', 684: '', 685: '-', 686: '-', 687: '+', 688: '-', 689: '', 690: '+', 691: '', 692: '+', 693: '-', 694: '+', 695: '-', 696: '-', 697: '+', 698: '+', 699: '+', 700: '', 701: '-', 702: '-', 703: '+', 704: '+', 705: '+', 706: '+', 707: '+', 708: '-', 709: '', 710: '', 711: '', 712: '+', 713: '-', 714: '-', 715: '+', 716: '', 717: '+', 718: '', 719: '-', 720: '-', 721: '-', 722: '-', 723: '+', 724: '+', 725: '-', 726: '+', 727: '', 728: '-', 729: '-', 730: '-', 731: '+', 732: '+', 733: '-', 734: '-', 735: '', 736: '-', 737: '+', 738: '+', 739: '+', 740: '+', 741: '-', 742: '+', 743: '-', 744: '', 745: '+', 746: '+', 747: '+', 748: '+', 749: '-', 750: '+', 751: '', 752: '+', 753: '+', 754: '-', 755: '+', 756: '', 757: '-', 758: '+', 759: '-', 760: '-', 761: '-', 762: '', 763: '+', 764: '-', 765: '-', 766: '', 767: '-', 768: '+', 769: '-', 770: '-', 771: '-', 772: '', 773: '+', 774: '-', 775: '-', 776: '', 777: '', 778: '', 779: '-', 780: '-', 781: '-', 782: '-', 783: '-', 784: '+', 785: '-', 786: '+', 787: '', 788: '+', 789: '', 790: '+', 791: '+', 792: '-', 793: '+', 794: '-', 795: '', 796: '+', 797: '+', 798: '+', 799: '-', 800: '-', 801: '+', 802: '+', 803: '+', 804: '+', 805: '+', 806: '', 807: '+', 808: '+', 809: '+', 810: '-', 811: '-', 812: '+', 813: '', 814: '', 815: '-', 816: '+', 817: '+', 818: '+', 819: '', 820: '', 821: '-', 822: '+', 823: '', 824: '-', 825: '+', 826: '', 827: '', 828: '', 829: '-', 830: '-', 831: '', 832: '-', 833: '-', 834: '+', 835: '+', 836: '+', 837: '-', 838: '+', 839: '+', 840: '-', 841: '+', 842: '+', 843: '', 844: '+', 845: '-', 846: '+', 847: '-', 848: '', 849: '+', 850: '-', 851: '-', 852: '-', 853: '-', 854: '-', 855: '-', 856: '', 857: '-', 858: '-', 859: '-', 860: '-', 861: '', 862: '+', 863: '+', 864: '-', 865: '-', 866: '+', 867: '-', 868: '', 869: '', 870: '-', 871: '-', 872: '+', 873: '', 874: '', 875: '', 876: '', 877: '+', 878: '+', 879: '-', 880: '+', 881: '', 882: '+', 883: '-', 884: '', 885: '+', 886: '+', 887: '', 888: '-', 889: '-', 890: '', 891: '+', 892: '', 893: '+', 894: '+', 895: '', 896: '+', 897: '', 898: '', 899: '', 900: '', 901: '+', 902: '+', 903: '+', 904: '-', 905: '+', 906: '+', 907: '-', 908: '', 909: '+', 910: '+', 911: '-', 912: '+', 913: '+', 914: '-', 915: '+', 916: '', 917: '-', 918: '+', 919: '+', 920: '', 921: '-', 922: '-', 923: '-', 924: '', 925: '+', 926: '-', 927: '', 928: '', 929: '-', 930: '-', 931: '+', 932: '', 933: '', 934: '-', 935: '-', 936: '+', 937: '+', 938: '', 939: '+', 940: '+', 941: '', 942: '+', 943: '+', 944: '+', 945: '+', 946: '+', 947: '-', 948: '-', 949: '-', 950: '', 951: '-', 952: '+', 953: '', 954: '+', 955: '+', 956: '-', 957: '+', 958: '-', 959: '+', 960: '+', 961: '+', 962: '-', 963: '-', 964: '-', 965: '+', 966: '', 967: '-', 968: '+', 969: '-', 970: '+', 971: '', 972: '-', 973: '-', 974: '-', 975: '-', 976: '-', 977: '-', 978: '', 979: '-', 980: '-', 981: '-', 982: '', 983: '-', 984: '-', 985: '-', 986: '-', 987: '-', 988: '-', 989: '-', 990: '-', 991: '+', 992: '', 993: '', 994: '', 995: '', 996: '+', 997: '+', 998: '-', 999: '-', 1000: '-', 1001: '', 1002: '-', 1003: '-', 1004: '+', 1005: '', 1006: '+', 1007: '-', 1008: '+', 1009: '+', 1010: ''},
                                    'prev_con': {0: -1, 1: 66325, 2: 151140, 3: 131695, 4: 118, 5: 145106, 6: 145106, 7: 145106, 8: -1, 9: 151140, 10: 131695, 11: -1, 12: 118, 13: 18939, 14: 158997, 15: 125700, 16: 98361, 17: 158877, 18: 229, 19: 230, 20: 34961, 21: -1, 22: 151140, 23: 54365, 24: 131695, 25: 118, 26: 158997, 27: -1, 28: 118, 29: 84697, 30: 51778, 31: 158997, 32: 129953, 33: 98361, 34: 158877, 35: 229, 36: 230, 37: 34961, 38: -1, 39: 33789, 40: -1, 41: 118, 42: 131695, 43: 54440, 44: 151140, 45: 25112, 46: -1, 47: 118, 48: -1, 49: 118, 50: 145106, 51: -1, 52: -1, 53: -1, 54: 118, 55: 131695, 56: 151140, 57: -1, 58: -1, 59: 118, 60: 131695, 61: 151140, 62: 25112, 63: 132623, 64: 169477, 65: 114749, 66: 122377, 67: -1, 68: 118, 69: 131695, 70: -1, 71: 131695, 72: -1, 73: 158997, 74: -1, 75: 163573, 76: 105008, 77: 86693, 78: 65653, 79: 167755, 80: 122, 81: 122377, 82: 114749, 83: 151140, 84: 131695, 85: -1, 86: 118, 87: 131695, 88: 151140, 89: 25112, 90: -1, 91: 118, 92: -1, 93: 118, 94: 131695, 95: -1, 96: 158997, 97: 18939, 98: 51778, 99: -1, 100: 118, 101: 145106, 102: 145106, 103: 145106, 104: 145106, 105: 118, 106: 131695, 107: 151140, 108: 124554, 109: -1, 110: 158997, 111: 18939, 112: 118, 113: 145106, 114: 145106, 115: 145106, 116: -1, 117: 151140, 118: -1, 119: 24516, 120: 151140, 121: 131695, 122: 118, 123: 18939, 124: -1, 125: 167804, 126: 151140, 127: 54440, 128: 131695, 129: 118, 130: 158997, 131: 141077, 132: 98361, 133: -1, 134: 118, 135: -1, 136: 118, 137: 131695, 138: -1, 139: 124554, 140: 151140, 141: 131695, 142: 118, 143: 158997, 144: -1, 145: 151140, 146: 131695, 147: 118, 148: -1, 149: 118, 150: 131695, 151: 54440, 152: 151140, 153: 25112, 154: 142381, 155: -1, 156: 118, 157: 33789, 158: 51778, 159: 158997, 160: 125700, 161: 155864, 162: 98361, 163: 158877, 164: -1, 165: 25112, 166: 151140, 167: 131695, 168: -1, 169: -1, 170: 145106, 171: 145106, 172: 145106, 173: 118, 174: 131695, 175: 151140, 176: -1, 177: 158997, 178: 118, 179: 138012, 180: 138013, 181: 10034, 182: 79436, 183: -1, 184: 229, 185: 158877, 186: 98361, 187: 91080, 188: 125700, 189: 158997, 190: 18939, 191: -1, 192: 124554, 193: 151140, 194: 54365, 195: 131695, 196: 118, 197: 158997, 198: 125700, 199: -1, 200: 267, 201: 230, 202: 229, 203: 158877, 204: 98361, 205: 158997, 206: 118, 207: 131695, 208: 151140, 209: 25112, 210: -1, 211: 118, 212: 131695, 213: 151140, 214: 124554, 215: -1, 216: 124554, 217: 24516, 218: 151140, 219: 131695, 220: -1, 221: 230, 222: 229, 223: 158877, 224: 54413, 225: 98361, 226: 174486, 227: 125700, 228: 158997, 229: 51778, 230: -1, 231: -1, 232: 118, 233: 131695, 234: 151140, 235: -1, 236: 114749, 237: 151140, 238: 131695, 239: 118, 240: -1, 241: -1, 242: 125948, 243: 158997, 244: 51778, 245: 118, 246: 131695, 247: 54440, 248: -1, 249: 114749, 250: 66325, 251: 124554, 252: 151140, 253: 131695, 254: 118, 255: 51778, 256: -1, 257: 125700, 258: 158997, 259: -1, 260: 125700, 261: 158997, 262: 51778, 263: -1, 264: 122, 265: 122377, 266: 42820, 267: 151140, 268: 54440, 269: 131695, 270: -1, 271: 54440, 272: 131695, 273: -1, 274: 118, 275: 51778, 276: 158997, 277: 125700, 278: 98361, 279: 158877, 280: -1, 281: 158997, 282: 51778, 283: 118, 284: 131695, 285: 151140, 286: -1, 287: 118, 288: -1, 289: 118, 290: 131695, 291: 54440, 292: 151140, 293: 128586, 294: 114749, 295: 122377, 296: 122, 297: -1, 298: -1, 299: 118, 300: 158997, 301: 157626, 302: 158877, 303: 229, 304: 230, 305: -1, 306: 147715, 307: 98361, 308: 170148, 309: 158997, 310: 118, 311: 131695, 312: 151140, 313: -1, 314: 118, 315: 158997, 316: -1, 317: 151140, 318: 131695, 319: 118, 320: 158997, 321: -1, 322: 229, 323: 158877, 324: 98361, 325: 128565, 326: 158997, 327: 51778, 328: 118, 329: 131695, 330: 151140, 331: 153497, 332: 114749, 333: -1, 334: -1, 335: 82672, 336: 163573, 337: 267, 338: 86346, 339: 34961, 340: 230, 341: 229, 342: 158877, 343: 98361, 344: 125700, 345: 158997, 346: -1, 347: 118, 348: 131695, 349: -1, 350: 151140, 351: 131695, 352: 118, 353: 158997, 354: 125700, 355: 154857, 356: -1, 357: 118, 358: 131695, 359: 151140, 360: -1, 361: 118, 362: 158997, 363: 125700, 364: 167253, 365: 98361, 366: -1, 367: -1, 368: 125700, 369: 158997, 370: 118, 371: 131695, 372: 151140, 373: 33116, 374: 150138, 375: 104442, 376: -1, 377: 118, 378: 131695, 379: 151140, 380: 124554, 381: 66325, 382: -1, 383: 118, 384: 51778, 385: 158997, 386: 154254, 387: 98361, 388: 158877, 389: 229, 390: 230, 391: 86346, 392: 267, 393: 163573, 394: 20210, 395: 82672, 396: 105008, 397: 20745, 398: 167755, 399: -1, 400: -1, 401: 125700, 402: 158997, 403: -1, 404: 151140, 405: 131695, 406: 118, 407: 158997, 408: 125700, 409: 141660, 410: 98361, 411: 158877, 412: -1, 413: 118, 414: 131695, 415: 21391, 416: 151140, 417: 24516, 418: 124554, 419: 122377, 420: -1, 421: 118, 422: 51778, 423: 158997, 424: 125700, 425: -1, 426: 151140, 427: 131695, 428: -1, 429: 229, 430: 158877, 431: 98361, 432: 145088, 433: 158997, 434: 51778, 435: -1, 436: 51778, 437: -1, 438: 114749, 439: 170196, 440: 151140, 441: -1, 442: 122, 443: 122377, 444: 114749, 445: 159149, 446: 24516, 447: 151140, 448: 54440, 449: 131695, 450: -1, 451: 229, 452: 158877, 453: 54413, 454: 98361, 455: 114497, 456: 166956, 457: 158997, 458: 118, 459: 131695, 460: -1, 461: -1, 462: 118, 463: 18939, 464: -1, 465: 151140, 466: 54440, 467: 131695, 468: -1, 469: 151140, 470: 131695, 471: 145106, 472: 145106, 473: 145106, 474: 145106, 475: 145106, 476: -1, 477: 147715, 478: 98361, 479: 125700, 480: 158997, 481: -1, 482: 158997, 483: 18939, 484: 118, 485: 131695, 486: 54365, 487: 151140, 488: 24516, 489: -1, 490: 118, 491: -1, 492: 118, 493: 18939, 494: -1, 495: 118, 496: 158997, 497: 125700, 498: 98361, 499: 158877, 500: -1, 501: -1, 502: -1, 503: -1, 504: 98361, 505: 114477, 506: 125700, 507: 158997, 508: -1, 509: 118, 510: 51778, 511: 158997, 512: -1, 513: 118, 514: 158997, 515: 125700, 516: 123790, 517: 158877, 518: -1, 519: 118, 520: 131695, 521: 151140, 522: -1, 523: -1, 524: 114749, 525: 127906, 526: 151140, 527: 54461, 528: 54440, 529: 131695, 530: -1, 531: 151140, 532: 54440, 533: 131695, 534: -1, 535: 151140, 536: 42562, 537: 54440, 538: 131695, 539: -1, 540: 118, 541: 158997, 542: -1, 543: 118, 544: 131695, 545: -1, 546: 118, 547: 145106, 548: 145106, 549: 145106, 550: 118, 551: 131695, 552: 151140, 553: -1, 554: 118, 555: -1, 556: 145106, 557: -1, 558: 134846, 559: 158997, 560: 18939, 561: -1, 562: 131695, 563: -1, 564: 118, 565: 131695, 566: 151140, 567: 32173, 568: 164355, 569: -1, 570: -1, 571: 118, 572: 158997, 573: -1, 574: 267, 575: 86346, 576: 230, 577: 229, 578: 158877, 579: 98361, 580: 166651, 581: 158997, 582: -1, 583: 150550, 584: 151140, 585: 131695, 586: 118, 587: 158997, 588: 125700, 589: -1, 590: 174263, 591: 151140, 592: 21391, 593: 131695, 594: -1, 595: 118, 596: 158997, 597: 98361, 598: 158877, 599: 229, 600: 230, 601: 83601, 602: -1, 603: 118, 604: 131695, 605: -1, 606: 118, 607: 131695, 608: -1, 609: 118, 610: -1, 611: 118, 612: -1, 613: 118, 614: 131695, 615: 151140, 616: -1, 617: 158997, 618: 118, 619: 131695, 620: 151140, 621: -1, 622: 147289, 623: -1, 624: 118, 625: -1, 626: 118, 627: 131695, 628: 151140, 629: 158660, 630: -1, 631: 118, 632: -1, 633: 125700, 634: 158997, 635: 51778, 636: 118, 637: 131695, 638: 54440, 639: 151140, 640: 25112, 641: -1, 642: -1, 643: 18139, 644: 140128, 645: 66325, 646: 151140, 647: 21391, 648: 131695, 649: 118, 650: 18939, 651: 158997, 652: 125700, 653: 163044, 654: 43088, 655: 158877, 656: -1, 657: 151140, 658: -1, 659: -1, 660: 118, 661: -1, 662: 125700, 663: 158997, 664: -1, 665: -1, 666: -1, 667: 118, 668: 131695, 669: 151140, 670: -1, 671: 158997, 672: 34295, 673: 118, 674: 145106, 675: 161745, 676: 145106, 677: 145106, 678: -1, 679: 98361, 680: 158997, 681: 118, 682: 131695, 683: -1, 684: 151140, 685: -1, 686: 151140, 687: 131695, 688: 118, 689: 158997, 690: -1, 691: 158997, 692: -1, 693: 147715, 694: 98361, 695: 171365, 696: 125700, 697: 158997, 698: 51778, 699: 118, 700: 131695, 701: -1, 702: 158997, 703: 18939, 704: 51778, 705: 118, 706: 131695, 707: 151140, 708: 124554, 709: 66325, 710: -1, 711: -1, 712: -1, 713: 151140, 714: 33799, 715: 131695, 716: 118, 717: -1, 718: 118, 719: -1, 720: 118, 721: 145106, 722: 145106, 723: 145106, 724: 118, 725: 51778, 726: 158997, 727: 98361, 728: -1, 729: 150550, 730: 151140, 731: 131695, 732: 118, 733: 51778, 734: 158997, 735: 125700, 736: -1, 737: 118, 738: 161745, 739: 145106, 740: 118, 741: 131695, 742: 151140, 743: 45737, 744: 124554, 745: -1, 746: 118, 747: 131695, 748: 151140, 749: 66325, 750: 138205, 751: 114749, 752: -1, 753: 147715, 754: 98361, 755: 128239, 756: 158997, 757: -1, 758: 55412, 759: 129870, 760: 42820, 761: 151140, 762: 131695, 763: -1, 764: 118, 765: 158997, 766: 176226, 767: -1, 768: 122, 769: 122377, 770: 114749, 771: 151140, 772: 131695, 773: -1, 774: 118, 775: 158997, 776: 98361, 777: -1, 778: -1, 779: -1, 780: 267, 781: 86346, 782: 83601, 783: 230, 784: 229, 785: 158877, 786: 125700, 787: 158997, 788: -1, 789: 158997, 790: -1, 791: 118, 792: 131695, 793: 151140, 794: 24516, 795: 124554, 796: -1, 797: 147715, 798: 98361, 799: 128841, 800: 125700, 801: 158997, 802: 18939, 803: 118, 804: 131695, 805: 151140, 806: 27433, 807: -1, 808: 147715, 809: 98361, 810: 166651, 811: 125700, 812: 158997, 813: 18939, 814: -1, 815: -1, 816: 118, 817: 20934, 818: 131695, 819: 151140, 820: -1, 821: -1, 822: 158997, 823: 51778, 824: -1, 825: 158997, 826: 51778, 827: -1, 828: -1, 829: -1, 830: 151140, 831: 131695, 832: -1, 833: 172920, 834: 131695, 835: 118, 836: 84697, 837: 51778, 838: 158997, 839: 125700, 840: 111219, 841: 165576, 842: 98361, 843: 158877, 844: -1, 845: 118, 846: 131695, 847: 54440, 848: 151140, 849: -1, 850: 151140, 851: 54440, 852: 131695, 853: 118, 854: 145106, 855: 145106, 856: 145106, 857: -1, 858: 114749, 859: 174138, 860: 151140, 861: 131695, 862: -1, 863: 118, 864: 131695, 865: 151140, 866: 33116, 867: 25112, 868: 18139, 869: -1, 870: -1, 871: 54440, 872: 131695, 873: 118, 874: -1, 875: -1, 876: -1, 877: -1, 878: 158997, 879: 118, 880: 131695, 881: 128176, 882: -1, 883: 118, 884: 158997, 885: -1, 886: 118, 887: 18939, 888: -1, 889: 54440, 890: 131695, 891: -1, 892: 118, 893: -1, 894: 118, 895: 118, 896: -1, 897: 118, 898: -1, 899: -1, 900: -1, 901: -1, 902: 118, 903: 131695, 904: 151140, 905: 124554, 906: 42820, 907: 18139, 908: 19041, 909: -1, 910: 95506, 911: 118, 912: 131695, 913: 54440, 914: 151140, 915: 124554, 916: 114749, 917: -1, 918: 118, 919: 161745, 920: 145106, 921: -1, 922: 114749, 923: 151140, 924: 131695, 925: -1, 926: 118, 927: 158997, 928: -1, 929: -1, 930: 176571, 931: 158997, 932: 51778, 933: -1, 934: -1, 935: 125700, 936: 158997, 937: 51778, 938: 118, 939: -1, 940: 118, 941: 145106, 942: -1, 943: 145106, 944: 145106, 945: 145106, 946: 118, 947: 131695, 948: 151140, 949: 32041, 950: 25112, 951: -1, 952: 146259, 953: 158997, 954: -1, 955: 118, 956: 18939, 957: 158997, 958: 125700, 959: 130619, 960: 98361, 961: 158877, 962: 229, 963: 230, 964: 267, 965: 163573, 966: 105008, 967: -1, 968: 98361, 969: 164479, 970: 125700, 971: 158997, 972: -1, 973: 150550, 974: 90009, 975: 151140, 976: 131695, 977: 118, 978: 145106, 979: -1, 980: 151140, 981: 131695, 982: 118, 983: -1, 984: 151140, 985: 131695, 986: 118, 987: 145106, 988: 145106, 989: 145106, 990: 145106, 991: 145106, 992: 118, 993: -1, 994: -1, 995: -1, 996: -1, 997: 118, 998: 51778, 999: 158997, 1000: 120543, 1001: 98361, 1002: -1, 1003: 98361, 1004: 131253, 1005: 158997, 1006: -1, 1007: 118, 1008: 131695, 1009: 54440, 1010: 151140},
                                    'prev_strand': {0: '', 1: '-', 2: '-', 3: '-', 4: '-', 5: '-', 6: '-', 7: '-', 8: '', 9: '-', 10: '-', 11: '', 12: '-', 13: '+', 14: '+', 15: '-', 16: '-', 17: '+', 18: '+', 19: '+', 20: '+', 21: '', 22: '-', 23: '+', 24: '-', 25: '-', 26: '+', 27: '', 28: '-', 29: '+', 30: '+', 31: '+', 32: '-', 33: '-', 34: '+', 35: '+', 36: '+', 37: '+', 38: '', 39: '-', 40: '', 41: '+', 42: '+', 43: '-', 44: '+', 45: '-', 46: '', 47: '+', 48: '', 49: '-', 50: '-', 51: '', 52: '', 53: '', 54: '+', 55: '+', 56: '+', 57: '', 58: '', 59: '+', 60: '+', 61: '+', 62: '-', 63: '-', 64: '+', 65: '-', 66: '+', 67: '', 68: '+', 69: '+', 70: '', 71: '-', 72: '', 73: '-', 74: '', 75: '-', 76: '-', 77: '+', 78: '+', 79: '+', 80: '+', 81: '-', 82: '+', 83: '-', 84: '-', 85: '', 86: '+', 87: '+', 88: '+', 89: '-', 90: '', 91: '-', 92: '', 93: '+', 94: '+', 95: '', 96: '-', 97: '-', 98: '-', 99: '', 100: '+', 101: '+', 102: '+', 103: '+', 104: '+', 105: '+', 106: '+', 107: '+', 108: '+', 109: '', 110: '-', 111: '-', 112: '+', 113: '+', 114: '+', 115: '+', 116: '', 117: '-', 118: '', 119: '+', 120: '-', 121: '-', 122: '-', 123: '+', 124: '', 125: '-', 126: '-', 127: '+', 128: '-', 129: '-', 130: '+', 131: '+', 132: '-', 133: '', 134: '+', 135: '', 136: '+', 137: '+', 138: '', 139: '-', 140: '-', 141: '-', 142: '-', 143: '+', 144: '', 145: '-', 146: '-', 147: '-', 148: '', 149: '+', 150: '+', 151: '-', 152: '+', 153: '-', 154: '+', 155: '', 156: '-', 157: '+', 158: '+', 159: '+', 160: '-', 161: '-', 162: '-', 163: '+', 164: '', 165: '+', 166: '-', 167: '-', 168: '', 169: '', 170: '+', 171: '+', 172: '+', 173: '+', 174: '+', 175: '+', 176: '', 177: '-', 178: '+', 179: '+', 180: '+', 181: '-', 182: '-', 183: '', 184: '-', 185: '-', 186: '+', 187: '-', 188: '+', 189: '-', 190: '-', 191: '', 192: '-', 193: '-', 194: '+', 195: '-', 196: '-', 197: '+', 198: '-', 199: '', 200: '+', 201: '-', 202: '-', 203: '-', 204: '+', 205: '-', 206: '+', 207: '+', 208: '+', 209: '-', 210: '', 211: '+', 212: '+', 213: '+', 214: '+', 215: '', 216: '-', 217: '+', 218: '-', 219: '-', 220: '', 221: '-', 222: '-', 223: '-', 224: '-', 225: '+', 226: '+', 227: '+', 228: '-', 229: '-', 230: '', 231: '', 232: '+', 233: '+', 234: '+', 235: '', 236: '+', 237: '-', 238: '-', 239: '-', 240: '', 241: '', 242: '+', 243: '-', 244: '-', 245: '+', 246: '+', 247: '-', 248: '', 249: '+', 250: '-', 251: '-', 252: '-', 253: '-', 254: '-', 255: '+', 256: '', 257: '+', 258: '-', 259: '', 260: '+', 261: '-', 262: '-', 263: '', 264: '+', 265: '-', 266: '+', 267: '-', 268: '+', 269: '-', 270: '', 271: '+', 272: '-', 273: '', 274: '-', 275: '+', 276: '+', 277: '-', 278: '-', 279: '+', 280: '', 281: '-', 282: '-', 283: '+', 284: '+', 285: '+', 286: '', 287: '+', 288: '', 289: '+', 290: '+', 291: '-', 292: '+', 293: '+', 294: '-', 295: '+', 296: '-', 297: '', 298: '', 299: '-', 300: '+', 301: '+', 302: '+', 303: '+', 304: '+', 305: '', 306: '+', 307: '+', 308: '-', 309: '-', 310: '+', 311: '+', 312: '+', 313: '', 314: '-', 315: '+', 316: '', 317: '-', 318: '-', 319: '-', 320: '+', 321: '', 322: '-', 323: '-', 324: '+', 325: '-', 326: '-', 327: '-', 328: '+', 329: '+', 330: '+', 331: '-', 332: '-', 333: '', 334: '', 335: '+', 336: '+', 337: '+', 338: '-', 339: '-', 340: '-', 341: '-', 342: '-', 343: '+', 344: '+', 345: '-', 346: '', 347: '+', 348: '+', 349: '', 350: '-', 351: '-', 352: '-', 353: '+', 354: '-', 355: '-', 356: '', 357: '+', 358: '+', 359: '+', 360: '', 361: '-', 362: '+', 363: '-', 364: '+', 365: '-', 366: '', 367: '', 368: '+', 369: '-', 370: '+', 371: '+', 372: '+', 373: '-', 374: '-', 375: '+', 376: '', 377: '+', 378: '+', 379: '+', 380: '+', 381: '+', 382: '', 383: '-', 384: '+', 385: '+', 386: '+', 387: '-', 388: '+', 389: '+', 390: '+', 391: '+', 392: '-', 393: '-', 394: '-', 395: '-', 396: '-', 397: '+', 398: '+', 399: '', 400: '', 401: '+', 402: '-', 403: '', 404: '-', 405: '-', 406: '-', 407: '+', 408: '-', 409: '+', 410: '-', 411: '+', 412: '', 413: '+', 414: '+', 415: '-', 416: '+', 417: '-', 418: '+', 419: '+', 420: '', 421: '-', 422: '+', 423: '+', 424: '-', 425: '', 426: '-', 427: '-', 428: '', 429: '-', 430: '-', 431: '+', 432: '-', 433: '-', 434: '-', 435: '', 436: '-', 437: '', 438: '+', 439: '-', 440: '-', 441: '', 442: '+', 443: '-', 444: '+', 445: '-', 446: '+', 447: '-', 448: '+', 449: '-', 450: '', 451: '-', 452: '-', 453: '-', 454: '+', 455: '+', 456: '+', 457: '-', 458: '+', 459: '+', 460: '', 461: '', 462: '-', 463: '+', 464: '', 465: '-', 466: '+', 467: '-', 468: '', 469: '-', 470: '-', 471: '+', 472: '+', 473: '+', 474: '+', 475: '+', 476: '', 477: '+', 478: '+', 479: '+', 480: '-', 481: '', 482: '-', 483: '-', 484: '+', 485: '+', 486: '-', 487: '+', 488: '-', 489: '', 490: '-', 491: '', 492: '-', 493: '+', 494: '', 495: '-', 496: '+', 497: '-', 498: '-', 499: '+', 500: '', 501: '', 502: '', 503: '', 504: '+', 505: '+', 506: '+', 507: '-', 508: '', 509: '-', 510: '+', 511: '+', 512: '', 513: '-', 514: '+', 515: '-', 516: '-', 517: '+', 518: '', 519: '+', 520: '+', 521: '+', 522: '', 523: '', 524: '+', 525: '-', 526: '-', 527: '-', 528: '+', 529: '-', 530: '', 531: '-', 532: '+', 533: '-', 534: '', 535: '-', 536: '-', 537: '+', 538: '-', 539: '', 540: '-', 541: '+', 542: '', 543: '+', 544: '+', 545: '', 546: '+', 547: '+', 548: '+', 549: '+', 550: '+', 551: '+', 552: '+', 553: '', 554: '-', 555: '', 556: '-', 557: '', 558: '+', 559: '-', 560: '-', 561: '', 562: '-', 563: '', 564: '+', 565: '+', 566: '+', 567: '-', 568: '+', 569: '', 570: '', 571: '-', 572: '+', 573: '', 574: '+', 575: '-', 576: '-', 577: '-', 578: '-', 579: '+', 580: '+', 581: '-', 582: '', 583: '-', 584: '-', 585: '-', 586: '-', 587: '+', 588: '-', 589: '', 590: '+', 591: '-', 592: '+', 593: '-', 594: '', 595: '-', 596: '+', 597: '-', 598: '+', 599: '+', 600: '+', 601: '+', 602: '', 603: '+', 604: '+', 605: '', 606: '+', 607: '+', 608: '', 609: '-', 610: '', 611: '-', 612: '', 613: '+', 614: '+', 615: '+', 616: '', 617: '-', 618: '+', 619: '+', 620: '+', 621: '', 622: '+', 623: '', 624: '-', 625: '', 626: '+', 627: '+', 628: '+', 629: '+', 630: '', 631: '+', 632: '', 633: '+', 634: '-', 635: '-', 636: '+', 637: '+', 638: '-', 639: '+', 640: '-', 641: '', 642: '', 643: '-', 644: '+', 645: '-', 646: '-', 647: '+', 648: '-', 649: '-', 650: '+', 651: '+', 652: '-', 653: '+', 654: '+', 655: '+', 656: '', 657: '-', 658: '', 659: '', 660: '-', 661: '', 662: '+', 663: '-', 664: '', 665: '', 666: '', 667: '+', 668: '+', 669: '+', 670: '', 671: '-', 672: '-', 673: '+', 674: '+', 675: '-', 676: '+', 677: '+', 678: '', 679: '+', 680: '-', 681: '+', 682: '+', 683: '', 684: '-', 685: '', 686: '-', 687: '-', 688: '-', 689: '+', 690: '', 691: '-', 692: '', 693: '+', 694: '+', 695: '-', 696: '+', 697: '-', 698: '-', 699: '+', 700: '+', 701: '', 702: '-', 703: '-', 704: '-', 705: '+', 706: '+', 707: '+', 708: '+', 709: '+', 710: '', 711: '', 712: '', 713: '-', 714: '+', 715: '-', 716: '-', 717: '', 718: '-', 719: '', 720: '-', 721: '-', 722: '-', 723: '-', 724: '-', 725: '+', 726: '+', 727: '-', 728: '', 729: '-', 730: '-', 731: '-', 732: '-', 733: '+', 734: '+', 735: '-', 736: '', 737: '+', 738: '-', 739: '+', 740: '+', 741: '+', 742: '+', 743: '-', 744: '+', 745: '', 746: '+', 747: '+', 748: '+', 749: '+', 750: '+', 751: '-', 752: '', 753: '+', 754: '+', 755: '+', 756: '-', 757: '', 758: '+', 759: '-', 760: '+', 761: '-', 762: '-', 763: '', 764: '-', 765: '+', 766: '-', 767: '', 768: '+', 769: '-', 770: '+', 771: '-', 772: '-', 773: '', 774: '-', 775: '+', 776: '-', 777: '', 778: '', 779: '', 780: '+', 781: '-', 782: '-', 783: '-', 784: '-', 785: '-', 786: '+', 787: '-', 788: '', 789: '-', 790: '', 791: '+', 792: '+', 793: '+', 794: '-', 795: '+', 796: '', 797: '+', 798: '+', 799: '+', 800: '+', 801: '-', 802: '-', 803: '+', 804: '+', 805: '+', 806: '+', 807: '', 808: '+', 809: '+', 810: '+', 811: '+', 812: '-', 813: '-', 814: '', 815: '', 816: '+', 817: '-', 818: '+', 819: '+', 820: '', 821: '', 822: '-', 823: '-', 824: '', 825: '-', 826: '-', 827: '', 828: '', 829: '', 830: '-', 831: '-', 832: '', 833: '-', 834: '-', 835: '-', 836: '+', 837: '+', 838: '+', 839: '-', 840: '+', 841: '+', 842: '-', 843: '+', 844: '', 845: '+', 846: '+', 847: '-', 848: '+', 849: '', 850: '-', 851: '+', 852: '-', 853: '-', 854: '-', 855: '-', 856: '-', 857: '', 858: '+', 859: '-', 860: '-', 861: '-', 862: '', 863: '+', 864: '+', 865: '+', 866: '-', 867: '-', 868: '+', 869: '', 870: '', 871: '+', 872: '-', 873: '-', 874: '', 875: '', 876: '', 877: '', 878: '-', 879: '+', 880: '+', 881: '-', 882: '', 883: '-', 884: '+', 885: '', 886: '-', 887: '+', 888: '', 889: '+', 890: '-', 891: '', 892: '-', 893: '', 894: '+', 895: '+', 896: '', 897: '+', 898: '', 899: '', 900: '', 901: '', 902: '+', 903: '+', 904: '+', 905: '+', 906: '-', 907: '+', 908: '+', 909: '', 910: '+', 911: '+', 912: '+', 913: '-', 914: '+', 915: '+', 916: '-', 917: '', 918: '+', 919: '-', 920: '+', 921: '', 922: '+', 923: '-', 924: '-', 925: '', 926: '-', 927: '+', 928: '', 929: '', 930: '+', 931: '-', 932: '-', 933: '', 934: '', 935: '+', 936: '-', 937: '-', 938: '+', 939: '', 940: '+', 941: '+', 942: '', 943: '+', 944: '+', 945: '+', 946: '+', 947: '+', 948: '+', 949: '-', 950: '-', 951: '', 952: '+', 953: '-', 954: '', 955: '-', 956: '+', 957: '+', 958: '-', 959: '+', 960: '-', 961: '+', 962: '+', 963: '+', 964: '-', 965: '-', 966: '-', 967: '', 968: '+', 969: '-', 970: '+', 971: '-', 972: '', 973: '-', 974: '-', 975: '-', 976: '-', 977: '-', 978: '-', 979: '', 980: '-', 981: '-', 982: '-', 983: '', 984: '-', 985: '-', 986: '-', 987: '-', 988: '-', 989: '-', 990: '-', 991: '-', 992: '-', 993: '', 994: '', 995: '', 996: '', 997: '-', 998: '+', 999: '+', 1000: '-', 1001: '-', 1002: '', 1003: '+', 1004: '-', 1005: '-', 1006: '', 1007: '+', 1008: '+', 1009: '-', 1010: '+'},
                                    'read_start': {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0, 101: 0, 102: 0, 103: 0, 104: 0, 105: 0, 106: 0, 107: 0, 108: 0, 109: 0, 110: 0, 111: 0, 112: 0, 113: 0, 114: 0, 115: 0, 116: 0, 117: 0, 118: 0, 119: 0, 120: 0, 121: 0, 122: 0, 123: 0, 124: 0, 125: 0, 126: 0, 127: 0, 128: 0, 129: 0, 130: 0, 131: 0, 132: 0, 133: 0, 134: 0, 135: 0, 136: 0, 137: 0, 138: 0, 139: 0, 140: 0, 141: 0, 142: 0, 143: 0, 144: 0, 145: 0, 146: 0, 147: 0, 148: 0, 149: 0, 150: 0, 151: 0, 152: 0, 153: 0, 154: 0, 155: 0, 156: 0, 157: 0, 158: 0, 159: 0, 160: 0, 161: 0, 162: 0, 163: 0, 164: 0, 165: 0, 166: 0, 167: 0, 168: 0, 169: 0, 170: 0, 171: 0, 172: 0, 173: 0, 174: 0, 175: 0, 176: 0, 177: 0, 178: 0, 179: 0, 180: 0, 181: 0, 182: 0, 183: 0, 184: 0, 185: 0, 186: 0, 187: 0, 188: 0, 189: 0, 190: 0, 191: 0, 192: 0, 193: 0, 194: 0, 195: 0, 196: 0, 197: 0, 198: 0, 199: 0, 200: 0, 201: 0, 202: 0, 203: 0, 204: 0, 205: 0, 206: 0, 207: 0, 208: 0, 209: 0, 210: 0, 211: 0, 212: 0, 213: 0, 214: 0, 215: 0, 216: 0, 217: 0, 218: 0, 219: 0, 220: 0, 221: 0, 222: 0, 223: 0, 224: 0, 225: 0, 226: 0, 227: 0, 228: 0, 229: 0, 230: 0, 231: 0, 232: 0, 233: 0, 234: 0, 235: 0, 236: 0, 237: 0, 238: 0, 239: 0, 240: 0, 241: 0, 242: 0, 243: 0, 244: 0, 245: 0, 246: 0, 247: 0, 248: 0, 249: 0, 250: 0, 251: 0, 252: 0, 253: 0, 254: 0, 255: 0, 256: 0, 257: 0, 258: 0, 259: 0, 260: 0, 261: 0, 262: 0, 263: 0, 264: 0, 265: 0, 266: 0, 267: 0, 268: 0, 269: 0, 270: 0, 271: 0, 272: 0, 273: 0, 274: 0, 275: 0, 276: 0, 277: 0, 278: 0, 279: 0, 280: 0, 281: 0, 282: 0, 283: 0, 284: 0, 285: 0, 286: 0, 287: 0, 288: 0, 289: 0, 290: 0, 291: 0, 292: 0, 293: 0, 294: 0, 295: 0, 296: 0, 297: 0, 298: 0, 299: 0, 300: 0, 301: 0, 302: 0, 303: 0, 304: 0, 305: 0, 306: 0, 307: 0, 308: 0, 309: 0, 310: 0, 311: 0, 312: 0, 313: 0, 314: 0, 315: 0, 316: 0, 317: 0, 318: 0, 319: 0, 320: 0, 321: 0, 322: 0, 323: 0, 324: 0, 325: 0, 326: 0, 327: 0, 328: 0, 329: 0, 330: 0, 331: 0, 332: 0, 333: 0, 334: 0, 335: 0, 336: 0, 337: 0, 338: 0, 339: 0, 340: 0, 341: 0, 342: 0, 343: 0, 344: 0, 345: 0, 346: 0, 347: 0, 348: 0, 349: 0, 350: 0, 351: 0, 352: 0, 353: 0, 354: 0, 355: 0, 356: 0, 357: 0, 358: 0, 359: 0, 360: 0, 361: 0, 362: 0, 363: 0, 364: 0, 365: 0, 366: 0, 367: 0, 368: 0, 369: 0, 370: 0, 371: 0, 372: 0, 373: 0, 374: 0, 375: 0, 376: 0, 377: 0, 378: 0, 379: 0, 380: 0, 381: 0, 382: 0, 383: 0, 384: 0, 385: 0, 386: 0, 387: 0, 388: 0, 389: 0, 390: 0, 391: 0, 392: 0, 393: 0, 394: 0, 395: 0, 396: 0, 397: 0, 398: 0, 399: 0, 400: 0, 401: 0, 402: 0, 403: 0, 404: 0, 405: 0, 406: 0, 407: 0, 408: 0, 409: 0, 410: 0, 411: 0, 412: 0, 413: 0, 414: 0, 415: 0, 416: 0, 417: 0, 418: 0, 419: 0, 420: 0, 421: 0, 422: 0, 423: 0, 424: 0, 425: 0, 426: 0, 427: 0, 428: 0, 429: 0, 430: 0, 431: 0, 432: 0, 433: 0, 434: 0, 435: 0, 436: 0, 437: 0, 438: 0, 439: 0, 440: 0, 441: 0, 442: 0, 443: 0, 444: 0, 445: 0, 446: 0, 447: 0, 448: 0, 449: 0, 450: 0, 451: 0, 452: 0, 453: 0, 454: 0, 455: 0, 456: 0, 457: 0, 458: 0, 459: 0, 460: 0, 461: 0, 462: 0, 463: 0, 464: 0, 465: 0, 466: 0, 467: 0, 468: 0, 469: 0, 470: 0, 471: 0, 472: 0, 473: 0, 474: 0, 475: 0, 476: 0, 477: 0, 478: 0, 479: 0, 480: 0, 481: 0, 482: 0, 483: 0, 484: 0, 485: 0, 486: 0, 487: 0, 488: 0, 489: 0, 490: 0, 491: 0, 492: 0, 493: 0, 494: 0, 495: 0, 496: 0, 497: 0, 498: 0, 499: 0, 500: 0, 501: 0, 502: 0, 503: 0, 504: 0, 505: 0, 506: 0, 507: 0, 508: 0, 509: 0, 510: 0, 511: 0, 512: 0, 513: 0, 514: 0, 515: 0, 516: 0, 517: 0, 518: 0, 519: 0, 520: 0, 521: 0, 522: 0, 523: 0, 524: 0, 525: 0, 526: 0, 527: 0, 528: 0, 529: 0, 530: 0, 531: 0, 532: 0, 533: 0, 534: 0, 535: 0, 536: 0, 537: 0, 538: 0, 539: 0, 540: 0, 541: 0, 542: 0, 543: 0, 544: 0, 545: 0, 546: 0, 547: 0, 548: 0, 549: 0, 550: 0, 551: 0, 552: 0, 553: 0, 554: 0, 555: 0, 556: 0, 557: 0, 558: 0, 559: 0, 560: 0, 561: 0, 562: 0, 563: 0, 564: 0, 565: 0, 566: 0, 567: 0, 568: 0, 569: 0, 570: 0, 571: 0, 572: 0, 573: 0, 574: 0, 575: 0, 576: 0, 577: 0, 578: 0, 579: 0, 580: 0, 581: 0, 582: 0, 583: 0, 584: 0, 585: 0, 586: 0, 587: 0, 588: 0, 589: 0, 590: 0, 591: 0, 592: 0, 593: 0, 594: 0, 595: 0, 596: 0, 597: 0, 598: 0, 599: 0, 600: 0, 601: 0, 602: 0, 603: 0, 604: 0, 605: 0, 606: 0, 607: 0, 608: 0, 609: 0, 610: 0, 611: 0, 612: 0, 613: 0, 614: 0, 615: 0, 616: 0, 617: 0, 618: 0, 619: 0, 620: 0, 621: 0, 622: 0, 623: 0, 624: 0, 625: 0, 626: 0, 627: 0, 628: 0, 629: 0, 630: 0, 631: 0, 632: 0, 633: 0, 634: 0, 635: 0, 636: 0, 637: 0, 638: 0, 639: 0, 640: 0, 641: 0, 642: 0, 643: 0, 644: 0, 645: 0, 646: 0, 647: 0, 648: 0, 649: 0, 650: 0, 651: 0, 652: 0, 653: 0, 654: 0, 655: 0, 656: 0, 657: 0, 658: 0, 659: 0, 660: 0, 661: 0, 662: 0, 663: 0, 664: 0, 665: 0, 666: 0, 667: 0, 668: 0, 669: 0, 670: 0, 671: 0, 672: 0, 673: 0, 674: 0, 675: 0, 676: 0, 677: 0, 678: 0, 679: 0, 680: 0, 681: 0, 682: 0, 683: 0, 684: 0, 685: 0, 686: 0, 687: 0, 688: 0, 689: 0, 690: 0, 691: 0, 692: 0, 693: 0, 694: 0, 695: 0, 696: 0, 697: 0, 698: 0, 699: 0, 700: 0, 701: 0, 702: 0, 703: 0, 704: 0, 705: 0, 706: 0, 707: 0, 708: 0, 709: 0, 710: 0, 711: 0, 712: 0, 713: 0, 714: 0, 715: 0, 716: 0, 717: 0, 718: 0, 719: 0, 720: 0, 721: 0, 722: 0, 723: 0, 724: 0, 725: 0, 726: 0, 727: 0, 728: 0, 729: 0, 730: 0, 731: 0, 732: 0, 733: 0, 734: 0, 735: 0, 736: 0, 737: 0, 738: 0, 739: 0, 740: 0, 741: 0, 742: 0, 743: 0, 744: 0, 745: 0, 746: 0, 747: 0, 748: 0, 749: 0, 750: 0, 751: 0, 752: 0, 753: 0, 754: 0, 755: 0, 756: 0, 757: 0, 758: 0, 759: 0, 760: 0, 761: 0, 762: 0, 763: 0, 764: 0, 765: 0, 766: 0, 767: 0, 768: 0, 769: 0, 770: 0, 771: 0, 772: 0, 773: 0, 774: 0, 775: 0, 776: 0, 777: 0, 778: 0, 779: 0, 780: 0, 781: 0, 782: 0, 783: 0, 784: 0, 785: 0, 786: 0, 787: 0, 788: 0, 789: 0, 790: 0, 791: 0, 792: 0, 793: 0, 794: 0, 795: 0, 796: 0, 797: 0, 798: 0, 799: 0, 800: 0, 801: 0, 802: 0, 803: 0, 804: 0, 805: 0, 806: 0, 807: 0, 808: 0, 809: 0, 810: 0, 811: 0, 812: 0, 813: 0, 814: 0, 815: 0, 816: 0, 817: 0, 818: 0, 819: 0, 820: 0, 821: 0, 822: 0, 823: 0, 824: 0, 825: 0, 826: 0, 827: 0, 828: 0, 829: 0, 830: 0, 831: 0, 832: 0, 833: 0, 834: 0, 835: 0, 836: 0, 837: 0, 838: 0, 839: 0, 840: 0, 841: 0, 842: 0, 843: 0, 844: 0, 845: 0, 846: 0, 847: 0, 848: 0, 849: 0, 850: 0, 851: 0, 852: 0, 853: 0, 854: 0, 855: 0, 856: 0, 857: 0, 858: 0, 859: 0, 860: 0, 861: 0, 862: 0, 863: 0, 864: 0, 865: 0, 866: 0, 867: 0, 868: 0, 869: 0, 870: 0, 871: 0, 872: 0, 873: 0, 874: 0, 875: 0, 876: 0, 877: 0, 878: 0, 879: 0, 880: 0, 881: 0, 882: 0, 883: 0, 884: 0, 885: 0, 886: 0, 887: 0, 888: 0, 889: 0, 890: 0, 891: 0, 892: 0, 893: 0, 894: 0, 895: 0, 896: 0, 897: 0, 898: 0, 899: 0, 900: 0, 901: 0, 902: 0, 903: 0, 904: 0, 905: 0, 906: 0, 907: 0, 908: 0, 909: 0, 910: 0, 911: 0, 912: 0, 913: 0, 914: 0, 915: 0, 916: 0, 917: 0, 918: 0, 919: 0, 920: 0, 921: 0, 922: 0, 923: 0, 924: 0, 925: 0, 926: 0, 927: 0, 928: 0, 929: 0, 930: 0, 931: 0, 932: 0, 933: 0, 934: 0, 935: 0, 936: 0, 937: 0, 938: 0, 939: 0, 940: 0, 941: 0, 942: 0, 943: 0, 944: 0, 945: 0, 946: 0, 947: 0, 948: 0, 949: 0, 950: 0, 951: 0, 952: 0, 953: 0, 954: 0, 955: 0, 956: 0, 957: 0, 958: 0, 959: 0, 960: 0, 961: 0, 962: 0, 963: 0, 964: 0, 965: 0, 966: 0, 967: 0, 968: 0, 969: 0, 970: 0, 971: 0, 972: 0, 973: 0, 974: 0, 975: 0, 976: 0, 977: 0, 978: 0, 979: 0, 980: 0, 981: 0, 982: 0, 983: 0, 984: 0, 985: 0, 986: 0, 987: 0, 988: 0, 989: 0, 990: 0, 991: 0, 992: 0, 993: 0, 994: 0, 995: 0, 996: 0, 997: 0, 998: 0, 999: 0, 1000: 0, 1001: 0, 1002: 0, 1003: 0, 1004: 0, 1005: 0, 1006: 0, 1007: 0, 1008: 0, 1009: 0, 1010: 0},
                                    'read_end': {0: 9150, 1: 9150, 2: 9150, 3: 9150, 4: 9150, 5: 9150, 6: 9150, 7: 9150, 8: 8461, 9: 8461, 10: 8461, 11: 24573, 12: 24573, 13: 24573, 14: 24573, 15: 24573, 16: 24573, 17: 24573, 18: 24573, 19: 24573, 20: 24573, 21: 16833, 22: 16833, 23: 16833, 24: 16833, 25: 16833, 26: 16833, 27: 20179, 28: 20179, 29: 20179, 30: 20179, 31: 20179, 32: 20179, 33: 20179, 34: 20179, 35: 20179, 36: 20179, 37: 20179, 38: 8816, 39: 8816, 40: 13313, 41: 13313, 42: 13313, 43: 13313, 44: 13313, 45: 13313, 46: 11064, 47: 11064, 48: 5019, 49: 5019, 50: 5019, 51: 1607, 52: 4493, 53: 8177, 54: 8177, 55: 8177, 56: 8177, 57: 3768, 58: 16501, 59: 16501, 60: 16501, 61: 16501, 62: 16501, 63: 16501, 64: 16501, 65: 16501, 66: 16501, 67: 2941, 68: 2941, 69: 2941, 70: 5653, 71: 5653, 72: 9319, 73: 9319, 74: 34620, 75: 34620, 76: 34620, 77: 34620, 78: 34620, 79: 34620, 80: 34620, 81: 34620, 82: 34620, 83: 34620, 84: 34620, 85: 9522, 86: 9522, 87: 9522, 88: 9522, 89: 9522, 90: 10411, 91: 10411, 92: 8230, 93: 8230, 94: 8230, 95: 9365, 96: 9365, 97: 9365, 98: 9365, 99: 15930, 100: 15930, 101: 15930, 102: 15930, 103: 15930, 104: 15930, 105: 15930, 106: 15930, 107: 15930, 108: 15930, 109: 15314, 110: 15314, 111: 15314, 112: 15314, 113: 15314, 114: 15314, 115: 15314, 116: 16386, 117: 16386, 118: 17633, 119: 17633, 120: 17633, 121: 17633, 122: 17633, 123: 17633, 124: 25544, 125: 25544, 126: 25544, 127: 25544, 128: 25544, 129: 25544, 130: 25544, 131: 25544, 132: 25544, 133: 5710, 134: 5710, 135: 9038, 136: 9038, 137: 9038, 138: 17534, 139: 17534, 140: 17534, 141: 17534, 142: 17534, 143: 17534, 144: 14944, 145: 14944, 146: 14944, 147: 14944, 148: 14959, 149: 14959, 150: 14959, 151: 14959, 152: 14959, 153: 14959, 154: 14959, 155: 13733, 156: 13733, 157: 13733, 158: 13733, 159: 13733, 160: 13733, 161: 13733, 162: 13733, 163: 13733, 164: 10987, 165: 10987, 166: 10987, 167: 10987, 168: 1407, 169: 8679, 170: 8679, 171: 8679, 172: 8679, 173: 8679, 174: 8679, 175: 8679, 176: 14354, 177: 14354, 178: 14354, 179: 14354, 180: 14354, 181: 14354, 182: 14354, 183: 15720, 184: 15720, 185: 15720, 186: 15720, 187: 15720, 188: 15720, 189: 15720, 190: 15720, 191: 19396, 192: 19396, 193: 19396, 194: 19396, 195: 19396, 196: 19396, 197: 19396, 198: 19396, 199: 33316, 200: 33316, 201: 33316, 202: 33316, 203: 33316, 204: 33316, 205: 33316, 206: 33316, 207: 33316, 208: 33316, 209: 33316, 210: 13857, 211: 13857, 212: 13857, 213: 13857, 214: 13857, 215: 7981, 216: 7981, 217: 7981, 218: 7981, 219: 7981, 220: 18529, 221: 18529, 222: 18529, 223: 18529, 224: 18529, 225: 18529, 226: 18529, 227: 18529, 228: 18529, 229: 18529, 230: 4174, 231: 27197, 232: 27197, 233: 27197, 234: 27197, 235: 13361, 236: 13361, 237: 13361, 238: 13361, 239: 13361, 240: 2310, 241: 14531, 242: 14531, 243: 14531, 244: 14531, 245: 14531, 246: 14531, 247: 14531, 248: 20463, 249: 20463, 250: 20463, 251: 20463, 252: 20463, 253: 20463, 254: 20463, 255: 20463, 256: 5045, 257: 5045, 258: 5045, 259: 8994, 260: 8994, 261: 8994, 262: 8994, 263: 20695, 264: 20695, 265: 20695, 266: 20695, 267: 20695, 268: 20695, 269: 20695, 270: 3179, 271: 3179, 272: 3179, 273: 17932, 274: 17932, 275: 17932, 276: 17932, 277: 17932, 278: 17932, 279: 17932, 280: 20975, 281: 20975, 282: 20975, 283: 20975, 284: 20975, 285: 20975, 286: 9741, 287: 9741, 288: 27567, 289: 27567, 290: 27567, 291: 27567, 292: 27567, 293: 27567, 294: 27567, 295: 27567, 296: 27567, 297: 4177, 298: 23833, 299: 23833, 300: 23833, 301: 23833, 302: 23833, 303: 23833, 304: 23833, 305: 26470, 306: 26470, 307: 26470, 308: 26470, 309: 26470, 310: 26470, 311: 26470, 312: 26470, 313: 8497, 314: 8497, 315: 8497, 316: 17978, 317: 17978, 318: 17978, 319: 17978, 320: 17978, 321: 32503, 322: 32503, 323: 32503, 324: 32503, 325: 32503, 326: 32503, 327: 32503, 328: 32503, 329: 32503, 330: 32503, 331: 32503, 332: 32503, 333: 1445, 334: 25695, 335: 25695, 336: 25695, 337: 25695, 338: 25695, 339: 25695, 340: 25695, 341: 25695, 342: 25695, 343: 25695, 344: 25695, 345: 25695, 346: 9788, 347: 9788, 348: 9788, 349: 18777, 350: 18777, 351: 18777, 352: 18777, 353: 18777, 354: 18777, 355: 18777, 356: 7884, 357: 7884, 358: 7884, 359: 7884, 360: 12418, 361: 12418, 362: 12418, 363: 12418, 364: 12418, 365: 12418, 366: 4864, 367: 20893, 368: 20893, 369: 20893, 370: 20893, 371: 20893, 372: 20893, 373: 20893, 374: 20893, 375: 20893, 376: 14405, 377: 14405, 378: 14405, 379: 14405, 380: 14405, 381: 14405, 382: 30887, 383: 30887, 384: 30887, 385: 30887, 386: 30887, 387: 30887, 388: 30887, 389: 30887, 390: 30887, 391: 30887, 392: 30887, 393: 30887, 394: 30887, 395: 30887, 396: 30887, 397: 30887, 398: 30887, 399: 508, 400: 11918, 401: 11918, 402: 11918, 403: 32838, 404: 32838, 405: 32838, 406: 32838, 407: 32838, 408: 32838, 409: 32838, 410: 32838, 411: 32838, 412: 21062, 413: 21062, 414: 21062, 415: 21062, 416: 21062, 417: 21062, 418: 21062, 419: 21062, 420: 6674, 421: 6674, 422: 6674, 423: 6674, 424: 6674, 425: 4694, 426: 4694, 427: 4694, 428: 13109, 429: 13109, 430: 13109, 431: 13109, 432: 13109, 433: 13109, 434: 13109, 435: 1557, 436: 1557, 437: 12350, 438: 12350, 439: 12350, 440: 12350, 441: 18592, 442: 18592, 443: 18592, 444: 18592, 445: 18592, 446: 18592, 447: 18592, 448: 18592, 449: 18592, 450: 22695, 451: 22695, 452: 22695, 453: 22695, 454: 22695, 455: 22695, 456: 22695, 457: 22695, 458: 22695, 459: 22695, 460: 2065, 461: 9110, 462: 9110, 463: 9110, 464: 7477, 465: 7477, 466: 7477, 467: 7477, 468: 15619, 469: 15619, 470: 15619, 471: 15619, 472: 15619, 473: 15619, 474: 15619, 475: 15619, 476: 32049, 477: 32049, 478: 32049, 479: 32049, 480: 32049, 481: 14236, 482: 14236, 483: 14236, 484: 14236, 485: 14236, 486: 14236, 487: 14236, 488: 14236, 489: 6753, 490: 6753, 491: 7148, 492: 7148, 493: 7148, 494: 14824, 495: 14824, 496: 14824, 497: 14824, 498: 14824, 499: 14824, 500: 7235, 501: 3161, 502: 6926, 503: 12855, 504: 12855, 505: 12855, 506: 12855, 507: 12855, 508: 6887, 509: 6887, 510: 6887, 511: 6887, 512: 16636, 513: 16636, 514: 16636, 515: 16636, 516: 16636, 517: 16636, 518: 26204, 519: 26204, 520: 26204, 521: 26204, 522: 6964, 523: 15086, 524: 15086, 525: 15086, 526: 15086, 527: 15086, 528: 15086, 529: 15086, 530: 4134, 531: 4134, 532: 4134, 533: 4134, 534: 11302, 535: 11302, 536: 11302, 537: 11302, 538: 11302, 539: 7173, 540: 7173, 541: 7173, 542: 8028, 543: 8028, 544: 8028, 545: 10248, 546: 10248, 547: 10248, 548: 10248, 549: 10248, 550: 10248, 551: 10248, 552: 10248, 553: 4499, 554: 4499, 555: 1810, 556: 1810, 557: 7249, 558: 7249, 559: 7249, 560: 7249, 561: 5590, 562: 5590, 563: 13371, 564: 13371, 565: 13371, 566: 13371, 567: 13371, 568: 13371, 569: 2522, 570: 5749, 571: 5749, 572: 5749, 573: 19358, 574: 19358, 575: 19358, 576: 19358, 577: 19358, 578: 19358, 579: 19358, 580: 19358, 581: 19358, 582: 16654, 583: 16654, 584: 16654, 585: 16654, 586: 16654, 587: 16654, 588: 16654, 589: 11815, 590: 11815, 591: 11815, 592: 11815, 593: 11815, 594: 22103, 595: 22103, 596: 22103, 597: 22103, 598: 22103, 599: 22103, 600: 22103, 601: 22103, 602: 2546, 603: 2546, 604: 2546, 605: 9802, 606: 9802, 607: 9802, 608: 6947, 609: 6947, 610: 5988, 611: 5988, 612: 14583, 613: 14583, 614: 14583, 615: 14583, 616: 16223, 617: 16223, 618: 16223, 619: 16223, 620: 16223, 621: 7811, 622: 7811, 623: 11846, 624: 11846, 625: 17351, 626: 17351, 627: 17351, 628: 17351, 629: 17351, 630: 4962, 631: 4962, 632: 18363, 633: 18363, 634: 18363, 635: 18363, 636: 18363, 637: 18363, 638: 18363, 639: 18363, 640: 18363, 641: 5058, 642: 24065, 643: 24065, 644: 24065, 645: 24065, 646: 24065, 647: 24065, 648: 24065, 649: 24065, 650: 24065, 651: 24065, 652: 24065, 653: 24065, 654: 24065, 655: 24065, 656: 12920, 657: 12920, 658: 8020, 659: 6059, 660: 6059, 661: 7347, 662: 7347, 663: 7347, 664: 316, 665: 6651, 666: 35228, 667: 35228, 668: 35228, 669: 35228, 670: 16069, 671: 16069, 672: 16069, 673: 16069, 674: 16069, 675: 16069, 676: 16069, 677: 16069, 678: 20286, 679: 20286, 680: 20286, 681: 20286, 682: 20286, 683: 12756, 684: 12756, 685: 16222, 686: 16222, 687: 16222, 688: 16222, 689: 16222, 690: 10300, 691: 10300, 692: 19242, 693: 19242, 694: 19242, 695: 19242, 696: 19242, 697: 19242, 698: 19242, 699: 19242, 700: 19242, 701: 20648, 702: 20648, 703: 20648, 704: 20648, 705: 20648, 706: 20648, 707: 20648, 708: 20648, 709: 20648, 710: 3808, 711: 2591, 712: 15199, 713: 15199, 714: 15199, 715: 15199, 716: 15199, 717: 27498, 718: 27498, 719: 18975, 720: 18975, 721: 18975, 722: 18975, 723: 18975, 724: 18975, 725: 18975, 726: 18975, 727: 18975, 728: 22181, 729: 22181, 730: 22181, 731: 22181, 732: 22181, 733: 22181, 734: 22181, 735: 22181, 736: 14154, 737: 14154, 738: 14154, 739: 14154, 740: 14154, 741: 14154, 742: 14154, 743: 14154, 744: 14154, 745: 20042, 746: 20042, 747: 20042, 748: 20042, 749: 20042, 750: 20042, 751: 20042, 752: 11475, 753: 11475, 754: 11475, 755: 11475, 756: 11475, 757: 8804, 758: 8804, 759: 8804, 760: 8804, 761: 8804, 762: 8804, 763: 10674, 764: 10674, 765: 10674, 766: 10674, 767: 30974, 768: 30974, 769: 30974, 770: 30974, 771: 30974, 772: 30974, 773: 14920, 774: 14920, 775: 14920, 776: 14920, 777: 1047, 778: 1243, 779: 19962, 780: 19962, 781: 19962, 782: 19962, 783: 19962, 784: 19962, 785: 19962, 786: 19962, 787: 19962, 788: 1921, 789: 1921, 790: 16704, 791: 16704, 792: 16704, 793: 16704, 794: 16704, 795: 16704, 796: 34897, 797: 34897, 798: 34897, 799: 34897, 800: 34897, 801: 34897, 802: 34897, 803: 34897, 804: 34897, 805: 34897, 806: 34897, 807: 20868, 808: 20868, 809: 20868, 810: 20868, 811: 20868, 812: 20868, 813: 20868, 814: 2252, 815: 8646, 816: 8646, 817: 8646, 818: 8646, 819: 8646, 820: 1027, 821: 8131, 822: 8131, 823: 8131, 824: 4869, 825: 4869, 826: 4869, 827: 7762, 828: 646, 829: 5216, 830: 5216, 831: 5216, 832: 20403, 833: 20403, 834: 20403, 835: 20403, 836: 20403, 837: 20403, 838: 20403, 839: 20403, 840: 20403, 841: 20403, 842: 20403, 843: 20403, 844: 12096, 845: 12096, 846: 12096, 847: 12096, 848: 12096, 849: 9665, 850: 9665, 851: 9665, 852: 9665, 853: 9665, 854: 9665, 855: 9665, 856: 9665, 857: 19900, 858: 19900, 859: 19900, 860: 19900, 861: 19900, 862: 17916, 863: 17916, 864: 17916, 865: 17916, 866: 17916, 867: 17916, 868: 17916, 869: 2778, 870: 12577, 871: 12577, 872: 12577, 873: 12577, 874: 2924, 875: 5950, 876: 998, 877: 17387, 878: 17387, 879: 17387, 880: 17387, 881: 17387, 882: 10339, 883: 10339, 884: 10339, 885: 3638, 886: 3638, 887: 3638, 888: 8378, 889: 8378, 890: 8378, 891: 6579, 892: 6579, 893: 10475, 894: 10475, 895: 10475, 896: 11821, 897: 11821, 898: 8769, 899: 1036, 900: 2694, 901: 16175, 902: 16175, 903: 16175, 904: 16175, 905: 16175, 906: 16175, 907: 16175, 908: 16175, 909: 20427, 910: 20427, 911: 20427, 912: 20427, 913: 20427, 914: 20427, 915: 20427, 916: 20427, 917: 7712, 918: 7712, 919: 7712, 920: 7712, 921: 17637, 922: 17637, 923: 17637, 924: 17637, 925: 12062, 926: 12062, 927: 12062, 928: 4742, 929: 11139, 930: 11139, 931: 11139, 932: 11139, 933: 4940, 934: 11724, 935: 11724, 936: 11724, 937: 11724, 938: 11724, 939: 4577, 940: 4577, 941: 4577, 942: 17504, 943: 17504, 944: 17504, 945: 17504, 946: 17504, 947: 17504, 948: 17504, 949: 17504, 950: 17504, 951: 5887, 952: 5887, 953: 5887, 954: 25238, 955: 25238, 956: 25238, 957: 25238, 958: 25238, 959: 25238, 960: 25238, 961: 25238, 962: 25238, 963: 25238, 964: 25238, 965: 25238, 966: 25238, 967: 16036, 968: 16036, 969: 16036, 970: 16036, 971: 16036, 972: 11112, 973: 11112, 974: 11112, 975: 11112, 976: 11112, 977: 11112, 978: 11112, 979: 16838, 980: 16838, 981: 16838, 982: 16838, 983: 20960, 984: 20960, 985: 20960, 986: 20960, 987: 20960, 988: 20960, 989: 20960, 990: 20960, 991: 20960, 992: 20960, 993: 778, 994: 2879, 995: 3567, 996: 10885, 997: 10885, 998: 10885, 999: 10885, 1000: 10885, 1001: 10885, 1002: 10008, 1003: 10008, 1004: 10008, 1005: 10008, 1006: 18409, 1007: 18409, 1008: 18409, 1009: 18409, 1010: 18409}} ) )
    mappings = pd.concat(mappings, ignore_index=True).drop_duplicates()
    cov_probs = pd.DataFrame( {'length': {0: 1322, 1: 2310, 2: 3423, 3: 4747, 4: 6352, 5: 8342, 6: 10885, 7: 14391, 8: 19515},
                               'mu': {0: 23.584247754696893, 1: 20.504403471788635, 2: 17.50634409745549, 3: 14.479950280985337, 4: 11.469365157471334, 5: 8.499501941440332, 6: 5.644689191431619, 7: 2.9181378061105048, 8: 0.6972656560039101},
                               'sigma': {0: 10.77667641634545, 1: 9.626709919730942, 2: 8.528159984470921, 3: 7.409683003018617, 4: 6.278824836158851, 5: 5.134680019120394, 6: 3.951599912388506, 7: 2.706556531266621, 8: 1.6980926441778164}} )
    covered_regions = mappings.groupby(['t_id']).agg({'t_start':'min','t_end':'max'}).reset_index().rename(columns={'t_id':'con_id','t_start':'from','t_end':'to'})
    # Run and validate in steps
    pdf = None
    pot_breaks = GetBrokenMappings(mappings, covered_regions, max_dist_contig_end, min_length_contig_break, pdf)
    pot_breaks = pot_breaks[np.isin(pot_breaks['contig_id'], [62, 118])].copy()
    break_points, bp_ext_len = PreparePotBreakPoints(pot_breaks, max_break_point_distance, min_mapping_length, min_num_reads)
    test = []
    # str(break_points[break_points['contig_id'] == 118].reset_index(drop=True).to_dict())
    test.append( pd.DataFrame( {'contig_id': {0: 62, 1: 62, 2: 62, 3: 62, 4: 62, 5: 62, 6: 62, 7: 62, 8: 62, 9: 62, 10: 62, 11: 62},
                                'side': {0: 'r', 1: 'r', 2: 'r', 3: 'r', 4: 'r', 5: 'r', 6: 'r', 7: 'r', 8: 'r', 9: 'r', 10: 'r', 11: 'r'},
                                'position': {0: 2533, 1: 2545, 2: 3243, 3: 3402, 4: 3535, 5: 3604, 6: 3648, 7: 3648, 8: 3648, 9: 3651, 10: 3651, 11: 3651},
                                'mapq': {0: 44, 1: 60, 2: 59, 3: 53, 4: 48, 5: 60, 6: 59, 7: 44, 8: 28, 9: 60, 10: 56, 11: 40},
                                'connected': {0: True, 1: True, 2: False, 3: True, 4: True, 5: True, 6: True, 7: False, 8: True, 9: True, 10: False, 11: True},
                                'bindex': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11}} ) )
    test.append( pd.DataFrame( {'contig_id': {0: 118, 1: 118, 2: 118, 3: 118, 4: 118, 5: 118, 6: 118, 7: 118, 8: 118, 9: 118, 10: 118, 11: 118, 12: 118, 13: 118, 14: 118, 15: 118, 16: 118, 17: 118, 18: 118, 19: 118, 20: 118, 21: 118, 22: 118, 23: 118, 24: 118, 25: 118, 26: 118, 27: 118, 28: 118, 29: 118, 30: 118, 31: 118, 32: 118, 33: 118},
                                'side': {0: 'r', 1: 'l', 2: 'r', 3: 'l', 4: 'r', 5: 'r', 6: 'l', 7: 'r', 8: 'l', 9: 'l', 10: 'l', 11: 'r', 12: 'r', 13: 'r', 14: 'r', 15: 'r', 16: 'r', 17: 'r', 18: 'r', 19: 'r', 20: 'r', 21: 'r', 22: 'r', 23: 'r', 24: 'l', 25: 'l', 26: 'l', 27: 'l', 28: 'l', 29: 'l', 30: 'l', 31: 'l', 32: 'l', 33: 'r'},
                                'position': {0: 2493, 1: 2499, 2: 2608, 3: 2705, 4: 3246, 5: 3345, 6: 5459, 7: 5502, 8: 5541, 9: 5551, 10: 5557, 11: 5559, 12: 5559, 13: 5561, 14: 5561, 15: 5561, 16: 5561, 17: 5561, 18: 5561, 19: 5561, 20: 5561, 21: 5561, 22: 5561, 23: 5562, 24: 5564, 25: 5564, 26: 5564, 27: 5566, 28: 5566, 29: 5566, 30: 5566, 31: 5566, 32: 5566, 33: 5566},
                                'mapq': {0: 60, 1: 60, 2: 60, 3: 36, 4: 60, 5: 60, 6: 60, 7: 60, 8: 60, 9: 60, 10: 60, 11: 60, 12: 60, 13: 60, 14: 60, 15: 60, 16: 60, 17: 60, 18: 60, 19: 60, 20: 60, 21: 60, 22: 51, 23: 60, 24: 60, 25: 60, 26: 60, 27: 60, 28: 60, 29: 60, 30: 60, 31: 60, 32: 60, 33: 60},
                                'connected': {0: True, 1: False, 2: True, 3: True, 4: True, 5: True, 6: False, 7: False, 8: True, 9: True, 10: True, 11: True, 12: True, 13: True, 14: True, 15: True, 16: True, 17: True, 18: True, 19: True, 20: True, 21: True, 22: True, 23: True, 24: True, 25: True, 26: True, 27: True, 28: True, 29: True, 30: True, 31: True, 32: True, 33: True},
                                'bindex': {0: 12, 1: 13, 2: 14, 3: 15, 4: 16, 5: 17, 6: 18, 7: 19, 8: 20, 9: 21, 10: 22, 11: 23, 12: 24, 13: 25, 14: 26, 15: 27, 16: 28, 17: 29, 18: 30, 19: 31, 20: 32, 21: 33, 22: 34, 23: 35, 24: 36, 25: 37, 26: 38, 27: 39, 28: 40, 29: 41, 30: 42, 31: 43, 32: 44, 33: 45}} ) )
    test = pd.concat(test, ignore_index=True)
    test = test.merge(break_points, on=list(test.columns), how='outer', indicator=True)
    test = test[test['_merge'] != "both"].copy()
    if len(test):
        print("TestFindBreakPoints failed at PreparePotBreakPoints:")
        print(test)
        return True
#
    break_points = CountBreakSupport(break_points, max_break_point_distance, min_num_reads)
    test = []
    test.append( pd.DataFrame( {'contig_id': {0: 62, 1: 62, 2: 62, 3: 62, 4: 62, 5: 62, 6: 62, 7: 62, 8: 62, 9: 62, 10: 62, 11: 62, 12: 62, 13: 62, 14: 62, 15: 62, 16: 62, 17: 62, 18: 62, 19: 62, 20: 62, 21: 62, 22: 62, 23: 62, 24: 62, 25: 62, 26: 62, 27: 62, 28: 62, 29: 62, 30: 62, 31: 62, 32: 62, 33: 62, 34: 62, 35: 62, 36: 62, 37: 62},
                                'position': {0: 2533, 1: 2533, 2: 2545, 3: 2545, 4: 3243, 5: 3243, 6: 3402, 7: 3402, 8: 3402, 9: 3535, 10: 3535, 11: 3535, 12: 3535, 13: 3535, 14: 3535, 15: 3535, 16: 3535, 17: 3604, 18: 3604, 19: 3604, 20: 3604, 21: 3604, 22: 3604, 23: 3604, 24: 3648, 25: 3648, 26: 3648, 27: 3648, 28: 3648, 29: 3648, 30: 3648, 31: 3651, 32: 3651, 33: 3651, 34: 3651, 35: 3651, 36: 3651, 37: 3651},
                                'mapq': {0: 60, 1: 44, 2: 60, 3: 44, 4: 59, 5: 53, 6: 59, 7: 53, 8: 48, 9: 60, 10: 59, 11: 56, 12: 53, 13: 48, 14: 44, 15: 40, 16: 28, 17: 60, 18: 59, 19: 56, 20: 48, 21: 44, 22: 40, 23: 28, 24: 60, 25: 59, 26: 56, 27: 48, 28: 44, 29: 40, 30: 28, 31: 60, 32: 59, 33: 56, 34: 48, 35: 44, 36: 40, 37: 28},
                                'support': {0: 1, 1: 2, 2: 1, 3: 2, 4: 1, 5: 2, 6: 1, 7: 2, 8: 3, 9: 2, 10: 3, 11: 4, 12: 5, 13: 6, 14: 7, 15: 8, 16: 9, 17: 2, 18: 3, 19: 4, 20: 5, 21: 6, 22: 7, 23: 8, 24: 2, 25: 3, 26: 4, 27: 5, 28: 6, 29: 7, 30: 8, 31: 2, 32: 3, 33: 4, 34: 5, 35: 6, 36: 7, 37: 8},
                                'con_supp': {0: 1, 1: 2, 2: 1, 3: 2, 4: 0, 5: 1, 6: 0, 7: 1, 8: 2, 9: 2, 10: 3, 11: 3, 12: 4, 13: 5, 14: 5, 15: 6, 16: 7, 17: 2, 18: 3, 19: 3, 20: 4, 21: 4, 22: 5, 23: 6, 24: 2, 25: 3, 26: 3, 27: 4, 28: 4, 29: 5, 30: 6, 31: 2, 32: 3, 33: 3, 34: 4, 35: 4, 36: 5, 37: 6},
                                'ifrom': {0: 0, 1: 0, 2: 0, 3: 0, 4: 2, 5: 2, 6: 2, 7: 2, 8: 2, 9: 3, 10: 3, 11: 3, 12: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4, 28: 4, 29: 4, 30: 4, 31: 4, 32: 4, 33: 4, 34: 4, 35: 4, 36: 4, 37: 4},
                                'ito': {0: 1, 1: 1, 2: 1, 3: 1, 4: 3, 5: 3, 6: 4, 7: 4, 8: 4, 9: 11, 10: 11, 11: 11, 12: 11, 13: 11, 14: 11, 15: 11, 16: 11, 17: 11, 18: 11, 19: 11, 20: 11, 21: 11, 22: 11, 23: 11, 24: 11, 25: 11, 26: 11, 27: 11, 28: 11, 29: 11, 30: 11, 31: 11, 32: 11, 33: 11, 34: 11, 35: 11, 36: 11, 37: 11}} ) )
    test.append( pd.DataFrame( {'contig_id': {0: 118, 1: 118, 2: 118, 3: 118, 4: 118, 5: 118, 6: 118, 7: 118, 8: 118, 9: 118, 10: 118, 11: 118, 12: 118, 13: 118, 14: 118, 15: 118, 16: 118, 17: 118, 18: 118, 19: 118, 20: 118, 21: 118, 22: 118, 23: 118, 24: 118, 25: 118, 26: 118, 27: 118},
                                'position': {0: 2493, 1: 2499, 2: 2608, 3: 2608, 4: 2705, 5: 2705, 6: 3246, 7: 3345, 8: 5459, 9: 5459, 10: 5502, 11: 5502, 12: 5541, 13: 5541, 14: 5551, 15: 5551, 16: 5557, 17: 5557, 18: 5559, 19: 5559, 20: 5561, 21: 5561, 22: 5562, 23: 5562, 24: 5564, 25: 5564, 26: 5566, 27: 5566},
                                'mapq': {0: 60, 1: 60, 2: 60, 3: 36, 4: 60, 5: 36, 6: 60, 7: 60, 8: 60, 9: 51, 10: 60, 11: 51, 12: 60, 13: 51, 14: 60, 15: 51, 16: 60, 17: 51, 18: 60, 19: 51, 20: 60, 21: 51, 22: 60, 23: 51, 24: 60, 25: 51, 26: 60, 27: 51},
                                'support': {0: 3, 1: 3, 2: 3, 3: 4, 4: 1, 5: 2, 6: 2, 7: 2, 8: 27, 9: 28, 10: 27, 11: 28, 12: 27, 13: 28, 14: 27, 15: 28, 16: 27, 17: 28, 18: 27, 19: 28, 20: 27, 21: 28, 22: 27, 23: 28, 24: 27, 25: 28, 26: 27, 27: 28},
                                'con_supp': {0: 2, 1: 2, 2: 2, 3: 3, 4: 1, 5: 2, 6: 2, 7: 2, 8: 25, 9: 26, 10: 25, 11: 26, 12: 25, 13: 26, 14: 25, 15: 26, 16: 25, 17: 26, 18: 25, 19: 26, 20: 25, 21: 26, 22: 25, 23: 26, 24: 25, 25: 26, 26: 25, 27: 26},
                                'ifrom': {0: 12, 1: 12, 2: 12, 3: 12, 4: 14, 5: 14, 6: 16, 7: 16, 8: 18, 9: 18, 10: 18, 11: 18, 12: 18, 13: 18, 14: 18, 15: 18, 16: 18, 17: 18, 18: 18, 19: 18, 20: 18, 21: 18, 22: 18, 23: 18, 24: 18, 25: 18, 26: 18, 27: 18},
                                'ito': {0: 14, 1: 14, 2: 15, 3: 15, 4: 15, 5: 15, 6: 17, 7: 17, 8: 45, 9: 45, 10: 45, 11: 45, 12: 45, 13: 45, 14: 45, 15: 45, 16: 45, 17: 45, 18: 45, 19: 45, 20: 45, 21: 45, 22: 45, 23: 45, 24: 45, 25: 45, 26: 45, 27: 45}} ) )
    test = pd.concat(test, ignore_index=True)
    test = test.merge(break_points, on=list(test.columns), how='outer', indicator=True)
    test = test[test['_merge'] != "both"].copy()
    if len(test):
        print("TestFindBreakPoints failed at CountBreakSupport:")
        print(test)
        return True
#
    break_points, unconnected_break_points = CountAndApplyBreakVetos(break_points, mappings, pot_breaks, bp_ext_len, cov_probs, covered_regions, max_dist_contig_end, max_break_point_distance, min_mapping_length, min_num_reads, min_length_contig_break, prob_factor, merge_block_length, prematurity_threshold)
    test = []
    test.append( pd.DataFrame( {'contig_id': {0: 118, 1: 118, 2: 118, 3: 118, 4: 118, 5: 118, 6: 118, 7: 118, 8: 118, 9: 118},
                                'position': {0: 5459, 1: 5502, 2: 5541, 3: 5551, 4: 5557, 5: 5559, 6: 5561, 7: 5562, 8: 5564, 9: 5566},
                                'mapq': {0: 60, 1: 60, 2: 60, 3: 60, 4: 60, 5: 60, 6: 60, 7: 60, 8: 60, 9: 60},
                                'support': {0: 27, 1: 27, 2: 27, 3: 27, 4: 27, 5: 27, 6: 27, 7: 27, 8: 27, 9: 27},
                                'con_supp': {0: 25, 1: 25, 2: 25, 3: 25, 4: 25, 5: 25, 6: 25, 7: 25, 8: 25, 9: 25},
                                'ifrom': {0: 18, 1: 18, 2: 18, 3: 18, 4: 18, 5: 18, 6: 18, 7: 18, 8: 18, 9: 18},
                                'ito': {0: 45, 1: 45, 2: 45, 3: 45, 4: 45, 5: 45, 6: 45, 7: 45, 8: 45, 9: 45},
                                'vetos': {0: 92, 1: 93, 2: 93, 3: 93, 4: 93, 5: 93, 6: 93, 7: 93, 8: 93, 9: 93},
                                'break_prob': {0: 0.5, 1: 0.5, 2: 0.5, 3: 0.5, 4: 0.5, 5: 0.5, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.5},
                                'veto_prob': {0: 0.5, 1: 0.5, 2: 0.5, 3: 0.5, 4: 0.5, 5: 0.5, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.5},
                                'con_prob': {0: 0.08242225585651375, 1: 0.08242225585651375, 2: 0.08242225585651375, 3: 0.08242225585651375, 4: 0.08242225585651375, 5: 0.08242225585651375, 6: 0.08242225585651375, 7: 0.08242225585651375, 8: 0.08242225585651375, 9: 0.08242225585651375}} ) )
    test = pd.concat(test, ignore_index=True)
    test = test.merge(break_points, on=list(test.columns), how='outer', indicator=True)
    test = test[test['_merge'] != "both"].copy()
    if len(test):
        print("TestFindBreakPoints failed at CountBreakSupport:")
        print(test)
        return True
#
    return False # Did not fail

def TestDuplicationConflictResolution():
    # Define test cases
    test_dups = pd.concat([
        #abab#
        #||  #
        #ab--#
        pd.DataFrame({'apid':0,'apos':[0,1,2,3],'ahap':0,'bpid':1,'bpos':[0,1,0,1],'bhap':0,'samedir':True, 'correct':[True,True,False,False]}),
        #aab#
        # ||#
        #-ab#
        pd.DataFrame({'apid':2,'apos':[0,1,2],'ahap':0,'bpid':3,'bpos':[0,0,1],'bhap':0,'samedir':True, 'correct':[False,True,True]}),
        #abbc#
        #||||#
        #abbc#
        pd.DataFrame({'apid':4,'apos':[0,1,1,2,2,3],'ahap':0,'bpid':5,'bpos':[0,1,2,1,2,3],'bhap':0,'samedir':True, 'correct':[True,True,False,False,True,True]}),
        #aabb#
        # || #
        #-ab-#
        pd.DataFrame({'apid':6,'apos':[0,1,2,3],'ahap':0,'bpid':7,'bpos':[0,0,1,1],'bhap':0,'samedir':True, 'correct':[False,True,True,False]}),
        #abcde#
        #| | |#
        #adcbe#
        pd.DataFrame({'apid':8,'apos':[0,1,2,3,4],'ahap':0,'bpid':9,'bpos':[0,3,2,1,4],'bhap':0,'samedir':True, 'correct':[True,False,True,False,True]}),
        #afghibcde#
        #|    |  |#
        #a--dcb--e#
        pd.DataFrame({'apid':10,'apos':[0,5,6,7,8],'ahap':0,'bpid':11,'bpos':[0,3,2,1,4],'bhap':0,'samedir':True, 'correct':[True,True,False,False,True]}),
        #abcb#
        #||| #
        #abc-#
        pd.DataFrame({'apid':12,'apos':[0,1,2,3],'ahap':0,'bpid':13,'bpos':[0,1,2,1],'bhap':0,'samedir':True, 'correct':[True,True,True,False]}),
        #abcb#
        #| ||#
        #a-cb#
        pd.DataFrame({'apid':14,'apos':[0,1,2,3],'ahap':0,'bpid':15,'bpos':[0,2,1,2],'bhap':0,'samedir':True, 'correct':[True,False,True,True]}),
        #aa#
        #||#
        #aa#
        pd.DataFrame({'apid':16,'apos':[0,0,1,1],'ahap':0,'bpid':17,'bpos':[0,1,0,1],'bhap':0,'samedir':True, 'correct':[True,False,False,True]}),
        #abbac#
        #  |||#
        #--bac#
        pd.DataFrame({'apid':18,'apos':[0,1,2,3,4],'ahap':0,'bpid':19,'bpos':[1,0,0,1,2],'bhap':0,'samedir':True, 'correct':[False,False,True,True,True]}),
        #abbc#
        #| ||#
        #adbc#
        pd.DataFrame({'apid':20,'apos':[0,1,2,3],'ahap':0,'bpid':21,'bpos':[0,2,2,3],'bhap':0,'samedir':True, 'correct':[True,False,True,True]}),
        #abcadc#
        #|  | |#
        #adeafc#
        pd.DataFrame({'apid':22,'apos':[0,0,3,3,5],'ahap':0,'bpid':23,'bpos':[0,3,0,3,5],'bhap':0,'samedir':True, 'correct':[True,False,False,True,True]}),
        #ab-cdaefca#
        #|  |||    #
        #agecda----#
        pd.DataFrame({'apid':24,'apos':[0,0,2,3,4,4,5,7,8,8],'ahap':0,'bpid':25,'bpos':[0,5,3,4,0,5,2,3,0,5],'bhap':0,'samedir':True, 'correct':[True,False,True,True,False,True,False,False,False,False]}),
        #a-bcd#
        #| | |#
        #acb-d#
        pd.DataFrame({'apid':26,'apos':[0,1,2,3],'ahap':0,'bpid':27,'bpos':[0,2,1,3],'bhap':0,'samedir':True, 'correct':[True,True,False,True]}),
        #aabcdbd#
        # || |  #
        #-ab-d--#
        pd.DataFrame({'apid':28,'apos':[0,1,2,4,5,6],'ahap':0,'bpid':29,'bpos':[0,0,1,2,1,2],'bhap':0,'samedir':True, 'correct':[False,True,True,True,False,False]}),
        #abc#
        #| |#
        #aac#
        pd.DataFrame({'apid':30,'apos':[0,0,2],'ahap':0,'bpid':31,'bpos':[0,1,2],'bhap':0,'samedir':True, 'correct':[True,False,True]}),
        #abc#
        #| |#
        #acc#
        pd.DataFrame({'apid':32,'apos':[0,2,2],'ahap':0,'bpid':33,'bpos':[0,1,2],'bhap':0,'samedir':True, 'correct':[True,False,True]}),
        #abc-#
        #| | #
        #aacc#
        pd.DataFrame({'apid':34,'apos':[0,0,2,2],'ahap':0,'bpid':35,'bpos':[0,1,2,3],'bhap':0,'samedir':True, 'correct':[True,False,True,False]}),
        #abcadd#
        #||  | #
        #ab--d-#
        pd.DataFrame({'apid':36,'apos':[0,1,3,4,5],'ahap':0,'bpid':37,'bpos':[0,1,0,2,2],'bhap':0,'samedir':True, 'correct':[True,True,False,True,False]})
        ], ignore_index=True)
#
    # Invert the test cases to check that they are consistent for both options
    test_dups = pd.concat([test_dups, test_dups.rename(columns={'apid':'bpid','apos':'bpos','ahap':'bhap','bpid':'apid','bpos':'apos','bhap':'ahap'})], ignore_index=True)
#
    # Run function
    duplications = test_dups.drop(columns=['correct'])
    duplications = RequireContinuousDirectionForDuplications(duplications)
#
    # Compare and report failed tests
    cols = [col for col in test_dups.columns if col != 'correct']
    test_dups['chosen'] = test_dups.merge(duplications[cols].drop_duplicates(), on=cols, how='left', indicator=True)['_merge'].values == "both"
    test_dups = test_dups[np.isin(test_dups['apid'].values, test_dups.loc[test_dups['correct'] != test_dups['chosen'], 'apid'].values)].copy()
#
    if len(test_dups) == 0:
        return False # Success
    else:
        print("TestDuplicationConflictResolution failed in", len(np.unique(test_dups['apid'].values)), "cases:")
        print(test_dups)
        return True

def FillMissingHaplotypes(scaffold_paths, ploidy):
    for i in range(len(scaffold_paths)):
        for h in range(1, ploidy):
            if f'scaf{h}' not in scaffold_paths[i].columns:
                scaffold_paths[i][f'phase{h}'] = -(scaffold_paths[i]['pid']*10 + h)
                scaffold_paths[i][[f'scaf{h}',f'strand{h}',f'dist{h}']] = [-1,'',0]
#
    return scaffold_paths

def CheckConsistencyOfScaffoldGraph(scaffold_graph):
    inconsistent = []
    for l in np.unique(scaffold_graph['length']):
        if f'scaf{l-1}' not in scaffold_graph.columns:
            inconsistent.append( scaffold_graph[scaffold_graph['length'] == l].copy() ) # If the column does not exist all entries with this length are wrong
        else:
            inconsistent.append( scaffold_graph[(scaffold_graph['length'] == l) & np.isnan(scaffold_graph[f'scaf{l-1}'])].copy() )
        if f'scaf{l}' in scaffold_graph.columns:
            inconsistent.append( scaffold_graph[(scaffold_graph['length'] == l) & (np.isnan(scaffold_graph[f'scaf{l}']) == False)].copy() )
    if len(inconsistent):
        inconsistent = pd.concat(inconsistent, ignore_index=False)
    if len(inconsistent):
        print("Warning: Inconsistent entries in scaffold_graph for test:")
        print(inconsistent)

def TestFilterInvalidConnections():
    # Define test cases
    ploidy = 4
    scaffold_paths = []
    scaffold_graph = []
    splits = []
    man_patha = []
    man_pathb = []
    man_ends = []
    # Case 1
    scaffold_paths.append( pd.DataFrame({
    'pid':      [    1,    1,    1,    1,    1],
    'pos':      [    0,    1,    2,    3,    4],
    'phase0':   [   10,   10,   10,   10,   10],
    'scaf0':    [  101,  102,  105,  106,  109],
    'strand0':  [  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0],
    'phase1':   [  -11,   11,  -11,   11,  -11],
    'scaf1':    [   -1,  103,   -1,  107,   -1],
    'strand1':  [   '',  '+',   '',  '+',   ''],
    'dist1':    [    0,    0,    0,    0,    0],
    'phase2':   [  -12,   12,  -12,   12,  -12],
    'scaf2':    [   -1,  104,   -1,  108,   -1],
    'strand2':  [   '',  '+',   '',  '+',   ''],
    'dist2':    [    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from':  101, 'from_side': 'r', 'length':  5, 'scaf1':  102, 'strand1': '+', 'dist1':    0, 'scaf2':  105, 'strand2': '+', 'dist2':    0, 'scaf3':  106, 'strand3': '+', 'dist3':    0, 'scaf4':  109, 'strand4': '+', 'dist4':    0},
        {'from':  101, 'from_side': 'r', 'length':  5, 'scaf1':  103, 'strand1': '+', 'dist1':    0, 'scaf2':  105, 'strand2': '+', 'dist2':    0, 'scaf3':  107, 'strand3': '+', 'dist3':    0, 'scaf4':  109, 'strand4': '+', 'dist4':    0},
        {'from':  101, 'from_side': 'r', 'length':  3, 'scaf1':  104, 'strand1': '+', 'dist1':    0, 'scaf2':  105, 'strand2': '+', 'dist2':    0},
        {'from':  105, 'from_side': 'r', 'length':  3, 'scaf1':  108, 'strand1': '+', 'dist1':    0, 'scaf2':  109, 'strand2': '+', 'dist2':    0}
        ]) )
    for p in range(1,4):
        splits.append(pd.DataFrame({'pid': 1, 'pos': p, 'ahap':[0,0,1,1,2,2,2], 'bhap':[0,2,1,2,0,1,2], 'valid_path':'ab'}))
        #splits.append(pd.DataFrame({'pid': 1, 'pos': p, 'ahap':[0,1,2], 'bhap':[0,1,2], 'valid_path':'ab'}))
    splits.append(pd.DataFrame({'pid': 1, 'pos': 1, 'ahap':[0,1], 'bhap':[1,0], 'valid_path':'b'}))
    #splits.append(pd.DataFrame({'pid': 1, 'pos': 1, 'ahap':[0,0,1,1,2,2], 'bhap':[1,2,0,2,0,1], 'valid_path':'b'}))
    splits.append(pd.DataFrame({'pid': 1, 'pos': 3, 'ahap':[0,1], 'bhap':[1,0], 'valid_path':'a'}))
    #splits.append(pd.DataFrame({'pid': 1, 'pos': 3, 'ahap':[0,0,1,1,2,2], 'bhap':[1,2,0,2,0,1], 'valid_path':'a'}))
    # Case 2
    scaffold_paths.append( pd.DataFrame({
    'pid':      [    2,    2,    2,    2,    2,    2],
    'pos':      [    0,    1,    2,    3,    4,    5],
    'phase0':   [   20,   20,   20,   20,   20,   20],
    'scaf0':    [  201,  202,  205,  206,  207,  210],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0],
    'phase1':   [  -21,   21,  -21,  -21,   21,  -21],
    'scaf1':    [   -1,  203,   -1,   -1,  208,   -1],
    'strand1':  [   '',  '+',   '',   '',  '+',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0],
    'phase2':   [  -22,   22,  -22,  -22,   22,  -22],
    'scaf2':    [   -1,  204,   -1,   -1,  209,   -1],
    'strand2':  [   '',  '+',   '',   '',  '+',   ''],
    'dist2':    [    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from':  201, 'from_side': 'r', 'length':  6, 'scaf1':  202, 'strand1': '+', 'dist1':    0, 'scaf2':  205, 'strand2': '+', 'dist2':    0, 'scaf3':  206, 'strand3': '+', 'dist3':    0, 'scaf4':  207, 'strand4': '+', 'dist4':    0, 'scaf5':  210, 'strand5': '+', 'dist5':    0},
        {'from':  201, 'from_side': 'r', 'length':  6, 'scaf1':  203, 'strand1': '+', 'dist1':    0, 'scaf2':  205, 'strand2': '+', 'dist2':    0, 'scaf3':  206, 'strand3': '+', 'dist3':    0, 'scaf4':  208, 'strand4': '+', 'dist4':    0, 'scaf5':  210, 'strand5': '+', 'dist5':    0},
        {'from':  201, 'from_side': 'r', 'length':  3, 'scaf1':  204, 'strand1': '+', 'dist1':    0, 'scaf2':  205, 'strand2': '+', 'dist2':    0},
        {'from':  206, 'from_side': 'r', 'length':  3, 'scaf1':  209, 'strand1': '+', 'dist1':    0, 'scaf2':  210, 'strand2': '+', 'dist2':    0}
        ]) )
    for p in range(1,5):
        splits.append(pd.DataFrame({'pid': 2, 'pos': p, 'ahap':[0,0,1,1,2,2,2], 'bhap':[0,2,1,2,0,1,2], 'valid_path':'ab'}))
    splits.append(pd.DataFrame({'pid': 2, 'pos': 1, 'ahap':[0,1], 'bhap':[1,0], 'valid_path':'b'}))
    splits.append(pd.DataFrame({'pid': 2, 'pos': 4, 'ahap':[0,1], 'bhap':[1,0], 'valid_path':'a'}))
    # Case 3
    scaffold_paths.append( pd.DataFrame({
    'pid':      [    3,    3,    3,    3,    3,    3,    3],
    'pos':      [    0,    1,    2,    3,    4,    5,    6],
    'phase0':   [   30,   30,   30,   30,   30,   30,   30],
    'scaf0':    [  301,  302,  305,  306,  307,  308,  311],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0,    0],
    'phase1':   [  -31,   31,  -31,  -31,  -31,   31,  -31],
    'scaf1':    [   -1,  303,   -1,   -1,   -1,  309,   -1],
    'strand1':  [   '',  '+',   '',   '',   '',  '+',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0,    0],
    'phase2':   [  -32,   32,  -32,  -32,  -32,   32,  -32],
    'scaf2':    [   -1,  304,   -1,   -1,   -1,  310,   -1],
    'strand2':  [   '',  '+',   '',   '',   '',  '+',   ''],
    'dist2':    [    0,    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from':  301, 'from_side': 'r', 'length':  7, 'scaf1':  302, 'strand1': '+', 'dist1':    0, 'scaf2':  305, 'strand2': '+', 'dist2':    0, 'scaf3':  306, 'strand3': '+', 'dist3':    0, 'scaf4':  307, 'strand4': '+', 'dist4':    0, 'scaf5':  308, 'strand5': '+', 'dist5':    0, 'scaf6':  311, 'strand6': '+', 'dist6':    0},
        {'from':  301, 'from_side': 'r', 'length':  7, 'scaf1':  303, 'strand1': '+', 'dist1':    0, 'scaf2':  305, 'strand2': '+', 'dist2':    0, 'scaf3':  306, 'strand3': '+', 'dist3':    0, 'scaf4':  307, 'strand4': '+', 'dist4':    0, 'scaf5':  309, 'strand5': '+', 'dist5':    0, 'scaf6':  311, 'strand6': '+', 'dist6':    0},
        {'from':  301, 'from_side': 'r', 'length':  3, 'scaf1':  304, 'strand1': '+', 'dist1':    0, 'scaf2':  305, 'strand2': '+', 'dist2':    0},
        {'from':  307, 'from_side': 'r', 'length':  3, 'scaf1':  310, 'strand1': '+', 'dist1':    0, 'scaf2':  311, 'strand2': '+', 'dist2':    0}
        ]) )
    for p in range(1,6):
        splits.append(pd.DataFrame({'pid': 3, 'pos': p, 'ahap':[0,0,1,1,2,2,2], 'bhap':[0,2,1,2,0,1,2], 'valid_path':'ab'}))
    splits.append(pd.DataFrame({'pid': 3, 'pos': 1, 'ahap':[0,1], 'bhap':[1,0], 'valid_path':'b'}))
    splits.append(pd.DataFrame({'pid': 3, 'pos': 5, 'ahap':[0,1], 'bhap':[1,0], 'valid_path':'a'}))
    # Case 4
    scaffold_paths.append( pd.DataFrame({
    'pid':      [    4,    4,    4,    4,    4,    4],
    'pos':      [    0,    1,    2,    3,    4,    5],
    'phase0':   [   40,   40,   40,   40,   40,   40],
    'scaf0':    [  401,  402,  404,  405,  406,  408],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0],
    'phase1':   [  -41,   41,  -41,  -41,   41,  -41],
    'scaf1':    [   -1,  403,   -1,   -1,  407,   -1],
    'strand1':  [   '',  '+',   '',   '',  '+',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from':  401, 'from_side': 'r', 'length':  3, 'scaf1':  402, 'strand1': '+', 'dist1':    0, 'scaf2':  404, 'strand2': '+', 'dist2':    0},
        {'from':  401, 'from_side': 'r', 'length':  3, 'scaf1':  403, 'strand1': '+', 'dist1':    0, 'scaf2':  404, 'strand2': '+', 'dist2':    0},
        {'from':  404, 'from_side': 'r', 'length':  2, 'scaf1':  405, 'strand1': '+', 'dist1':    0},
        {'from':  405, 'from_side': 'r', 'length':  3, 'scaf1':  406, 'strand1': '+', 'dist1':    0, 'scaf2':  408, 'strand2': '+', 'dist2':    0},
        {'from':  405, 'from_side': 'r', 'length':  3, 'scaf1':  407, 'strand1': '+', 'dist1':    0, 'scaf2':  408, 'strand2': '+', 'dist2':    0}
        ]) )
    for p in range(1,5):
        splits.append(pd.DataFrame({'pid': 4, 'pos': p, 'ahap':[0,0,1,1], 'bhap':[0,1,0,1], 'valid_path':'ab'}))
    # Case 5
    scaffold_paths.append( pd.DataFrame({
    'pid':      [    5,    5,    5,    5,    5,    5],
    'pos':      [    0,    1,    2,    3,    4,    5],
    'phase0':   [   50,   50,   50,   50,   50,   50],
    'scaf0':    [  501,  502,  504,  505,  506,  508],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0],
    'phase1':   [  -51,   51,  -51,  -51,   51,  -51],
    'scaf1':    [   -1,  503,   -1,   -1,  507,   -1],
    'strand1':  [   '',  '+',   '',   '',  '+',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from':  501, 'from_side': 'r', 'length':  4, 'scaf1':  502, 'strand1': '+', 'dist1':    0, 'scaf2':  504, 'strand2': '+', 'dist2':    0, 'scaf3':  505, 'strand3': '+', 'dist3':    0},
        {'from':  501, 'from_side': 'r', 'length':  3, 'scaf1':  503, 'strand1': '+', 'dist1':    0, 'scaf2':  504, 'strand2': '+', 'dist2':    0},
        {'from':  505, 'from_side': 'r', 'length':  3, 'scaf1':  506, 'strand1': '+', 'dist1':    0, 'scaf2':  508, 'strand2': '+', 'dist2':    0},
        {'from':  505, 'from_side': 'r', 'length':  3, 'scaf1':  507, 'strand1': '+', 'dist1':    0, 'scaf2':  508, 'strand2': '+', 'dist2':    0}
        ]) )
    for p in range(1,5):
        splits.append(pd.DataFrame({'pid': 5, 'pos': p, 'ahap':[0,0,1,1], 'bhap':[0,1,0,1], 'valid_path':'ab'}))
    # Case 6
    scaffold_paths.append( pd.DataFrame({
    'pid':      [    6,    6,    6,    6,    6,    6],
    'pos':      [    0,    1,    2,    3,    4,    5],
    'phase0':   [   60,   60,   60,   60,   60,   60],
    'scaf0':    [  601,  602,  604,  605,  606,  607],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0],
    'phase1':   [  -61,   61,  -61,  -61,   61,  -61],
    'scaf1':    [   -1,  603,   -1,   -1,  606,   -1],
    'strand1':  [   '',  '+',   '',   '',  '+',   ''],
    'dist1':    [    0,    0,    0,    0,  100,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from':  601, 'from_side': 'r', 'length':  4, 'scaf1':  602, 'strand1': '+', 'dist1':    0, 'scaf2':  604, 'strand2': '+', 'dist2':    0, 'scaf3':  605, 'strand3': '+', 'dist3':    0},
        {'from':  601, 'from_side': 'r', 'length':  3, 'scaf1':  603, 'strand1': '+', 'dist1':    0, 'scaf2':  604, 'strand2': '+', 'dist2':    0},
        {'from':  604, 'from_side': 'r', 'length':  4, 'scaf1':  605, 'strand1': '+', 'dist1':    0, 'scaf2':  606, 'strand2': '+', 'dist2':  100, 'scaf3':  607, 'strand3': '+', 'dist3':    0},
        {'from':  605, 'from_side': 'r', 'length':  3, 'scaf1':  606, 'strand1': '+', 'dist1':    0, 'scaf2':  607, 'strand2': '+', 'dist2':    0}
        ]) )
    for p in range(1,5):
        splits.append(pd.DataFrame({'pid': 6, 'pos': p, 'ahap':[0,0,1,1], 'bhap':[0,1,0,1], 'valid_path':'ab'}))
    # Case 7
    scaffold_paths.append( pd.DataFrame({
    'pid':      [    7,    7,    7,    7,    7],
    'pos':      [    0,    1,    2,    3,    4],
    'phase0':   [   70,   70,   70,   70,   70],
    'scaf0':    [  701,  702,  704,  705,  707],
    'strand0':  [  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0],
    'phase1':   [  -71,   71,  -71,   71,  -71],
    'scaf1':    [   -1,  703,   -1,  706,   -1],
    'strand1':  [   '',  '+',   '',  '+',   ''],
    'dist1':    [    0,    0,    0,    0,    0],
    'phase2':   [  -72,   72,  -72,   72,  -72],
    'scaf2':    [   -1,   -1,   -1,   -1,   -1],
    'strand2':  [   '',   '',   '',   '',   ''],
    'dist2':    [    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from':  701, 'from_side': 'r', 'length':  5, 'scaf1':  702, 'strand1': '+', 'dist1':    0, 'scaf2':  704, 'strand2': '+', 'dist2':    0, 'scaf3':  705, 'strand3': '+', 'dist3':    0, 'scaf4':  707, 'strand4': '+', 'dist4':    0},
        {'from':  701, 'from_side': 'r', 'length':  5, 'scaf1':  703, 'strand1': '+', 'dist1':    0, 'scaf2':  704, 'strand2': '+', 'dist2':    0, 'scaf3':  706, 'strand3': '+', 'dist3':    0, 'scaf4':  707, 'strand4': '+', 'dist4':    0},
        {'from':  701, 'from_side': 'r', 'length':  4, 'scaf1':  703, 'strand1': '+', 'dist1':    0, 'scaf2':  704, 'strand2': '+', 'dist2':    0, 'scaf3':  707, 'strand3': '+', 'dist3':    0},
        {'from':  701, 'from_side': 'r', 'length':  4, 'scaf1':  704, 'strand1': '+', 'dist1':    0, 'scaf2':  706, 'strand2': '+', 'dist2':    0, 'scaf3':  707, 'strand3': '+', 'dist3':    0},
        {'from':  701, 'from_side': 'r', 'length':  3, 'scaf1':  704, 'strand1': '+', 'dist1':    0, 'scaf2':  707, 'strand2': '+', 'dist2':    0}
        ]) )
    splits.append(pd.DataFrame({'pid': 7, 'pos': 2, 'ahap':[0,1,1,2,2], 'bhap':[0,1,2,1,2], 'valid_path':'ab'}))
    # Case 8
    scaffold_paths.append( pd.DataFrame({
    'pid':      [    8,    8,    8,    8,    8],
    'pos':      [    0,    1,    2,    3,    4],
    'phase0':   [   80,   80,   80,   80,   80],
    'scaf0':    [  801,  802,  804,  805,  806],
    'strand0':  [  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0],
    'phase1':   [  -81,   81,  -81,  -81,  -81],
    'scaf1':    [   -1,  803,   -1,   -1,   -1],
    'strand1':  [   '',  '+',   '',   '',   ''],
    'dist1':    [    0,    0,    0,    0,    0],
    'phase2':   [  -82,   82,  -82,  -82,  -82],
    'scaf2':    [   -1,   -1,   -1,   -1,   -1],
    'strand2':  [   '',   '',   '',   '',   ''],
    'dist2':    [    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from':  801, 'from_side': 'r', 'length':  5, 'scaf1':  802, 'strand1': '+', 'dist1':    0, 'scaf2':  804, 'strand2': '+', 'dist2':    0, 'scaf3':  805, 'strand3': '+', 'dist3':    0, 'scaf4':  806, 'strand4': '+', 'dist4':    0},
        {'from':  801, 'from_side': 'r', 'length':  5, 'scaf1':  803, 'strand1': '+', 'dist1':    0, 'scaf2':  804, 'strand2': '+', 'dist2':    0, 'scaf3':  805, 'strand3': '+', 'dist3':    0, 'scaf4':  806, 'strand4': '+', 'dist4':    0},
        {'from':  801, 'from_side': 'r', 'length':  2, 'scaf1':  804, 'strand1': '+', 'dist1':    0}
        ]) )
    for p in range(2,4):
        splits.append(pd.DataFrame({'pid': 8, 'pos': p, 'ahap':[0,0,0,1,1,1,2,2,2], 'bhap':[0,1,2,0,1,2,0,1,2], 'valid_path':'ab'}))
    # Case 9
    scaffold_paths.append( pd.DataFrame({
    'pid':      [    9,    9,    9,    9,    9,    9,    9,    9],
    'pos':      [    0,    1,    2,    3,    4,    5,    6,    7],
    'phase0':   [   90,   90,   90,   90,   90,   90,   90,   90],
    'scaf0':    [  901,  902,  904,  905,  906,  905,  906,  907],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0,    0,    0],
    'phase1':   [  -91,   91,  -91,  -91,  -91,  -91,  -91,  -91],
    'scaf1':    [   -1,  903,   -1,   -1,   -1,   -1,   -1,   -1],
    'strand1':  [   '',  '+',   '',   '',   '',   '',   '',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from':  901, 'from_side': 'r', 'length':  4, 'scaf1':  902, 'strand1': '+', 'dist1':    0, 'scaf2':  904, 'strand2': '+', 'dist2':    0, 'scaf3':  905, 'strand3': '+', 'dist3':    0},
        {'from':  901, 'from_side': 'r', 'length':  3, 'scaf1':  903, 'strand1': '+', 'dist1':    0, 'scaf2':  904, 'strand2': '+', 'dist2':    0},
        {'from':  904, 'from_side': 'r', 'length':  6, 'scaf1':  905, 'strand1': '+', 'dist1':    0, 'scaf2':  906, 'strand2': '+', 'dist2':    0, 'scaf3':  905, 'strand3': '+', 'dist3':    0, 'scaf4':  906, 'strand4': '+', 'dist4':    0, 'scaf5':  907, 'strand5': '+', 'dist5':    0}
        ]) )
    for p in range(1,7):
        splits.append(pd.DataFrame({'pid': 9, 'pos': p, 'ahap':[0,0,1,1], 'bhap':[0,1,0,1], 'valid_path':'ab'}))
    # Case 10
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   10,   10,   10,   10,   10,   10],
    'pos':      [    0,    1,    2,    3,    4,    5],
    'phase0':   [  100,  100,  100,  100,  100,  100],
    'scaf0':    [ 1001, 1002, 1004, 1005, 1006, 1007],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0],
    'phase1':   [ -101,  101, -101, -101, -101, -101],
    'scaf1':    [   -1, 1003,   -1,   -1,   -1,   -1],
    'strand1':  [   '',  '+',   '',   '',   '',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 1001, 'from_side': 'r', 'length':  4, 'scaf1': 1002, 'strand1': '+', 'dist1':    0, 'scaf2': 1004, 'strand2': '+', 'dist2':    0, 'scaf3': 1005, 'strand3': '+', 'dist3':    0},
        {'from': 1001, 'from_side': 'r', 'length':  3, 'scaf1': 1003, 'strand1': '+', 'dist1':    0, 'scaf2': 1004, 'strand2': '+', 'dist2':    0},
        {'from': 1004, 'from_side': 'r', 'length':  6, 'scaf1': 1005, 'strand1': '+', 'dist1':    0, 'scaf2': 1006, 'strand2': '+', 'dist2':    0, 'scaf3': 1005, 'strand3': '+', 'dist3':    0, 'scaf4': 1006, 'strand4': '+', 'dist4':    0, 'scaf5': 1007, 'strand5': '+', 'dist5':    0}
        ]) )
    splits.append(pd.DataFrame({'pid': 10, 'pos': [3,4], 'ahap':-1, 'bhap':-1, 'valid_path':''}))
    # Case 11
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   11,   11,   11,   11,   11,   11,   11,   11],
    'pos':      [    0,    1,    2,    3,    4,    5,    6,    7],
    'phase0':   [  110,  110,  110,  110,  110,  110,  110,  110],
    'scaf0':    [ 1101, 1102, 1104, 1105, 1106, 1105, 1106, 1107],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0,    0,    0],
    'phase1':   [ -111,  111, -111, -111, -111, -111, -111, -111],
    'scaf1':    [   -1, 1103,   -1,   -1,   -1,   -1,   -1,   -1],
    'strand1':  [   '',  '+',   '',   '',   '',   '',   '',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 1101, 'from_side': 'r', 'length':  4, 'scaf1': 1102, 'strand1': '+', 'dist1':    0, 'scaf2': 1104, 'strand2': '+', 'dist2':    0, 'scaf3': 1105, 'strand3': '+', 'dist3':    0},
        {'from': 1101, 'from_side': 'r', 'length':  3, 'scaf1': 1103, 'strand1': '+', 'dist1':    0, 'scaf2': 1104, 'strand2': '+', 'dist2':    0},
        {'from': 1105, 'from_side': 'r', 'length':  5, 'scaf1': 1106, 'strand1': '+', 'dist1':    0, 'scaf2': 1105, 'strand2': '+', 'dist2':    0, 'scaf3': 1106, 'strand3': '+', 'dist3':    0, 'scaf4': 1107, 'strand4': '+', 'dist4':    0}
        ]) )
    for p in range(1,7):
        splits.append(pd.DataFrame({'pid': 11, 'pos': p, 'ahap':[0,0,1,1], 'bhap':[0,1,0,1], 'valid_path':'ab'}))
    # Case 12
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   12,   12,   12,   12,   12,   12],
    'pos':      [    0,    1,    2,    3,    4,    5],
    'phase0':   [  120,  120,  120,  120,  120,  120],
    'scaf0':    [ 1201, 1202, 1204, 1205, 1206, 1207],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0],
    'phase1':   [ -121,  121, -121, -121, -121, -121],
    'scaf1':    [   -1, 1203,   -1,   -1,   -1,   -1],
    'strand1':  [   '',  '+',   '',   '',   '',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 1201, 'from_side': 'r', 'length':  4, 'scaf1': 1202, 'strand1': '+', 'dist1':    0, 'scaf2': 1204, 'strand2': '+', 'dist2':    0, 'scaf3': 1205, 'strand3': '+', 'dist3':    0},
        {'from': 1201, 'from_side': 'r', 'length':  3, 'scaf1': 1203, 'strand1': '+', 'dist1':    0, 'scaf2': 1204, 'strand2': '+', 'dist2':    0},
        {'from': 1205, 'from_side': 'r', 'length':  5, 'scaf1': 1206, 'strand1': '+', 'dist1':    0, 'scaf2': 1205, 'strand2': '+', 'dist2':    0, 'scaf3': 1206, 'strand3': '+', 'dist3':    0, 'scaf4': 1207, 'strand4': '+', 'dist4':    0}
        ]) )
    for p in [3,4]:
        splits.append(pd.DataFrame({'pid': 12, 'pos': p, 'ahap':[0,0,1,1], 'bhap':[0,1,0,1], 'valid_path':'ab'})) # Except for bridged repeats, over or underrepresented repeats are not something this function can take care of (Instead we allow everything and later block due to ambiguity, so that we do not make errors and other functions can handle it)
    # Case 13
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   13,   13,   13,   13,   13,   13,   13,   13],
    'pos':      [    0,    1,    2,    3,    4,    5,    6,    7],
    'phase0':   [  130,  130,  130,  130,  130,  130,  130,  130],
    'scaf0':    [ 1301, 1302, 1304, 1305, 1306, 1305, 1306, 1307],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0,    0,    0],
    'phase1':   [  -91,  131,  -91,  -91,  -91,  -91,  -91,  -91],
    'scaf1':    [   -1, 1303,   -1,   -1,   -1,   -1,   -1,   -1],
    'strand1':  [   '',  '+',   '',   '',   '',   '',   '',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 1301, 'from_side': 'r', 'length':  4, 'scaf1': 1302, 'strand1': '+', 'dist1':    0, 'scaf2': 1304, 'strand2': '+', 'dist2':    0, 'scaf3': 1305, 'strand3': '+', 'dist3':    0},
        {'from': 1301, 'from_side': 'r', 'length':  3, 'scaf1': 1303, 'strand1': '+', 'dist1':    0, 'scaf2': 1304, 'strand2': '+', 'dist2':    0},
        {'from': 1306, 'from_side': 'r', 'length':  4, 'scaf1': 1305, 'strand1': '+', 'dist1':    0, 'scaf2': 1306, 'strand2': '+', 'dist2':    0, 'scaf3': 1307, 'strand3': '+', 'dist3':    0}
        ]) )
    for p in range(1,7):
        splits.append(pd.DataFrame({'pid': 13, 'pos': p, 'ahap':[0,0,1,1], 'bhap':[0,1,0,1], 'valid_path':'ab'}))
    # Case 14
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   14,   14,   14,   14,   14,   14],
    'pos':      [    0,    1,    2,    3,    4,    5],
    'phase0':   [  140,  140,  140,  140,  140,  140],
    'scaf0':    [ 1401, 1402, 1404, 1405, 1406, 1407],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0],
    'phase1':   [ -141,  141, -141, -141, -141, -141],
    'scaf1':    [   -1, 1403,   -1,   -1,   -1,   -1],
    'strand1':  [   '',  '+',   '',   '',   '',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 1401, 'from_side': 'r', 'length':  4, 'scaf1': 1402, 'strand1': '+', 'dist1':    0, 'scaf2': 1404, 'strand2': '+', 'dist2':    0, 'scaf3': 1405, 'strand3': '+', 'dist3':    0},
        {'from': 1401, 'from_side': 'r', 'length':  3, 'scaf1': 1403, 'strand1': '+', 'dist1':    0, 'scaf2': 1404, 'strand2': '+', 'dist2':    0},
        {'from': 1406, 'from_side': 'r', 'length':  4, 'scaf1': 1405, 'strand1': '+', 'dist1':    0, 'scaf2': 1406, 'strand2': '+', 'dist2':    0, 'scaf3': 1407, 'strand3': '+', 'dist3':    0}
        ]) )
    for p in [3,4]:
        splits.append(pd.DataFrame({'pid': 14, 'pos': p, 'ahap':[0,0,1,1], 'bhap':[0,1,0,1], 'valid_path':'ab'})) # Except for bridged repeats, over or underrepresented repeats are not something this function can take care of (Instead we allow everything and later block due to ambiguity, so that we do not make errors and other functions can handle it)
    # Case 15
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   15,   15,   15,   15,   15,   15,   15],
    'pos':      [    0,    1,    2,    3,    4,    5,    6],
    'phase0':   [  150,  150,  150,  150,  150,  150,  150],
    'scaf0':    [ 1501, 1502, 1505, 1506, 1507, 1509, 1511],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0,    0],
    'phase1':   [ -151,  151, -151, -151,  151,  151, -151],
    'scaf1':    [   -1, 1503,   -1,   -1, 1508, 1510,   -1],
    'strand1':  [   '',  '+',   '',   '',  '+',  '+',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0,    0],
    'phase2':   [ -152,  152, -152, -152, -152,  152, -152],
    'scaf2':    [   -1, 1504,   -1,   -1,   -1, 1510,   -1],
    'strand2':  [   '',  '+',   '',   '',   '',  '+',   ''],
    'dist2':    [    0,    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 1501, 'from_side': 'r', 'length':  7, 'scaf1': 1502, 'strand1': '+', 'dist1':    0, 'scaf2': 1505, 'strand2': '+', 'dist2':    0, 'scaf3': 1506, 'strand3': '+', 'dist3':    0, 'scaf4': 1507, 'strand4': '+', 'dist4':    0, 'scaf5': 1509, 'strand5': '+', 'dist5':    0, 'scaf6': 1511, 'strand6': '+', 'dist6':    0},
        {'from': 1501, 'from_side': 'r', 'length':  3, 'scaf1': 1503, 'strand1': '+', 'dist1':    0, 'scaf2': 1505, 'strand2': '+', 'dist2':    0},
        {'from': 1501, 'from_side': 'r', 'length':  3, 'scaf1': 1504, 'strand1': '+', 'dist1':    0, 'scaf2': 1505, 'strand2': '+', 'dist2':    0},
        {'from': 1506, 'from_side': 'r', 'length':  4, 'scaf1': 1508, 'strand1': '+', 'dist1':    0, 'scaf2': 1510, 'strand2': '+', 'dist2':    0, 'scaf3': 1511, 'strand3': '+', 'dist3':    0},
        {'from': 1507, 'from_side': 'r', 'length':  3, 'scaf1': 1510, 'strand1': '+', 'dist1':    0, 'scaf2': 1511, 'strand2': '+', 'dist2':    0}
        ]) )
    for p in range(1,4):
        splits.append(pd.DataFrame({'pid': 15, 'pos': p, 'ahap':[0,0,0,1,1,1,2,2,2], 'bhap':[0,1,2,0,1,2,0,1,2], 'valid_path':'ab'}))
    for p in [4,5]:
        splits.append(pd.DataFrame({'pid': 15, 'pos': p, 'ahap':[0,0,0,1,1,2,2,2], 'bhap':[0,1,2,1,2,0,1,2], 'valid_path':'ab'}))
    splits.append(pd.DataFrame({'pid': 15, 'pos': [4,5], 'ahap':[1,1], 'bhap':[0,0], 'valid_path':['b','a']}))
    # Case 16
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   16,   16,   16,   16,   16,   16,   16],
    'pos':      [    0,    1,    2,    3,    4,    5,    6],
    'phase0':   [  160,  160,  160,  160,  160,  160,  160],
    'scaf0':    [ 1601, 1602, 1603, 1604, 1605, 1606, 1607],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0,    0],
    'phase1':   [ -161, -161, -161,  161, -161,  161, -161],
    'scaf1':    [   -1,   -1,   -1,   -1,   -1, 1606,   -1],
    'strand1':  [   '',   '',   '',   '',   '',  '-',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 1601, 'from_side': 'r', 'length':  5, 'scaf1': 1602, 'strand1': '+', 'dist1':    0, 'scaf2': 1603, 'strand2': '+', 'dist2':    0, 'scaf3': 1604, 'strand3': '+', 'dist3':    0, 'scaf4': 1605, 'strand4': '+', 'dist4':    0},
        {'from': 1602, 'from_side': 'r', 'length':  6, 'scaf1': 1603, 'strand1': '+', 'dist1':    0, 'scaf2': 1604, 'strand2': '+', 'dist2':    0, 'scaf3': 1605, 'strand3': '+', 'dist3':    0, 'scaf4': 1606, 'strand4': '+', 'dist4':    0, 'scaf5': 1607, 'strand5': '+', 'dist5':    0},
        {'from': 1603, 'from_side': 'r', 'length':  4, 'scaf1': 1605, 'strand1': '+', 'dist1':    0, 'scaf2': 1606, 'strand2': '-', 'dist2':    0, 'scaf3': 1607, 'strand3': '+', 'dist3':    0}
        ]) )
    for p in [1,2]:
        splits.append(pd.DataFrame({'pid': 16, 'pos': p, 'ahap':[0,0,1,1], 'bhap':[0,1,0,1], 'valid_path':'ab'}))
    splits.append(pd.DataFrame({'pid': 16, 'pos': 4, 'ahap':[0,1], 'bhap':[0,1], 'valid_path':'ab'}))
    # Case 17
    man_patha.append( pd.DataFrame({
    'pid':      [   17,   17,   17,   17,   17],
    'pos':      [    0,    1,    2,    3,    4],
    'phase0':   [  170,  170,  170,  170,  170],
    'scaf0':    [ 1701, 1702, 1703, 1704, 1705],
    'strand0':  [  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0]
    }) )
    man_pathb.append( pd.DataFrame({
    'pid':      [   17,   17,   17,   17,   17],
    'pos':      [    0,    1,    2,    3,    4],
    'phase0':   [  170,  170,  170,  170,  170],
    'scaf0':    [ 1703, 1704, 1705, 1706, 1707],
    'strand0':  [  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0],
    'phase1':   [ -171,  171, -171,  171, -171],
    'scaf1':    [   -1,   -1,   -1, 1706,   -1],
    'strand1':  [   '',   '',   '',  '-',   ''],
    'dist1':    [    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 1701, 'from_side': 'r', 'length':  5, 'scaf1': 1702, 'strand1': '+', 'dist1':    0, 'scaf2': 1703, 'strand2': '+', 'dist2':    0, 'scaf3': 1704, 'strand3': '+', 'dist3':    0, 'scaf4': 1705, 'strand4': '+', 'dist4':    0},
        {'from': 1702, 'from_side': 'r', 'length':  6, 'scaf1': 1703, 'strand1': '+', 'dist1':    0, 'scaf2': 1704, 'strand2': '+', 'dist2':    0, 'scaf3': 1705, 'strand3': '+', 'dist3':    0, 'scaf4': 1706, 'strand4': '+', 'dist4':    0, 'scaf5': 1707, 'strand5': '+', 'dist5':    0},
        {'from': 1703, 'from_side': 'r', 'length':  4, 'scaf1': 1705, 'strand1': '+', 'dist1':    0, 'scaf2': 1706, 'strand2': '-', 'dist2':    0, 'scaf3': 1707, 'strand3': '+', 'dist3':    0}
    ]) )
    man_ends.append( pd.DataFrame([ # Differences in the overlapping region need to be ignored and need separate hanlding to be able to combine these two paths
        {'apid': 17, 'ahap': 0, 'bpid': 17, 'bhap': 0, 'amin':  2, 'amax':  4, 'bmin':  0, 'bmax':  2, 'matches':  3, 'alen':  4, 'blen':  4, 'aside': 'r', 'bside': 'l', 'valid_path':'ab'},
        {'apid': 17, 'ahap': 0, 'bpid': 17, 'bhap': 1, 'amin':  2, 'amax':  4, 'bmin':  0, 'bmax':  2, 'matches':  3, 'alen':  4, 'blen':  4, 'aside': 'r', 'bside': 'l', 'valid_path':'b'},
        ]) )
    # Case 18
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   18,   18,   18,   18],
    'pos':      [    0,    1,    2,    3],
    'phase0':   [  180,  180,  180,  180],
    'scaf0':    [ 1801, 1802, 1803, 1804],
    'strand0':  [  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0],
    'phase1':   [ -181,  181, -181, -181],
    'scaf1':    [   -1, 1802,   -1,   -1],
    'strand1':  [   '',  '+',   '',   ''],
    'dist1':    [    0,  100,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 1801, 'from_side': 'r', 'length':  4, 'scaf1': 1802, 'strand1': '+', 'dist1':    0, 'scaf2': 1803, 'strand2': '+', 'dist2':    0, 'scaf3': 1805, 'strand3': '+', 'dist3':    0},
        {'from': 1801, 'from_side': 'r', 'length':  4, 'scaf1': 1802, 'strand1': '+', 'dist1':  100, 'scaf2': 1803, 'strand2': '+', 'dist2':    0, 'scaf3': 1804, 'strand3': '+', 'dist3':    0},
        {'from': 1806, 'from_side': 'r', 'length':  3, 'scaf1': 1801, 'strand1': '+', 'dist1':    0, 'scaf2': 1802, 'strand2': '+', 'dist2':    0},
        {'from': 1807, 'from_side': 'r', 'length':  4, 'scaf1': 1801, 'strand1': '+', 'dist1':    0, 'scaf2': 1802, 'strand2': '+', 'dist2':    0, 'scaf3': 1803, 'strand3': '+', 'dist3':    0}
        ]) )
    splits.append(pd.DataFrame({'pid': 18, 'pos': 1, 'ahap':1, 'bhap':[0,1], 'valid_path':'ab'}))
    # Case 19
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   19,   19,   19,   19,   19,   19],
    'pos':      [    0,    1,    2,    3,    4,    5],
    'phase0':   [  190,  190,  190,  190,  190,  190],
    'scaf0':    [ 1901, 1902, 1904, 1905, 1906, 1907],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0],
    'phase1':   [ -191,  191, -191, -191, -191, -191],
    'scaf1':    [   -1, 1903,   -1,   -1,   -1,   -1],
    'strand1':  [   '',  '+',   '',   '',   '',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 1901, 'from_side': 'r', 'length':  6, 'scaf1': 1902, 'strand1': '+', 'dist1':    0, 'scaf2': 1904, 'strand2': '+', 'dist2':    0, 'scaf3': 1905, 'strand3': '+', 'dist3':    0, 'scaf4': 1906, 'strand4': '+', 'dist4':    0, 'scaf5': 1907, 'strand5': '+', 'dist5':    0},
        {'from': 1901, 'from_side': 'r', 'length':  5, 'scaf1': 1903, 'strand1': '+', 'dist1':    0, 'scaf2': 1904, 'strand2': '+', 'dist2':    0, 'scaf3': 1905, 'strand3': '+', 'dist3':    0, 'scaf4': 1908, 'strand4': '+', 'dist4':    0}
        ]) )
    splits.append(pd.DataFrame({'pid': 19, 'pos': 1, 'ahap':[0,0,1], 'bhap':[0,1,0], 'valid_path':['ab','a','b']}))
    for p in range(2,5):
        splits.append(pd.DataFrame({'pid': 19, 'pos': p, 'ahap':[0,0], 'bhap':[0,1], 'valid_path':'ab'}))
    # Case 20
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   20,   20,   20,   20,   20,   20,   20,   20,   20],
    'pos':      [    0,    1,    2,    3,    4,    5,    6,    7,    8],
    'phase0':   [  200,  200,  200,  200,  200,  200,  200,  200,  200],
    'scaf0':    [ 2001, 2002, 2004, 2005, 2006, 2007, 2008, 2009, 2011],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0,    0,    0,    0],
    'phase1':   [ -201,  201, -201, -201,  201, -201, -201,  201, -201],
    'scaf1':    [   -1, 2003,   -1,   -1,   -1,   -1,   -1, 2010,   -1],
    'strand1':  [   '',  '+',   '',   '',   '',   '',   '',  '+',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 2001, 'from_side': 'r', 'length':  9, 'scaf1': 2002, 'strand1': '+', 'dist1':    0, 'scaf2': 2004, 'strand2': '+', 'dist2':    0, 'scaf3': 2005, 'strand3': '+', 'dist3':    0, 'scaf4': 2006, 'strand4': '+', 'dist4':    0, 'scaf5': 2007, 'strand5': '+', 'dist5':    0, 'scaf6': 2008, 'strand6': '+', 'dist6':    0, 'scaf7': 2009, 'strand7': '+', 'dist7':    0, 'scaf8': 2011, 'strand8': '+', 'dist8':    0},
        {'from': 2001, 'from_side': 'r', 'length':  3, 'scaf1': 2003, 'strand1': '+', 'dist1':    0, 'scaf2': 2004, 'strand2': '+', 'dist2':    0},
        {'from': 2005, 'from_side': 'r', 'length':  2, 'scaf1': 2007, 'strand1': '+', 'dist1':    0},
        {'from': 2008, 'from_side': 'r', 'length':  3, 'scaf1': 2010, 'strand1': '+', 'dist1':    0, 'scaf2': 2011, 'strand2': '+', 'dist2':    0}
        ]) )
    for p in [2,3,5,6]:
        splits.append(pd.DataFrame({'pid': 20, 'pos': p, 'ahap':[0,0,1,1], 'bhap':[0,1,0,1], 'valid_path':'ab'}))
    # Case 21
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   21,   21,   21,   21,   21,   21],
    'pos':      [    0,    1,    2,    3,    4,    5],
    'phase0':   [  210,  210,  210,  210,  210,  210],
    'scaf0':    [ 2101, 2102,   -1, 2103, 2104, 2106],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0],
    'phase1':   [ -211, -211,  211, -211,  211, -211],
    'scaf1':    [   -1,   -1, 2102,   -1, 2105,   -1],
    'strand1':  [   '',   '',  '+',   '',  '+',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 2101, 'from_side': 'r', 'length':  4, 'scaf1': 2102, 'strand1': '+', 'dist1':    0, 'scaf2': 2102, 'strand2': '+', 'dist2':    0, 'scaf3': 2103, 'strand3': '+', 'dist3':    0},
        {'from': 2101, 'from_side': 'r', 'length':  3, 'scaf1': 2102, 'strand1': '+', 'dist1':    0, 'scaf2': 2103, 'strand2': '+', 'dist2':    0},
        {'from': 2103, 'from_side': 'r', 'length':  3, 'scaf1': 2104, 'strand1': '+', 'dist1':    0, 'scaf2': 2106, 'strand2': '+', 'dist2':    0},
        {'from': 2103, 'from_side': 'r', 'length':  3, 'scaf1': 2105, 'strand1': '+', 'dist1':    0, 'scaf2': 2106, 'strand2': '+', 'dist2':    0}
        ]) )
    for p in [1,3]:
        splits.append(pd.DataFrame({'pid': 21, 'pos': p, 'ahap':[0,0,1,1], 'bhap':[0,1,0,1], 'valid_path':'ab'}))
    # Case 22
    man_patha.append( pd.DataFrame({
    'pid':      [   22,   22,   22],
    'pos':      [    0,    1,    2],
    'phase0':   [  220,  220,  220],
    'scaf0':    [ 2201, 2202,   -1],
    'strand0':  [  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0],
    'phase1':   [ -221, -221,  221],
    'scaf1':    [   -1,   -1, 2202],
    'strand1':  [   '',   '',  '+'],
    'dist1':    [    0,    0,    0]
    }) )
    man_pathb.append( pd.DataFrame({
    'pid':      [   22,   22,   22,   22],
    'pos':      [    0,    1,    2,    3],
    'phase0':   [  220,  220,  220,  220],
    'scaf0':    [   -1, 2203, 2204, 2206],
    'strand0':  [  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0],
    'phase1':   [  221, -221,  221, -221],
    'scaf1':    [ 2202,   -1, 2205,   -1],
    'strand1':  [  '+',   '',  '+',   ''],
    'dist1':    [    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 2201, 'from_side': 'r', 'length':  4, 'scaf1': 2202, 'strand1': '+', 'dist1':    0, 'scaf2': 2202, 'strand2': '+', 'dist2':    0, 'scaf3': 2203, 'strand3': '+', 'dist3':    0},
        {'from': 2201, 'from_side': 'r', 'length':  3, 'scaf1': 2202, 'strand1': '+', 'dist1':    0, 'scaf2': 2203, 'strand2': '+', 'dist2':    0},
        {'from': 2203, 'from_side': 'r', 'length':  3, 'scaf1': 2204, 'strand1': '+', 'dist1':    0, 'scaf2': 2206, 'strand2': '+', 'dist2':    0},
        {'from': 2203, 'from_side': 'r', 'length':  3, 'scaf1': 2205, 'strand1': '+', 'dist1':    0, 'scaf2': 2206, 'strand2': '+', 'dist2':    0}
    ]) )
    man_ends.append( pd.DataFrame([ # Differences in the overlapping region need to be ignored and need separate hanlding to be able to combine these two paths
        {'apid': 22, 'ahap': 1, 'bpid': 22, 'bhap': 1, 'amin':  2, 'amax':  2, 'bmin':  0, 'bmax':  0, 'matches':  1, 'alen':  2, 'blen':  3, 'aside': 'r', 'bside': 'l', 'valid_path':'ab'}
        ]) )
    # Case 23
    man_patha.append( pd.DataFrame({
    'pid':      [   23,   23,   23,   23,   23],
    'pos':      [    0,    1,    2,    3,    4],
    'phase0':   [  230,  230,  230,  230,  230],
    'scaf0':    [ 2301, 2302, 2303, 2304, 2305],
    'strand0':  [  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0]
    }) )
    man_pathb.append( pd.DataFrame({
    'pid':      [   23,   23,   23,   23,   23],
    'pos':      [    0,    1,    2,    3,    4],
    'phase0':   [  230,  230,  230,  230,  230],
    'scaf0':    [ 2303, 2304, 2305, 2306, 2307],
    'strand0':  [  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0],
    'phase1':   [ -231,  231, -231, -231, -231],
    'scaf1':    [   -1,   -1,   -1,   -1,   -1],
    'strand1':  [   '',   '',   '',   '',   ''],
    'dist1':    [    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 2301, 'from_side': 'r', 'length':  5, 'scaf1': 2302, 'strand1': '+', 'dist1':    0, 'scaf2': 2303, 'strand2': '+', 'dist2':    0, 'scaf3': 2304, 'strand3': '+', 'dist3':    0, 'scaf4': 2305, 'strand4': '+', 'dist4':    0},
        {'from': 2302, 'from_side': 'r', 'length':  6, 'scaf1': 2303, 'strand1': '+', 'dist1':    0, 'scaf2': 2304, 'strand2': '+', 'dist2':    0, 'scaf3': 2305, 'strand3': '+', 'dist3':    0, 'scaf4': 2306, 'strand4': '+', 'dist4':    0, 'scaf5': 2307, 'strand5': '+', 'dist5':    0},
        {'from': 2308, 'from_side': 'r', 'length':  5, 'scaf1': 2303, 'strand1': '+', 'dist1':    0, 'scaf2': 2305, 'strand2': '+', 'dist2':    0, 'scaf3': 2306, 'strand3': '+', 'dist3':    0, 'scaf4': 2307, 'strand4': '+', 'dist4':    0}
    ]) )
    man_ends.append( pd.DataFrame([ # Differences in the overlapping region need to be ignored and need separate hanlding to be able to combine these two paths
        {'apid': 23, 'ahap': 0, 'bpid': 23, 'bhap': 0, 'amin':  2, 'amax':  4, 'bmin':  0, 'bmax':  2, 'matches':  3, 'alen':  4, 'blen':  4, 'aside': 'r', 'bside': 'l', 'valid_path':'ab'},
        {'apid': 23, 'ahap': 0, 'bpid': 23, 'bhap': 1, 'amin':  2, 'amax':  4, 'bmin':  0, 'bmax':  2, 'matches':  3, 'alen':  4, 'blen':  4, 'aside': 'r', 'bside': 'l', 'valid_path':'a'},
        ]) )
    # Case 24
    man_patha.append( pd.DataFrame({
    'pid':      [   24,   24,   24,   24],
    'pos':      [    0,    1,    2,    3],
    'phase0':   [  240,  240,  240,  240],
    'scaf0':    [ 2401, 2402, 2403, 2404],
    'strand0':  [  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0]
    }) )
    man_pathb.append( pd.DataFrame({
    'pid':      [   24,   24,   24,   24,   24],
    'pos':      [    0,    1,    2,    3,    4],
    'phase0':   [  240,  240,  240,  240,  240],
    'scaf0':    [ 2403, 2404, 2406, 2407, 2408],
    'strand0':  [  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0],
    'phase1':   [ -241,  241, -241, -241, -241],
    'scaf1':    [   -1, 2405,   -1,   -1,   -1],
    'strand1':  [   '',   '',   '',   '',   ''],
    'dist1':    [    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 2401, 'from_side': 'r', 'length':  5, 'scaf1': 2402, 'strand1': '+', 'dist1':    0, 'scaf2': 2403, 'strand2': '+', 'dist2':    0, 'scaf3': 2404, 'strand3': '+', 'dist3':    0, 'scaf4': 2406, 'strand4': '+', 'dist4':    0},
        {'from': 2402, 'from_side': 'r', 'length':  6, 'scaf1': 2403, 'strand1': '+', 'dist1':    0, 'scaf2': 2404, 'strand2': '+', 'dist2':    0, 'scaf3': 2406, 'strand3': '+', 'dist3':    0, 'scaf4': 2407, 'strand4': '+', 'dist4':    0, 'scaf5': 2408, 'strand5': '+', 'dist5':    0},
        {'from': 2409, 'from_side': 'r', 'length':  6, 'scaf1': 2403, 'strand1': '+', 'dist1':    0, 'scaf2': 2405, 'strand2': '+', 'dist2':    0, 'scaf3': 2406, 'strand3': '+', 'dist3':    0, 'scaf4': 2407, 'strand4': '+', 'dist4':    0, 'scaf5': 2408, 'strand5': '+', 'dist5':    0}
    ]) )
    man_ends.append( pd.DataFrame([ # Differences in the overlapping region need to be ignored and need separate hanlding to be able to combine these two paths
        {'apid': 24, 'ahap': 0, 'bpid': 24, 'bhap': 0, 'amin':  2, 'amax':  3, 'bmin':  0, 'bmax':  1, 'matches':  2, 'alen':  3, 'blen':  4, 'aside': 'r', 'bside': 'l', 'valid_path':'ab'},
        {'apid': 24, 'ahap': 0, 'bpid': 24, 'bhap': 1, 'amin':  2, 'amax':  3, 'bmin':  0, 'bmax':  1, 'matches':  2, 'alen':  3, 'blen':  4, 'aside': 'r', 'bside': 'l', 'valid_path':'a'},
        ]) )
    # Case 25
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   25,   25,   25,   25,   25],
    'pos':      [    0,    1,    2,    3,    4],
    'phase0':   [  250,  250,  250,  250,  250],
    'scaf0':    [ 2501, 2502, 2503, 2504, 2505],
    'strand0':  [  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 2501, 'from_side': 'r', 'length':  4, 'scaf1': 2502, 'strand1': '+', 'dist1':    0, 'scaf2': 2503, 'strand2': '+', 'dist2':    0, 'scaf3': 2504, 'strand3': '+', 'dist3':    0},
        {'from': 2506, 'from_side': 'r', 'length':  5, 'scaf1': 2502, 'strand1': '+', 'dist1':    0, 'scaf2': 2503, 'strand2': '+', 'dist2':    0, 'scaf3': 2504, 'strand3': '+', 'dist3':    0, 'scaf4': 2505, 'strand4': '+', 'dist4':    0},
        {'from': 2506, 'from_side': 'r', 'length':  5, 'scaf1': 2502, 'strand1': '+', 'dist1':    0, 'scaf2': 2503, 'strand2': '+', 'dist2':    0, 'scaf3': 2504, 'strand3': '+', 'dist3':    0, 'scaf4': 2509, 'strand4': '+', 'dist4':    0},
        {'from': 2510, 'from_side': 'r', 'length':  5, 'scaf1': 2501, 'strand1': '+', 'dist1':    0, 'scaf2': 2502, 'strand2': '+', 'dist2':    0, 'scaf3': 2503, 'strand3': '+', 'dist3':    0, 'scaf4': 2508, 'strand4': '+', 'dist4':    0},
        {'from': 2507, 'from_side': 'r', 'length':  5, 'scaf1': 2501, 'strand1': '+', 'dist1':    0, 'scaf2': 2502, 'strand2': '+', 'dist2':    0, 'scaf3': 2503, 'strand3': '+', 'dist3':    0, 'scaf4': 2508, 'strand4': '+', 'dist4':    0}
        ]) )
    splits.append(pd.DataFrame({'pid': 25, 'pos': [1,2,3], 'ahap':0, 'bhap':0, 'valid_path':'ab'}))
    # Case 26
    scaffold_paths.append( pd.DataFrame({
    'pid':      [   26,   26,   26,   26,   26,   26,   26],
    'pos':      [    0,    1,    2,    3,    4,    5,    6],
    'phase0':   [  260,  260,  260,  260,  260,  260,  260],
    'scaf0':    [ 2601, 2602, 2605, 2606, 2607, 2608, 2611],
    'strand0':  [  '+',  '+',  '+',  '+',  '+',  '+',  '+'],
    'dist0':    [    0,    0,    0,    0,    0,    0,    0],
    'phase1':   [ -261,  261, -261, -261, -261,  261, -261],
    'scaf1':    [   -1, 2603,   -1,   -1,   -1, 2609,   -1],
    'strand1':  [   '',  '+',   '',   '',   '',  '+',   ''],
    'dist1':    [    0,    0,    0,    0,    0,    0,    0],
    'phase2':   [ -262,  262, -262, -262, -262,  262, -262],
    'scaf2':    [   -1, 2604,   -1,   -1,   -1, 2610,   -1],
    'strand2':  [   '',  '+',   '',   '',   '',  '+',   ''],
    'dist2':    [    0,    0,    0,    0,    0,    0,    0]
    }) )
    scaffold_graph.append( pd.DataFrame([
        {'from': 2601, 'from_side': 'r', 'length':  7, 'scaf1': 2602, 'strand1': '+', 'dist1':    0, 'scaf2': 2605, 'strand2': '+', 'dist2':    0, 'scaf3': 2606, 'strand3': '+', 'dist3':    0, 'scaf4': 2607, 'strand4': '+', 'dist4':    0, 'scaf5': 2608, 'strand5': '+', 'dist5':    0, 'scaf6': 2611, 'strand6': '+', 'dist6':    0},
        {'from': 2601, 'from_side': 'r', 'length':  7, 'scaf1': 2603, 'strand1': '+', 'dist1':    0, 'scaf2': 2605, 'strand2': '+', 'dist2':    0, 'scaf3': 2606, 'strand3': '+', 'dist3':    0, 'scaf4': 2607, 'strand4': '+', 'dist4':    0, 'scaf5': 2609, 'strand5': '+', 'dist5':    0, 'scaf6': 2611, 'strand6': '+', 'dist6':    0},
        {'from': 2601, 'from_side': 'r', 'length':  4, 'scaf1': 2604, 'strand1': '+', 'dist1':    0, 'scaf2': 2605, 'strand2': '+', 'dist2':    0, 'scaf3': 2606, 'strand3': '+', 'dist3':    0},
        {'from': 2606, 'from_side': 'r', 'length':  4, 'scaf1': 2607, 'strand1': '+', 'dist1':    0, 'scaf2': 2610, 'strand2': '+', 'dist2':    0, 'scaf3': 2611, 'strand3': '+', 'dist3':    0}
        ]) )
    for p in range(2,5):
        splits.append(pd.DataFrame({'pid': 3, 'pos': p, 'ahap':[0,0,1,1,2,2,2], 'bhap':[0,2,1,2,0,1,2], 'valid_path':'ab'}))
        #splits.append(pd.DataFrame({'pid': 3, 'pos': p, 'ahap':[0,1,2], 'bhap':[0,1,2], 'valid_path':'ab'}))
#
    # Fill missing haplotype with empty ones and combine scaffold_paths
    scaffold_paths = pd.concat(FillMissingHaplotypes(scaffold_paths, ploidy), ignore_index=True)
    man_patha = pd.concat(FillMissingHaplotypes(man_patha, ploidy), ignore_index=True)
    man_pathb = pd.concat(FillMissingHaplotypes(man_pathb, ploidy), ignore_index=True)
    # Combine scaffold_graph and add the shorter and reverse paths
    scaffold_graph = pd.concat(scaffold_graph, ignore_index=True)
    CheckConsistencyOfScaffoldGraph(scaffold_graph)
    reverse_graph = scaffold_graph.rename(columns={'from':'scaf0','from_side':'strand0'})
    reverse_graph['strand0'] = np.where(reverse_graph['strand0'] == 'r', '+', '-')
    reverse_graph = ReverseVerticalPaths(reverse_graph).drop(columns=['lindex'])
    reverse_graph.rename(columns={'scaf0':'from','strand0':'from_side'}, inplace=True)
    reverse_graph['from_side'] = np.where(reverse_graph['from_side'] == '+', 'r', 'l')
    scaffold_graph = pd.concat([scaffold_graph,reverse_graph], ignore_index=True)
    tmp_graph = [ scaffold_graph.copy() ]
    while len(scaffold_graph):
        scaffold_graph = scaffold_graph[scaffold_graph['length'] > 2].copy()
        if len(scaffold_graph):
            scaffold_graph.drop(columns=['from','from_side','dist1'], inplace=True)
            scaffold_graph.rename(columns={**{'scaf1':'from', 'strand1':'from_side'}, **{f'{n}{s}':f'{n}{s-1}' for s in range(2,scaffold_graph['length'].max()) for n in ['scaf','strand','dist']}}, inplace=True)
            scaffold_graph['from_side'] = np.where(scaffold_graph['from_side'] == '+', 'r', 'l')
            scaffold_graph['length'] -= 1
            tmp_graph.append( scaffold_graph.copy() )
    scaffold_graph = pd.concat(tmp_graph, ignore_index=True)
    scaffold_graph[['from','scaf1','dist1']] = scaffold_graph[['from','scaf1','dist1']].astype(int).values
    scaffold_graph.sort_values(['from','from_side'] + [f'{n}{s}' for s in range(1,scaffold_graph['length'].max()) for n in ['scaf','strand','dist']], inplace=True)
    scaffold_graph = RemoveRedundantEntriesInScaffoldGraph(scaffold_graph)
    # Prepare ends
    splits = pd.concat(splits, ignore_index=True)
    split_pos = splits[['pid','pos']].drop_duplicates()
    splits = splits[splits['ahap'] >= 0].copy()
    split_pos['apid'] = np.arange(len(split_pos))*2
    split_pos['bpid'] = split_pos['apid'] + 1
    splits[['apid','bpid']] = splits[['pid','pos']].merge(split_pos, on=['pid','pos'], how='left')[['apid','bpid']].values
    ends = split_pos.copy()
    ends['len'] = ends[['pid']].merge(scaffold_paths.groupby(['pid'])['pos'].max().reset_index(), on=['pid'], how='left')['pos'].values
    ends['nhaps'] = ends[['pid']].merge(GetNumberOfHaplotypes(scaffold_paths, ploidy), on=['pid'], how='left')['nhaps'].values
    ends['index'] = ends.index.values
    ends = ends.loc[np.repeat(ends.index.values, ends['nhaps'].values ** 2)].copy()
    ends['ahap'] = ends.groupby(['index'], sort=False).cumcount() // ends['nhaps'].values
    ends['bhap'] = ends.groupby(['index'], sort=False).cumcount() % ends['nhaps'].values
    ends['amin'] = ends['pos']
    ends['amax'] = ends['amin']
    ends[['bmin','bmax']] = 0
    ends['matches'] = 1
    ends['alen'] = ends['amax']
    ends['blen'] = ends['len'] - ends['pos']
    ends['aside'] = 'r'
    ends['bside'] = 'l'
    ends = ends[['apid','ahap','bpid','bhap','amin','amax','bmin','bmax','matches','alen','blen','aside','bside']].copy()
    # Split scaffold_path according to splits(split_pos)
    apath = scaffold_paths.merge(split_pos[['pid','apid','pos']].rename(columns={'pos':'mpos'}), on=['pid'], how='left')
    apath['pid'] = apath['apid']
    apath = apath[apath['pos'] <= apath['mpos']].drop(columns=['apid','mpos'])
    bpath = scaffold_paths.merge(split_pos[['pid','bpid','pos']].rename(columns={'pos':'mpos'}), on=['pid'], how='left')
    bpath['pid'] = bpath['bpid']
    bpath = bpath[bpath['pos'] >= bpath['mpos']].drop(columns=['bpid','mpos'])
    bpath['pos'] = bpath.groupby(['pid'], sort=False).cumcount()
    # Add manual entries
    man_ends = pd.concat(man_ends, ignore_index=True)
    man_test_pids = man_ends[['apid']].drop_duplicates()
    man_test_pids.rename(columns={'apid':'pid'}, inplace=True)
    man_test_pids['apid'] = np.arange(len(man_test_pids))*2 + split_pos['apid'].max() + 2
    man_test_pids['bpid'] = man_test_pids['apid'] + 1
    split_pos = pd.concat([split_pos, man_test_pids], ignore_index=True)
    man_ends[['apid','bpid']] = man_ends[['apid']].rename(columns={'apid':'pid'}).merge(man_test_pids, on=['pid'], how='left')[['apid','bpid']].values
    splits = pd.concat([splits, man_ends.loc[man_ends['valid_path'] != '', ['apid','bpid','ahap','bhap','valid_path']]], ignore_index=True)
    ends = pd.concat([ends, man_ends.drop(columns=['valid_path'])], ignore_index=True)
    man_patha['pid'] = man_patha[['pid']].merge(man_test_pids[['pid','apid']], on=['pid'], how='left')['apid'].values
    man_pathb['pid'] = man_pathb[['pid']].merge(man_test_pids[['pid','bpid']], on=['pid'], how='left')['bpid'].values
    scaffold_paths = pd.concat([apath, bpath, man_patha, man_pathb], ignore_index=True)
    scaffold_paths.sort_values(['pid','pos'], inplace=True)
    scaffold_paths = SetDistanceAtFirstPositionToZero(scaffold_paths, ploidy)
    # Make ends symmetrical
    ends = pd.concat([ends, ends.rename(columns={**{col:f'b{col[1:]}' for col in ends.columns if col[0] == "a"}, **{col:f'a{col[1:]}' for col in ends.columns if col[0] == "b"}})], ignore_index=True)
#
    # Run function
    graph_ext = FindValidExtensionsInScaffoldGraph(scaffold_graph)
    ends = FilterInvalidConnections(ends, scaffold_paths, graph_ext, ploidy)
#
    # Compare and report failed tests
    splits['correct'] = True
    if len(ends):
        ends = ends[ends['apid'] < ends['bpid']].copy()
        ends['obtained'] = True
        splits = splits.merge(ends[['ahap','bhap','apid','bpid','valid_path','obtained']], on=['ahap','bhap','apid','bpid','valid_path'], how='outer')
        splits[['pid','pos']] = splits[['apid']].merge(split_pos[['apid','pid','pos']], on=['apid'], how='left')[['pid','pos']].values
        splits['pid'] = splits['pid'].astype(int)
        splits[['correct','obtained']] = splits[['correct','obtained']].fillna(False).values
    else:
        splits['obtained'] = False
    splits = splits.loc[splits['correct'] != splits['obtained'], ['pid','pos','apid','bpid','ahap','bhap','correct','valid_path','obtained']].copy()
    if len(splits) == 0:
        # Everything as it should be
        return False
    else:
        print("TestFilterInvalidConnections failed:")
        splits.sort_values(['pid','apid','bpid','ahap','bhap'], inplace=True)
        print(splits)
        return True

def TestTraverseScaffoldGraph():
    # Define test cases
    ploidy = 2
    max_loop_units = 10
    scaffolds = [] # (test==new scaffold_graph): ', '.join(np.unique(test['from']).astype(str))
    scaffold_graph = [] # print(",\n".join([str(" "*8+"{") + ', '.join(["'{}': {:>6}".format(k,v) for k,v in zip(entry[entry.isnull() == False].index.values,[val[:-2] if val[-2:] == ".0" else (f"'{val}'" if val in ['-','+','l','r'] else val) for val in entry[entry.isnull() == False].astype(str).values])]) + "}" for entry in [scaffold_graph.loc[i, ['from','from_side','length']+[f'{n}{s}' for s in range(1,scaffold_graph['length'].max()) for n in ['scaf','strand','dist']]] for i in scaffold_graph[scaffold_graph['from'] == x].index.values]]))
    scaf_bridges = [] # tmp = test[['from','from_side','scaf1','strand1','dist1']].rename(columns={'scaf1':'to','strand1':'to_side','dist1':'mean_dist'}); tmp['to_side'] = np.where(tmp['to_side'] == '+', 'l', 'r'); tmp=tmp.merge(scaf_bridges, on=list(tmp.columns), how='inner').drop_duplicates(); print(",\n".join([str(" "*8+"{") + ', '.join(["'{}': {:>6}".format(k,v) for k,v in zip(entry[entry.isnull() == False].index.values,["{:8.6f}".format(float(val)) if val[:2] == "0." else (f"'{val}'" if val in ['-','+','l','r'] else val) for val in entry[entry.isnull() == False].astype(str).values])]) + "}" for entry in [tmp.loc[i] for i in tmp.index.values]]))
    org_scaf_conns = [pd.DataFrame({'from':[],'from_side':[],'to':[],'to_side':[],'distance':[]})]
    result_paths = []
    result_unique_paths = []
    result_loops = []
    result_inversions = []
    result_untraversed = []
#
    # Test 1
    scaffolds.append( pd.DataFrame({'case':1, 'scaffold':[44, 69, 114, 115, 372, 674, 929, 1306, 2722, 2725, 2799, 2885, 9344, 10723, 11659, 12896, 12910, 13029, 13434, 13452, 13455, 13591, 14096, 15177, 15812, 20727, 26855, 30179, 30214, 31749, 31756, 32229, 33144, 33994, 40554, 41636, 47404, 47516, 49093, 51660, 53480, 56740, 56987, 58443, 70951, 71091, 76860, 96716, 99004, 99341, 99342, 101215, 101373, 107483, 107484, 107485, 107486, 109207, 110827, 112333, 115803, 117303, 117304, 118890, 118892]}) )
    scaffold_graph.append( pd.DataFrame([
        {'from':     44, 'from_side':    'l', 'length':      5, 'scaf1':  99342, 'strand1':    '-', 'dist1':    -44, 'scaf2':  99341, 'strand2':    '-', 'dist2':      0, 'scaf3':   9344, 'strand3':    '+', 'dist3':      0, 'scaf4': 118890, 'strand4':    '-', 'dist4':    -13},
        {'from':     44, 'from_side':    'r', 'length':      3, 'scaf1':   1306, 'strand1':    '-', 'dist1':    -43, 'scaf2':  71091, 'strand2':    '+', 'dist2':    -45},
        {'from':     44, 'from_side':    'r', 'length':      2, 'scaf1':  71091, 'strand1':    '+', 'dist1':      1},
        {'from':     69, 'from_side':    'l', 'length':      3, 'scaf1':  47404, 'strand1':    '-', 'dist1':    -43, 'scaf2':  30179, 'strand2':    '-', 'dist2':    -32},
        {'from':     69, 'from_side':    'l', 'length':      2, 'scaf1': 110827, 'strand1':    '-', 'dist1':    -43},
        {'from':     69, 'from_side':    'r', 'length':      5, 'scaf1':  13434, 'strand1':    '-', 'dist1':    -41, 'scaf2': 112333, 'strand2':    '+', 'dist2':     38, 'scaf3':    114, 'strand3':    '+', 'dist3':   -819, 'scaf4':    115, 'strand4':    '+', 'dist4':   1131},
        {'from':     69, 'from_side':    'r', 'length':      5, 'scaf1':  13455, 'strand1':    '-', 'dist1':    -43, 'scaf2': 112333, 'strand2':    '+', 'dist2':    -43, 'scaf3':  41636, 'strand3':    '-', 'dist3':   -710, 'scaf4':    115, 'strand4':    '+', 'dist4':   1932},
        {'from':    114, 'from_side':    'l', 'length':      4, 'scaf1': 112333, 'strand1':    '-', 'dist1':   -819, 'scaf2':  13434, 'strand2':    '+', 'dist2':     38, 'scaf3':     69, 'strand3':    '-', 'dist3':    -41},
        {'from':    114, 'from_side':    'r', 'length':      3, 'scaf1':    115, 'strand1':    '+', 'dist1':   1131, 'scaf2': 115803, 'strand2':    '+', 'dist2':    885},
        {'from':    115, 'from_side':    'l', 'length':      5, 'scaf1':    114, 'strand1':    '-', 'dist1':   1131, 'scaf2': 112333, 'strand2':    '-', 'dist2':   -819, 'scaf3':  13434, 'strand3':    '+', 'dist3':     38, 'scaf4':     69, 'strand4':    '-', 'dist4':    -41},
        {'from':    115, 'from_side':    'l', 'length':      5, 'scaf1':  41636, 'strand1':    '+', 'dist1':   1932, 'scaf2': 112333, 'strand2':    '-', 'dist2':   -710, 'scaf3':  13455, 'strand3':    '+', 'dist3':    -43, 'scaf4':     69, 'strand4':    '-', 'dist4':    -43},
        {'from':    115, 'from_side':    'r', 'length':      3, 'scaf1':  15177, 'strand1':    '+', 'dist1':    -42, 'scaf2': 115803, 'strand2':    '+', 'dist2':      5},
        {'from':    115, 'from_side':    'r', 'length':      2, 'scaf1': 115803, 'strand1':    '+', 'dist1':    885},
        {'from':    372, 'from_side':    'r', 'length':      5, 'scaf1': 107485, 'strand1':    '+', 'dist1':  -1436, 'scaf2': 107486, 'strand2':    '+', 'dist2':      0, 'scaf3': 101373, 'strand3':    '-', 'dist3':  -1922, 'scaf4':  70951, 'strand4':    '+', 'dist4':      0},
        {'from':    674, 'from_side':    'l', 'length':      2, 'scaf1':   2799, 'strand1':    '+', 'dist1':     60},
        {'from':    674, 'from_side':    'r', 'length':      2, 'scaf1':  20727, 'strand1':    '-', 'dist1':    -68},
        {'from':    674, 'from_side':    'r', 'length':      2, 'scaf1':  20727, 'strand1':    '-', 'dist1':    -18},
        {'from':    929, 'from_side':    'l', 'length':      6, 'scaf1': 117303, 'strand1':    '-', 'dist1':   2544, 'scaf2': 107484, 'strand2':    '+', 'dist2':   -888, 'scaf3': 107485, 'strand3':    '+', 'dist3':      0, 'scaf4': 107486, 'strand4':    '+', 'dist4':      0, 'scaf5':  33994, 'strand5':    '+', 'dist5':    -44},
        {'from':    929, 'from_side':    'l', 'length':      2, 'scaf1': 117304, 'strand1':    '-', 'dist1':   2489},
        {'from':    929, 'from_side':    'r', 'length':      2, 'scaf1':   2725, 'strand1':    '+', 'dist1':   1205},
        {'from':    929, 'from_side':    'r', 'length':      3, 'scaf1':  14096, 'strand1':    '+', 'dist1':    -44, 'scaf2':   2725, 'strand2':    '+', 'dist2':    642},
        {'from':   1306, 'from_side':    'l', 'length':      2, 'scaf1':  71091, 'strand1':    '+', 'dist1':    -45},
        {'from':   1306, 'from_side':    'r', 'length':      2, 'scaf1':     44, 'strand1':    '-', 'dist1':    -43},
        {'from':   2722, 'from_side':    'l', 'length':      3, 'scaf1':  53480, 'strand1':    '+', 'dist1':      0, 'scaf2':  40554, 'strand2':    '-', 'dist2':      0},
        {'from':   2725, 'from_side':    'l', 'length':      2, 'scaf1':    929, 'strand1':    '-', 'dist1':   1205},
        {'from':   2725, 'from_side':    'l', 'length':      3, 'scaf1':  14096, 'strand1':    '-', 'dist1':    642, 'scaf2':    929, 'strand2':    '-', 'dist2':    -44},
        {'from':   2725, 'from_side':    'r', 'length':      3, 'scaf1':  11659, 'strand1':    '-', 'dist1':    136, 'scaf2':  13591, 'strand2':    '-', 'dist2':    -43},
        {'from':   2725, 'from_side':    'r', 'length':      3, 'scaf1':  13029, 'strand1':    '+', 'dist1':    143, 'scaf2':  13591, 'strand2':    '-', 'dist2':    -43},
        {'from':   2725, 'from_side':    'r', 'length':      2, 'scaf1':  13591, 'strand1':    '-', 'dist1':    734},
        {'from':   2799, 'from_side':    'l', 'length':      2, 'scaf1':    674, 'strand1':    '+', 'dist1':     60},
        {'from':   2885, 'from_side':    'l', 'length':      2, 'scaf1':  20727, 'strand1':    '+', 'dist1':    -44},
        {'from':   2885, 'from_side':    'l', 'length':      2, 'scaf1':  20727, 'strand1':    '+', 'dist1':    245},
        {'from':   2885, 'from_side':    'r', 'length':      3, 'scaf1':  13452, 'strand1':    '-', 'dist1':    -45, 'scaf2':  10723, 'strand2':    '-', 'dist2':    -41},
        {'from':   2885, 'from_side':    'r', 'length':      3, 'scaf1':  15812, 'strand1':    '+', 'dist1':    -44, 'scaf2':  10723, 'strand2':    '-', 'dist2':    -43},
        {'from':   9344, 'from_side':    'l', 'length':      4, 'scaf1':  99341, 'strand1':    '+', 'dist1':      0, 'scaf2':  99342, 'strand2':    '+', 'dist2':      0, 'scaf3':     44, 'strand3':    '+', 'dist3':    -44},
        {'from':   9344, 'from_side':    'l', 'length':      7, 'scaf1':  99342, 'strand1':    '+', 'dist1':     73, 'scaf2': 107483, 'strand2':    '+', 'dist2':    -44, 'scaf3': 107484, 'strand3':    '+', 'dist3':      0, 'scaf4': 107485, 'strand4':    '+', 'dist4':      0, 'scaf5': 107486, 'strand5':    '+', 'dist5':      0, 'scaf6':  99004, 'strand6':    '-', 'dist6':    838},
        {'from':   9344, 'from_side':    'r', 'length':      2, 'scaf1': 118890, 'strand1':    '-', 'dist1':    -13},
        {'from':   9344, 'from_side':    'r', 'length':      2, 'scaf1': 118892, 'strand1':    '-', 'dist1':      8},
        {'from':  10723, 'from_side':    'l', 'length':      3, 'scaf1':  12896, 'strand1':    '-', 'dist1':    -43, 'scaf2':  76860, 'strand2':    '-', 'dist2':    -46},
        {'from':  10723, 'from_side':    'l', 'length':      3, 'scaf1':  12910, 'strand1':    '-', 'dist1':    -43, 'scaf2':  76860, 'strand2':    '-', 'dist2':    -42},
        {'from':  10723, 'from_side':    'r', 'length':      3, 'scaf1':  13452, 'strand1':    '+', 'dist1':    -41, 'scaf2':   2885, 'strand2':    '-', 'dist2':    -45},
        {'from':  10723, 'from_side':    'r', 'length':      3, 'scaf1':  15812, 'strand1':    '-', 'dist1':    -43, 'scaf2':   2885, 'strand2':    '-', 'dist2':    -44},
        {'from':  11659, 'from_side':    'l', 'length':      2, 'scaf1':  13591, 'strand1':    '-', 'dist1':    -43},
        {'from':  11659, 'from_side':    'r', 'length':      2, 'scaf1':   2725, 'strand1':    '-', 'dist1':    136},
        {'from':  12896, 'from_side':    'l', 'length':      2, 'scaf1':  76860, 'strand1':    '-', 'dist1':    -46},
        {'from':  12896, 'from_side':    'r', 'length':      2, 'scaf1':  10723, 'strand1':    '+', 'dist1':    -43},
        {'from':  12910, 'from_side':    'l', 'length':      2, 'scaf1':  76860, 'strand1':    '-', 'dist1':    -42},
        {'from':  12910, 'from_side':    'r', 'length':      2, 'scaf1':  10723, 'strand1':    '+', 'dist1':    -43},
        {'from':  13029, 'from_side':    'l', 'length':      2, 'scaf1':   2725, 'strand1':    '-', 'dist1':    143},
        {'from':  13029, 'from_side':    'r', 'length':      2, 'scaf1':  13591, 'strand1':    '-', 'dist1':    -43},
        {'from':  13434, 'from_side':    'l', 'length':      4, 'scaf1': 112333, 'strand1':    '+', 'dist1':     38, 'scaf2':    114, 'strand2':    '+', 'dist2':   -819, 'scaf3':    115, 'strand3':    '+', 'dist3':   1131},
        {'from':  13434, 'from_side':    'r', 'length':      2, 'scaf1':     69, 'strand1':    '-', 'dist1':    -41},
        {'from':  13452, 'from_side':    'l', 'length':      2, 'scaf1':  10723, 'strand1':    '-', 'dist1':    -41},
        {'from':  13452, 'from_side':    'r', 'length':      2, 'scaf1':   2885, 'strand1':    '-', 'dist1':    -45},
        {'from':  13455, 'from_side':    'l', 'length':      4, 'scaf1': 112333, 'strand1':    '+', 'dist1':    -43, 'scaf2':  41636, 'strand2':    '-', 'dist2':   -710, 'scaf3':    115, 'strand3':    '+', 'dist3':   1932},
        {'from':  13455, 'from_side':    'r', 'length':      2, 'scaf1':     69, 'strand1':    '-', 'dist1':    -43},
        {'from':  13591, 'from_side':    'r', 'length':      2, 'scaf1':   2725, 'strand1':    '-', 'dist1':    734},
        {'from':  13591, 'from_side':    'r', 'length':      3, 'scaf1':  11659, 'strand1':    '+', 'dist1':    -43, 'scaf2':   2725, 'strand2':    '-', 'dist2':    136},
        {'from':  13591, 'from_side':    'r', 'length':      3, 'scaf1':  13029, 'strand1':    '-', 'dist1':    -43, 'scaf2':   2725, 'strand2':    '-', 'dist2':    143},
        {'from':  14096, 'from_side':    'l', 'length':      2, 'scaf1':    929, 'strand1':    '-', 'dist1':    -44},
        {'from':  14096, 'from_side':    'r', 'length':      2, 'scaf1':   2725, 'strand1':    '+', 'dist1':    642},
        {'from':  15177, 'from_side':    'l', 'length':      2, 'scaf1':    115, 'strand1':    '-', 'dist1':    -42},
        {'from':  15177, 'from_side':    'r', 'length':      2, 'scaf1': 115803, 'strand1':    '+', 'dist1':      5},
        {'from':  15812, 'from_side':    'l', 'length':      2, 'scaf1':   2885, 'strand1':    '-', 'dist1':    -44},
        {'from':  15812, 'from_side':    'r', 'length':      2, 'scaf1':  10723, 'strand1':    '-', 'dist1':    -43},
        {'from':  20727, 'from_side':    'l', 'length':      2, 'scaf1':   2885, 'strand1':    '+', 'dist1':    -44},
        {'from':  20727, 'from_side':    'l', 'length':      2, 'scaf1':   2885, 'strand1':    '+', 'dist1':    245},
        {'from':  20727, 'from_side':    'r', 'length':      2, 'scaf1':    674, 'strand1':    '-', 'dist1':    -68},
        {'from':  20727, 'from_side':    'r', 'length':      2, 'scaf1':    674, 'strand1':    '-', 'dist1':    -18},
        {'from':  26855, 'from_side':    'l', 'length':      3, 'scaf1':  33144, 'strand1':    '-', 'dist1':    -40, 'scaf2':  47404, 'strand2':    '+', 'dist2':  -4062},
        {'from':  26855, 'from_side':    'r', 'length':      2, 'scaf1': 109207, 'strand1':    '-', 'dist1':    -43},
        {'from':  30179, 'from_side':    'l', 'length':      2, 'scaf1': 109207, 'strand1':    '-', 'dist1':    -40},
        {'from':  30179, 'from_side':    'r', 'length':      3, 'scaf1':  47404, 'strand1':    '+', 'dist1':    -32, 'scaf2':     69, 'strand2':    '+', 'dist2':    -43},
        {'from':  30214, 'from_side':    'l', 'length':      2, 'scaf1': 109207, 'strand1':    '+', 'dist1':     -6},
        {'from':  30214, 'from_side':    'r', 'length':      4, 'scaf1':  49093, 'strand1':    '-', 'dist1':     17, 'scaf2':  51660, 'strand2':    '+', 'dist2':    -14, 'scaf3': 101215, 'strand3':    '+', 'dist3':      5},
        {'from':  31749, 'from_side':    'l', 'length':      2, 'scaf1':  31756, 'strand1':    '-', 'dist1':     -1},
        {'from':  31749, 'from_side':    'l', 'length':      3, 'scaf1':  53480, 'strand1':    '-', 'dist1':   -646, 'scaf2': 101373, 'strand2':    '-', 'dist2':  -1383},
        {'from':  31749, 'from_side':    'r', 'length':      4, 'scaf1': 101373, 'strand1':    '+', 'dist1':   -350, 'scaf2': 107486, 'strand2':    '-', 'dist2':  -1922, 'scaf3': 107485, 'strand3':    '-', 'dist3':      0},
        {'from':  31756, 'from_side':    'r', 'length':      3, 'scaf1':  31749, 'strand1':    '+', 'dist1':     -1, 'scaf2': 101373, 'strand2':    '+', 'dist2':   -350},
        {'from':  32229, 'from_side':    'l', 'length':      2, 'scaf1':  76860, 'strand1':    '+', 'dist1':   -456},
        {'from':  32229, 'from_side':    'r', 'length':      2, 'scaf1':  71091, 'strand1':    '-', 'dist1':      3},
        {'from':  33144, 'from_side':    'l', 'length':      2, 'scaf1':  47404, 'strand1':    '+', 'dist1':  -4062},
        {'from':  33144, 'from_side':    'r', 'length':      3, 'scaf1':  26855, 'strand1':    '+', 'dist1':    -40, 'scaf2': 109207, 'strand2':    '-', 'dist2':    -43},
        {'from':  33994, 'from_side':    'l', 'length':      6, 'scaf1': 107486, 'strand1':    '-', 'dist1':    -44, 'scaf2': 107485, 'strand2':    '-', 'dist2':      0, 'scaf3': 107484, 'strand3':    '-', 'dist3':      0, 'scaf4': 117303, 'strand4':    '+', 'dist4':   -888, 'scaf5':    929, 'strand5':    '+', 'dist5':   2544},
        {'from':  40554, 'from_side':    'r', 'length':      3, 'scaf1':  53480, 'strand1':    '-', 'dist1':      0, 'scaf2':   2722, 'strand2':    '+', 'dist2':      0},
        {'from':  41636, 'from_side':    'l', 'length':      2, 'scaf1':    115, 'strand1':    '+', 'dist1':   1932},
        {'from':  41636, 'from_side':    'r', 'length':      4, 'scaf1': 112333, 'strand1':    '-', 'dist1':   -710, 'scaf2':  13455, 'strand2':    '+', 'dist2':    -43, 'scaf3':     69, 'strand3':    '-', 'dist3':    -43},        {'from':  47404, 'from_side':    'l', 'length':      3, 'scaf1':  30179, 'strand1':    '-', 'dist1':    -32, 'scaf2': 109207, 'strand2':    '-', 'dist2':    -40},
        {'from':  47404, 'from_side':    'l', 'length':      3, 'scaf1':  30179, 'strand1':    '-', 'dist1':    -32, 'scaf2': 109207, 'strand2':    '-', 'dist2':    -40},
        {'from':  47404, 'from_side':    'l', 'length':      3, 'scaf1':  33144, 'strand1':    '+', 'dist1':  -4062, 'scaf2':  26855, 'strand2':    '+', 'dist2':    -40},
        {'from':  47404, 'from_side':    'r', 'length':      2, 'scaf1':     69, 'strand1':    '+', 'dist1':    -43},
        {'from':  49093, 'from_side':    'l', 'length':      3, 'scaf1':  51660, 'strand1':    '+', 'dist1':    -14, 'scaf2': 101215, 'strand2':    '+', 'dist2':      5},
        {'from':  49093, 'from_side':    'l', 'length':      5, 'scaf1':  56987, 'strand1':    '+', 'dist1':      4, 'scaf2': 101215, 'strand2':    '+', 'dist2':    -10, 'scaf3':  56740, 'strand3':    '+', 'dist3':    469, 'scaf4':  96716, 'strand4':    '-', 'dist4':    -45},
        {'from':  49093, 'from_side':    'r', 'length':      3, 'scaf1':  30214, 'strand1':    '-', 'dist1':     17, 'scaf2': 109207, 'strand2':    '+', 'dist2':     -6},
        {'from':  49093, 'from_side':    'r', 'length':      2, 'scaf1': 109207, 'strand1':    '+', 'dist1':    780},
        {'from':  47516, 'from_side':    'l', 'length':      3, 'scaf1': 101215, 'strand1':    '-', 'dist1':    453, 'scaf2':  51660, 'strand2':    '-', 'dist2':      5},
        {'from':  47516, 'from_side':    'r', 'length':      3, 'scaf1':  96716, 'strand1':    '-', 'dist1':    -45, 'scaf2': 118892, 'strand2':    '+', 'dist2':    -38},
        {'from':  51660, 'from_side':    'l', 'length':      4, 'scaf1':  49093, 'strand1':    '+', 'dist1':    -14, 'scaf2':  30214, 'strand2':    '-', 'dist2':     17, 'scaf3': 109207, 'strand3':    '+', 'dist3':     -6},
        {'from':  51660, 'from_side':    'r', 'length':      5, 'scaf1': 101215, 'strand1':    '+', 'dist1':      5, 'scaf2':  47516, 'strand2':    '+', 'dist2':    453, 'scaf3':  96716, 'strand3':    '-', 'dist3':    -45, 'scaf4': 118892, 'strand4':    '+', 'dist4':    -38},
        {'from':  53480, 'from_side':    'l', 'length':      2, 'scaf1':   2722, 'strand1':    '+', 'dist1':      0},
        {'from':  53480, 'from_side':    'l', 'length':      2, 'scaf1': 101373, 'strand1':    '-', 'dist1':  -1383},
        {'from':  53480, 'from_side':    'r', 'length':      3, 'scaf1':  31749, 'strand1':    '+', 'dist1':   -646, 'scaf2': 101373, 'strand2':    '+', 'dist2':   -350},
        {'from':  53480, 'from_side':    'r', 'length':      2, 'scaf1':  40554, 'strand1':    '-', 'dist1':      0},
        {'from':  56740, 'from_side':    'l', 'length':      5, 'scaf1': 101215, 'strand1':    '-', 'dist1':    469, 'scaf2':  56987, 'strand2':    '-', 'dist2':    -10, 'scaf3':  49093, 'strand3':    '+', 'dist3':      4, 'scaf4': 109207, 'strand4':    '+', 'dist4':    780},
        {'from':  56740, 'from_side':    'r', 'length':      3, 'scaf1':  96716, 'strand1':    '-', 'dist1':    -45, 'scaf2': 118890, 'strand2':    '+', 'dist2':     -3},
        {'from':  56987, 'from_side':    'l', 'length':      3, 'scaf1':  49093, 'strand1':    '+', 'dist1':      4, 'scaf2': 109207, 'strand2':    '+', 'dist2':    780},
        {'from':  56987, 'from_side':    'r', 'length':      4, 'scaf1': 101215, 'strand1':    '+', 'dist1':    -10, 'scaf2':  56740, 'strand2':    '+', 'dist2':    469, 'scaf3':  96716, 'strand3':    '-', 'dist3':    -45},
        {'from':  58443, 'from_side':    'r', 'length':      2, 'scaf1': 107486, 'strand1':    '+', 'dist1':    -83},
        {'from':  70951, 'from_side':    'l', 'length':      5, 'scaf1': 101373, 'strand1':    '+', 'dist1':      0, 'scaf2': 107486, 'strand2':    '-', 'dist2':  -1922, 'scaf3': 107485, 'strand3':    '-', 'dist3':      0, 'scaf4':    372, 'strand4':    '-', 'dist4':  -1436},
        {'from':  71091, 'from_side':    'l', 'length':      2, 'scaf1':     44, 'strand1':    '-', 'dist1':      1},
        {'from':  71091, 'from_side':    'l', 'length':      3, 'scaf1':   1306, 'strand1':    '+', 'dist1':    -45, 'scaf2':     44, 'strand2':    '-', 'dist2':    -43},
        {'from':  71091, 'from_side':    'r', 'length':      3, 'scaf1':  32229, 'strand1':    '-', 'dist1':      3, 'scaf2':  76860, 'strand2':    '+', 'dist2':   -456},
        {'from':  71091, 'from_side':    'r', 'length':      2, 'scaf1':  76860, 'strand1':    '+', 'dist1':   2134},
        {'from':  76860, 'from_side':    'l', 'length':      3, 'scaf1':  32229, 'strand1':    '+', 'dist1':   -456, 'scaf2':  71091, 'strand2':    '-', 'dist2':      3},
        {'from':  76860, 'from_side':    'l', 'length':      2, 'scaf1':  71091, 'strand1':    '-', 'dist1':   2134},
        {'from':  76860, 'from_side':    'r', 'length':      3, 'scaf1':  12896, 'strand1':    '+', 'dist1':    -46, 'scaf2':  10723, 'strand2':    '+', 'dist2':    -43},
        {'from':  76860, 'from_side':    'r', 'length':      3, 'scaf1':  12910, 'strand1':    '+', 'dist1':    -42, 'scaf2':  10723, 'strand2':    '+', 'dist2':    -43},
        {'from':  96716, 'from_side':    'l', 'length':      2, 'scaf1': 118890, 'strand1':    '+', 'dist1':     -3},
        {'from':  96716, 'from_side':    'l', 'length':      2, 'scaf1': 118892, 'strand1':    '+', 'dist1':    -38},
        {'from':  96716, 'from_side':    'r', 'length':      4, 'scaf1':  47516, 'strand1':    '-', 'dist1':    -45, 'scaf2': 101215, 'strand2':    '-', 'dist2':    453, 'scaf3':  51660, 'strand3':    '-', 'dist3':      5},
        {'from':  96716, 'from_side':    'r', 'length':      5, 'scaf1':  56740, 'strand1':    '-', 'dist1':    -45, 'scaf2': 101215, 'strand2':    '-', 'dist2':    469, 'scaf3':  56987, 'strand3':    '-', 'dist3':    -10, 'scaf4':  49093, 'strand4':    '+', 'dist4':      4},
        {'from':  99004, 'from_side':    'r', 'length':      7, 'scaf1': 107486, 'strand1':    '-', 'dist1':    838, 'scaf2': 107485, 'strand2':    '-', 'dist2':      0, 'scaf3': 107484, 'strand3':    '-', 'dist3':      0, 'scaf4': 107483, 'strand4':    '-', 'dist4':      0, 'scaf5':  99342, 'strand5':    '-', 'dist5':    -44, 'scaf6':   9344, 'strand6':    '+', 'dist6':     73},
        {'from':  99341, 'from_side':    'l', 'length':      3, 'scaf1':   9344, 'strand1':    '+', 'dist1':      0, 'scaf2': 118890, 'strand2':    '-', 'dist2':    -13},
        {'from':  99341, 'from_side':    'r', 'length':      3, 'scaf1':  99342, 'strand1':    '+', 'dist1':      0, 'scaf2':     44, 'strand2':    '+', 'dist2':    -44},
        {'from':  99342, 'from_side':    'l', 'length':      2, 'scaf1':   9344, 'strand1':    '+', 'dist1':     73},
        {'from':  99342, 'from_side':    'l', 'length':      4, 'scaf1':  99341, 'strand1':    '-', 'dist1':      0, 'scaf2':   9344, 'strand2':    '+', 'dist2':      0, 'scaf3': 118890, 'strand3':    '-', 'dist3':    -13},
        {'from':  99342, 'from_side':    'r', 'length':      2, 'scaf1':     44, 'strand1':    '+', 'dist1':    -44},
        {'from':  99342, 'from_side':    'r', 'length':      6, 'scaf1': 107483, 'strand1':    '+', 'dist1':    -44, 'scaf2': 107484, 'strand2':    '+', 'dist2':      0, 'scaf3': 107485, 'strand3':    '+', 'dist3':      0, 'scaf4': 107486, 'strand4':    '+', 'dist4':      0, 'scaf5':  99004, 'strand5':    '-', 'dist5':    838},
        {'from': 101215, 'from_side':    'l', 'length':      5, 'scaf1':  51660, 'strand1':    '-', 'dist1':      5, 'scaf2':  49093, 'strand2':    '+', 'dist2':    -14, 'scaf3':  30214, 'strand3':    '-', 'dist3':     17, 'scaf4': 109207, 'strand4':    '+', 'dist4':     -6},
        {'from': 101215, 'from_side':    'l', 'length':      4, 'scaf1':  56987, 'strand1':    '-', 'dist1':    -10, 'scaf2':  49093, 'strand2':    '+', 'dist2':      4, 'scaf3': 109207, 'strand3':    '+', 'dist3':    780},
        {'from': 101215, 'from_side':    'r', 'length':      4, 'scaf1':  47516, 'strand1':    '+', 'dist1':    453, 'scaf2':  96716, 'strand2':    '-', 'dist2':    -45, 'scaf3': 118892, 'strand3':    '+', 'dist3':    -38},
        {'from': 101215, 'from_side':    'r', 'length':      4, 'scaf1':  56740, 'strand1':    '+', 'dist1':    469, 'scaf2':  96716, 'strand2':    '-', 'dist2':    -45, 'scaf3': 118890, 'strand3':    '+', 'dist3':     -3},
        {'from': 101373, 'from_side':    'l', 'length':      3, 'scaf1':  31749, 'strand1':    '-', 'dist1':   -350, 'scaf2':  31756, 'strand2':    '-', 'dist2':     -1},
        {'from': 101373, 'from_side':    'l', 'length':      4, 'scaf1':  31749, 'strand1':    '-', 'dist1':   -350, 'scaf2':  53480, 'strand2':    '-', 'dist2':   -646, 'scaf3': 101373, 'strand3':    '-', 'dist3':  -1383},
        {'from': 101373, 'from_side':    'l', 'length':      2, 'scaf1':  70951, 'strand1':    '+', 'dist1':      0},
        {'from': 101373, 'from_side':    'r', 'length':      4, 'scaf1':  53480, 'strand1':    '+', 'dist1':  -1383, 'scaf2':  31749, 'strand2':    '+', 'dist2':   -646, 'scaf3': 101373, 'strand3':    '+', 'dist3':   -350},
        {'from': 101373, 'from_side':    'r', 'length':      4, 'scaf1': 107486, 'strand1':    '-', 'dist1':  -1922, 'scaf2': 107485, 'strand2':    '-', 'dist2':      0, 'scaf3':    372, 'strand3':    '-', 'dist3':  -1436},
        {'from': 101373, 'from_side':    'r', 'length':      5, 'scaf1': 107486, 'strand1':    '-', 'dist1':  -1922, 'scaf2': 107485, 'strand2':    '-', 'dist2':      0, 'scaf3': 107484, 'strand3':    '-', 'dist3':      0, 'scaf4': 107483, 'strand4':    '-', 'dist4':      0},
        {'from': 107483, 'from_side':    'l', 'length':      3, 'scaf1':  99342, 'strand1':    '-', 'dist1':    -44, 'scaf2':   9344, 'strand2':    '+', 'dist2':     73},
        {'from': 107483, 'from_side':    'r', 'length':      5, 'scaf1': 107484, 'strand1':    '+', 'dist1':      0, 'scaf2': 107485, 'strand2':    '+', 'dist2':      0, 'scaf3': 107486, 'strand3':    '+', 'dist3':      0, 'scaf4':  99004, 'strand4':    '-', 'dist4':    838},
        {'from': 107483, 'from_side':    'r', 'length':      5, 'scaf1': 107484, 'strand1':    '+', 'dist1':      0, 'scaf2': 107485, 'strand2':    '+', 'dist2':      0, 'scaf3': 107486, 'strand3':    '+', 'dist3':      0, 'scaf4': 101373, 'strand4':    '-', 'dist4':  -1922},
        {'from': 107484, 'from_side':    'l', 'length':      4, 'scaf1': 107483, 'strand1':    '-', 'dist1':      0, 'scaf2':  99342, 'strand2':    '-', 'dist2':    -44, 'scaf3':   9344, 'strand3':    '+', 'dist3':     73},
        {'from': 107484, 'from_side':    'l', 'length':      3, 'scaf1': 117303, 'strand1':    '+', 'dist1':   -888, 'scaf2':    929, 'strand2':    '+', 'dist2':   2544},
        {'from': 107484, 'from_side':    'r', 'length':      4, 'scaf1': 107485, 'strand1':    '+', 'dist1':      0, 'scaf2': 107486, 'strand2':    '+', 'dist2':      0, 'scaf3':  33994, 'strand3':    '+', 'dist3':    -44},
        {'from': 107484, 'from_side':    'r', 'length':      4, 'scaf1': 107485, 'strand1':    '+', 'dist1':      0, 'scaf2': 107486, 'strand2':    '+', 'dist2':      0, 'scaf3':  99004, 'strand3':    '-', 'dist3':    838},
        {'from': 107484, 'from_side':    'r', 'length':      4, 'scaf1': 107485, 'strand1':    '+', 'dist1':      0, 'scaf2': 107486, 'strand2':    '+', 'dist2':      0, 'scaf3': 101373, 'strand3':    '-', 'dist3':  -1922},
        {'from': 107485, 'from_side':    'l', 'length':      2, 'scaf1':    372, 'strand1':    '-', 'dist1':  -1436},
        {'from': 107485, 'from_side':    'l', 'length':      5, 'scaf1': 107484, 'strand1':    '-', 'dist1':      0, 'scaf2': 107483, 'strand2':    '-', 'dist2':      0, 'scaf3':  99342, 'strand3':    '-', 'dist3':    -44, 'scaf4':   9344, 'strand4':    '+', 'dist4':     73},
        {'from': 107485, 'from_side':    'l', 'length':      4, 'scaf1': 107484, 'strand1':    '-', 'dist1':      0, 'scaf2': 117303, 'strand2':    '+', 'dist2':   -888, 'scaf3':    929, 'strand3':    '+', 'dist3':   2544},
        {'from': 107485, 'from_side':    'r', 'length':      3, 'scaf1': 107486, 'strand1':    '+', 'dist1':      0, 'scaf2':  33994, 'strand2':    '+', 'dist2':    -44},
        {'from': 107485, 'from_side':    'r', 'length':      3, 'scaf1': 107486, 'strand1':    '+', 'dist1':      0, 'scaf2':  99004, 'strand2':    '-', 'dist2':    838},
        {'from': 107485, 'from_side':    'r', 'length':      4, 'scaf1': 107486, 'strand1':    '+', 'dist1':      0, 'scaf2': 101373, 'strand2':    '-', 'dist2':  -1922, 'scaf3':  31749, 'strand3':    '-', 'dist3':   -350},
        {'from': 107485, 'from_side':    'r', 'length':      4, 'scaf1': 107486, 'strand1':    '+', 'dist1':      0, 'scaf2': 101373, 'strand2':    '-', 'dist2':  -1922, 'scaf3':  70951, 'strand3':    '+', 'dist3':      0},
        {'from': 107486, 'from_side':    'l', 'length':      2, 'scaf1':  58443, 'strand1':    '-', 'dist1':    -83},
        {'from': 107486, 'from_side':    'l', 'length':      3, 'scaf1': 107485, 'strand1':    '-', 'dist1':      0, 'scaf2':    372, 'strand2':    '-', 'dist2':  -1436},
        {'from': 107486, 'from_side':    'l', 'length':      6, 'scaf1': 107485, 'strand1':    '-', 'dist1':      0, 'scaf2': 107484, 'strand2':    '-', 'dist2':      0, 'scaf3': 107483, 'strand3':    '-', 'dist3':      0, 'scaf4':  99342, 'strand4':    '-', 'dist4':    -44, 'scaf5':   9344, 'strand5':    '+', 'dist5':     73},
        {'from': 107486, 'from_side':    'l', 'length':      5, 'scaf1': 107485, 'strand1':    '-', 'dist1':      0, 'scaf2': 107484, 'strand2':    '-', 'dist2':      0, 'scaf3': 117303, 'strand3':    '+', 'dist3':   -888, 'scaf4':    929, 'strand4':    '+', 'dist4':   2544},
        {'from': 107486, 'from_side':    'r', 'length':      2, 'scaf1':  33994, 'strand1':    '+', 'dist1':    -44},
        {'from': 107486, 'from_side':    'r', 'length':      2, 'scaf1':  99004, 'strand1':    '-', 'dist1':    838},
        {'from': 107486, 'from_side':    'r', 'length':      3, 'scaf1': 101373, 'strand1':    '-', 'dist1':  -1922, 'scaf2':  31749, 'strand2':    '-', 'dist2':   -350},
        {'from': 107486, 'from_side':    'r', 'length':      3, 'scaf1': 101373, 'strand1':    '-', 'dist1':  -1922, 'scaf2':  70951, 'strand2':    '+', 'dist2':      0},
        {'from': 109207, 'from_side':    'l', 'length':      5, 'scaf1':  30214, 'strand1':    '+', 'dist1':     -6, 'scaf2':  49093, 'strand2':    '-', 'dist2':     17, 'scaf3':  51660, 'strand3':    '+', 'dist3':    -14, 'scaf4': 101215, 'strand4':    '+', 'dist4':      5},
        {'from': 109207, 'from_side':    'l', 'length':      5, 'scaf1':  49093, 'strand1':    '-', 'dist1':    780, 'scaf2':  56987, 'strand2':    '+', 'dist2':      4, 'scaf3': 101215, 'strand3':    '+', 'dist3':    -10, 'scaf4':  56740, 'strand4':    '+', 'dist4':    469},
        {'from': 109207, 'from_side':    'r', 'length':      3, 'scaf1':  26855, 'strand1':    '-', 'dist1':    -43, 'scaf2':  33144, 'strand2':    '-', 'dist2':    -40},
        {'from': 109207, 'from_side':    'r', 'length':      3, 'scaf1':  30179, 'strand1':    '+', 'dist1':    -40, 'scaf2':  47404, 'strand2':    '+', 'dist2':    -32},
        {'from': 110827, 'from_side':    'r', 'length':      2, 'scaf1':     69, 'strand1':    '+', 'dist1':    -43},
        {'from': 112333, 'from_side':    'l', 'length':      3, 'scaf1':  13434, 'strand1':    '+', 'dist1':     38, 'scaf2':     69, 'strand2':    '-', 'dist2':    -41},
        {'from': 112333, 'from_side':    'l', 'length':      3, 'scaf1':  13455, 'strand1':    '+', 'dist1':    -43, 'scaf2':     69, 'strand2':    '-', 'dist2':    -43},
        {'from': 112333, 'from_side':    'r', 'length':      4, 'scaf1':    114, 'strand1':    '+', 'dist1':   -819, 'scaf2':    115, 'strand2':    '+', 'dist2':   1131, 'scaf3': 115803, 'strand3':    '+', 'dist3':    885},
        {'from': 112333, 'from_side':    'r', 'length':      3, 'scaf1':  41636, 'strand1':    '-', 'dist1':   -710, 'scaf2':    115, 'strand2':    '+', 'dist2':   1932},
        {'from': 115803, 'from_side':    'l', 'length':      4, 'scaf1':    115, 'strand1':    '-', 'dist1':    885, 'scaf2':    114, 'strand2':    '-', 'dist2':   1131, 'scaf3': 112333, 'strand3':    '-', 'dist3':   -819},
        {'from': 115803, 'from_side':    'l', 'length':      3, 'scaf1':  15177, 'strand1':    '-', 'dist1':      5, 'scaf2':    115, 'strand2':    '-', 'dist2':    -42},
        {'from': 117303, 'from_side':    'l', 'length':      5, 'scaf1': 107484, 'strand1':    '+', 'dist1':   -888, 'scaf2': 107485, 'strand2':    '+', 'dist2':      0, 'scaf3': 107486, 'strand3':    '+', 'dist3':      0, 'scaf4':  33994, 'strand4':    '+', 'dist4':    -44},
        {'from': 117303, 'from_side':    'r', 'length':      2, 'scaf1':    929, 'strand1':    '+', 'dist1':   2544},
        {'from': 117304, 'from_side':    'r', 'length':      2, 'scaf1':    929, 'strand1':    '+', 'dist1':   2489},
        {'from': 118890, 'from_side':    'l', 'length':      4, 'scaf1':  96716, 'strand1':    '+', 'dist1':     -3, 'scaf2':  56740, 'strand2':    '-', 'dist2':    -45, 'scaf3': 101215, 'strand3':    '-', 'dist3':    469},
        {'from': 118890, 'from_side':    'r', 'length':      5, 'scaf1':   9344, 'strand1':    '-', 'dist1':    -13, 'scaf2':  99341, 'strand2':    '+', 'dist2':      0, 'scaf3':  99342, 'strand3':    '+', 'dist3':      0, 'scaf4':     44, 'strand4':    '+', 'dist4':    -44},
        {'from': 118892, 'from_side':    'l', 'length':      5, 'scaf1':  96716, 'strand1':    '+', 'dist1':    -38, 'scaf2':  47516, 'strand2':    '-', 'dist2':    -45, 'scaf3': 101215, 'strand3':    '-', 'dist3':    453, 'scaf4':  51660, 'strand4':    '-', 'dist4':      5},
        {'from': 118892, 'from_side':    'r', 'length':      2, 'scaf1':   9344, 'strand1':    '-', 'dist1':      8}
        ]) )
    scaf_bridges.append( pd.DataFrame([
        {'from':     44, 'from_side':    'l', 'to':  99342, 'to_side':    'r', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     19, 'min_dist':    -49, 'max_dist':    -33, 'probability': 0.273725, 'to_alt':      2, 'from_alt':      1},
        {'from':     44, 'from_side':    'r', 'to':   1306, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     12, 'min_dist':    -46, 'max_dist':    -39, 'probability': 0.064232, 'to_alt':      1, 'from_alt':      2},
        {'from':     44, 'from_side':    'r', 'to':  71091, 'to_side':    'l', 'mean_dist':      1, 'mapq':  60060, 'bcount':      5, 'min_dist':     -1, 'max_dist':      6, 'probability': 0.007368, 'to_alt':      2, 'from_alt':      2},
        {'from':     69, 'from_side':    'l', 'to':  47404, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     15, 'min_dist':    -49, 'max_dist':    -38, 'probability': 0.129976, 'to_alt':      1, 'from_alt':      2},
        {'from':     69, 'from_side':    'l', 'to': 110827, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     24, 'min_dist':    -50, 'max_dist':    -16, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':     69, 'from_side':    'r', 'to':  13434, 'to_side':    'r', 'mean_dist':    -41, 'mapq':  60060, 'bcount':     18, 'min_dist':    -51, 'max_dist':    -31, 'probability': 0.231835, 'to_alt':      1, 'from_alt':      2},
        {'from':     69, 'from_side':    'r', 'to':  13455, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     27, 'min_dist':    -60, 'max_dist':    -17, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':    114, 'from_side':    'l', 'to': 112333, 'to_side':    'r', 'mean_dist':   -819, 'mapq':  60060, 'bcount':     22, 'min_dist':   -931, 'max_dist':   -785, 'probability': 0.417654, 'to_alt':      2, 'from_alt':      1},
        {'from':    114, 'from_side':    'r', 'to':    115, 'to_side':    'l', 'mean_dist':   1131, 'mapq':  60060, 'bcount':     21, 'min_dist':   1080, 'max_dist':   1263, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':    115, 'from_side':    'l', 'to':    114, 'to_side':    'r', 'mean_dist':   1131, 'mapq':  60060, 'bcount':     21, 'min_dist':   1080, 'max_dist':   1263, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':    115, 'from_side':    'l', 'to':  41636, 'to_side':    'l', 'mean_dist':   1932, 'mapq':  60060, 'bcount':     18, 'min_dist':   1811, 'max_dist':   2087, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':    115, 'from_side':    'r', 'to':  15177, 'to_side':    'l', 'mean_dist':    -42, 'mapq':  60060, 'bcount':     14, 'min_dist':    -52, 'max_dist':    -20, 'probability': 0.104244, 'to_alt':      1, 'from_alt':      2},
        {'from':    115, 'from_side':    'r', 'to': 115803, 'to_side':    'l', 'mean_dist':    885, 'mapq':  60060, 'bcount':     15, 'min_dist':    818, 'max_dist':   1120, 'probability': 0.209365, 'to_alt':      2, 'from_alt':      2},
        {'from':    372, 'from_side':    'r', 'to': 107485, 'to_side':    'l', 'mean_dist':  -1436, 'mapq':  60060, 'bcount':     14, 'min_dist':  -1605, 'max_dist':  -1350, 'probability': 0.169655, 'to_alt':      2, 'from_alt':      1},
        {'from':    674, 'from_side':    'l', 'to':   2799, 'to_side':    'l', 'mean_dist':     60, 'mapq':  60060, 'bcount':     19, 'min_dist':     43, 'max_dist':    107, 'probability': 0.273725, 'to_alt':      2, 'from_alt':      1},
        {'from':    674, 'from_side':    'r', 'to':  20727, 'to_side':    'r', 'mean_dist':    -68, 'mapq':  60060, 'bcount':     10, 'min_dist':    -79, 'max_dist':    -63, 'probability': 0.037322, 'to_alt':      2, 'from_alt':      2},
        {'from':    674, 'from_side':    'r', 'to':  20727, 'to_side':    'r', 'mean_dist':    -18, 'mapq':  60060, 'bcount':      8, 'min_dist':    -22, 'max_dist':    -11, 'probability': 0.020422, 'to_alt':      2, 'from_alt':      2},
        {'from':    929, 'from_side':    'l', 'to': 117303, 'to_side':    'r', 'mean_dist':   2544, 'mapq':  60060, 'bcount':      7, 'min_dist':   2451, 'max_dist':   2631, 'probability': 0.076700, 'to_alt':      1, 'from_alt':      2},
        {'from':    929, 'from_side':    'l', 'to': 117304, 'to_side':    'r', 'mean_dist':   2489, 'mapq':  60060, 'bcount':      2, 'min_dist':   2450, 'max_dist':   2534, 'probability': 0.008611, 'to_alt':      1, 'from_alt':      2},
        {'from':    929, 'from_side':    'r', 'to':   2725, 'to_side':    'l', 'mean_dist':   1205, 'mapq':  60060, 'bcount':      6, 'min_dist':   1175, 'max_dist':   1229, 'probability': 0.016554, 'to_alt':      2, 'from_alt':      2},
        {'from':    929, 'from_side':    'r', 'to':  14096, 'to_side':    'l', 'mean_dist':    -44, 'mapq':  60060, 'bcount':      9, 'min_dist':    -49, 'max_dist':    -34, 'probability': 0.027818, 'to_alt':      1, 'from_alt':      2},
        {'from':   1306, 'from_side':    'l', 'to':  71091, 'to_side':    'l', 'mean_dist':    -45, 'mapq':  60060, 'bcount':      8, 'min_dist':    -49, 'max_dist':    -40, 'probability': 0.020422, 'to_alt':      2, 'from_alt':      1},
        {'from':   1306, 'from_side':    'r', 'to':     44, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     12, 'min_dist':    -46, 'max_dist':    -39, 'probability': 0.064232, 'to_alt':      2, 'from_alt':      1},
        {'from':   2722, 'from_side':    'l', 'to':  53480, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     16, 'min_dist':      0, 'max_dist':      0, 'probability': 0.159802, 'to_alt':      2, 'from_alt':      1},
        {'from':   2725, 'from_side':    'l', 'to':    929, 'to_side':    'r', 'mean_dist':   1205, 'mapq':  60060, 'bcount':      6, 'min_dist':   1175, 'max_dist':   1229, 'probability': 0.016554, 'to_alt':      2, 'from_alt':      2},
        {'from':   2725, 'from_side':    'l', 'to':  14096, 'to_side':    'r', 'mean_dist':    642, 'mapq':  60060, 'bcount':      8, 'min_dist':    586, 'max_dist':    773, 'probability': 0.033108, 'to_alt':      1, 'from_alt':      2},
        {'from':   2725, 'from_side':    'r', 'to':  11659, 'to_side':    'r', 'mean_dist':    136, 'mapq':  60060, 'bcount':      7, 'min_dist':    129, 'max_dist':    146, 'probability': 0.014765, 'to_alt':      1, 'from_alt':      3},
        {'from':   2725, 'from_side':    'r', 'to':  13029, 'to_side':    'l', 'mean_dist':    143, 'mapq':  60060, 'bcount':      3, 'min_dist':    126, 'max_dist':    201, 'probability': 0.003454, 'to_alt':      1, 'from_alt':      3},
        {'from':   2725, 'from_side':    'r', 'to':  13591, 'to_side':    'r', 'mean_dist':    734, 'mapq':  60060, 'bcount':      5, 'min_dist':    684, 'max_dist':    797, 'probability': 0.011373, 'to_alt':      3, 'from_alt':      3},
        {'from':   2799, 'from_side':    'l', 'to':    674, 'to_side':    'l', 'mean_dist':     60, 'mapq':  60060, 'bcount':     19, 'min_dist':     43, 'max_dist':    107, 'probability': 0.273725, 'to_alt':      1, 'from_alt':      2},
        {'from':   2885, 'from_side':    'l', 'to':  20727, 'to_side':    'l', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     14, 'min_dist':    -55, 'max_dist':    -35, 'probability': 0.104244, 'to_alt':      2, 'from_alt':      2},
        {'from':   2885, 'from_side':    'l', 'to':  20727, 'to_side':    'l', 'mean_dist':    245, 'mapq':  60060, 'bcount':     12, 'min_dist':    220, 'max_dist':    324, 'probability': 0.064232, 'to_alt':      2, 'from_alt':      2},
        {'from':   2885, 'from_side':    'r', 'to':  13452, 'to_side':    'r', 'mean_dist':    -45, 'mapq':  60060, 'bcount':      6, 'min_dist':    -47, 'max_dist':    -43, 'probability': 0.010512, 'to_alt':      1, 'from_alt':      2},
        {'from':   2885, 'from_side':    'r', 'to':  15812, 'to_side':    'l', 'mean_dist':    -44, 'mapq':  60060, 'bcount':      7, 'min_dist':    -47, 'max_dist':    -38, 'probability': 0.014765, 'to_alt':      1, 'from_alt':      2},
        {'from':   9344, 'from_side':    'l', 'to':  99341, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     20, 'min_dist':      0, 'max_dist':      0, 'probability': 0.319050, 'to_alt':      1, 'from_alt':      2},
        {'from':   9344, 'from_side':    'l', 'to':  99342, 'to_side':    'l', 'mean_dist':     73, 'mapq':  60060, 'bcount':     12, 'min_dist':     64, 'max_dist':     80, 'probability': 0.064232, 'to_alt':      2, 'from_alt':      2},
        {'from':   9344, 'from_side':    'r', 'to': 118890, 'to_side':    'r', 'mean_dist':    -13, 'mapq':  60060, 'bcount':     25, 'min_dist':    -20, 'max_dist':     39, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':   9344, 'from_side':    'r', 'to': 118892, 'to_side':    'r', 'mean_dist':      8, 'mapq':  60060, 'bcount':     20, 'min_dist':      0, 'max_dist':     35, 'probability': 0.319050, 'to_alt':      1, 'from_alt':      2},
        {'from':  10723, 'from_side':    'l', 'to':  12896, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':      7, 'min_dist':    -47, 'max_dist':    -24, 'probability': 0.014765, 'to_alt':      1, 'from_alt':      2},
        {'from':  10723, 'from_side':    'l', 'to':  12910, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':      7, 'min_dist':    -47, 'max_dist':    -36, 'probability': 0.014765, 'to_alt':      1, 'from_alt':      2},
        {'from':  10723, 'from_side':    'r', 'to':  13452, 'to_side':    'l', 'mean_dist':    -41, 'mapq':  60060, 'bcount':      5, 'min_dist':    -46, 'max_dist':    -30, 'probability': 0.007368, 'to_alt':      1, 'from_alt':      2},
        {'from':  10723, 'from_side':    'r', 'to':  15812, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     10, 'min_dist':    -47, 'max_dist':    -33, 'probability': 0.037322, 'to_alt':      1, 'from_alt':      2},
        {'from':  11659, 'from_side':    'l', 'to':  13591, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':      7, 'min_dist':    -47, 'max_dist':    -36, 'probability': 0.014765, 'to_alt':      3, 'from_alt':      1},
        {'from':  11659, 'from_side':    'r', 'to':   2725, 'to_side':    'r', 'mean_dist':    136, 'mapq':  60060, 'bcount':      7, 'min_dist':    129, 'max_dist':    146, 'probability': 0.014765, 'to_alt':      3, 'from_alt':      1},
        {'from':  12896, 'from_side':    'l', 'to':  76860, 'to_side':    'r', 'mean_dist':    -46, 'mapq':  60060, 'bcount':      6, 'min_dist':    -51, 'max_dist':    -43, 'probability': 0.010512, 'to_alt':      2, 'from_alt':      1},
        {'from':  12896, 'from_side':    'r', 'to':  10723, 'to_side':    'l', 'mean_dist':    -43, 'mapq':  60060, 'bcount':      7, 'min_dist':    -47, 'max_dist':    -24, 'probability': 0.014765, 'to_alt':      2, 'from_alt':      1},
        {'from':  12910, 'from_side':    'l', 'to':  76860, 'to_side':    'r', 'mean_dist':    -42, 'mapq':  60060, 'bcount':      5, 'min_dist':    -52, 'max_dist':    -30, 'probability': 0.007368, 'to_alt':      2, 'from_alt':      1},
        {'from':  12910, 'from_side':    'r', 'to':  10723, 'to_side':    'l', 'mean_dist':    -43, 'mapq':  60060, 'bcount':      7, 'min_dist':    -47, 'max_dist':    -36, 'probability': 0.014765, 'to_alt':      2, 'from_alt':      1},
        {'from':  13029, 'from_side':    'l', 'to':   2725, 'to_side':    'r', 'mean_dist':    143, 'mapq':  60060, 'bcount':      3, 'min_dist':    126, 'max_dist':    201, 'probability': 0.003454, 'to_alt':      3, 'from_alt':      1},
        {'from':  13029, 'from_side':    'r', 'to':  13591, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':      3, 'min_dist':    -47, 'max_dist':    -39, 'probability': 0.003454, 'to_alt':      3, 'from_alt':      1},
        {'from':  13434, 'from_side':    'l', 'to': 112333, 'to_side':    'l', 'mean_dist':     38, 'mapq':  60060, 'bcount':     18, 'min_dist':     32, 'max_dist':     52, 'probability': 0.231835, 'to_alt':      2, 'from_alt':      1},
        {'from':  13434, 'from_side':    'r', 'to':     69, 'to_side':    'r', 'mean_dist':    -41, 'mapq':  60060, 'bcount':     18, 'min_dist':    -51, 'max_dist':    -31, 'probability': 0.231835, 'to_alt':      2, 'from_alt':      1},
        {'from':  13452, 'from_side':    'l', 'to':  10723, 'to_side':    'r', 'mean_dist':    -41, 'mapq':  60060, 'bcount':      5, 'min_dist':    -46, 'max_dist':    -30, 'probability': 0.007368, 'to_alt':      2, 'from_alt':      1},
        {'from':  13452, 'from_side':    'r', 'to':   2885, 'to_side':    'r', 'mean_dist':    -45, 'mapq':  60060, 'bcount':      6, 'min_dist':    -47, 'max_dist':    -43, 'probability': 0.010512, 'to_alt':      2, 'from_alt':      1},
        {'from':  13455, 'from_side':    'l', 'to': 112333, 'to_side':    'l', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     29, 'min_dist':    -49, 'max_dist':    -34, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  13455, 'from_side':    'r', 'to':     69, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     27, 'min_dist':    -60, 'max_dist':    -17, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  13591, 'from_side':    'r', 'to':   2725, 'to_side':    'r', 'mean_dist':    734, 'mapq':  60060, 'bcount':      5, 'min_dist':    684, 'max_dist':    797, 'probability': 0.011373, 'to_alt':      3, 'from_alt':      3},
        {'from':  13591, 'from_side':    'r', 'to':  11659, 'to_side':    'l', 'mean_dist':    -43, 'mapq':  60060, 'bcount':      7, 'min_dist':    -47, 'max_dist':    -36, 'probability': 0.014765, 'to_alt':      1, 'from_alt':      3},
        {'from':  13591, 'from_side':    'r', 'to':  13029, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':      3, 'min_dist':    -47, 'max_dist':    -39, 'probability': 0.003454, 'to_alt':      1, 'from_alt':      3},
        {'from':  14096, 'from_side':    'l', 'to':    929, 'to_side':    'r', 'mean_dist':    -44, 'mapq':  60060, 'bcount':      9, 'min_dist':    -49, 'max_dist':    -34, 'probability': 0.027818, 'to_alt':      2, 'from_alt':      1},
        {'from':  14096, 'from_side':    'r', 'to':   2725, 'to_side':    'l', 'mean_dist':    642, 'mapq':  60060, 'bcount':      8, 'min_dist':    586, 'max_dist':    773, 'probability': 0.033108, 'to_alt':      2, 'from_alt':      1},
        {'from':  15177, 'from_side':    'l', 'to':    115, 'to_side':    'r', 'mean_dist':    -42, 'mapq':  60060, 'bcount':     14, 'min_dist':    -52, 'max_dist':    -20, 'probability': 0.104244, 'to_alt':      2, 'from_alt':      1},
        {'from':  15177, 'from_side':    'r', 'to': 115803, 'to_side':    'l', 'mean_dist':      5, 'mapq':  60060, 'bcount':     12, 'min_dist':      1, 'max_dist':     15, 'probability': 0.064232, 'to_alt':      2, 'from_alt':      1},
        {'from':  15812, 'from_side':    'l', 'to':   2885, 'to_side':    'r', 'mean_dist':    -44, 'mapq':  60060, 'bcount':      7, 'min_dist':    -47, 'max_dist':    -38, 'probability': 0.014765, 'to_alt':      2, 'from_alt':      1},
        {'from':  15812, 'from_side':    'r', 'to':  10723, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     10, 'min_dist':    -47, 'max_dist':    -33, 'probability': 0.037322, 'to_alt':      2, 'from_alt':      1},
        {'from':  20727, 'from_side':    'l', 'to':   2885, 'to_side':    'l', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     14, 'min_dist':    -55, 'max_dist':    -35, 'probability': 0.104244, 'to_alt':      2, 'from_alt':      2},
        {'from':  20727, 'from_side':    'l', 'to':   2885, 'to_side':    'l', 'mean_dist':    245, 'mapq':  60060, 'bcount':     12, 'min_dist':    220, 'max_dist':    324, 'probability': 0.064232, 'to_alt':      2, 'from_alt':      2},
        {'from':  20727, 'from_side':    'r', 'to':    674, 'to_side':    'r', 'mean_dist':    -68, 'mapq':  60060, 'bcount':     10, 'min_dist':    -79, 'max_dist':    -63, 'probability': 0.037322, 'to_alt':      2, 'from_alt':      2},
        {'from':  20727, 'from_side':    'r', 'to':    674, 'to_side':    'r', 'mean_dist':    -18, 'mapq':  60060, 'bcount':      8, 'min_dist':    -22, 'max_dist':    -11, 'probability': 0.020422, 'to_alt':      2, 'from_alt':      2},
        {'from':  26855, 'from_side':    'l', 'to':  33144, 'to_side':    'r', 'mean_dist':    -40, 'mapq':  60060, 'bcount':      4, 'min_dist':    -46, 'max_dist':    -31, 'probability': 0.005085, 'to_alt':      1, 'from_alt':      2},
        {'from':  26855, 'from_side':    'r', 'to': 109207, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     13, 'min_dist':    -51, 'max_dist':    -14, 'probability': 0.082422, 'to_alt':      2, 'from_alt':      1},
        {'from':  30179, 'from_side':    'l', 'to': 109207, 'to_side':    'r', 'mean_dist':    -40, 'mapq':  60060, 'bcount':      8, 'min_dist':    -47, 'max_dist':    -23, 'probability': 0.020422, 'to_alt':      2, 'from_alt':      1},
        {'from':  30179, 'from_side':    'r', 'to':  47404, 'to_side':    'l', 'mean_dist':    -32, 'mapq':  60060, 'bcount':     13, 'min_dist':    -39, 'max_dist':    -18, 'probability': 0.082422, 'to_alt':      2, 'from_alt':      1},
        {'from':  30214, 'from_side':    'l', 'to': 109207, 'to_side':    'l', 'mean_dist':     -6, 'mapq':  60060, 'bcount':     13, 'min_dist':    -10, 'max_dist':     21, 'probability': 0.082422, 'to_alt':      2, 'from_alt':      1},
        {'from':  30214, 'from_side':    'r', 'to':  49093, 'to_side':    'r', 'mean_dist':     17, 'mapq':  60060, 'bcount':     12, 'min_dist':     13, 'max_dist':     25, 'probability': 0.064232, 'to_alt':      2, 'from_alt':      1},
        {'from':  31749, 'from_side':    'l', 'to':  31756, 'to_side':    'r', 'mean_dist':     -1, 'mapq':  60060, 'bcount':     12, 'min_dist':     -8, 'max_dist':     26, 'probability': 0.064232, 'to_alt':      1, 'from_alt':      2},
        {'from':  31749, 'from_side':    'l', 'to':  53480, 'to_side':    'r', 'mean_dist':   -646, 'mapq':  60060, 'bcount':     18, 'min_dist':   -716, 'max_dist':   -614, 'probability': 0.231835, 'to_alt':      2, 'from_alt':      2},
        {'from':  31749, 'from_side':    'r', 'to': 101373, 'to_side':    'l', 'mean_dist':   -350, 'mapq':  60060, 'bcount':    109, 'min_dist':   -433, 'max_dist':   -288, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  31756, 'from_side':    'r', 'to':  31749, 'to_side':    'l', 'mean_dist':     -1, 'mapq':  60060, 'bcount':     12, 'min_dist':     -8, 'max_dist':     26, 'probability': 0.064232, 'to_alt':      2, 'from_alt':      1},
        {'from':  32229, 'from_side':    'l', 'to':  76860, 'to_side':    'l', 'mean_dist':   -456, 'mapq':  60060, 'bcount':      7, 'min_dist':   -527, 'max_dist':   -412, 'probability': 0.014765, 'to_alt':      2, 'from_alt':      1},
        {'from':  32229, 'from_side':    'r', 'to':  71091, 'to_side':    'r', 'mean_dist':      3, 'mapq':  60060, 'bcount':     12, 'min_dist':      1, 'max_dist':      7, 'probability': 0.064232, 'to_alt':      2, 'from_alt':      1},
        {'from':  33144, 'from_side':    'l', 'to':  47404, 'to_side':    'l', 'mean_dist':  -4062, 'mapq':  60060, 'bcount':      2, 'min_dist':  -4070, 'max_dist':  -4055, 'probability': 0.008611, 'to_alt':      2, 'from_alt':      1},
        {'from':  33144, 'from_side':    'r', 'to':  26855, 'to_side':    'l', 'mean_dist':    -40, 'mapq':  60060, 'bcount':      4, 'min_dist':    -46, 'max_dist':    -31, 'probability': 0.005085, 'to_alt':      2, 'from_alt':      1},
        {'from':  33994, 'from_side':    'l', 'to': 107486, 'to_side':    'r', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     25, 'min_dist':    -52, 'max_dist':    -15, 'probability': 0.500000, 'to_alt':      3, 'from_alt':      1},
        {'from':  40554, 'from_side':    'r', 'to':  53480, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     19, 'min_dist':      0, 'max_dist':      0, 'probability': 0.273725, 'to_alt':      2, 'from_alt':      1},
        {'from':  41636, 'from_side':    'l', 'to':    115, 'to_side':    'l', 'mean_dist':   1932, 'mapq':  60060, 'bcount':     18, 'min_dist':   1811, 'max_dist':   2087, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  41636, 'from_side':    'r', 'to': 112333, 'to_side':    'r', 'mean_dist':   -710, 'mapq':  60060, 'bcount':     20, 'min_dist':   -775, 'max_dist':   -662, 'probability': 0.319050, 'to_alt':      2, 'from_alt':      1},
        {'from':  47404, 'from_side':    'l', 'to':  30179, 'to_side':    'r', 'mean_dist':    -32, 'mapq':  60060, 'bcount':     13, 'min_dist':    -39, 'max_dist':    -18, 'probability': 0.082422, 'to_alt':      1, 'from_alt':      2},
        {'from':  47404, 'from_side':    'l', 'to':  33144, 'to_side':    'l', 'mean_dist':  -4062, 'mapq':  60060, 'bcount':      2, 'min_dist':  -4070, 'max_dist':  -4055, 'probability': 0.008611, 'to_alt':      1, 'from_alt':      2},
        {'from':  47404, 'from_side':    'r', 'to':     69, 'to_side':    'l', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     15, 'min_dist':    -49, 'max_dist':    -38, 'probability': 0.129976, 'to_alt':      2, 'from_alt':      1},
        {'from':  49093, 'from_side':    'l', 'to':  51660, 'to_side':    'l', 'mean_dist':    -14, 'mapq':  60060, 'bcount':     18, 'min_dist':    -19, 'max_dist':     14, 'probability': 0.231835, 'to_alt':      1, 'from_alt':      2},
        {'from':  49093, 'from_side':    'l', 'to':  56987, 'to_side':    'l', 'mean_dist':      4, 'mapq':  60060, 'bcount':     21, 'min_dist':      0, 'max_dist':     15, 'probability': 0.367256, 'to_alt':      1, 'from_alt':      2},
        {'from':  49093, 'from_side':    'r', 'to':  30214, 'to_side':    'r', 'mean_dist':     17, 'mapq':  60060, 'bcount':     12, 'min_dist':     13, 'max_dist':     25, 'probability': 0.064232, 'to_alt':      1, 'from_alt':      2},
        {'from':  49093, 'from_side':    'r', 'to': 109207, 'to_side':    'l', 'mean_dist':    780, 'mapq':  60060, 'bcount':     11, 'min_dist':    722, 'max_dist':   1043, 'probability': 0.081320, 'to_alt':      2, 'from_alt':      2},
        {'from':  47516, 'from_side':    'l', 'to': 101215, 'to_side':    'r', 'mean_dist':    453, 'mapq':  60060, 'bcount':     13, 'min_dist':    422, 'max_dist':    595, 'probability': 0.135136, 'to_alt':      2, 'from_alt':      1},
        {'from':  47516, 'from_side':    'r', 'to':  96716, 'to_side':    'r', 'mean_dist':    -45, 'mapq':  60060, 'bcount':     16, 'min_dist':    -49, 'max_dist':    -36, 'probability': 0.159802, 'to_alt':      2, 'from_alt':      1},
        {'from':  51660, 'from_side':    'l', 'to':  49093, 'to_side':    'l', 'mean_dist':    -14, 'mapq':  60060, 'bcount':     18, 'min_dist':    -19, 'max_dist':     14, 'probability': 0.231835, 'to_alt':      2, 'from_alt':      1},
        {'from':  51660, 'from_side':    'r', 'to': 101215, 'to_side':    'l', 'mean_dist':      5, 'mapq':  60060, 'bcount':     17, 'min_dist':      0, 'max_dist':     18, 'probability': 0.193782, 'to_alt':      2, 'from_alt':      1},
        {'from':  53480, 'from_side':    'l', 'to':   2722, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     16, 'min_dist':      0, 'max_dist':      0, 'probability': 0.159802, 'to_alt':      1, 'from_alt':      2},
        {'from':  53480, 'from_side':    'l', 'to': 101373, 'to_side':    'r', 'mean_dist':  -1383, 'mapq':  60060, 'bcount':     12, 'min_dist':  -1547, 'max_dist':  -1281, 'probability': 0.105770, 'to_alt':      2, 'from_alt':      2},
        {'from':  53480, 'from_side':    'r', 'to':  31749, 'to_side':    'l', 'mean_dist':   -646, 'mapq':  60060, 'bcount':     18, 'min_dist':   -716, 'max_dist':   -614, 'probability': 0.231835, 'to_alt':      2, 'from_alt':      2},
        {'from':  53480, 'from_side':    'r', 'to':  40554, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     19, 'min_dist':      0, 'max_dist':      0, 'probability': 0.273725, 'to_alt':      1, 'from_alt':      2},
        {'from':  56740, 'from_side':    'l', 'to': 101215, 'to_side':    'r', 'mean_dist':    469, 'mapq':  60060, 'bcount':     15, 'min_dist':    418, 'max_dist':    704, 'probability': 0.209365, 'to_alt':      2, 'from_alt':      1},
        {'from':  56740, 'from_side':    'r', 'to':  96716, 'to_side':    'r', 'mean_dist':    -45, 'mapq':  60060, 'bcount':     14, 'min_dist':    -51, 'max_dist':    -23, 'probability': 0.104244, 'to_alt':      2, 'from_alt':      1},
        {'from':  56987, 'from_side':    'l', 'to':  49093, 'to_side':    'l', 'mean_dist':      4, 'mapq':  60060, 'bcount':     21, 'min_dist':      0, 'max_dist':     15, 'probability': 0.367256, 'to_alt':      2, 'from_alt':      1},
        {'from':  56987, 'from_side':    'r', 'to': 101215, 'to_side':    'l', 'mean_dist':    -10, 'mapq':  60060, 'bcount':     19, 'min_dist':    -12, 'max_dist':     -7, 'probability': 0.273725, 'to_alt':      2, 'from_alt':      1},
        {'from':  58443, 'from_side':    'r', 'to': 107486, 'to_side':    'l', 'mean_dist':    -83, 'mapq':  60060, 'bcount':     15, 'min_dist':   -103, 'max_dist':    -76, 'probability': 0.129976, 'to_alt':      2, 'from_alt':      1},
        {'from':  70951, 'from_side':    'l', 'to': 101373, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     29, 'min_dist':      0, 'max_dist':      0, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  71091, 'from_side':    'l', 'to':     44, 'to_side':    'r', 'mean_dist':      1, 'mapq':  60060, 'bcount':      5, 'min_dist':     -1, 'max_dist':      6, 'probability': 0.007368, 'to_alt':      2, 'from_alt':      2},
        {'from':  71091, 'from_side':    'l', 'to':   1306, 'to_side':    'l', 'mean_dist':    -45, 'mapq':  60060, 'bcount':      8, 'min_dist':    -49, 'max_dist':    -40, 'probability': 0.020422, 'to_alt':      1, 'from_alt':      2},
        {'from':  71091, 'from_side':    'r', 'to':  32229, 'to_side':    'r', 'mean_dist':      3, 'mapq':  60060, 'bcount':     12, 'min_dist':      1, 'max_dist':      7, 'probability': 0.064232, 'to_alt':      1, 'from_alt':      2},
        {'from':  71091, 'from_side':    'r', 'to':  76860, 'to_side':    'l', 'mean_dist':   2134, 'mapq':  60060, 'bcount':      4, 'min_dist':   2026, 'max_dist':   2303, 'probability': 0.012554, 'to_alt':      2, 'from_alt':      2},
        {'from':  76860, 'from_side':    'l', 'to':  32229, 'to_side':    'l', 'mean_dist':   -456, 'mapq':  60060, 'bcount':      7, 'min_dist':   -527, 'max_dist':   -412, 'probability': 0.014765, 'to_alt':      1, 'from_alt':      2},
        {'from':  76860, 'from_side':    'l', 'to':  71091, 'to_side':    'r', 'mean_dist':   2134, 'mapq':  60060, 'bcount':      4, 'min_dist':   2026, 'max_dist':   2303, 'probability': 0.012554, 'to_alt':      2, 'from_alt':      2},
        {'from':  76860, 'from_side':    'r', 'to':  12896, 'to_side':    'l', 'mean_dist':    -46, 'mapq':  60060, 'bcount':      6, 'min_dist':    -51, 'max_dist':    -43, 'probability': 0.010512, 'to_alt':      1, 'from_alt':      2},
        {'from':  76860, 'from_side':    'r', 'to':  12910, 'to_side':    'l', 'mean_dist':    -42, 'mapq':  60060, 'bcount':      5, 'min_dist':    -52, 'max_dist':    -30, 'probability': 0.007368, 'to_alt':      1, 'from_alt':      2},
        {'from':  96716, 'from_side':    'l', 'to': 118890, 'to_side':    'l', 'mean_dist':     -3, 'mapq':  60060, 'bcount':     16, 'min_dist':     -6, 'max_dist':     16, 'probability': 0.159802, 'to_alt':      1, 'from_alt':      2},
        {'from':  96716, 'from_side':    'l', 'to': 118892, 'to_side':    'l', 'mean_dist':    -38, 'mapq':  60060, 'bcount':     17, 'min_dist':    -43, 'max_dist':    -27, 'probability': 0.193782, 'to_alt':      1, 'from_alt':      2},
        {'from':  96716, 'from_side':    'r', 'to':  47516, 'to_side':    'r', 'mean_dist':    -45, 'mapq':  60060, 'bcount':     16, 'min_dist':    -49, 'max_dist':    -36, 'probability': 0.159802, 'to_alt':      1, 'from_alt':      2},
        {'from':  96716, 'from_side':    'r', 'to':  56740, 'to_side':    'r', 'mean_dist':    -45, 'mapq':  60060, 'bcount':     14, 'min_dist':    -51, 'max_dist':    -23, 'probability': 0.104244, 'to_alt':      1, 'from_alt':      2},
        {'from':  99004, 'from_side':    'r', 'to': 107486, 'to_side':    'r', 'mean_dist':    838, 'mapq':  60060, 'bcount':     14, 'min_dist':    787, 'max_dist':    885, 'probability': 0.169655, 'to_alt':      3, 'from_alt':      1},
        {'from':  99341, 'from_side':    'l', 'to':   9344, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     20, 'min_dist':      0, 'max_dist':      0, 'probability': 0.319050, 'to_alt':      2, 'from_alt':      1},
        {'from':  99341, 'from_side':    'r', 'to':  99342, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     21, 'min_dist':      0, 'max_dist':      0, 'probability': 0.367256, 'to_alt':      2, 'from_alt':      1},
        {'from':  99342, 'from_side':    'l', 'to':   9344, 'to_side':    'l', 'mean_dist':     73, 'mapq':  60060, 'bcount':     12, 'min_dist':     64, 'max_dist':     80, 'probability': 0.064232, 'to_alt':      2, 'from_alt':      2},
        {'from':  99342, 'from_side':    'l', 'to':  99341, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     21, 'min_dist':      0, 'max_dist':      0, 'probability': 0.367256, 'to_alt':      1, 'from_alt':      2},
        {'from':  99342, 'from_side':    'r', 'to':     44, 'to_side':    'l', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     19, 'min_dist':    -49, 'max_dist':    -33, 'probability': 0.273725, 'to_alt':      1, 'from_alt':      2},
        {'from':  99342, 'from_side':    'r', 'to': 107483, 'to_side':    'l', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     11, 'min_dist':    -49, 'max_dist':    -37, 'probability': 0.049326, 'to_alt':      1, 'from_alt':      2},
        {'from': 101215, 'from_side':    'l', 'to':  51660, 'to_side':    'r', 'mean_dist':      5, 'mapq':  60060, 'bcount':     17, 'min_dist':      0, 'max_dist':     18, 'probability': 0.193782, 'to_alt':      1, 'from_alt':      2},
        {'from': 101215, 'from_side':    'l', 'to':  56987, 'to_side':    'r', 'mean_dist':    -10, 'mapq':  60060, 'bcount':     19, 'min_dist':    -12, 'max_dist':     -7, 'probability': 0.273725, 'to_alt':      1, 'from_alt':      2},
        {'from': 101215, 'from_side':    'r', 'to':  47516, 'to_side':    'l', 'mean_dist':    453, 'mapq':  60060, 'bcount':     13, 'min_dist':    422, 'max_dist':    595, 'probability': 0.135136, 'to_alt':      1, 'from_alt':      2},
        {'from': 101215, 'from_side':    'r', 'to':  56740, 'to_side':    'l', 'mean_dist':    469, 'mapq':  60060, 'bcount':     15, 'min_dist':    418, 'max_dist':    704, 'probability': 0.209365, 'to_alt':      1, 'from_alt':      2},
        {'from': 101373, 'from_side':    'l', 'to':  31749, 'to_side':    'r', 'mean_dist':   -350, 'mapq':  60060, 'bcount':    109, 'min_dist':   -433, 'max_dist':   -288, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from': 101373, 'from_side':    'l', 'to':  70951, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     29, 'min_dist':      0, 'max_dist':      0, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from': 101373, 'from_side':    'r', 'to':  53480, 'to_side':    'l', 'mean_dist':  -1383, 'mapq':  60060, 'bcount':     12, 'min_dist':  -1547, 'max_dist':  -1281, 'probability': 0.105770, 'to_alt':      2, 'from_alt':      2},
        {'from': 101373, 'from_side':    'r', 'to': 107486, 'to_side':    'r', 'mean_dist':  -1922, 'mapq':  60060, 'bcount':     26, 'min_dist':  -2024, 'max_dist':  -1836, 'probability': 0.500000, 'to_alt':      3, 'from_alt':      2},
        {'from': 107483, 'from_side':    'l', 'to':  99342, 'to_side':    'r', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     11, 'min_dist':    -49, 'max_dist':    -37, 'probability': 0.049326, 'to_alt':      2, 'from_alt':      1},
        {'from': 107483, 'from_side':    'r', 'to': 107484, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     19, 'min_dist':      0, 'max_dist':      0, 'probability': 0.273725, 'to_alt':      2, 'from_alt':      1},
        {'from': 107484, 'from_side':    'l', 'to': 107483, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     19, 'min_dist':      0, 'max_dist':      0, 'probability': 0.273725, 'to_alt':      1, 'from_alt':      2},
        {'from': 107484, 'from_side':    'l', 'to': 117303, 'to_side':    'l', 'mean_dist':   -888, 'mapq':  60060, 'bcount':      8, 'min_dist':   -912, 'max_dist':   -865, 'probability': 0.033108, 'to_alt':      1, 'from_alt':      2},
        {'from': 107484, 'from_side':    'r', 'to': 107485, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     49, 'min_dist':      0, 'max_dist':      0, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from': 107485, 'from_side':    'l', 'to':    372, 'to_side':    'r', 'mean_dist':  -1436, 'mapq':  60060, 'bcount':     14, 'min_dist':  -1605, 'max_dist':  -1350, 'probability': 0.169655, 'to_alt':      1, 'from_alt':      2},
        {'from': 107485, 'from_side':    'l', 'to': 107484, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     49, 'min_dist':      0, 'max_dist':      0, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from': 107485, 'from_side':    'r', 'to': 107486, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':    106, 'min_dist':      0, 'max_dist':      0, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from': 107486, 'from_side':    'l', 'to':  58443, 'to_side':    'r', 'mean_dist':    -83, 'mapq':  60060, 'bcount':     15, 'min_dist':   -103, 'max_dist':    -76, 'probability': 0.129976, 'to_alt':      1, 'from_alt':      2},
        {'from': 107486, 'from_side':    'l', 'to': 107485, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':    106, 'min_dist':      0, 'max_dist':      0, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from': 107486, 'from_side':    'r', 'to':  33994, 'to_side':    'l', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     25, 'min_dist':    -52, 'max_dist':    -15, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      3},
        {'from': 107486, 'from_side':    'r', 'to':  99004, 'to_side':    'r', 'mean_dist':    838, 'mapq':  60060, 'bcount':     14, 'min_dist':    787, 'max_dist':    885, 'probability': 0.169655, 'to_alt':      1, 'from_alt':      3},
        {'from': 107486, 'from_side':    'r', 'to': 101373, 'to_side':    'r', 'mean_dist':  -1922, 'mapq':  60060, 'bcount':     26, 'min_dist':  -2024, 'max_dist':  -1836, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      3},
        {'from': 109207, 'from_side':    'l', 'to':  30214, 'to_side':    'l', 'mean_dist':     -6, 'mapq':  60060, 'bcount':     13, 'min_dist':    -10, 'max_dist':     21, 'probability': 0.082422, 'to_alt':      1, 'from_alt':      2},
        {'from': 109207, 'from_side':    'l', 'to':  49093, 'to_side':    'r', 'mean_dist':    780, 'mapq':  60060, 'bcount':     11, 'min_dist':    722, 'max_dist':   1043, 'probability': 0.081320, 'to_alt':      2, 'from_alt':      2},
        {'from': 109207, 'from_side':    'r', 'to':  26855, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     13, 'min_dist':    -51, 'max_dist':    -14, 'probability': 0.082422, 'to_alt':      1, 'from_alt':      2},
        {'from': 109207, 'from_side':    'r', 'to':  30179, 'to_side':    'l', 'mean_dist':    -40, 'mapq':  60060, 'bcount':      8, 'min_dist':    -47, 'max_dist':    -23, 'probability': 0.020422, 'to_alt':      1, 'from_alt':      2},
        {'from': 110827, 'from_side':    'r', 'to':     69, 'to_side':    'l', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     24, 'min_dist':    -50, 'max_dist':    -16, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from': 112333, 'from_side':    'l', 'to':  13434, 'to_side':    'l', 'mean_dist':     38, 'mapq':  60060, 'bcount':     18, 'min_dist':     32, 'max_dist':     52, 'probability': 0.231835, 'to_alt':      1, 'from_alt':      2},
        {'from': 112333, 'from_side':    'l', 'to':  13455, 'to_side':    'l', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     29, 'min_dist':    -49, 'max_dist':    -34, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from': 112333, 'from_side':    'r', 'to':    114, 'to_side':    'l', 'mean_dist':   -819, 'mapq':  60060, 'bcount':     22, 'min_dist':   -931, 'max_dist':   -785, 'probability': 0.417654, 'to_alt':      1, 'from_alt':      2},
        {'from': 112333, 'from_side':    'r', 'to':  41636, 'to_side':    'r', 'mean_dist':   -710, 'mapq':  60060, 'bcount':     20, 'min_dist':   -775, 'max_dist':   -662, 'probability': 0.319050, 'to_alt':      1, 'from_alt':      2},
        {'from': 115803, 'from_side':    'l', 'to':    115, 'to_side':    'r', 'mean_dist':    885, 'mapq':  60060, 'bcount':     15, 'min_dist':    818, 'max_dist':   1120, 'probability': 0.209365, 'to_alt':      2, 'from_alt':      2},
        {'from': 115803, 'from_side':    'l', 'to':  15177, 'to_side':    'r', 'mean_dist':      5, 'mapq':  60060, 'bcount':     12, 'min_dist':      1, 'max_dist':     15, 'probability': 0.064232, 'to_alt':      1, 'from_alt':      2},
        {'from': 117303, 'from_side':    'l', 'to': 107484, 'to_side':    'l', 'mean_dist':   -888, 'mapq':  60060, 'bcount':      8, 'min_dist':   -912, 'max_dist':   -865, 'probability': 0.033108, 'to_alt':      2, 'from_alt':      1},
        {'from': 117303, 'from_side':    'r', 'to':    929, 'to_side':    'l', 'mean_dist':   2544, 'mapq':  60060, 'bcount':      7, 'min_dist':   2451, 'max_dist':   2631, 'probability': 0.076700, 'to_alt':      2, 'from_alt':      1},
        {'from': 117304, 'from_side':    'r', 'to':    929, 'to_side':    'l', 'mean_dist':   2489, 'mapq':  60060, 'bcount':      2, 'min_dist':   2450, 'max_dist':   2534, 'probability': 0.008611, 'to_alt':      2, 'from_alt':      1},
        {'from': 118890, 'from_side':    'l', 'to':  96716, 'to_side':    'l', 'mean_dist':     -3, 'mapq':  60060, 'bcount':     16, 'min_dist':     -6, 'max_dist':     16, 'probability': 0.159802, 'to_alt':      2, 'from_alt':      1},
        {'from': 118890, 'from_side':    'r', 'to':   9344, 'to_side':    'r', 'mean_dist':    -13, 'mapq':  60060, 'bcount':     25, 'min_dist':    -20, 'max_dist':     39, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from': 118892, 'from_side':    'l', 'to':  96716, 'to_side':    'l', 'mean_dist':    -38, 'mapq':  60060, 'bcount':     17, 'min_dist':    -43, 'max_dist':    -27, 'probability': 0.193782, 'to_alt':      2, 'from_alt':      1},
        {'from': 118892, 'from_side':    'r', 'to':   9344, 'to_side':    'r', 'mean_dist':      8, 'mapq':  60060, 'bcount':     20, 'min_dist':      0, 'max_dist':     35, 'probability': 0.319050, 'to_alt':      2, 'from_alt':      1}
        ]) )
    result_paths.append( pd.DataFrame({
        'pid':      [    100,    100,    100,    100,    100,    100,    100,    100,    100,    100,    100,    100,    100,    100],
        'pos':      [      0,      1,      2,      3,      4,      5,      6,      7,      8,      9,     10,     11,     12,     13],
        'phase0':   [    101,    101,    101,    101,    101,    101,    101,    101,    101,    101,    101,    101,    101,    101],
        'scaf0':    [   2799,    674,  20727,   2885,  15812,  10723,  12896,  76860,  32229,  71091,   1306,     44,  99342,  99341],
        'strand0':  [    '-',    '+',    '-',    '+',    '+',    '-',    '-',    '-',    '+',    '-',    '+',    '-',    '-',    '-'],
        'dist0':    [      0,     60,    -68,    -44,    -44,    -43,    -43,    -46,   -456,      3,    -45,    -43,    -44,      0],
        'phase1':   [   -102,   -102,    102,    102,    102,    102,    102,    102,    102,    102,    102,    102,   -102,   -102],
        'scaf1':    [     -1,     -1,  20727,   2885,  13452,  10723,  12910,  76860,     -1,  71091,     -1,     44,     -1,     -1],
        'strand1':  [     '',     '',    '-',    '+',    '-',    '-',    '-',    '-',     '',    '-',     '',    '-',     '',     ''],
        'dist1':    [      0,      0,    -18,    245,    -45,    -41,    -43,    -42,      0,   2134,      0,      1,      0,      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    101,    101],
        'pos':      [      0,      1],
        'phase0':   [    103,    103],
        'scaf0':    [  99342, 107483],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,    -44],
        'phase1':   [   -104,   -104],
        'scaf1':    [     -1,     -1],
        'strand1':  [     '',     ''],
        'dist1':    [      0,      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    102,    102,    102,    102,    102,    102,    102,    102,    102,    102,    102,    102],
        'pos':      [      0,      1,      2,      3,      4,      5,      6,      7,      8,      9,     10,     11],
        'phase0':   [    105,    105,    105,    105,    105,    105,    105,    105,    105,    105,    105,    105],
        'scaf0':    [   9344, 118892,  96716,  47516, 101215,  51660,  49093,  30214, 109207,  30179,     -1,  47404],
        'strand0':  [    '+',    '-',    '+',    '-',    '-',    '-',    '+',    '-',    '+',    '+',     '',    '+'],
        'dist0':    [      0,      8,    -38,    -45,    453,      5,    -14,     17,     -6,    -40,      0,    -32],
        'phase1':   [   -106,    106,    106,    106,    106,    106,    106,    106,    106,    106,    106,    106],
        'scaf1':    [     -1, 118890,  96716,  56740, 101215,  56987,  49093,     -1, 109207,  26855,  33144,  47404],
        'strand1':  [     '',    '-',    '+',    '-',    '-',    '-',    '+',     '',    '+',    '-',    '-',    '+'],
        'dist1':    [      0,    -13,     -3,    -45,    469,    -10,      4,      0,    780,    -43,    -40,  -4062]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    103,    103,    103,    103,    103],
        'pos':      [      0,      1,      2,      3,      4],
        'phase0':   [    107,    107,    107,    107,    107],
        'scaf0':    [  13591,  11659,   2725,  14096,    929],
        'strand0':  [    '+',    '+',    '-',    '-',    '-'],
        'dist0':    [      0,    -43,    136,    642,    -44],
        'phase1':   [   -108,    108,    108,    108,    108],
        'scaf1':    [     -1,  13029,   2725,     -1,    929],
        'strand1':  [     '',    '-',    '-',     '',    '-'],
        'dist1':    [      0,    -43,    143,      0,   1205]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    104],
        'pos':      [      0],
        'phase0':   [    109],
        'scaf0':    [ 117304],
        'strand0':  [    '+'],
        'dist0':    [      0],
        'phase1':   [   -110],
        'scaf1':    [     -1],
        'strand1':  [     ''],
        'dist1':    [      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    105,    105,    105,    105,    105],
        'pos':      [      0,      1,      2,      3,      4],
        'phase0':   [    111,    111,    111,    111,    111],
        'scaf0':    [ 117303, 107484, 107485, 107486,  33994],
        'strand0':  [    '-',    '+',    '+',    '+',    '+'],
        'dist0':    [      0,   -888,      0,      0,    -44],
        'phase1':   [   -112,   -112,   -112,   -112,   -112],
        'scaf1':    [     -1,     -1,     -1,     -1,     -1],
        'strand1':  [     '',     '',     '',     '',     ''],
        'dist1':    [      0,      0,      0,      0,      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    106],
        'pos':      [      0],
        'phase0':   [    113],
        'scaf0':    [  58443],
        'strand0':  [    '+'],
        'dist0':    [      0],
        'phase1':   [   -114],
        'scaf1':    [     -1],
        'strand1':  [     ''],
        'dist1':    [      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    107],
        'pos':      [      0],
        'phase0':   [    115],
        'scaf0':    [  99004],
        'strand0':  [    '+'],
        'dist0':    [      0],
        'phase1':   [   -116],
        'scaf1':    [     -1],
        'strand1':  [     ''],
        'dist1':    [      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    108],
        'pos':      [      0],
        'phase0':   [    117],
        'scaf0':    [ 110827],
        'strand0':  [    '+'],
        'dist0':    [      0],
        'phase1':   [   -118],
        'scaf1':    [     -1],
        'strand1':  [     ''],
        'dist1':    [      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    109,    109,    109,    109,    109,    109,    109],
        'pos':      [      0,      1,      2,      3,      4,      5,      6],
        'phase0':   [    119,    119,    119,    119,    119,    119,    119],
        'scaf0':    [     69,  13434, 112333,    114,    115,     -1, 115803],
        'strand0':  [    '+',    '-',    '+',    '+',    '+',     '',    '+'],
        'dist0':    [      0,    -41,     38,   -819,   1131,      0,    885],
        'phase1':   [   -120,    120,    120,    120,    120,    120,    120],
        'scaf1':    [     -1,  13455, 112333,  41636,    115,  15177, 115803],
        'strand1':  [     '',    '-',    '+',    '-',    '+',    '+',    '+'],
        'dist1':    [      0,    -43,    -43,   -710,   1932,    -42,      5]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    110,    110,    110,    110,    110],
        'pos':      [      0,      1,      2,      3,      4],
        'phase0':   [    121,    121,    121,    121,    121],
        'scaf0':    [    372, 107485, 107486, 101373,  70951],
        'strand0':  [    '+',    '+',    '+',    '-',    '+'],
        'dist0':    [      0,  -1436,      0,  -1922,      0],
        'phase1':   [   -122,   -122,   -122,   -122,   -122],
        'scaf1':    [     -1,     -1,     -1,     -1,     -1],
        'strand1':  [     '',     '',     '',     '',     ''],
        'dist1':    [      0,      0,      0,      0,      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    111,    111,    111,    111,    111],
        'pos':      [      0,      1,      2,      3,      4],
        'phase0':   [    123,    123,    123,    123,    123],
        'scaf0':    [  31749,  53480, 101373,  31749,  31756],
        'strand0':  [    '-',    '-',    '-',    '-',    '-'],
        'dist0':    [   -350,   -646,  -1383,   -350,     -1],
        'phase1':   [   -124,   -124,   -124,   -124,   -124],
        'scaf1':    [     -1,     -1,     -1,     -1,     -1],
        'strand1':  [     '',     '',     '',     '',     ''],
        'dist1':    [      0,      0,      0,      0,      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    112,    112,    112],
        'pos':      [      0,      1,      2],
        'phase0':   [    125,    125,    125],
        'scaf0':    [  40554,  53480,   2722],
        'strand0':  [    '+',    '-',    '+'],
        'dist0':    [      0,      0,      0],
        'phase1':   [   -126,   -126,   -126],
        'scaf1':    [     -1,     -1,     -1],
        'strand1':  [     '',     '',     ''],
        'dist1':    [      0,      0,      0]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    100,    100,    100],
        'pos':      [      0,      1,      2],
        'scaf0':    [   2885,  15812,  10723],
        'strand0':  [    '+',    '+',    '-'],
        'dist0':    [      0,    -44,    -43]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    101,    101,    101],
        'pos':      [      0,      1,      2],
        'scaf0':    [   2885,  13452,  10723],
        'strand0':  [    '+',    '-',    '-'],
        'dist0':    [      0,    -45,    -41]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    102,    102,    102],
        'pos':      [      0,      1,      2],
        'scaf0':    [  10723,  12896,  76860],
        'strand0':  [    '-',    '-',    '-'],
        'dist0':    [      0,    -43,    -46]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    103,    103,    103],
        'pos':      [      0,      1,      2],
        'scaf0':    [  10723,  12910,  76860],
        'strand0':  [    '-',    '-',    '-'],
        'dist0':    [      0,    -43,    -42]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    104,    104,    104],
        'pos':      [      0,      1,      2],
        'scaf0':    [  76860,  32229,  71091],
        'strand0':  [    '-',    '+',    '-'],
        'dist0':    [      0,   -456,      3]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    105,    105,    105],
        'pos':      [      0,      1,      2],
        'scaf0':    [  71091,   1306,     44],
        'strand0':  [    '-',    '+',    '-'],
        'dist0':    [      0,    -45,    -43]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    106,    106,    106,    106],
        'pos':      [      0,      1,      2,      3],
        'scaf0':    [     44,  99342,  99341,   9344],
        'strand0':  [    '-',    '-',    '-',    '+'],
        'dist0':    [      0,    -44,      0,      0]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    107,    107,    107,    107,    107,    107,    107,    107],
        'pos':      [      0,      1,      2,      3,      4,      5,      6,      7],
        'scaf0':    [   9344, 118890,  96716,  56740, 101215,  56987,  49093, 109207],
        'strand0':  [    '+',    '-',    '+',    '-',    '-',    '-',    '+',    '+'],
        'dist0':    [      0,    -13,     -3,    -45,    469,    -10,      4,    780]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    108,    108,    108],
        'pos':      [      0,      1,      2],
        'scaf0':    [  109207,  30179, 47404],
        'strand0':  [    '+',    '+',    '+'],
        'dist0':    [      0,    -40,    -32]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    109,    109,    109,    109],
        'pos':      [      0,      1,      2,      3],
        'scaf0':    [ 109207,  26855,  33144,  47404],
        'strand0':  [    '+',    '-',    '-',    '+'],
        'dist0':    [      0,    -43,    -40,  -4062]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    110,    110,    110,    110,    110,    110,    110,    110,    110],
        'pos':      [      0,      1,      2,      3,      4,      5,      6,      7,      8],
        'scaf0':    [ 109207,  30214,  49093,  51660, 101215,  47516,  96716, 118892,   9344],
        'strand0':  [    '-',    '+',    '-',    '+',    '+',    '+',    '-',    '+',    '-'],
        'dist0':    [      0,     -6,     17,    -14,      5,    453,    -45,    -38,      8]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    111,    111,    111,    111,    111],
        'pos':      [      0,      1,      2,      3,      4],
        'scaf0':    [   9344,  99342, 107483, 107484, 107485],
        'strand0':  [    '-',    '+',    '+',    '+',    '+'],
        'dist0':    [      0,     73,    -44,      0,      0]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    112,    112,    112],
        'pos':      [      0,      1,      2],
        'scaf0':    [  13591,  11659,   2725],
        'strand0':  [    '+',    '+',    '-'],
        'dist0':    [      0,    -43,    136]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    113,    113,    113],
        'pos':      [      0,      1,      2],
        'scaf0':    [  13591,  13029,   2725],
        'strand0':  [    '+',    '-',    '-'],
        'dist0':    [      0,    -43,    143]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    114,    114,    114],
        'pos':      [      0,      1,      2],
        'scaf0':    [   2725,  14096,    929],
        'strand0':  [    '-',    '-',    '-'],
        'dist0':    [      0,    642,    -44]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    115,    115,    115,    115,    115],
        'pos':      [      0,      1,      2,      3,      4],
        'scaf0':    [    929, 117303, 107484, 107485, 107486],
        'strand0':  [    '-',    '-',    '+',    '+',    '+'],
        'dist0':    [      0,   2544,   -888,      0,      0]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    116,    116,    116,    116,    116],
        'pos':      [      0,      1,      2,      3,      4],
        'scaf0':    [     69,  13434, 112333,    114,    115],
        'strand0':  [    '+',    '-',    '+',    '+',    '+'],
        'dist0':    [      0,    -41,     38,   -819,   1131]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    117,    117,    117,    117,    117],
        'pos':      [      0,      1,      2,      3,      4],
        'scaf0':    [     69,  13455, 112333,  41636,    115],
        'strand0':  [    '+',    '-',    '+',    '-',    '+'],
        'dist0':    [      0,    -43,    -43,   -710,   1932]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    118,    118,    118],
        'pos':      [      0,      1,      2],
        'scaf0':    [    115,  15177, 115803],
        'strand0':  [    '+',    '+',    '+'],
        'dist0':    [      0,    -42,      5]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    121,    121,    121],
        'pos':      [      0,      1,      2],
        'scaf0':    [  31749,  53480, 101373],
        'strand0':  [    '-',    '-',    '-'],
        'dist0':    [      0,   -646,  -1383]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    122,    122,    122],
        'pos':      [      0,      1,      2],
        'scaf0':    [  40554,  53480,   2722],
        'strand0':  [    '+',    '-',    '+'],
        'dist0':    [      0,      0,      0]
        }) )
    result_loops.append( pd.DataFrame({
        'pid':      [    100,    100,    100],
        'pos':      [      0,      1,      2],
        'scaf0':    [ 107486, 101373,  70951],
        'strand0':  [    '+',    '-',    '+'],
        'dist0':    [      0,  -1922,      0]
        }) )
    result_loops.append( pd.DataFrame({
        'pid':      [    101,    101,    101,    101,    101,    101,    101],
        'pos':      [      0,      1,      2,      3,      4,      5,      6],
        'scaf0':    [ 107486, 101373,  31749,  53480, 101373,  31749,  31756],
        'strand0':  [    '+',    '-',    '-',    '-',    '-',    '-',    '-'],
        'dist0':    [      0,  -1922,   -350,   -646,  -1383,   -350,     -1]
        }) )
    result_loops.append( pd.DataFrame({
        'pid':      [    102,    102,    102],
        'pos':      [      0,      1,      2],
        'scaf0':    [  40554,  53480,   2722],
        'strand0':  [    '+',    '-',    '+'],
        'dist0':    [      0,      0,      0]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    100,    100],
        'pos':      [      0,      1],
        'scaf0':    [   2799,    674],
        'strand0':  [    '-',    '+'],
        'dist0':    [      0,     60]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    101,    101],
        'pos':      [      0,      1],
        'scaf0':    [    674,  20727],
        'strand0':  [    '+',    '-'],
        'dist0':    [      0,    -68]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    102,    102],
        'pos':      [      0,      1],
        'scaf0':    [    674,  20727],
        'strand0':  [    '+',    '-'],
        'dist0':    [      0,    -18]
        }) )        
    result_untraversed.append( pd.DataFrame({
        'pid':      [    103,    103],
        'pos':      [      0,      1],
        'scaf0':    [  20727,   2885],
        'strand0':  [    '-',    '+'],
        'dist0':    [      0,    -44]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    104,    104],
        'pos':      [      0,      1],
        'scaf0':    [  20727,   2885],
        'strand0':  [    '-',    '+'],
        'dist0':    [      0,    245]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    105,    105],
        'pos':      [      0,      1],
        'scaf0':    [  76860,  71091],
        'strand0':  [    '-',    '-'],
        'dist0':    [      0,   2134]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    106,    106],
        'pos':      [      0,      1],
        'scaf0':    [  71091,     44],
        'strand0':  [    '-',    '-'],
        'dist0':    [      0,      1]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    107,    107],
        'pos':      [      0,      1],
        'scaf0':    [  13591,   2725],
        'strand0':  [    '+',    '-'],
        'dist0':    [      0,    734]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    108,    108],
        'pos':      [      0,      1],
        'scaf0':    [   2725,    929],
        'strand0':  [    '-',    '-'],
        'dist0':    [      0,   1205]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    109,    109],
        'pos':      [      0,      1],
        'scaf0':    [    929, 117304],
        'strand0':  [    '-',    '-'],
        'dist0':    [      0,   2489]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    110,    110],
        'pos':      [      0,      1],
        'scaf0':    [     69,  47404],
        'strand0':  [    '-',    '-'],
        'dist0':    [      0,    -43]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    111,    111],
        'pos':      [      0,      1],
        'scaf0':    [     69, 110827],
        'strand0':  [    '-',    '-'],
        'dist0':    [      0,    -43]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    112,    112],
        'pos':      [      0,      1],
        'scaf0':    [  58443, 107486],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,    -83]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    113,    113],
        'pos':      [      0,      1],
        'scaf0':    [  99004, 107486],
        'strand0':  [    '+',    '-'],
        'dist0':    [      0,    838]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    114,    114],
        'pos':      [      0,      1],
        'scaf0':    [ 107486,  33994],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,    -44]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    115,    115],
        'pos':      [      0,      1],
        'scaf0':    [    115, 115803],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,    885]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    116,    116],
        'pos':      [      0,      1],
        'scaf0':    [    372, 107485],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,  -1436]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    117,    117],
        'pos':      [      0,      1],
        'scaf0':    [ 107485, 107486],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,      0]
        }) )
#
    # Test 2
    scaffolds.append( pd.DataFrame({'case':2, 'scaffold':[19382, 66038 ,115947, 115948, 115949, 115950, 115951, 115952]}) )
    scaffold_graph.append( pd.DataFrame([
        {'from':  19382, 'from_side':    'l', 'length':      4, 'scaf1': 115947, 'strand1':    '+', 'dist1':    -41, 'scaf2': 115949, 'strand2':    '+', 'dist2':    515, 'scaf3': 115951, 'strand3':    '+', 'dist3':   9325},
        {'from':  66038, 'from_side':    'r', 'length':      3, 'scaf1': 115947, 'strand1':    '+', 'dist1':    381, 'scaf2': 115948, 'strand2':    '+', 'dist2':      0},
        {'from': 115947, 'from_side':    'l', 'length':      2, 'scaf1':  19382, 'strand1':    '+', 'dist1':    -41},
        {'from': 115947, 'from_side':    'l', 'length':      2, 'scaf1':  66038, 'strand1':    '-', 'dist1':    381},
        {'from': 115947, 'from_side':    'r', 'length':      2, 'scaf1': 115948, 'strand1':    '+', 'dist1':      0},
        {'from': 115947, 'from_side':    'r', 'length':      4, 'scaf1': 115949, 'strand1':    '+', 'dist1':    515, 'scaf2': 115951, 'strand2':    '+', 'dist2':   9325, 'scaf3': 115952, 'strand3':    '-', 'dist3':    -43},
        {'from': 115948, 'from_side':    'l', 'length':      3, 'scaf1': 115947, 'strand1':    '-', 'dist1':      0, 'scaf2':  66038, 'strand2':    '-', 'dist2':    381},
        {'from': 115948, 'from_side':    'r', 'length':      5, 'scaf1': 115949, 'strand1':    '+', 'dist1':      0, 'scaf2': 115950, 'strand2':    '+', 'dist2':      0, 'scaf3': 115951, 'strand3':    '+', 'dist3':      0, 'scaf4': 115952, 'strand4':    '+', 'dist4':    -42},
        {'from': 115949, 'from_side':    'l', 'length':      3, 'scaf1': 115947, 'strand1':    '-', 'dist1':    515, 'scaf2':  19382, 'strand2':    '+', 'dist2':    -41},
        {'from': 115949, 'from_side':    'l', 'length':      2, 'scaf1': 115948, 'strand1':    '-', 'dist1':      0},
        {'from': 115949, 'from_side':    'r', 'length':      4, 'scaf1': 115950, 'strand1':    '+', 'dist1':      0, 'scaf2': 115951, 'strand2':    '+', 'dist2':      0, 'scaf3': 115952, 'strand3':    '+', 'dist3':    -42},
        {'from': 115949, 'from_side':    'r', 'length':      3, 'scaf1': 115951, 'strand1':    '+', 'dist1':   9325, 'scaf2': 115952, 'strand2':    '-', 'dist2':    -43},
        {'from': 115950, 'from_side':    'l', 'length':      3, 'scaf1': 115949, 'strand1':    '-', 'dist1':      0, 'scaf2': 115948, 'strand2':    '-', 'dist2':      0},
        {'from': 115950, 'from_side':    'r', 'length':      3, 'scaf1': 115951, 'strand1':    '+', 'dist1':      0, 'scaf2': 115952, 'strand2':    '+', 'dist2':    -42},
        {'from': 115951, 'from_side':    'l', 'length':      4, 'scaf1': 115949, 'strand1':    '-', 'dist1':   9325, 'scaf2': 115947, 'strand2':    '-', 'dist2':    515, 'scaf3':  19382, 'strand3':    '+', 'dist3':    -41},
        {'from': 115951, 'from_side':    'l', 'length':      4, 'scaf1': 115950, 'strand1':    '-', 'dist1':      0, 'scaf2': 115949, 'strand2':    '-', 'dist2':      0, 'scaf3': 115948, 'strand3':    '-', 'dist3':      0},
        {'from': 115951, 'from_side':    'r', 'length':      2, 'scaf1': 115952, 'strand1':    '+', 'dist1':    -42},
        {'from': 115951, 'from_side':    'r', 'length':      2, 'scaf1': 115952, 'strand1':    '-', 'dist1':    -43},
        {'from': 115952, 'from_side':    'l', 'length':      5, 'scaf1': 115951, 'strand1':    '-', 'dist1':    -42, 'scaf2': 115950, 'strand2':    '-', 'dist2':      0, 'scaf3': 115949, 'strand3':    '-', 'dist3':      0, 'scaf4': 115948, 'strand4':    '-', 'dist4':      0},
        {'from': 115952, 'from_side':    'r', 'length':      4, 'scaf1': 115951, 'strand1':    '-', 'dist1':    -43, 'scaf2': 115949, 'strand2':    '-', 'dist2':   9325, 'scaf3': 115947, 'strand3':    '-', 'dist3':    515}
        ]) )
    scaf_bridges.append( pd.DataFrame([
        {'from':  19382, 'from_side':    'l', 'to': 115947, 'to_side':    'l', 'mean_dist':    -41, 'mapq':  60060, 'bcount':     22, 'min_dist':    -48, 'max_dist':    -25, 'probability': 0.417654, 'to_alt':      2, 'from_alt':      1},
        {'from':  66038, 'from_side':    'r', 'to': 115947, 'to_side':    'l', 'mean_dist':    381, 'mapq':  60060, 'bcount':     22, 'min_dist':    354, 'max_dist':    485, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from': 115947, 'from_side':    'l', 'to':  19382, 'to_side':    'l', 'mean_dist':    -41, 'mapq':  60060, 'bcount':     22, 'min_dist':    -48, 'max_dist':    -25, 'probability': 0.417654, 'to_alt':      1, 'from_alt':      2},
        {'from': 115947, 'from_side':    'l', 'to':  66038, 'to_side':    'r', 'mean_dist':    381, 'mapq':  60060, 'bcount':     22, 'min_dist':    354, 'max_dist':    485, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from': 115947, 'from_side':    'r', 'to': 115948, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     23, 'min_dist':      0, 'max_dist':      0, 'probability': 0.469443, 'to_alt':      1, 'from_alt':      2},
        {'from': 115947, 'from_side':    'r', 'to': 115949, 'to_side':    'l', 'mean_dist':    515, 'mapq':  60060, 'bcount':     20, 'min_dist':    473, 'max_dist':    642, 'probability': 0.470466, 'to_alt':      2, 'from_alt':      2},
        {'from': 115948, 'from_side':    'l', 'to': 115947, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     23, 'min_dist':      0, 'max_dist':      0, 'probability': 0.469443, 'to_alt':      2, 'from_alt':      1},
        {'from': 115948, 'from_side':    'r', 'to': 115949, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     17, 'min_dist':      0, 'max_dist':      0, 'probability': 0.193782, 'to_alt':      2, 'from_alt':      1},
        {'from': 115949, 'from_side':    'l', 'to': 115947, 'to_side':    'r', 'mean_dist':    515, 'mapq':  60060, 'bcount':     20, 'min_dist':    473, 'max_dist':    642, 'probability': 0.470466, 'to_alt':      2, 'from_alt':      2},
        {'from': 115949, 'from_side':    'l', 'to': 115948, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     17, 'min_dist':      0, 'max_dist':      0, 'probability': 0.193782, 'to_alt':      1, 'from_alt':      2},
        {'from': 115949, 'from_side':    'r', 'to': 115950, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     16, 'min_dist':      0, 'max_dist':      0, 'probability': 0.159802, 'to_alt':      1, 'from_alt':      2},
        {'from': 115949, 'from_side':    'r', 'to': 115951, 'to_side':    'l', 'mean_dist':   9325, 'mapq':  60060, 'bcount':      6, 'min_dist':   8844, 'max_dist':  10272, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      2},
        {'from': 115950, 'from_side':    'l', 'to': 115949, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     16, 'min_dist':      0, 'max_dist':      0, 'probability': 0.159802, 'to_alt':      2, 'from_alt':      1},
        {'from': 115950, 'from_side':    'r', 'to': 115951, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     13, 'min_dist':      0, 'max_dist':      0, 'probability': 0.082422, 'to_alt':      2, 'from_alt':      1},
        {'from': 115951, 'from_side':    'l', 'to': 115949, 'to_side':    'r', 'mean_dist':   9325, 'mapq':  60060, 'bcount':      6, 'min_dist':   8844, 'max_dist':  10272, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      2},
        {'from': 115951, 'from_side':    'l', 'to': 115950, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     13, 'min_dist':      0, 'max_dist':      0, 'probability': 0.082422, 'to_alt':      1, 'from_alt':      2},
        {'from': 115951, 'from_side':    'r', 'to': 115952, 'to_side':    'l', 'mean_dist':    -42, 'mapq':  60060, 'bcount':     10, 'min_dist':    -49, 'max_dist':    -37, 'probability': 0.037322, 'to_alt':      1, 'from_alt':      2},
        {'from': 115951, 'from_side':    'r', 'to': 115952, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     20, 'min_dist':    -49, 'max_dist':    -28, 'probability': 0.319050, 'to_alt':      1, 'from_alt':      2},
        {'from': 115952, 'from_side':    'l', 'to': 115951, 'to_side':    'r', 'mean_dist':    -42, 'mapq':  60060, 'bcount':     10, 'min_dist':    -49, 'max_dist':    -37, 'probability': 0.037322, 'to_alt':      2, 'from_alt':      1},
        {'from': 115952, 'from_side':    'r', 'to': 115951, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     20, 'min_dist':    -49, 'max_dist':    -28, 'probability': 0.319050, 'to_alt':      2, 'from_alt':      1}
        ]) )
    result_paths.append( pd.DataFrame({
        'pid':      [    200,    200,    200,    200,    200,    200,    200,    200,    200,    200,    200],
        'pos':      [      0,      1,      2,      3,      4,      5,      6,      7,      8,      9,     10],
        'phase0':   [    201,    201,    201,    201,    201,    201,    201,    201,    201,    201,    201],
        'scaf0':    [  66038, 115947, 115948, 115949, 115950, 115951, 115952, 115951, 115949, 115947,  19382],
        'strand0':  [    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '-',    '-',    '-',    '+'],
        'dist0':    [      0,    381,      0,      0,      0,      0,    -42,    -43,   9325,    515,    -41],
        'phase1':   [   -202,   -202,   -202,   -202,   -202,   -202,   -202,   -202,   -202,   -202,   -202],
        'scaf1':    [     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1],
        'strand1':  [     '',     '',     '',     '',     '',     '',     '',     '',     '',     '',     ''],
        'dist1':    [      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    200,    200,    200,    200,    200,    200,    200,    200,    200,    200,    200],
        'pos':      [      0,      1,      2,      3,      4,      5,      6,      7,      8,      9,     10],
        'scaf0':    [  66038, 115947, 115948, 115949, 115950, 115951, 115952, 115951, 115949, 115947,  19382],
        'strand0':  [    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '-',    '-',    '-',    '+'],
        'dist0':    [      0,    381,      0,      0,      0,      0,    -42,    -43,   9325,    515,    -41]
        }) )
#
    # Test 3
    scaffolds.append( pd.DataFrame({'case':3, 'scaffold':[30387,  95786, 108403, 110072]}) )
    scaffold_graph.append( pd.DataFrame([
        {'from':  30387, 'from_side':    'l', 'length':      3, 'scaf1': 110072, 'strand1':    '+', 'dist1':    758, 'scaf2': 108403, 'strand2':    '+', 'dist2':    -30},
        {'from':  95786, 'from_side':    'l', 'length':      3, 'scaf1': 110072, 'strand1':    '-', 'dist1':    -29, 'scaf2': 108403, 'strand2':    '-', 'dist2':    813},
        {'from': 108403, 'from_side':    'l', 'length':      3, 'scaf1': 110072, 'strand1':    '-', 'dist1':    -30, 'scaf2':  30387, 'strand2':    '+', 'dist2':    758},
        {'from': 108403, 'from_side':    'l', 'length':      3, 'scaf1': 110072, 'strand1':    '-', 'dist1':    -30, 'scaf2': 108403, 'strand2':    '-', 'dist2':    813},
        {'from': 108403, 'from_side':    'r', 'length':      3, 'scaf1': 110072, 'strand1':    '+', 'dist1':    813, 'scaf2':  95786, 'strand2':    '+', 'dist2':    -29},
        {'from': 108403, 'from_side':    'r', 'length':      3, 'scaf1': 110072, 'strand1':    '+', 'dist1':    813, 'scaf2': 108403, 'strand2':    '+', 'dist2':    -30},
        {'from': 110072, 'from_side':    'l', 'length':      2, 'scaf1':  30387, 'strand1':    '+', 'dist1':    758},
        {'from': 110072, 'from_side':    'l', 'length':      3, 'scaf1': 108403, 'strand1':    '-', 'dist1':    813, 'scaf2': 110072, 'strand2':    '-', 'dist2':    -30},
        {'from': 110072, 'from_side':    'r', 'length':      2, 'scaf1':  95786, 'strand1':    '+', 'dist1':    -29},
        {'from': 110072, 'from_side':    'r', 'length':      3, 'scaf1': 108403, 'strand1':    '+', 'dist1':    -30, 'scaf2': 110072, 'strand2':    '+', 'dist2':    813}
        ]) )
    scaf_bridges.append( pd.DataFrame([
        {'from':  30387, 'from_side':    'l', 'to': 110072, 'to_side':    'l', 'mean_dist':    758, 'mapq':  60060, 'bcount':     17, 'min_dist':    711, 'max_dist':    992, 'probability': 0.303341, 'to_alt':      2, 'from_alt':      1},
        {'from':  95786, 'from_side':    'l', 'to': 110072, 'to_side':    'r', 'mean_dist':    -29, 'mapq':  60060, 'bcount':     24, 'min_dist':    -35, 'max_dist':    -20, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from': 108403, 'from_side':    'l', 'to': 110072, 'to_side':    'r', 'mean_dist':    -30, 'mapq':  60060, 'bcount':     34, 'min_dist':    -38, 'max_dist':    -24, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from': 108403, 'from_side':    'r', 'to': 110072, 'to_side':    'l', 'mean_dist':    813, 'mapq':  60060, 'bcount':     31, 'min_dist':    745, 'max_dist':   1052, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from': 110072, 'from_side':    'l', 'to':  30387, 'to_side':    'l', 'mean_dist':    758, 'mapq':  60060, 'bcount':     17, 'min_dist':    711, 'max_dist':    992, 'probability': 0.303341, 'to_alt':      1, 'from_alt':      2},
        {'from': 110072, 'from_side':    'l', 'to': 108403, 'to_side':    'r', 'mean_dist':    813, 'mapq':  60060, 'bcount':     31, 'min_dist':    745, 'max_dist':   1052, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from': 110072, 'from_side':    'r', 'to':  95786, 'to_side':    'l', 'mean_dist':    -29, 'mapq':  60060, 'bcount':     24, 'min_dist':    -35, 'max_dist':    -20, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from': 110072, 'from_side':    'r', 'to': 108403, 'to_side':    'l', 'mean_dist':    -30, 'mapq':  60060, 'bcount':     34, 'min_dist':    -38, 'max_dist':    -24, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2}
        ]) )
    result_paths.append( pd.DataFrame({
        'pid':      [    300,    300,    300,    300,    300,    300,    300],
        'pos':      [      0,      1,      2,      3,      4,      5,      6],
        'phase0':   [    301,    301,    301,    301,    301,    301,    301],
        'scaf0':    [  30387, 110072, 108403, 110072, 108403, 110072,  95786],
        'strand0':  [    '-',    '+',    '+',    '+',    '+',    '+',    '+'],
        'dist0':    [      0,    758,    -30,    813,    -30,    813,    -29],
        'phase1':   [   -302,   -302,   -302,   -302,   -302,   -302,   -302],
        'scaf1':    [     -1,     -1,     -1,     -1,     -1,     -1,     -1],
        'strand1':  [     '',     '',     '',     '',     '',     '',     ''],
        'dist1':    [      0,      0,      0,      0,      0,      0,      0]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    300,    300,    300],
        'pos':      [      0,      1,      2],
        'scaf0':    [ 110072, 108403, 110072],
        'strand0':  [    '+',    '+',    '+'],
        'dist0':    [      0,    -30,    813]
        }) )
    result_loops.append( pd.DataFrame({
        'pid':      [    300,    300,    300,    300,    300,    300,    300],
        'pos':      [      0,      1,      2,      3,      4,      5,      6],
        'scaf0':    [  30387, 110072, 108403, 110072, 108403, 110072,  95786],
        'strand0':  [    '-',    '+',    '+',    '+',    '+',    '+',    '+'],
        'dist0':    [      0,    758,    -30,    813,    -30,    813,    -29]
        }) )
#
    # Test 4
    scaffolds.append( pd.DataFrame({'case':4, 'scaffold':[928, 9067, 12976, 13100, 20542, 45222, 80469]}) )
    scaffold_graph.append( pd.DataFrame([
        {'from':    928, 'from_side':    'r', 'length':      2, 'scaf1':  45222, 'strand1':    '+', 'dist1':    -44},
        {'from':    928, 'from_side':    'r', 'length':      2, 'scaf1':  45222, 'strand1':    '+', 'dist1':     20},
        {'from':   9067, 'from_side':    'l', 'length':      3, 'scaf1':   9067, 'strand1':    '-', 'dist1':   2982, 'scaf2':  80469, 'strand2':    '-', 'dist2':      4},
        {'from':   9067, 'from_side':    'l', 'length':      2, 'scaf1':  80469, 'strand1':    '-', 'dist1':      4},
        {'from':   9067, 'from_side':    'r', 'length':      3, 'scaf1':   9067, 'strand1':    '+', 'dist1':   2982, 'scaf2':  45222, 'strand2':    '-', 'dist2':    397},
        {'from':   9067, 'from_side':    'r', 'length':      2, 'scaf1':  45222, 'strand1':    '-', 'dist1':    397},
        {'from':  12976, 'from_side':    'l', 'length':      2, 'scaf1':  80469, 'strand1':    '+', 'dist1':    -44},
        {'from':  12976, 'from_side':    'r', 'length':      2, 'scaf1':  20542, 'strand1':    '+', 'dist1':    -46},
        {'from':  13100, 'from_side':    'l', 'length':      2, 'scaf1':  20542, 'strand1':    '+', 'dist1':    -43},
        {'from':  13100, 'from_side':    'r', 'length':      2, 'scaf1':  80469, 'strand1':    '+', 'dist1':    -42},
        {'from':  20542, 'from_side':    'l', 'length':      3, 'scaf1':  12976, 'strand1':    '-', 'dist1':    -46, 'scaf2':  80469, 'strand2':    '+', 'dist2':    -44},
        {'from':  20542, 'from_side':    'l', 'length':      3, 'scaf1':  13100, 'strand1':    '+', 'dist1':    -43, 'scaf2':  80469, 'strand2':    '+', 'dist2':    -42},
        {'from':  45222, 'from_side':    'l', 'length':      2, 'scaf1':    928, 'strand1':    '-', 'dist1':    -44},
        {'from':  45222, 'from_side':    'l', 'length':      2, 'scaf1':    928, 'strand1':    '-', 'dist1':     20},
        {'from':  45222, 'from_side':    'r', 'length':      4, 'scaf1':   9067, 'strand1':    '-', 'dist1':    397, 'scaf2':   9067, 'strand2':    '-', 'dist2':   2982, 'scaf3':  80469, 'strand3':    '-', 'dist3':      4},
        {'from':  45222, 'from_side':    'r', 'length':      3, 'scaf1':   9067, 'strand1':    '-', 'dist1':    397, 'scaf2':  80469, 'strand2':    '-', 'dist2':      4},
        {'from':  80469, 'from_side':    'l', 'length':      3, 'scaf1':  12976, 'strand1':    '+', 'dist1':    -44, 'scaf2':  20542, 'strand2':    '+', 'dist2':    -46},
        {'from':  80469, 'from_side':    'l', 'length':      3, 'scaf1':  13100, 'strand1':    '-', 'dist1':    -42, 'scaf2':  20542, 'strand2':    '+', 'dist2':    -43},
        {'from':  80469, 'from_side':    'r', 'length':      4, 'scaf1':   9067, 'strand1':    '+', 'dist1':      4, 'scaf2':   9067, 'strand2':    '+', 'dist2':   2982, 'scaf3':  45222, 'strand3':    '-', 'dist3':    397},
        {'from':  80469, 'from_side':    'r', 'length':      3, 'scaf1':   9067, 'strand1':    '+', 'dist1':      4, 'scaf2':  45222, 'strand2':    '-', 'dist2':    397}
        ]) )
    scaf_bridges.append( pd.DataFrame([
        {'from':    928, 'from_side':    'r', 'to':  45222, 'to_side':    'l', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     10, 'min_dist':    -49, 'max_dist':    -39, 'probability': 0.037322, 'to_alt':      2, 'from_alt':      2},
        {'from':    928, 'from_side':    'r', 'to':  45222, 'to_side':    'l', 'mean_dist':     20, 'mapq':  60060, 'bcount':     10, 'min_dist':      9, 'max_dist':     39, 'probability': 0.037322, 'to_alt':      2, 'from_alt':      2},
        {'from':   9067, 'from_side':    'l', 'to':   9067, 'to_side':    'r', 'mean_dist':   2982, 'mapq':  60060, 'bcount':      7, 'min_dist':   2857, 'max_dist':   3235, 'probability': 0.076700, 'to_alt':      2, 'from_alt':      2},
        {'from':   9067, 'from_side':    'l', 'to':  80469, 'to_side':    'r', 'mean_dist':      4, 'mapq':  60060, 'bcount':     19, 'min_dist':      1, 'max_dist':     19, 'probability': 0.273725, 'to_alt':      1, 'from_alt':      2},
        {'from':   9067, 'from_side':    'r', 'to':   9067, 'to_side':    'l', 'mean_dist':   2982, 'mapq':  60060, 'bcount':      7, 'min_dist':   2857, 'max_dist':   3235, 'probability': 0.076700, 'to_alt':      2, 'from_alt':      2},
        {'from':   9067, 'from_side':    'r', 'to':  45222, 'to_side':    'r', 'mean_dist':    397, 'mapq':  60060, 'bcount':     24, 'min_dist':    379, 'max_dist':    424, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':  12976, 'from_side':    'l', 'to':  80469, 'to_side':    'l', 'mean_dist':    -44, 'mapq':  60060, 'bcount':      9, 'min_dist':    -49, 'max_dist':    -37, 'probability': 0.027818, 'to_alt':      2, 'from_alt':      1},
        {'from':  12976, 'from_side':    'r', 'to':  20542, 'to_side':    'l', 'mean_dist':    -46, 'mapq':  60060, 'bcount':      5, 'min_dist':    -55, 'max_dist':    -35, 'probability': 0.007368, 'to_alt':      2, 'from_alt':      1},
        {'from':  13100, 'from_side':    'l', 'to':  20542, 'to_side':    'l', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     10, 'min_dist':    -54, 'max_dist':    -29, 'probability': 0.037322, 'to_alt':      2, 'from_alt':      1},
        {'from':  13100, 'from_side':    'r', 'to':  80469, 'to_side':    'l', 'mean_dist':    -42, 'mapq':  60060, 'bcount':      9, 'min_dist':    -47, 'max_dist':    -27, 'probability': 0.027818, 'to_alt':      2, 'from_alt':      1},
        {'from':  20542, 'from_side':    'l', 'to':  12976, 'to_side':    'r', 'mean_dist':    -46, 'mapq':  60060, 'bcount':      5, 'min_dist':    -55, 'max_dist':    -35, 'probability': 0.007368, 'to_alt':      1, 'from_alt':      2},
        {'from':  20542, 'from_side':    'l', 'to':  13100, 'to_side':    'l', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     10, 'min_dist':    -54, 'max_dist':    -29, 'probability': 0.037322, 'to_alt':      1, 'from_alt':      2},
        {'from':  45222, 'from_side':    'l', 'to':    928, 'to_side':    'r', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     10, 'min_dist':    -49, 'max_dist':    -39, 'probability': 0.037322, 'to_alt':      2, 'from_alt':      2},
        {'from':  45222, 'from_side':    'l', 'to':    928, 'to_side':    'r', 'mean_dist':     20, 'mapq':  60060, 'bcount':     10, 'min_dist':      9, 'max_dist':     39, 'probability': 0.037322, 'to_alt':      2, 'from_alt':      2},
        {'from':  45222, 'from_side':    'r', 'to':   9067, 'to_side':    'r', 'mean_dist':    397, 'mapq':  60060, 'bcount':     24, 'min_dist':    379, 'max_dist':    424, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  80469, 'from_side':    'l', 'to':  12976, 'to_side':    'l', 'mean_dist':    -44, 'mapq':  60060, 'bcount':      9, 'min_dist':    -49, 'max_dist':    -37, 'probability': 0.027818, 'to_alt':      1, 'from_alt':      2},
        {'from':  80469, 'from_side':    'l', 'to':  13100, 'to_side':    'r', 'mean_dist':    -42, 'mapq':  60060, 'bcount':      9, 'min_dist':    -47, 'max_dist':    -27, 'probability': 0.027818, 'to_alt':      1, 'from_alt':      2},
        {'from':  80469, 'from_side':    'r', 'to':   9067, 'to_side':    'l', 'mean_dist':      4, 'mapq':  60060, 'bcount':     19, 'min_dist':      1, 'max_dist':     19, 'probability': 0.273725, 'to_alt':      2, 'from_alt':      1}
        ]) )
    result_paths.append( pd.DataFrame({
        'pid':      [    400,    400,    400,    400,    400,    400,    400],
        'pos':      [      0,      1,      2,      3,      4,      5,      6],
        'phase0':   [    401,    401,    401,    401,    401,    401,    401],
        'scaf0':    [    928,  45222,   9067,     -1,  80469,  13100,  20542],
        'strand0':  [    '+',    '+',    '-',     '',    '-',    '-',    '+'],
        'dist0':    [      0,     20,    397,      0,      4,    -42,    -43],
        'phase1':   [   -402,    402,   -402,    402,   -402,    402,    402],
        'scaf1':    [     -1,  45222,     -1,   9067,     -1,  12976,  20542],
        'strand1':  [     '',    '+',     '',    '-',     '',    '+',    '+'],
        'dist1':    [      0,    -44,      0,   2982,      0,    -44,    -46]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    400,    400,    400],
        'pos':      [      0,      1,      2],
        'scaf0':    [  80469,  13100,  20542],
        'strand0':  [    '-',    '-',    '+'],
        'dist0':    [      0,    -42,    -43]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    401,    401,    401],
        'pos':      [      0,      1,      2],
        'scaf0':    [  80469,  12976,  20542],
        'strand0':  [    '-',    '+',    '+'],
        'dist0':    [      0,    -44,    -46]
        }) )
    result_loops.append( pd.DataFrame({
        'pid':      [    400,    400,    400],
        'pos':      [      0,      1,      2],
        'scaf0':    [  45222,   9067,  80469],
        'strand0':  [    '+',    '-',    '-'],
        'dist0':    [      0,    397,      4]
        }) )
    result_loops.append( pd.DataFrame({
        'pid':      [    401,    401,    401,    401],
        'pos':      [      0,      1,      2,      3],
        'scaf0':    [  45222,   9067,   9067,  80469],
        'strand0':  [    '+',    '-',    '-',    '-'],
        'dist0':    [      0,    397,   2982,      4]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    400,    400],
        'pos':      [      0,      1],
        'scaf0':    [    928,  45222],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,     20]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    401,    401],
        'pos':      [      0,      1],
        'scaf0':    [    928,  45222],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,    -44]
        }) )
#
    # Test 5
    scaffolds.append( pd.DataFrame({'case':5, 'scaffold':[7, 1440, 7349, 10945, 11769, 23515, 29100, 30446, 31108, 31729, 31737, 31758, 32135, 32420, 45782, 45783, 47750, 49372, 54753, 74998, 76037, 86633, 93920, 95291, 105853, 110006, 113898]}) )
    scaffold_graph.append( pd.DataFrame([
        {'from':      7, 'from_side':    'l', 'length':      4, 'scaf1':  11769, 'strand1':    '+', 'dist1':     76, 'scaf2':   1440, 'strand2':    '+', 'dist2':    -45, 'scaf3': 110006, 'strand3':    '-', 'dist3':    406},
        {'from':      7, 'from_side':    'l', 'length':      2, 'scaf1':  93920, 'strand1':    '+', 'dist1':    383},
        {'from':      7, 'from_side':    'r', 'length':      6, 'scaf1':  45782, 'strand1':    '+', 'dist1':   -230, 'scaf2':  45783, 'strand2':    '+', 'dist2':      0, 'scaf3':  31737, 'strand3':    '+', 'dist3':   1044, 'scaf4':  31758, 'strand4':    '+', 'dist4':     86, 'scaf5':  47750, 'strand5':    '+', 'dist5':    -42},
        {'from':      7, 'from_side':    'r', 'length':      5, 'scaf1':  74998, 'strand1':    '+', 'dist1':   -558, 'scaf2':  32135, 'strand2':    '-', 'dist2':    739, 'scaf3':  45782, 'strand3':    '+', 'dist3':   -502, 'scaf4':  45783, 'strand4':    '+', 'dist4':      0},
        {'from':      7, 'from_side':    'r', 'length':      6, 'scaf1':  74998, 'strand1':    '+', 'dist1':   -558, 'scaf2':  32135, 'strand2':    '-', 'dist2':    739, 'scaf3':  45782, 'strand3':    '+', 'dist3':   -502, 'scaf4':  49372, 'strand4':    '-', 'dist4':    334, 'scaf5':  30446, 'strand5':    '+', 'dist5':   9158},
        {'from':      7, 'from_side':    'r', 'length':      6, 'scaf1':  74998, 'strand1':    '+', 'dist1':   -558, 'scaf2':  32135, 'strand2':    '-', 'dist2':    739, 'scaf3':  45782, 'strand3':    '+', 'dist3':   -502, 'scaf4':  49372, 'strand4':    '-', 'dist4':    334, 'scaf5':  86633, 'strand5':    '-', 'dist5':    432},
        {'from':   1440, 'from_side':    'l', 'length':      5, 'scaf1':  11769, 'strand1':    '-', 'dist1':    -45, 'scaf2':      7, 'strand2':    '+', 'dist2':     76, 'scaf3':  74998, 'strand3':    '+', 'dist3':   -558, 'scaf4':  32135, 'strand4':    '-', 'dist4':    739},
        {'from':   1440, 'from_side':    'r', 'length':      2, 'scaf1': 110006, 'strand1':    '-', 'dist1':    406},
        {'from':   7349, 'from_side':    'l', 'length':      2, 'scaf1': 110006, 'strand1':    '-', 'dist1':    387},
        {'from':   7349, 'from_side':    'r', 'length':      2, 'scaf1':  11769, 'strand1':    '-', 'dist1':    -45},
        {'from':  10945, 'from_side':    'l', 'length':      4, 'scaf1':  95291, 'strand1':    '-', 'dist1':    -45, 'scaf2':  54753, 'strand2':    '-', 'dist2':    -18, 'scaf3':  31108, 'strand3':    '+', 'dist3':      0},
        {'from':  10945, 'from_side':    'r', 'length':      7, 'scaf1': 105853, 'strand1':    '-', 'dist1':    -43, 'scaf2':  32135, 'strand2':    '-', 'dist2':    -45, 'scaf3':  45782, 'strand3':    '+', 'dist3':   -502, 'scaf4':  49372, 'strand4':    '-', 'dist4':    334, 'scaf5':  86633, 'strand5':    '-', 'dist5':    432, 'scaf6':  30446, 'strand6':    '+', 'dist6':    -43},
        {'from':  11769, 'from_side':    'l', 'length':      7, 'scaf1':      7, 'strand1':    '+', 'dist1':     76, 'scaf2':  74998, 'strand2':    '+', 'dist2':   -558, 'scaf3':  32135, 'strand3':    '-', 'dist3':    739, 'scaf4':  45782, 'strand4':    '+', 'dist4':   -502, 'scaf5':  49372, 'strand5':    '-', 'dist5':    334, 'scaf6':  86633, 'strand6':    '-', 'dist6':    432},
        {'from':  11769, 'from_side':    'r', 'length':      3, 'scaf1':   1440, 'strand1':    '+', 'dist1':    -45, 'scaf2': 110006, 'strand2':    '-', 'dist2':    406},
        {'from':  11769, 'from_side':    'r', 'length':      3, 'scaf1':   7349, 'strand1':    '-', 'dist1':    -45, 'scaf2': 110006, 'strand2':    '-', 'dist2':    387},
        {'from':  23515, 'from_side':    'r', 'length':      5, 'scaf1':  95291, 'strand1':    '+', 'dist1':      3, 'scaf2':  29100, 'strand2':    '-', 'dist2':    -43, 'scaf3': 105853, 'strand3':    '-', 'dist3':    -44, 'scaf4':  32420, 'strand4':    '-', 'dist4':    -44},
        {'from':  29100, 'from_side':    'l', 'length':      3, 'scaf1': 105853, 'strand1':    '-', 'dist1':    -44, 'scaf2':  32420, 'strand2':    '-', 'dist2':    -44},
        {'from':  29100, 'from_side':    'r', 'length':      3, 'scaf1':  95291, 'strand1':    '-', 'dist1':    -43, 'scaf2':  23515, 'strand2':    '-', 'dist2':      3},
        {'from':  30446, 'from_side':    'l', 'length':      6, 'scaf1':  49372, 'strand1':    '+', 'dist1':   9158, 'scaf2':  45782, 'strand2':    '-', 'dist2':    334, 'scaf3':  32135, 'strand3':    '+', 'dist3':   -502, 'scaf4':  74998, 'strand4':    '-', 'dist4':    739, 'scaf5':      7, 'strand5':    '-', 'dist5':   -558},
        {'from':  30446, 'from_side':    'l', 'length':      8, 'scaf1':  86633, 'strand1':    '+', 'dist1':    -43, 'scaf2':  49372, 'strand2':    '+', 'dist2':    432, 'scaf3':  45782, 'strand3':    '-', 'dist3':    334, 'scaf4':  32135, 'strand4':    '+', 'dist4':   -502, 'scaf5': 105853, 'strand5':    '+', 'dist5':    -45, 'scaf6':  10945, 'strand6':    '-', 'dist6':    -43, 'scaf7':  95291, 'strand7':    '-', 'dist7':    -45},
        {'from':  30446, 'from_side':    'r', 'length':      4, 'scaf1':  31729, 'strand1':    '+', 'dist1':    630, 'scaf2':  31758, 'strand2':    '+', 'dist2':   -399, 'scaf3':  47750, 'strand3':    '+', 'dist3':    -42},
        {'from':  30446, 'from_side':    'r', 'length':      4, 'scaf1':  31729, 'strand1':    '+', 'dist1':    630, 'scaf2':  31758, 'strand2':    '+', 'dist2':   -399, 'scaf3': 113898, 'strand3':    '+', 'dist3':    686},
        {'from':  30446, 'from_side':    'r', 'length':      5, 'scaf1':  76037, 'strand1':    '-', 'dist1':      0, 'scaf2':  31729, 'strand2':    '+', 'dist2':   -162, 'scaf3':  31758, 'strand3':    '+', 'dist3':   -399, 'scaf4':  47750, 'strand4':    '+', 'dist4':    -42},
        {'from':  30446, 'from_side':    'r', 'length':      5, 'scaf1':  76037, 'strand1':    '-', 'dist1':      0, 'scaf2':  31729, 'strand2':    '+', 'dist2':   -162, 'scaf3':  31758, 'strand3':    '+', 'dist3':   -399, 'scaf4': 113898, 'strand4':    '+', 'dist4':    686},
        {'from':  30446, 'from_side':    'r', 'length':      3, 'scaf1':  76037, 'strand1':    '-', 'dist1':      0, 'scaf2':  31737, 'strand2':    '+', 'dist2':   -168},
        {'from':  31108, 'from_side':    'l', 'length':      8, 'scaf1':  54753, 'strand1':    '+', 'dist1':      0, 'scaf2':  95291, 'strand2':    '+', 'dist2':    -18, 'scaf3':  10945, 'strand3':    '+', 'dist3':    -45, 'scaf4': 105853, 'strand4':    '-', 'dist4':    -43, 'scaf5':  32135, 'strand5':    '-', 'dist5':    -45, 'scaf6':  45782, 'strand6':    '+', 'dist6':   -502, 'scaf7':  49372, 'strand7':    '-', 'dist7':    334},
        {'from':  31108, 'from_side':    'l', 'length':      2, 'scaf1':  93920, 'strand1':    '-', 'dist1':  -4681},
        {'from':  31729, 'from_side':    'l', 'length':      6, 'scaf1':  30446, 'strand1':    '-', 'dist1':    630, 'scaf2':  86633, 'strand2':    '+', 'dist2':    -43, 'scaf3':  49372, 'strand3':    '+', 'dist3':    432, 'scaf4':  45782, 'strand4':    '-', 'dist4':    334, 'scaf5':  32135, 'strand5':    '+', 'dist5':   -502},
        {'from':  31729, 'from_side':    'l', 'length':      5, 'scaf1':  76037, 'strand1':    '+', 'dist1':   -162, 'scaf2':  30446, 'strand2':    '-', 'dist2':      0, 'scaf3':  86633, 'strand3':    '+', 'dist3':    -43, 'scaf4':  49372, 'strand4':    '+', 'dist4':    432},
        {'from':  31729, 'from_side':    'r', 'length':      3, 'scaf1':  31758, 'strand1':    '+', 'dist1':   -399, 'scaf2':  47750, 'strand2':    '+', 'dist2':    -42},
        {'from':  31729, 'from_side':    'r', 'length':      3, 'scaf1':  31758, 'strand1':    '+', 'dist1':   -399, 'scaf2': 113898, 'strand2':    '+', 'dist2':    686},
        {'from':  31737, 'from_side':    'l', 'length':      5, 'scaf1':  45783, 'strand1':    '-', 'dist1':   1044, 'scaf2':  45782, 'strand2':    '-', 'dist2':      0, 'scaf3':      7, 'strand3':    '-', 'dist3':   -230, 'scaf4':  93920, 'strand4':    '+', 'dist4':    383},
        {'from':  31737, 'from_side':    'l', 'length':      3, 'scaf1':  76037, 'strand1':    '+', 'dist1':   -168, 'scaf2':  30446, 'strand2':    '-', 'dist2':      0},
        {'from':  31737, 'from_side':    'r', 'length':      4, 'scaf1':  31758, 'strand1':    '+', 'dist1':     86, 'scaf2':  47750, 'strand2':    '+', 'dist2':    -42, 'scaf3': 113898, 'strand3':    '+', 'dist3':    -10},
        {'from':  31758, 'from_side':    'l', 'length':      7, 'scaf1':  31729, 'strand1':    '-', 'dist1':   -399, 'scaf2':  30446, 'strand2':    '-', 'dist2':    630, 'scaf3':  86633, 'strand3':    '+', 'dist3':    -43, 'scaf4':  49372, 'strand4':    '+', 'dist4':    432, 'scaf5':  45782, 'strand5':    '-', 'dist5':    334, 'scaf6':  32135, 'strand6':    '+', 'dist6':   -502},
        {'from':  31758, 'from_side':    'l', 'length':      6, 'scaf1':  31729, 'strand1':    '-', 'dist1':   -399, 'scaf2':  76037, 'strand2':    '+', 'dist2':   -162, 'scaf3':  30446, 'strand3':    '-', 'dist3':      0, 'scaf4':  86633, 'strand4':    '+', 'dist4':    -43, 'scaf5':  49372, 'strand5':    '+', 'dist5':    432},
        {'from':  31758, 'from_side':    'l', 'length':      6, 'scaf1':  31737, 'strand1':    '-', 'dist1':     86, 'scaf2':  45783, 'strand2':    '-', 'dist2':   1044, 'scaf3':  45782, 'strand3':    '-', 'dist3':      0, 'scaf4':      7, 'strand4':    '-', 'dist4':   -230, 'scaf5':  93920, 'strand5':    '+', 'dist5':    383},
        {'from':  31758, 'from_side':    'l', 'length':      3, 'scaf1':  31737, 'strand1':    '-', 'dist1':     86, 'scaf2':  76037, 'strand2':    '+', 'dist2':   -168},
        {'from':  31758, 'from_side':    'r', 'length':      3, 'scaf1':  47750, 'strand1':    '+', 'dist1':    -42, 'scaf2': 113898, 'strand2':    '+', 'dist2':    -10},
        {'from':  31758, 'from_side':    'r', 'length':      2, 'scaf1': 113898, 'strand1':    '+', 'dist1':    686},
        {'from':  32135, 'from_side':    'l', 'length':      3, 'scaf1':  45782, 'strand1':    '+', 'dist1':   -502, 'scaf2':  45783, 'strand2':    '+', 'dist2':      0},
        {'from':  32135, 'from_side':    'l', 'length':      4, 'scaf1':  45782, 'strand1':    '+', 'dist1':   -502, 'scaf2':  49372, 'strand2':    '-', 'dist2':    334, 'scaf3':  30446, 'strand3':    '+', 'dist3':   9158},
        {'from':  32135, 'from_side':    'l', 'length':      8, 'scaf1':  45782, 'strand1':    '+', 'dist1':   -502, 'scaf2':  49372, 'strand2':    '-', 'dist2':    334, 'scaf3':  86633, 'strand3':    '-', 'dist3':    432, 'scaf4':  30446, 'strand4':    '+', 'dist4':    -43, 'scaf5':  31729, 'strand5':    '+', 'dist5':    630, 'scaf6':  31758, 'strand6':    '+', 'dist6':   -399, 'scaf7': 113898, 'strand7':    '+', 'dist7':    686},
        {'from':  32135, 'from_side':    'r', 'length':      6, 'scaf1':  74998, 'strand1':    '-', 'dist1':    739, 'scaf2':      7, 'strand2':    '-', 'dist2':   -558, 'scaf3':  11769, 'strand3':    '+', 'dist3':     76, 'scaf4':   1440, 'strand4':    '+', 'dist4':    -45, 'scaf5': 110006, 'strand5':    '-', 'dist5':    406},
        {'from':  32135, 'from_side':    'r', 'length':      4, 'scaf1':  74998, 'strand1':    '-', 'dist1':    739, 'scaf2':      7, 'strand2':    '-', 'dist2':   -558, 'scaf3':  93920, 'strand3':    '+', 'dist3':    383},
        {'from':  32135, 'from_side':    'r', 'length':      6, 'scaf1': 105853, 'strand1':    '+', 'dist1':    -45, 'scaf2':  10945, 'strand2':    '-', 'dist2':    -43, 'scaf3':  95291, 'strand3':    '-', 'dist3':    -45, 'scaf4':  54753, 'strand4':    '-', 'dist4':    -18, 'scaf5':  31108, 'strand5':    '+', 'dist5':      0},
        {'from':  32420, 'from_side':    'r', 'length':      5, 'scaf1': 105853, 'strand1':    '+', 'dist1':    -44, 'scaf2':  29100, 'strand2':    '+', 'dist2':    -44, 'scaf3':  95291, 'strand3':    '-', 'dist3':    -43, 'scaf4':  23515, 'strand4':    '-', 'dist4':      3},
        {'from':  45782, 'from_side':    'l', 'length':      3, 'scaf1':      7, 'strand1':    '-', 'dist1':   -230, 'scaf2':  93920, 'strand2':    '+', 'dist2':    383},
        {'from':  45782, 'from_side':    'l', 'length':      5, 'scaf1':  32135, 'strand1':    '+', 'dist1':   -502, 'scaf2':  74998, 'strand2':    '-', 'dist2':    739, 'scaf3':      7, 'strand3':    '-', 'dist3':   -558, 'scaf4':  11769, 'strand4':    '+', 'dist4':     76},
        {'from':  45782, 'from_side':    'l', 'length':      5, 'scaf1':  32135, 'strand1':    '+', 'dist1':   -502, 'scaf2':  74998, 'strand2':    '-', 'dist2':    739, 'scaf3':      7, 'strand3':    '-', 'dist3':   -558, 'scaf4':  93920, 'strand4':    '+', 'dist4':    383},
        {'from':  45782, 'from_side':    'l', 'length':      7, 'scaf1':  32135, 'strand1':    '+', 'dist1':   -502, 'scaf2': 105853, 'strand2':    '+', 'dist2':    -45, 'scaf3':  10945, 'strand3':    '-', 'dist3':    -43, 'scaf4':  95291, 'strand4':    '-', 'dist4':    -45, 'scaf5':  54753, 'strand5':    '-', 'dist5':    -18, 'scaf6':  31108, 'strand6':    '+', 'dist6':      0},
        {'from':  45782, 'from_side':    'r', 'length':      5, 'scaf1':  45783, 'strand1':    '+', 'dist1':      0, 'scaf2':  31737, 'strand2':    '+', 'dist2':   1044, 'scaf3':  31758, 'strand3':    '+', 'dist3':     86, 'scaf4':  47750, 'strand4':    '+', 'dist4':    -42},
        {'from':  45782, 'from_side':    'r', 'length':      3, 'scaf1':  49372, 'strand1':    '-', 'dist1':    334, 'scaf2':  30446, 'strand2':    '+', 'dist2':   9158},
        {'from':  45782, 'from_side':    'r', 'length':      7, 'scaf1':  49372, 'strand1':    '-', 'dist1':    334, 'scaf2':  86633, 'strand2':    '-', 'dist2':    432, 'scaf3':  30446, 'strand3':    '+', 'dist3':    -43, 'scaf4':  31729, 'strand4':    '+', 'dist4':    630, 'scaf5':  31758, 'strand5':    '+', 'dist5':   -399, 'scaf6': 113898, 'strand6':    '+', 'dist6':    686},
        {'from':  45783, 'from_side':    'l', 'length':      4, 'scaf1':  45782, 'strand1':    '-', 'dist1':      0, 'scaf2':      7, 'strand2':    '-', 'dist2':   -230, 'scaf3':  93920, 'strand3':    '+', 'dist3':    383},
        {'from':  45783, 'from_side':    'l', 'length':      6, 'scaf1':  45782, 'strand1':    '-', 'dist1':      0, 'scaf2':  32135, 'strand2':    '+', 'dist2':   -502, 'scaf3':  74998, 'strand3':    '-', 'dist3':    739, 'scaf4':      7, 'strand4':    '-', 'dist4':   -558, 'scaf5':  93920, 'strand5':    '+', 'dist5':    383},
        {'from':  45783, 'from_side':    'r', 'length':      5, 'scaf1':  31737, 'strand1':    '+', 'dist1':   1044, 'scaf2':  31758, 'strand2':    '+', 'dist2':     86, 'scaf3':  47750, 'strand3':    '+', 'dist3':    -42, 'scaf4': 113898, 'strand4':    '+', 'dist4':    -10},
        {'from':  47750, 'from_side':    'l', 'length':      4, 'scaf1':  31758, 'strand1':    '-', 'dist1':    -42, 'scaf2':  31729, 'strand2':    '-', 'dist2':   -399, 'scaf3':  30446, 'strand3':    '-', 'dist3':    630},
        {'from':  47750, 'from_side':    'l', 'length':      5, 'scaf1':  31758, 'strand1':    '-', 'dist1':    -42, 'scaf2':  31729, 'strand2':    '-', 'dist2':   -399, 'scaf3':  76037, 'strand3':    '+', 'dist3':   -162, 'scaf4':  30446, 'strand4':    '-', 'dist4':      0},
        {'from':  47750, 'from_side':    'l', 'length':      7, 'scaf1':  31758, 'strand1':    '-', 'dist1':    -42, 'scaf2':  31737, 'strand2':    '-', 'dist2':     86, 'scaf3':  45783, 'strand3':    '-', 'dist3':   1044, 'scaf4':  45782, 'strand4':    '-', 'dist4':      0, 'scaf5':      7, 'strand5':    '-', 'dist5':   -230, 'scaf6':  93920, 'strand6':    '+', 'dist6':    383},
        {'from':  47750, 'from_side':    'l', 'length':      4, 'scaf1':  31758, 'strand1':    '-', 'dist1':    -42, 'scaf2':  31737, 'strand2':    '-', 'dist2':     86, 'scaf3':  76037, 'strand3':    '+', 'dist3':   -168},
        {'from':  47750, 'from_side':    'r', 'length':      2, 'scaf1': 113898, 'strand1':    '+', 'dist1':    -10},
        {'from':  49372, 'from_side':    'l', 'length':      2, 'scaf1':  30446, 'strand1':    '+', 'dist1':   9158},
        {'from':  49372, 'from_side':    'l', 'length':      6, 'scaf1':  86633, 'strand1':    '-', 'dist1':    432, 'scaf2':  30446, 'strand2':    '+', 'dist2':    -43, 'scaf3':  31729, 'strand3':    '+', 'dist3':    630, 'scaf4':  31758, 'strand4':    '+', 'dist4':   -399, 'scaf5': 113898, 'strand5':    '+', 'dist5':    686},
        {'from':  49372, 'from_side':    'l', 'length':      6, 'scaf1':  86633, 'strand1':    '-', 'dist1':    432, 'scaf2':  30446, 'strand2':    '+', 'dist2':    -43, 'scaf3':  76037, 'strand3':    '-', 'dist3':      0, 'scaf4':  31729, 'strand4':    '+', 'dist4':   -162, 'scaf5':  31758, 'strand5':    '+', 'dist5':   -399},
        {'from':  49372, 'from_side':    'r', 'length':      6, 'scaf1':  45782, 'strand1':    '-', 'dist1':    334, 'scaf2':  32135, 'strand2':    '+', 'dist2':   -502, 'scaf3':  74998, 'strand3':    '-', 'dist3':    739, 'scaf4':      7, 'strand4':    '-', 'dist4':   -558, 'scaf5':  11769, 'strand5':    '+', 'dist5':     76},
        {'from':  49372, 'from_side':    'r', 'length':      8, 'scaf1':  45782, 'strand1':    '-', 'dist1':    334, 'scaf2':  32135, 'strand2':    '+', 'dist2':   -502, 'scaf3': 105853, 'strand3':    '+', 'dist3':    -45, 'scaf4':  10945, 'strand4':    '-', 'dist4':    -43, 'scaf5':  95291, 'strand5':    '-', 'dist5':    -45, 'scaf6':  54753, 'strand6':    '-', 'dist6':    -18, 'scaf7':  31108, 'strand7':    '+', 'dist7':      0},
        {'from':  54753, 'from_side':    'l', 'length':      2, 'scaf1':  31108, 'strand1':    '+', 'dist1':      0},
        {'from':  54753, 'from_side':    'r', 'length':      7, 'scaf1':  95291, 'strand1':    '+', 'dist1':    -18, 'scaf2':  10945, 'strand2':    '+', 'dist2':    -45, 'scaf3': 105853, 'strand3':    '-', 'dist3':    -43, 'scaf4':  32135, 'strand4':    '-', 'dist4':    -45, 'scaf5':  45782, 'strand5':    '+', 'dist5':   -502, 'scaf6':  49372, 'strand6':    '-', 'dist6':    334},
        {'from':  74998, 'from_side':    'l', 'length':      5, 'scaf1':      7, 'strand1':    '-', 'dist1':   -558, 'scaf2':  11769, 'strand2':    '+', 'dist2':     76, 'scaf3':   1440, 'strand3':    '+', 'dist3':    -45, 'scaf4': 110006, 'strand4':    '-', 'dist4':    406},
        {'from':  74998, 'from_side':    'l', 'length':      3, 'scaf1':      7, 'strand1':    '-', 'dist1':   -558, 'scaf2':  93920, 'strand2':    '+', 'dist2':    383},
        {'from':  74998, 'from_side':    'r', 'length':      4, 'scaf1':  32135, 'strand1':    '-', 'dist1':    739, 'scaf2':  45782, 'strand2':    '+', 'dist2':   -502, 'scaf3':  45783, 'strand3':    '+', 'dist3':      0},
        {'from':  74998, 'from_side':    'r', 'length':      5, 'scaf1':  32135, 'strand1':    '-', 'dist1':    739, 'scaf2':  45782, 'strand2':    '+', 'dist2':   -502, 'scaf3':  49372, 'strand3':    '-', 'dist3':    334, 'scaf4':  30446, 'strand4':    '+', 'dist4':   9158},
        {'from':  74998, 'from_side':    'r', 'length':      5, 'scaf1':  32135, 'strand1':    '-', 'dist1':    739, 'scaf2':  45782, 'strand2':    '+', 'dist2':   -502, 'scaf3':  49372, 'strand3':    '-', 'dist3':    334, 'scaf4':  86633, 'strand4':    '-', 'dist4':    432},
        {'from':  76037, 'from_side':    'l', 'length':      4, 'scaf1':  31729, 'strand1':    '+', 'dist1':   -162, 'scaf2':  31758, 'strand2':    '+', 'dist2':   -399, 'scaf3':  47750, 'strand3':    '+', 'dist3':    -42},
        {'from':  76037, 'from_side':    'l', 'length':      4, 'scaf1':  31729, 'strand1':    '+', 'dist1':   -162, 'scaf2':  31758, 'strand2':    '+', 'dist2':   -399, 'scaf3': 113898, 'strand3':    '+', 'dist3':    686},
        {'from':  76037, 'from_side':    'l', 'length':      5, 'scaf1':  31737, 'strand1':    '+', 'dist1':   -168, 'scaf2':  31758, 'strand2':    '+', 'dist2':     86, 'scaf3':  47750, 'strand3':    '+', 'dist3':    -42, 'scaf4': 113898, 'strand4':    '+', 'dist4':    -10},
        {'from':  76037, 'from_side':    'r', 'length':      4, 'scaf1':  30446, 'strand1':    '-', 'dist1':      0, 'scaf2':  86633, 'strand2':    '+', 'dist2':    -43, 'scaf3':  49372, 'strand3':    '+', 'dist3':    432},
        {'from':  86633, 'from_side':    'l', 'length':      5, 'scaf1':  30446, 'strand1':    '+', 'dist1':    -43, 'scaf2':  31729, 'strand2':    '+', 'dist2':    630, 'scaf3':  31758, 'strand3':    '+', 'dist3':   -399, 'scaf4': 113898, 'strand4':    '+', 'dist4':    686},
        {'from':  86633, 'from_side':    'l', 'length':      6, 'scaf1':  30446, 'strand1':    '+', 'dist1':    -43, 'scaf2':  76037, 'strand2':    '-', 'dist2':      0, 'scaf3':  31729, 'strand3':    '+', 'dist3':   -162, 'scaf4':  31758, 'strand4':    '+', 'dist4':   -399, 'scaf5': 113898, 'strand5':    '+', 'dist5':    686},
        {'from':  86633, 'from_side':    'r', 'length':      7, 'scaf1':  49372, 'strand1':    '+', 'dist1':    432, 'scaf2':  45782, 'strand2':    '-', 'dist2':    334, 'scaf3':  32135, 'strand3':    '+', 'dist3':   -502, 'scaf4':  74998, 'strand4':    '-', 'dist4':    739, 'scaf5':      7, 'strand5':    '-', 'dist5':   -558, 'scaf6':  11769, 'strand6':    '+', 'dist6':     76},
        {'from':  86633, 'from_side':    'r', 'length':      7, 'scaf1':  49372, 'strand1':    '+', 'dist1':    432, 'scaf2':  45782, 'strand2':    '-', 'dist2':    334, 'scaf3':  32135, 'strand3':    '+', 'dist3':   -502, 'scaf4': 105853, 'strand4':    '+', 'dist4':    -45, 'scaf5':  10945, 'strand5':    '-', 'dist5':    -43, 'scaf6':  95291, 'strand6':    '-', 'dist6':    -45},
        {'from':  93920, 'from_side':    'l', 'length':      7, 'scaf1':      7, 'strand1':    '+', 'dist1':    383, 'scaf2':  45782, 'strand2':    '+', 'dist2':   -230, 'scaf3':  45783, 'strand3':    '+', 'dist3':      0, 'scaf4':  31737, 'strand4':    '+', 'dist4':   1044, 'scaf5':  31758, 'strand5':    '+', 'dist5':     86, 'scaf6':  47750, 'strand6':    '+', 'dist6':    -42},
        {'from':  93920, 'from_side':    'l', 'length':      6, 'scaf1':      7, 'strand1':    '+', 'dist1':    383, 'scaf2':  74998, 'strand2':    '+', 'dist2':   -558, 'scaf3':  32135, 'strand3':    '-', 'dist3':    739, 'scaf4':  45782, 'strand4':    '+', 'dist4':   -502, 'scaf5':  45783, 'strand5':    '+', 'dist5':      0},
        {'from':  93920, 'from_side':    'r', 'length':      2, 'scaf1':  31108, 'strand1':    '+', 'dist1':  -4681},
        {'from':  95291, 'from_side':    'l', 'length':      2, 'scaf1':  23515, 'strand1':    '-', 'dist1':      3},
        {'from':  95291, 'from_side':    'l', 'length':      3, 'scaf1':  54753, 'strand1':    '-', 'dist1':    -18, 'scaf2':  31108, 'strand2':    '+', 'dist2':      0},
        {'from':  95291, 'from_side':    'r', 'length':      8, 'scaf1':  10945, 'strand1':    '+', 'dist1':    -45, 'scaf2': 105853, 'strand2':    '-', 'dist2':    -43, 'scaf3':  32135, 'strand3':    '-', 'dist3':    -45, 'scaf4':  45782, 'strand4':    '+', 'dist4':   -502, 'scaf5':  49372, 'strand5':    '-', 'dist5':    334, 'scaf6':  86633, 'strand6':    '-', 'dist6':    432, 'scaf7':  30446, 'strand7':    '+', 'dist7':    -43},
        {'from':  95291, 'from_side':    'r', 'length':      4, 'scaf1':  29100, 'strand1':    '-', 'dist1':    -43, 'scaf2': 105853, 'strand2':    '-', 'dist2':    -44, 'scaf3':  32420, 'strand3':    '-', 'dist3':    -44},
        {'from': 105853, 'from_side':    'l', 'length':      6, 'scaf1':  32135, 'strand1':    '-', 'dist1':    -45, 'scaf2':  45782, 'strand2':    '+', 'dist2':   -502, 'scaf3':  49372, 'strand3':    '-', 'dist3':    334, 'scaf4':  86633, 'strand4':    '-', 'dist4':    432, 'scaf5':  30446, 'strand5':    '+', 'dist5':    -43},
        {'from': 105853, 'from_side':    'l', 'length':      2, 'scaf1':  32420, 'strand1':    '-', 'dist1':    -44},
        {'from': 105853, 'from_side':    'r', 'length':      5, 'scaf1':  10945, 'strand1':    '-', 'dist1':    -43, 'scaf2':  95291, 'strand2':    '-', 'dist2':    -45, 'scaf3':  54753, 'strand3':    '-', 'dist3':    -18, 'scaf4':  31108, 'strand4':    '+', 'dist4':      0},
        {'from': 105853, 'from_side':    'r', 'length':      4, 'scaf1':  29100, 'strand1':    '+', 'dist1':    -44, 'scaf2':  95291, 'strand2':    '-', 'dist2':    -43, 'scaf3':  23515, 'strand3':    '-', 'dist3':      3},
        {'from': 110006, 'from_side':    'r', 'length':      6, 'scaf1':   1440, 'strand1':    '-', 'dist1':    406, 'scaf2':  11769, 'strand2':    '-', 'dist2':    -45, 'scaf3':      7, 'strand3':    '+', 'dist3':     76, 'scaf4':  74998, 'strand4':    '+', 'dist4':   -558, 'scaf5':  32135, 'strand5':    '-', 'dist5':    739},
        {'from': 110006, 'from_side':    'r', 'length':      3, 'scaf1':   7349, 'strand1':    '+', 'dist1':    387, 'scaf2':  11769, 'strand2':    '-', 'dist2':    -45},
        {'from': 113898, 'from_side':    'l', 'length':      8, 'scaf1':  31758, 'strand1':    '-', 'dist1':    686, 'scaf2':  31729, 'strand2':    '-', 'dist2':   -399, 'scaf3':  30446, 'strand3':    '-', 'dist3':    630, 'scaf4':  86633, 'strand4':    '+', 'dist4':    -43, 'scaf5':  49372, 'strand5':    '+', 'dist5':    432, 'scaf6':  45782, 'strand6':    '-', 'dist6':    334, 'scaf7':  32135, 'strand7':    '+', 'dist7':   -502},
        {'from': 113898, 'from_side':    'l', 'length':      6, 'scaf1':  31758, 'strand1':    '-', 'dist1':    686, 'scaf2':  31729, 'strand2':    '-', 'dist2':   -399, 'scaf3':  76037, 'strand3':    '+', 'dist3':   -162, 'scaf4':  30446, 'strand4':    '-', 'dist4':      0, 'scaf5':  86633, 'strand5':    '+', 'dist5':    -43},
        {'from': 113898, 'from_side':    'l', 'length':      5, 'scaf1':  47750, 'strand1':    '-', 'dist1':    -10, 'scaf2':  31758, 'strand2':    '-', 'dist2':    -42, 'scaf3':  31737, 'strand3':    '-', 'dist3':     86, 'scaf4':  45783, 'strand4':    '-', 'dist4':   1044},
        {'from': 113898, 'from_side':    'l', 'length':      5, 'scaf1':  47750, 'strand1':    '-', 'dist1':    -10, 'scaf2':  31758, 'strand2':    '-', 'dist2':    -42, 'scaf3':  31737, 'strand3':    '-', 'dist3':     86, 'scaf4':  76037, 'strand4':    '+', 'dist4':   -168},
        ]) )
    scaf_bridges.append( pd.DataFrame([
        {'from':      7, 'from_side':    'l', 'to':  11769, 'to_side':    'l', 'mean_dist':     76, 'mapq':  60060, 'bcount':     16, 'min_dist':     67, 'max_dist':     90, 'probability': 0.159802, 'to_alt':      1, 'from_alt':      2},
        {'from':      7, 'from_side':    'l', 'to':  93920, 'to_side':    'l', 'mean_dist':    383, 'mapq':  60060, 'bcount':     38, 'min_dist':    364, 'max_dist':    425, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':      7, 'from_side':    'r', 'to':  45782, 'to_side':    'l', 'mean_dist':   -230, 'mapq':  60060, 'bcount':     42, 'min_dist':   -253, 'max_dist':   -155, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      2},
        {'from':      7, 'from_side':    'r', 'to':  74998, 'to_side':    'l', 'mean_dist':   -558, 'mapq':  60060, 'bcount':     40, 'min_dist':   -659, 'max_dist':   -401, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':   1440, 'from_side':    'l', 'to':  11769, 'to_side':    'r', 'mean_dist':    -45, 'mapq':  60060, 'bcount':     13, 'min_dist':    -54, 'max_dist':    -37, 'probability': 0.082422, 'to_alt':      2, 'from_alt':      1},
        {'from':   1440, 'from_side':    'r', 'to': 110006, 'to_side':    'r', 'mean_dist':    406, 'mapq':  60060, 'bcount':     11, 'min_dist':    386, 'max_dist':    433, 'probability': 0.081320, 'to_alt':      2, 'from_alt':      1},
        {'from':   7349, 'from_side':    'l', 'to': 110006, 'to_side':    'r', 'mean_dist':    387, 'mapq':  60060, 'bcount':      6, 'min_dist':    371, 'max_dist':    408, 'probability': 0.016554, 'to_alt':      2, 'from_alt':      1},
        {'from':   7349, 'from_side':    'r', 'to':  11769, 'to_side':    'r', 'mean_dist':    -45, 'mapq':  60060, 'bcount':      8, 'min_dist':    -51, 'max_dist':    -39, 'probability': 0.020422, 'to_alt':      2, 'from_alt':      1},
        {'from':  10945, 'from_side':    'l', 'to':  95291, 'to_side':    'r', 'mean_dist':    -45, 'mapq':  60060, 'bcount':     16, 'min_dist':    -49, 'max_dist':    -40, 'probability': 0.159802, 'to_alt':      2, 'from_alt':      1},
        {'from':  10945, 'from_side':    'r', 'to': 105853, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     15, 'min_dist':    -49, 'max_dist':    -15, 'probability': 0.129976, 'to_alt':      2, 'from_alt':      1},
        {'from':  11769, 'from_side':    'l', 'to':      7, 'to_side':    'l', 'mean_dist':     76, 'mapq':  60060, 'bcount':     16, 'min_dist':     67, 'max_dist':     90, 'probability': 0.159802, 'to_alt':      2, 'from_alt':      1},
        {'from':  11769, 'from_side':    'r', 'to':   1440, 'to_side':    'l', 'mean_dist':    -45, 'mapq':  60060, 'bcount':     13, 'min_dist':    -54, 'max_dist':    -37, 'probability': 0.082422, 'to_alt':      1, 'from_alt':      2},
        {'from':  11769, 'from_side':    'r', 'to':   7349, 'to_side':    'r', 'mean_dist':    -45, 'mapq':  60060, 'bcount':      8, 'min_dist':    -51, 'max_dist':    -39, 'probability': 0.020422, 'to_alt':      1, 'from_alt':      2},
        {'from':  23515, 'from_side':    'r', 'to':  95291, 'to_side':    'l', 'mean_dist':      3, 'mapq':  60060, 'bcount':     13, 'min_dist':      1, 'max_dist':      8, 'probability': 0.082422, 'to_alt':      2, 'from_alt':      1},
        {'from':  29100, 'from_side':    'l', 'to': 105853, 'to_side':    'r', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     17, 'min_dist':    -48, 'max_dist':    -38, 'probability': 0.193782, 'to_alt':      2, 'from_alt':      1},
        {'from':  29100, 'from_side':    'r', 'to':  95291, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     14, 'min_dist':    -50, 'max_dist':    -34, 'probability': 0.104244, 'to_alt':      2, 'from_alt':      1},
        {'from':  30446, 'from_side':    'l', 'to':  49372, 'to_side':    'l', 'mean_dist':   9158, 'mapq':  60060, 'bcount':      2, 'min_dist':   8995, 'max_dist':   9322, 'probability': 0.096053, 'to_alt':      3, 'from_alt':      2},
        {'from':  30446, 'from_side':    'l', 'to':  86633, 'to_side':    'l', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     30, 'min_dist':    -50, 'max_dist':    -32, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':  30446, 'from_side':    'r', 'to':  31729, 'to_side':    'l', 'mean_dist':    630, 'mapq':  60060, 'bcount':     17, 'min_dist':    586, 'max_dist':    882, 'probability': 0.303341, 'to_alt':      2, 'from_alt':      2},
        {'from':  30446, 'from_side':    'r', 'to':  76037, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     27, 'min_dist':      0, 'max_dist':      0, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':  31108, 'from_side':    'l', 'to':  54753, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     14, 'min_dist':      0, 'max_dist':      0, 'probability': 0.104244, 'to_alt':      2, 'from_alt':      1},
        {'from':  31108, 'from_side':    'l', 'to':  93920, 'to_side':    'r', 'mean_dist':  -4681, 'mapq':  60060, 'bcount':      6, 'min_dist':  -4716, 'max_dist':  -4577, 'probability': 0.108994, 'to_alt':      2, 'from_alt':      1},
        {'from':  31729, 'from_side':    'l', 'to':  30446, 'to_side':    'r', 'mean_dist':    630, 'mapq':  60060, 'bcount':     17, 'min_dist':    586, 'max_dist':    882, 'probability': 0.303341, 'to_alt':      2, 'from_alt':      2},
        {'from':  31729, 'from_side':    'l', 'to':  76037, 'to_side':    'l', 'mean_dist':   -162, 'mapq':  60060, 'bcount':     14, 'min_dist':   -174, 'max_dist':   -147, 'probability': 0.104244, 'to_alt':      2, 'from_alt':      2},
        {'from':  31729, 'from_side':    'r', 'to':  31758, 'to_side':    'l', 'mean_dist':   -399, 'mapq':  60060, 'bcount':     21, 'min_dist':   -463, 'max_dist':   -335, 'probability': 0.367256, 'to_alt':      2, 'from_alt':      1},
        {'from':  31737, 'from_side':    'l', 'to':  45783, 'to_side':    'r', 'mean_dist':   1044, 'mapq':  60060, 'bcount':      9, 'min_dist':    996, 'max_dist':   1138, 'probability': 0.045509, 'to_alt':      1, 'from_alt':      2},
        {'from':  31737, 'from_side':    'l', 'to':  76037, 'to_side':    'l', 'mean_dist':   -168, 'mapq':  60060, 'bcount':      9, 'min_dist':   -186, 'max_dist':   -143, 'probability': 0.027818, 'to_alt':      2, 'from_alt':      2},
        {'from':  31737, 'from_side':    'r', 'to':  31758, 'to_side':    'l', 'mean_dist':     86, 'mapq':  60060, 'bcount':     18, 'min_dist':     69, 'max_dist':    120, 'probability': 0.231835, 'to_alt':      2, 'from_alt':      1},
        {'from':  31758, 'from_side':    'l', 'to':  31729, 'to_side':    'r', 'mean_dist':   -399, 'mapq':  60060, 'bcount':     21, 'min_dist':   -463, 'max_dist':   -335, 'probability': 0.367256, 'to_alt':      1, 'from_alt':      2},
        {'from':  31758, 'from_side':    'l', 'to':  31737, 'to_side':    'r', 'mean_dist':     86, 'mapq':  60060, 'bcount':     18, 'min_dist':     69, 'max_dist':    120, 'probability': 0.231835, 'to_alt':      1, 'from_alt':      2},
        {'from':  31758, 'from_side':    'r', 'to':  47750, 'to_side':    'l', 'mean_dist':    -42, 'mapq':  60060, 'bcount':     47, 'min_dist':    -49, 'max_dist':    -17, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':  31758, 'from_side':    'r', 'to': 113898, 'to_side':    'l', 'mean_dist':    686, 'mapq':  60060, 'bcount':     20, 'min_dist':    648, 'max_dist':    763, 'probability': 0.470466, 'to_alt':      2, 'from_alt':      2},
        {'from':  32135, 'from_side':    'l', 'to':  45782, 'to_side':    'l', 'mean_dist':   -502, 'mapq':  60060, 'bcount':     42, 'min_dist':   -560, 'max_dist':   -395, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  32135, 'from_side':    'r', 'to':  74998, 'to_side':    'r', 'mean_dist':    739, 'mapq':  60060, 'bcount':     24, 'min_dist':    660, 'max_dist':    984, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':  32135, 'from_side':    'r', 'to': 105853, 'to_side':    'l', 'mean_dist':    -45, 'mapq':  60060, 'bcount':     21, 'min_dist':    -50, 'max_dist':    -34, 'probability': 0.367256, 'to_alt':      2, 'from_alt':      2},
        {'from':  32420, 'from_side':    'r', 'to': 105853, 'to_side':    'l', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     18, 'min_dist':    -50, 'max_dist':    -32, 'probability': 0.231835, 'to_alt':      2, 'from_alt':      1},
        {'from':  45782, 'from_side':    'l', 'to':      7, 'to_side':    'r', 'mean_dist':   -230, 'mapq':  60060, 'bcount':     42, 'min_dist':   -253, 'max_dist':   -155, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      2},
        {'from':  45782, 'from_side':    'l', 'to':  32135, 'to_side':    'l', 'mean_dist':   -502, 'mapq':  60060, 'bcount':     42, 'min_dist':   -560, 'max_dist':   -395, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':  45782, 'from_side':    'r', 'to':  45783, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     43, 'min_dist':      0, 'max_dist':      0, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':  45782, 'from_side':    'r', 'to':  49372, 'to_side':    'r', 'mean_dist':    334, 'mapq':  60060, 'bcount':     35, 'min_dist':    308, 'max_dist':    376, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2},
        {'from':  45783, 'from_side':    'l', 'to':  45782, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     43, 'min_dist':      0, 'max_dist':      0, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  45783, 'from_side':    'r', 'to':  31737, 'to_side':    'l', 'mean_dist':   1044, 'mapq':  60060, 'bcount':      9, 'min_dist':    996, 'max_dist':   1138, 'probability': 0.045509, 'to_alt':      2, 'from_alt':      1},
        {'from':  47750, 'from_side':    'l', 'to':  31758, 'to_side':    'r', 'mean_dist':    -42, 'mapq':  60060, 'bcount':     47, 'min_dist':    -49, 'max_dist':    -17, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  47750, 'from_side':    'r', 'to': 113898, 'to_side':    'l', 'mean_dist':    -10, 'mapq':  60060, 'bcount':     31, 'min_dist':    -33, 'max_dist':     40, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  49372, 'from_side':    'l', 'to':  30446, 'to_side':    'l', 'mean_dist':   9158, 'mapq':  60060, 'bcount':      2, 'min_dist':   8995, 'max_dist':   9322, 'probability': 0.096053, 'to_alt':      2, 'from_alt':      3},
        {'from':  49372, 'from_side':    'l', 'to':  86633, 'to_side':    'r', 'mean_dist':    432, 'mapq':  60060, 'bcount':     18, 'min_dist':    409, 'max_dist':    461, 'probability': 0.356470, 'to_alt':      1, 'from_alt':      3},
        {'from':  49372, 'from_side':    'r', 'to':  45782, 'to_side':    'r', 'mean_dist':    334, 'mapq':  60060, 'bcount':     35, 'min_dist':    308, 'max_dist':    376, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  54753, 'from_side':    'l', 'to':  31108, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     14, 'min_dist':      0, 'max_dist':      0, 'probability': 0.104244, 'to_alt':      2, 'from_alt':      1},
        {'from':  54753, 'from_side':    'r', 'to':  95291, 'to_side':    'l', 'mean_dist':    -18, 'mapq':  60060, 'bcount':     16, 'min_dist':    -22, 'max_dist':    -14, 'probability': 0.159802, 'to_alt':      2, 'from_alt':      1},
        {'from':  74998, 'from_side':    'l', 'to':      7, 'to_side':    'r', 'mean_dist':   -558, 'mapq':  60060, 'bcount':     40, 'min_dist':   -659, 'max_dist':   -401, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  74998, 'from_side':    'r', 'to':  32135, 'to_side':    'r', 'mean_dist':    739, 'mapq':  60060, 'bcount':     24, 'min_dist':    660, 'max_dist':    984, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  76037, 'from_side':    'l', 'to':  31729, 'to_side':    'l', 'mean_dist':   -162, 'mapq':  60060, 'bcount':     14, 'min_dist':   -174, 'max_dist':   -147, 'probability': 0.104244, 'to_alt':      2, 'from_alt':      2},
        {'from':  76037, 'from_side':    'l', 'to':  31737, 'to_side':    'l', 'mean_dist':   -168, 'mapq':  60060, 'bcount':      9, 'min_dist':   -186, 'max_dist':   -143, 'probability': 0.027818, 'to_alt':      2, 'from_alt':      2},
        {'from':  76037, 'from_side':    'r', 'to':  30446, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     27, 'min_dist':      0, 'max_dist':      0, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  86633, 'from_side':    'l', 'to':  30446, 'to_side':    'l', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     30, 'min_dist':    -50, 'max_dist':    -32, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  86633, 'from_side':    'r', 'to':  49372, 'to_side':    'l', 'mean_dist':    432, 'mapq':  60060, 'bcount':     18, 'min_dist':    409, 'max_dist':    461, 'probability': 0.356470, 'to_alt':      3, 'from_alt':      1},
        {'from':  93920, 'from_side':    'l', 'to':      7, 'to_side':    'l', 'mean_dist':    383, 'mapq':  60060, 'bcount':     38, 'min_dist':    364, 'max_dist':    425, 'probability': 0.500000, 'to_alt':      2, 'from_alt':      1},
        {'from':  93920, 'from_side':    'r', 'to':  31108, 'to_side':    'l', 'mean_dist':  -4681, 'mapq':  60060, 'bcount':      6, 'min_dist':  -4716, 'max_dist':  -4577, 'probability': 0.108994, 'to_alt':      2, 'from_alt':      1},
        {'from':  95291, 'from_side':    'l', 'to':  23515, 'to_side':    'r', 'mean_dist':      3, 'mapq':  60060, 'bcount':     13, 'min_dist':      1, 'max_dist':      8, 'probability': 0.082422, 'to_alt':      1, 'from_alt':      2},
        {'from':  95291, 'from_side':    'l', 'to':  54753, 'to_side':    'r', 'mean_dist':    -18, 'mapq':  60060, 'bcount':     16, 'min_dist':    -22, 'max_dist':    -14, 'probability': 0.159802, 'to_alt':      1, 'from_alt':      2},
        {'from':  95291, 'from_side':    'r', 'to':  10945, 'to_side':    'l', 'mean_dist':    -45, 'mapq':  60060, 'bcount':     16, 'min_dist':    -49, 'max_dist':    -40, 'probability': 0.159802, 'to_alt':      1, 'from_alt':      2},
        {'from':  95291, 'from_side':    'r', 'to':  29100, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     14, 'min_dist':    -50, 'max_dist':    -34, 'probability': 0.104244, 'to_alt':      1, 'from_alt':      2},
        {'from': 105853, 'from_side':    'l', 'to':  32135, 'to_side':    'r', 'mean_dist':    -45, 'mapq':  60060, 'bcount':     21, 'min_dist':    -50, 'max_dist':    -34, 'probability': 0.367256, 'to_alt':      2, 'from_alt':      2},
        {'from': 105853, 'from_side':    'l', 'to':  32420, 'to_side':    'r', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     18, 'min_dist':    -50, 'max_dist':    -32, 'probability': 0.231835, 'to_alt':      1, 'from_alt':      2},
        {'from': 105853, 'from_side':    'r', 'to':  10945, 'to_side':    'r', 'mean_dist':    -43, 'mapq':  60060, 'bcount':     15, 'min_dist':    -49, 'max_dist':    -15, 'probability': 0.129976, 'to_alt':      1, 'from_alt':      2},
        {'from': 105853, 'from_side':    'r', 'to':  29100, 'to_side':    'l', 'mean_dist':    -44, 'mapq':  60060, 'bcount':     17, 'min_dist':    -48, 'max_dist':    -38, 'probability': 0.193782, 'to_alt':      1, 'from_alt':      2},
        {'from': 110006, 'from_side':    'r', 'to':   1440, 'to_side':    'r', 'mean_dist':    406, 'mapq':  60060, 'bcount':     11, 'min_dist':    386, 'max_dist':    433, 'probability': 0.081320, 'to_alt':      1, 'from_alt':      2},
        {'from': 110006, 'from_side':    'r', 'to':   7349, 'to_side':    'l', 'mean_dist':    387, 'mapq':  60060, 'bcount':      6, 'min_dist':    371, 'max_dist':    408, 'probability': 0.016554, 'to_alt':      1, 'from_alt':      2},
        {'from': 113898, 'from_side':    'l', 'to':  31758, 'to_side':    'r', 'mean_dist':    686, 'mapq':  60060, 'bcount':     20, 'min_dist':    648, 'max_dist':    763, 'probability': 0.470466, 'to_alt':      2, 'from_alt':      2},
        {'from': 113898, 'from_side':    'l', 'to':  47750, 'to_side':    'r', 'mean_dist':    -10, 'mapq':  60060, 'bcount':     31, 'min_dist':    -33, 'max_dist':     40, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      2}
        ]) )
    result_paths.append( pd.DataFrame({
        'pid':      [    500,    500,    500,    500,    500,    500,    500,    500,    500,    500],
        'pos':      [      0,      1,      2,      3,      4,      5,      6,      7,      8,      9],
        'phase0':   [    501,    501,    501,    501,    501,    501,    501,    501,    501,    501],
        'scaf0':    [ 110006,   1440,  11769,      7,  74998,  32135,  45782,  49372,  86633,  30446],
        'strand0':  [    '+',    '-',    '-',    '+',    '+',    '-',    '+',    '-',    '-',    '+'],
        'dist0':    [      0,    406,    -45,     76,   -558,    739,   -502,    334,    432,    -43],
        'phase1':   [   -502,    502,   -502,   -502,   -502,   -502,   -502,   -502,    502,    502],
        'scaf1':    [     -1,   7349,     -1,     -1,     -1,     -1,     -1,     -1,     -1,  30446],
        'strand1':  [     '',    '+',     '',     '',     '',     '',     '',     '',     '',    '+'],
        'dist1':    [      0,    387,      0,      0,      0,      0,      0,      0,      0,   9158]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    501,    501,    501,    501,    501,    501],
        'pos':      [      0,      1,      2,      3,      4,      5],
        'phase0':   [    503,    503,    503,    503,    503,    503],
        'scaf0':    [  93920,      7,     -1,     -1,  45782,  45783],
        'strand0':  [    '-',    '+',     '',     '',    '+',    '+'],
        'dist0':    [      0,    383,      0,      0,   -230,      0],
        'phase1':   [   -504,   -504,    504,    504,    504,   -504],
        'scaf1':    [     -1,     -1,  74998,  32135,  45782,     -1],
        'strand1':  [     '',     '',    '+',    '-',    '+',     ''],
        'dist1':    [      0,      0,   -558,    739,   -502,      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    502,    502,    502,    502],
        'pos':      [      0,      1,      2,      3],
        'phase0':   [    505,    505,    505,    505],
        'scaf0':    [  54753,  95291,  10945, 105853],
        'strand0':  [    '+',    '+',    '+',    '-'],
        'dist0':    [      0,    -18,    -45,    -43],
        'phase1':   [   -506,   -506,   -506,   -506],
        'scaf1':    [     -1,     -1,     -1,     -1],
        'strand1':  [     '',     '',     '',     ''],
        'dist1':    [      0,      0,      0,      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    503,    503,    503,    503,    503],
        'pos':      [      0,      1,      2,      3,      4],
        'phase0':   [    507,    507,    507,    507,    507],
        'scaf0':    [  23515,  95291,  29100, 105853,  32420],
        'strand0':  [    '+',    '+',    '-',    '-',    '-'],
        'dist0':    [      0,      3,    -43,    -44,    -44],
        'phase1':   [   -508,   -508,   -508,   -508,   -508],
        'scaf1':    [     -1,     -1,     -1,     -1,     -1],
        'strand1':  [     '',     '',     '',     '',     ''],
        'dist1':    [      0,      0,      0,      0,      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    504,    504,    504,    504],
        'pos':      [      0,      1,      2,      3],
        'phase0':   [    509,    509,    509,    509],
        'scaf0':    [  31729,  31758,  47750, 113898],
        'strand0':  [    '+',    '+',    '+',    '+'],
        'dist0':    [      0,   -399,    -42,    -10],
        'phase1':   [   -510,   -510,    510,    510],
        'scaf1':    [     -1,     -1,     -1, 113898],
        'strand1':  [     '',     '',     '',    '+'],
        'dist1':    [      0,      0,      0,    686]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    505],
        'pos':      [      0],
        'phase0':   [    511],
        'scaf0':    [  31737],
        'strand0':  [    '+'],
        'dist0':    [      0],
        'phase1':   [   -512],
        'scaf1':    [     -1],
        'strand1':  [     ''],
        'dist1':    [      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    506],
        'pos':      [      0],
        'phase0':   [    513],
        'scaf0':    [  76037],
        'strand0':  [    '+'],
        'dist0':    [      0],
        'phase1':   [   -514],
        'scaf1':    [     -1],
        'strand1':  [     ''],
        'dist1':    [      0]
        }) )
    result_paths.append( pd.DataFrame({
        'pid':      [    507],
        'pos':      [      0],
        'phase0':   [    515],
        'scaf0':    [  31108],
        'strand0':  [    '+'],
        'dist0':    [      0],
        'phase1':   [   -516],
        'scaf1':    [     -1],
        'strand1':  [     ''],
        'dist1':    [      0]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    500,    500,    500,    500],
        'pos':      [      0,      1,      2,      3],
        'scaf0':    [  11769,      7,  74998,  32135],
        'strand0':  [    '-',    '+',    '+',    '-'],
        'dist0':    [      0,     76,   -558,    739]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    501,    501,    501],
        'pos':      [      0,      1,      2],
        'scaf0':    [  32135,  45782,  49372],
        'strand0':  [    '-',    '+',    '-'],
        'dist0':    [      0,   -502,    334]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    502,    502,    502],
        'pos':      [      0,      1,      2],
        'scaf0':    [  49372,  86633,  30446],
        'strand0':  [    '-',    '-',    '+'],
        'dist0':    [      0,    432,    -43]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    503,    503,    503],
        'pos':      [      0,      1,      2],
        'scaf0':    [  31108,  93920,      7],
        'strand0':  [    '-',    '-',    '+'],
        'dist0':    [      0,  -4681,    383]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    504,    504,    504,    504],
        'pos':      [      0,      1,      2,      3],
        'scaf0':    [      7,  74998,  32135,  45782],
        'strand0':  [    '+',    '+',    '-',    '+'],
        'dist0':    [      0,   -558,    739,   -502],
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    505,    505,    505],
        'pos':      [      0,      1,      2],
        'scaf0':    [  45782,  45783,  31737],
        'strand0':  [    '+',    '+',    '+'],
        'dist0':    [      0,      0,   1044],
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    506,    506,    506],
        'pos':      [      0,      1,      2],
        'scaf0':    [ 110006,   1440,  11769],
        'strand0':  [    '+',    '-',    '-'],
        'dist0':    [      0,    406,    -45]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    507,    507,    507],
        'pos':      [      0,      1,      2],
        'scaf0':    [ 110006,   7349,  11769],
        'strand0':  [    '+',    '+',    '-'],
        'dist0':    [      0,    387,    -45]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    508,    508,    508,    508,    508,    508],
        'pos':      [      0,      1,      2,      3,      4,      5],
        'scaf0':    [  31108,  54753,  95291,  10945, 105853,  32135],
        'strand0':  [    '-',    '+',    '+',    '+',    '-',    '-'],
        'dist0':    [      0,      0,    -18,    -45,    -43,    -45]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    509,    509,    509],
        'pos':      [      0,      1,      2],
        'scaf0':    [  31758,  47750, 113898],
        'strand0':  [    '+',    '+',    '+'],
        'dist0':    [      0,    -42,    -10]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    510,    510,    510,    510,    510],
        'pos':      [      0,      1,      2,      3,      4],
        'scaf0':    [  23515,  95291,  29100, 105853,  32420],
        'strand0':  [    '+',    '+',    '-',    '-',    '-'],
        'dist0':    [      0,      3,    -43,    -44,    -44]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    500,    500],
        'pos':      [      0,      1],
        'scaf0':    [      7,  45782],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,   -230]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    501,    501],
        'pos':      [      0,      1],
        'scaf0':    [  32135,  45782],
        'strand0':  [    '-',    '+'],
        'dist0':    [      0,   -502]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    502,    502],
        'pos':      [      0,      1],
        'scaf0':    [  31737,  31758],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,     86]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    503,    503],
        'pos':      [      0,      1],
        'scaf0':    [  49372,  30446],
        'strand0':  [    '-',    '+'],
        'dist0':    [      0,   9158]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    504,    504],
        'pos':      [      0,      1],
        'scaf0':    [  31758, 113898],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,    686]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    505,    505],
        'pos':      [      0,      1],
        'scaf0':    [  76037,  31729],
        'strand0':  [    '-',    '+'],
        'dist0':    [      0,   -162]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    506,    506],
        'pos':      [      0,      1],
        'scaf0':    [  76037,  31737],
        'strand0':  [    '-',    '+'],
        'dist0':    [      0,   -168]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    507,    507],
        'pos':      [      0,      1],
        'scaf0':    [  30446,  31729],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,    630]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    508,    508],
        'pos':      [      0,      1],
        'scaf0':    [  30446,  76037],
        'strand0':  [    '+',    '-'],
        'dist0':    [      0,      0]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    509,    509],
        'pos':      [      0,      1],
        'scaf0':    [  31729,  31758],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,   -399]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    510,    510],
        'pos':      [      0,      1],
        'scaf0':    [  31737,  31758],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,     86]
        }) )
#
    # Test 6
    scaffolds.append( pd.DataFrame({'case':6, 'scaffold':[9064, 41269, 51925, 67414, 123224, 123225, 123226, 123227, 123228, 123229, 123230, 123231, 123236, 123237, 123238]}) )
    scaffold_graph.append( pd.DataFrame([
        {'from':   9064, 'from_side':    'l', 'length':      7, 'scaf1': 123226, 'strand1':    '+', 'dist1':     -5, 'scaf2': 123238, 'strand2':    '+', 'dist2':      3, 'scaf3': 123225, 'strand3':    '+', 'dist3':    -42, 'scaf4': 123231, 'strand4':    '+', 'dist4':    -42, 'scaf5': 123227, 'strand5':    '+', 'dist5':    549, 'scaf6': 123236, 'strand6':    '+', 'dist6':    224},
        {'from':   9064, 'from_side':    'l', 'length':      6, 'scaf1': 123226, 'strand1':    '+', 'dist1':     -5, 'scaf2': 123238, 'strand2':    '+', 'dist2':    400, 'scaf3': 123225, 'strand3':    '+', 'dist3':    -42, 'scaf4': 123231, 'strand4':    '+', 'dist4':    -42, 'scaf5': 123227, 'strand5':    '+', 'dist5':    549},
        {'from':  41269, 'from_side':    'l', 'length':      5, 'scaf1': 123224, 'strand1':    '+', 'dist1':   -519, 'scaf2': 123228, 'strand2':    '+', 'dist2':     43, 'scaf3':  51925, 'strand3':    '+', 'dist3':     55, 'scaf4':  67414, 'strand4':    '+', 'dist4':     87},
        {'from':  41269, 'from_side':    'r', 'length':      7, 'scaf1':  51925, 'strand1':    '-', 'dist1':      4, 'scaf2': 123224, 'strand2':    '-', 'dist2':    965, 'scaf3': 123229, 'strand3':    '-', 'dist3':    179, 'scaf4': 123225, 'strand4':    '-', 'dist4':    -42, 'scaf5': 123230, 'strand5':    '-', 'dist5':    -40, 'scaf6': 123227, 'strand6':    '-', 'dist6':    196},
        {'from':  41269, 'from_side':    'r', 'length':      5, 'scaf1':  51925, 'strand1':    '-', 'dist1':      4, 'scaf2': 123224, 'strand2':    '-', 'dist2':    965, 'scaf3': 123229, 'strand3':    '-', 'dist3':    179, 'scaf4': 123230, 'strand4':    '-', 'dist4':    590},
        {'from':  51925, 'from_side':    'l', 'length':      6, 'scaf1': 123224, 'strand1':    '-', 'dist1':    965, 'scaf2': 123229, 'strand2':    '-', 'dist2':    179, 'scaf3': 123225, 'strand3':    '-', 'dist3':    -42, 'scaf4': 123230, 'strand4':    '-', 'dist4':    -40, 'scaf5': 123227, 'strand5':    '-', 'dist5':    196},
        {'from':  51925, 'from_side':    'l', 'length':      4, 'scaf1': 123224, 'strand1':    '-', 'dist1':    965, 'scaf2': 123229, 'strand2':    '-', 'dist2':    179, 'scaf3': 123230, 'strand3':    '-', 'dist3':    590},
        {'from':  51925, 'from_side':    'l', 'length':      4, 'scaf1': 123228, 'strand1':    '-', 'dist1':     55, 'scaf2': 123224, 'strand2':    '-', 'dist2':     43, 'scaf3':  41269, 'strand3':    '+', 'dist3':   -519},
        {'from':  51925, 'from_side':    'r', 'length':      2, 'scaf1':  41269, 'strand1':    '-', 'dist1':      4},
        {'from':  51925, 'from_side':    'r', 'length':      2, 'scaf1':  67414, 'strand1':    '+', 'dist1':     87},
        {'from':  67414, 'from_side':    'l', 'length':      5, 'scaf1':  51925, 'strand1':    '-', 'dist1':     87, 'scaf2': 123228, 'strand2':    '-', 'dist2':     55, 'scaf3': 123224, 'strand3':    '-', 'dist3':     43, 'scaf4':  41269, 'strand4':    '+', 'dist4':   -519},
        {'from': 123224, 'from_side':    'l', 'length':      2, 'scaf1':  41269, 'strand1':    '+', 'dist1':   -519},
        {'from': 123224, 'from_side':    'l', 'length':      6, 'scaf1': 123229, 'strand1':    '-', 'dist1':    179, 'scaf2': 123225, 'strand2':    '-', 'dist2':    -42, 'scaf3': 123230, 'strand3':    '-', 'dist3':    -40, 'scaf4': 123227, 'strand4':    '-', 'dist4':    196, 'scaf5': 123237, 'strand5':    '-', 'dist5':    542},
        {'from': 123224, 'from_side':    'l', 'length':      3, 'scaf1': 123229, 'strand1':    '-', 'dist1':    179, 'scaf2': 123230, 'strand2':    '-', 'dist2':    590},
        {'from': 123224, 'from_side':    'r', 'length':      3, 'scaf1':  51925, 'strand1':    '+', 'dist1':    965, 'scaf2':  41269, 'strand2':    '-', 'dist2':      4},
        {'from': 123224, 'from_side':    'r', 'length':      4, 'scaf1': 123228, 'strand1':    '+', 'dist1':     43, 'scaf2':  51925, 'strand2':    '+', 'dist2':     55, 'scaf3':  67414, 'strand3':    '+', 'dist3':     87},
        {'from': 123225, 'from_side':    'l', 'length':      4, 'scaf1': 123230, 'strand1':    '-', 'dist1':    -40, 'scaf2': 123227, 'strand2':    '-', 'dist2':    196, 'scaf3': 123237, 'strand3':    '-', 'dist3':    542},
        {'from': 123225, 'from_side':    'l', 'length':      4, 'scaf1': 123238, 'strand1':    '-', 'dist1':    -42, 'scaf2': 123226, 'strand2':    '-', 'dist2':      3, 'scaf3':   9064, 'strand3':    '+', 'dist3':     -5},
        {'from': 123225, 'from_side':    'l', 'length':      4, 'scaf1': 123238, 'strand1':    '-', 'dist1':    -42, 'scaf2': 123226, 'strand2':    '-', 'dist2':    400, 'scaf3':   9064, 'strand3':    '+', 'dist3':     -5},
        {'from': 123225, 'from_side':    'r', 'length':      5, 'scaf1': 123229, 'strand1':    '+', 'dist1':    -42, 'scaf2': 123224, 'strand2':    '+', 'dist2':    179, 'scaf3':  51925, 'strand3':    '+', 'dist3':    965, 'scaf4':  41269, 'strand4':    '-', 'dist4':      4},
        {'from': 123225, 'from_side':    'r', 'length':      4, 'scaf1': 123231, 'strand1':    '+', 'dist1':    -42, 'scaf2': 123227, 'strand2':    '+', 'dist2':    549, 'scaf3': 123236, 'strand3':    '+', 'dist3':    224},
        {'from': 123226, 'from_side':    'l', 'length':      2, 'scaf1':   9064, 'strand1':    '+', 'dist1':     -5},
        {'from': 123226, 'from_side':    'l', 'length':      4, 'scaf1': 123236, 'strand1':    '-', 'dist1':      2, 'scaf2': 123227, 'strand2':    '-', 'dist2':    224, 'scaf3': 123231, 'strand3':    '-', 'dist3':    549},
        {'from': 123226, 'from_side':    'r', 'length':      4, 'scaf1': 123237, 'strand1':    '+', 'dist1':      0, 'scaf2': 123227, 'strand2':    '+', 'dist2':    542, 'scaf3': 123230, 'strand3':    '+', 'dist3':    196},
        {'from': 123226, 'from_side':    'r', 'length':      6, 'scaf1': 123238, 'strand1':    '+', 'dist1':      3, 'scaf2': 123225, 'strand2':    '+', 'dist2':    -42, 'scaf3': 123231, 'strand3':    '+', 'dist3':    -42, 'scaf4': 123227, 'strand4':    '+', 'dist4':    549, 'scaf5': 123236, 'strand5':    '+', 'dist5':    224},
        {'from': 123226, 'from_side':    'r', 'length':      4, 'scaf1': 123238, 'strand1':    '+', 'dist1':      3, 'scaf2': 123231, 'strand2':    '+', 'dist2':    564, 'scaf3': 123227, 'strand3':    '+', 'dist3':    549},
        {'from': 123226, 'from_side':    'r', 'length':      5, 'scaf1': 123238, 'strand1':    '+', 'dist1':    400, 'scaf2': 123225, 'strand2':    '+', 'dist2':    -42, 'scaf3': 123231, 'strand3':    '+', 'dist3':    -42, 'scaf4': 123227, 'strand4':    '+', 'dist4':    549},
        {'from': 123227, 'from_side':    'l', 'length':      6, 'scaf1': 123231, 'strand1':    '-', 'dist1':    549, 'scaf2': 123225, 'strand2':    '-', 'dist2':    -42, 'scaf3': 123238, 'strand3':    '-', 'dist3':    -42, 'scaf4': 123226, 'strand4':    '-', 'dist4':      3, 'scaf5':   9064, 'strand5':    '+', 'dist5':     -5},
        {'from': 123227, 'from_side':    'l', 'length':      6, 'scaf1': 123231, 'strand1':    '-', 'dist1':    549, 'scaf2': 123225, 'strand2':    '-', 'dist2':    -42, 'scaf3': 123238, 'strand3':    '-', 'dist3':    -42, 'scaf4': 123226, 'strand4':    '-', 'dist4':    400, 'scaf5':   9064, 'strand5':    '+', 'dist5':     -5},
        {'from': 123227, 'from_side':    'l', 'length':      4, 'scaf1': 123231, 'strand1':    '-', 'dist1':    549, 'scaf2': 123238, 'strand2':    '-', 'dist2':    564, 'scaf3': 123226, 'strand3':    '-', 'dist3':      3},
        {'from': 123227, 'from_side':    'l', 'length':      5, 'scaf1': 123237, 'strand1':    '-', 'dist1':    542, 'scaf2': 123226, 'strand2':    '-', 'dist2':      0, 'scaf3': 123236, 'strand3':    '-', 'dist3':      2, 'scaf4': 123227, 'strand4':    '-', 'dist4':    224},
        {'from': 123227, 'from_side':    'r', 'length':      7, 'scaf1': 123230, 'strand1':    '+', 'dist1':    196, 'scaf2': 123225, 'strand2':    '+', 'dist2':    -40, 'scaf3': 123229, 'strand3':    '+', 'dist3':    -42, 'scaf4': 123224, 'strand4':    '+', 'dist4':    179, 'scaf5':  51925, 'strand5':    '+', 'dist5':    965, 'scaf6':  41269, 'strand6':    '-', 'dist6':      4},
        {'from': 123227, 'from_side':    'r', 'length':      5, 'scaf1': 123236, 'strand1':    '+', 'dist1':    224, 'scaf2': 123226, 'strand2':    '+', 'dist2':      2, 'scaf3': 123237, 'strand3':    '+', 'dist3':      0, 'scaf4': 123227, 'strand4':    '+', 'dist4':    542},
        {'from': 123228, 'from_side':    'l', 'length':      3, 'scaf1': 123224, 'strand1':    '-', 'dist1':     43, 'scaf2':  41269, 'strand2':    '+', 'dist2':   -519},
        {'from': 123228, 'from_side':    'r', 'length':      3, 'scaf1':  51925, 'strand1':    '+', 'dist1':     55, 'scaf2':  67414, 'strand2':    '+', 'dist2':     87},
        {'from': 123229, 'from_side':    'l', 'length':      5, 'scaf1': 123225, 'strand1':    '-', 'dist1':    -42, 'scaf2': 123230, 'strand2':    '-', 'dist2':    -40, 'scaf3': 123227, 'strand3':    '-', 'dist3':    196, 'scaf4': 123237, 'strand4':    '-', 'dist4':    542},
        {'from': 123229, 'from_side':    'l', 'length':      2, 'scaf1': 123230, 'strand1':    '-', 'dist1':    590},
        {'from': 123229, 'from_side':    'r', 'length':      4, 'scaf1': 123224, 'strand1':    '+', 'dist1':    179, 'scaf2':  51925, 'strand2':    '+', 'dist2':    965, 'scaf3':  41269, 'strand3':    '-', 'dist3':      4},
        {'from': 123230, 'from_side':    'l', 'length':      4, 'scaf1': 123227, 'strand1':    '-', 'dist1':    196, 'scaf2': 123237, 'strand2':    '-', 'dist2':    542, 'scaf3': 123226, 'strand3':    '-', 'dist3':      0},
        {'from': 123230, 'from_side':    'r', 'length':      6, 'scaf1': 123225, 'strand1':    '+', 'dist1':    -40, 'scaf2': 123229, 'strand2':    '+', 'dist2':    -42, 'scaf3': 123224, 'strand3':    '+', 'dist3':    179, 'scaf4':  51925, 'strand4':    '+', 'dist4':    965, 'scaf5':  41269, 'strand5':    '-', 'dist5':      4},
        {'from': 123230, 'from_side':    'r', 'length':      5, 'scaf1': 123229, 'strand1':    '+', 'dist1':    590, 'scaf2': 123224, 'strand2':    '+', 'dist2':    179, 'scaf3':  51925, 'strand3':    '+', 'dist3':    965, 'scaf4':  41269, 'strand4':    '-', 'dist4':      4},
        {'from': 123231, 'from_side':    'l', 'length':      5, 'scaf1': 123225, 'strand1':    '-', 'dist1':    -42, 'scaf2': 123238, 'strand2':    '-', 'dist2':    -42, 'scaf3': 123226, 'strand3':    '-', 'dist3':      3, 'scaf4':   9064, 'strand4':    '+', 'dist4':     -5},
        {'from': 123231, 'from_side':    'l', 'length':      5, 'scaf1': 123225, 'strand1':    '-', 'dist1':    -42, 'scaf2': 123238, 'strand2':    '-', 'dist2':    -42, 'scaf3': 123226, 'strand3':    '-', 'dist3':    400, 'scaf4':   9064, 'strand4':    '+', 'dist4':     -5},
        {'from': 123231, 'from_side':    'l', 'length':      3, 'scaf1': 123238, 'strand1':    '-', 'dist1':    564, 'scaf2': 123226, 'strand2':    '-', 'dist2':      3},
        {'from': 123231, 'from_side':    'r', 'length':      4, 'scaf1': 123227, 'strand1':    '+', 'dist1':    549, 'scaf2': 123236, 'strand2':    '+', 'dist2':    224, 'scaf3': 123226, 'strand3':    '+', 'dist3':      2},
        {'from': 123236, 'from_side':    'l', 'length':      7, 'scaf1': 123227, 'strand1':    '-', 'dist1':    224, 'scaf2': 123231, 'strand2':    '-', 'dist2':    549, 'scaf3': 123225, 'strand3':    '-', 'dist3':    -42, 'scaf4': 123238, 'strand4':    '-', 'dist4':    -42, 'scaf5': 123226, 'strand5':    '-', 'dist5':      3, 'scaf6':   9064, 'strand6':    '+', 'dist6':     -5},
        {'from': 123236, 'from_side':    'l', 'length':      4, 'scaf1': 123227, 'strand1':    '-', 'dist1':    224, 'scaf2': 123231, 'strand2':    '-', 'dist2':    549, 'scaf3': 123238, 'strand3':    '-', 'dist3':    564},
        {'from': 123236, 'from_side':    'r', 'length':      4, 'scaf1': 123226, 'strand1':    '+', 'dist1':      2, 'scaf2': 123237, 'strand2':    '+', 'dist2':      0, 'scaf3': 123227, 'strand3':    '+', 'dist3':    542},
        {'from': 123237, 'from_side':    'l', 'length':      4, 'scaf1': 123226, 'strand1':    '-', 'dist1':      0, 'scaf2': 123236, 'strand2':    '-', 'dist2':      2, 'scaf3': 123227, 'strand3':    '-', 'dist3':    224},
        {'from': 123237, 'from_side':    'r', 'length':      6, 'scaf1': 123227, 'strand1':    '+', 'dist1':    542, 'scaf2': 123230, 'strand2':    '+', 'dist2':    196, 'scaf3': 123225, 'strand3':    '+', 'dist3':    -40, 'scaf4': 123229, 'strand4':    '+', 'dist4':    -42, 'scaf5': 123224, 'strand5':    '+', 'dist5':    179},
        {'from': 123238, 'from_side':    'l', 'length':      3, 'scaf1': 123226, 'strand1':    '-', 'dist1':      3, 'scaf2':   9064, 'strand2':    '+', 'dist2':     -5},
        {'from': 123238, 'from_side':    'l', 'length':      3, 'scaf1': 123226, 'strand1':    '-', 'dist1':    400, 'scaf2':   9064, 'strand2':    '+', 'dist2':     -5},
        {'from': 123238, 'from_side':    'r', 'length':      5, 'scaf1': 123225, 'strand1':    '+', 'dist1':    -42, 'scaf2': 123231, 'strand2':    '+', 'dist2':    -42, 'scaf3': 123227, 'strand3':    '+', 'dist3':    549, 'scaf4': 123236, 'strand4':    '+', 'dist4':    224},
        {'from': 123238, 'from_side':    'r', 'length':      4, 'scaf1': 123231, 'strand1':    '+', 'dist1':    564, 'scaf2': 123227, 'strand2':    '+', 'dist2':    549, 'scaf3': 123236, 'strand3':    '+', 'dist3':    224}
        ]) )
    scaf_bridges.append( pd.DataFrame([
        {'from':   9064, 'from_side':    'l', 'to': 123226, 'to_side':    'l', 'mean_dist':     -5, 'mapq':  60060, 'bcount':     16, 'min_dist':    -10, 'max_dist':      4, 'probability': 0.159802, 'to_alt':      2, 'from_alt':      1},
        {'from':  41269, 'from_side':    'l', 'to': 123224, 'to_side':    'l', 'mean_dist':   -519, 'mapq':  60060, 'bcount':      9, 'min_dist':   -553, 'max_dist':   -501, 'probability': 0.027818, 'to_alt':      2, 'from_alt':      1},
        {'from':  41269, 'from_side':    'r', 'to':  51925, 'to_side':    'r', 'mean_dist':      4, 'mapq':  60060, 'bcount':     14, 'min_dist':      1, 'max_dist':      9, 'probability': 0.104244, 'to_alt':      2, 'from_alt':      1},
        {'from':  51925, 'from_side':    'l', 'to': 123224, 'to_side':    'r', 'mean_dist':    965, 'mapq':  60060, 'bcount':     12, 'min_dist':    917, 'max_dist':   1099, 'probability': 0.105770, 'to_alt':      2, 'from_alt':      2},
        {'from':  51925, 'from_side':    'l', 'to': 123228, 'to_side':    'r', 'mean_dist':     55, 'mapq':  60060, 'bcount':      7, 'min_dist':     44, 'max_dist':     84, 'probability': 0.014765, 'to_alt':      1, 'from_alt':      2},
        {'from':  51925, 'from_side':    'r', 'to':  41269, 'to_side':    'r', 'mean_dist':      4, 'mapq':  60060, 'bcount':     14, 'min_dist':      1, 'max_dist':      9, 'probability': 0.104244, 'to_alt':      1, 'from_alt':      2},
        {'from':  51925, 'from_side':    'r', 'to':  67414, 'to_side':    'l', 'mean_dist':     87, 'mapq':  60060, 'bcount':      6, 'min_dist':     52, 'max_dist':    117, 'probability': 0.010512, 'to_alt':      1, 'from_alt':      2},
        {'from':  67414, 'from_side':    'l', 'to':  51925, 'to_side':    'r', 'mean_dist':     87, 'mapq':  60060, 'bcount':      6, 'min_dist':     52, 'max_dist':    117, 'probability': 0.010512, 'to_alt':      2, 'from_alt':      1},
        {'from': 123224, 'from_side':    'l', 'to':  41269, 'to_side':    'l', 'mean_dist':   -519, 'mapq':  60060, 'bcount':      9, 'min_dist':   -553, 'max_dist':   -501, 'probability': 0.027818, 'to_alt':      1, 'from_alt':      2},
        {'from': 123224, 'from_side':    'l', 'to': 123229, 'to_side':    'r', 'mean_dist':    179, 'mapq':  60060, 'bcount':      9, 'min_dist':    162, 'max_dist':    207, 'probability': 0.027818, 'to_alt':      1, 'from_alt':      2},
        {'from': 123224, 'from_side':    'r', 'to':  51925, 'to_side':    'l', 'mean_dist':    965, 'mapq':  60060, 'bcount':     12, 'min_dist':    917, 'max_dist':   1099, 'probability': 0.105770, 'to_alt':      2, 'from_alt':      2},
        {'from': 123224, 'from_side':    'r', 'to': 123228, 'to_side':    'l', 'mean_dist':     43, 'mapq':  60060, 'bcount':     10, 'min_dist':     32, 'max_dist':     89, 'probability': 0.037322, 'to_alt':      1, 'from_alt':      2},
        {'from': 123225, 'from_side':    'l', 'to': 123230, 'to_side':    'r', 'mean_dist':    -40, 'mapq':  60060, 'bcount':      8, 'min_dist':    -49, 'max_dist':    -20, 'probability': 0.020422, 'to_alt':      2, 'from_alt':      2},
        {'from': 123225, 'from_side':    'l', 'to': 123238, 'to_side':    'r', 'mean_dist':    -42, 'mapq':  60060, 'bcount':      9, 'min_dist':    -48, 'max_dist':    -32, 'probability': 0.027818, 'to_alt':      2, 'from_alt':      2},
        {'from': 123225, 'from_side':    'r', 'to': 123229, 'to_side':    'l', 'mean_dist':    -42, 'mapq':  60060, 'bcount':      7, 'min_dist':    -49, 'max_dist':    -36, 'probability': 0.014765, 'to_alt':      2, 'from_alt':      2},
        {'from': 123225, 'from_side':    'r', 'to': 123231, 'to_side':    'l', 'mean_dist':    -42, 'mapq':  60060, 'bcount':      7, 'min_dist':    -47, 'max_dist':    -37, 'probability': 0.014765, 'to_alt':      2, 'from_alt':      2},
        {'from': 123226, 'from_side':    'l', 'to':   9064, 'to_side':    'l', 'mean_dist':     -5, 'mapq':  60060, 'bcount':     16, 'min_dist':    -10, 'max_dist':      4, 'probability': 0.159802, 'to_alt':      1, 'from_alt':      2},
        {'from': 123226, 'from_side':    'l', 'to': 123236, 'to_side':    'r', 'mean_dist':      2, 'mapq':  60060, 'bcount':      9, 'min_dist':      0, 'max_dist':      4, 'probability': 0.027818, 'to_alt':      1, 'from_alt':      2},
        {'from': 123226, 'from_side':    'r', 'to': 123237, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':      6, 'min_dist':     -3, 'max_dist':      9, 'probability': 0.010512, 'to_alt':      1, 'from_alt':      3},
        {'from': 123226, 'from_side':    'r', 'to': 123238, 'to_side':    'l', 'mean_dist':      3, 'mapq':  60060, 'bcount':      8, 'min_dist':      1, 'max_dist':      9, 'probability': 0.020422, 'to_alt':      2, 'from_alt':      3},
        {'from': 123226, 'from_side':    'r', 'to': 123238, 'to_side':    'l', 'mean_dist':    400, 'mapq':  60060, 'bcount':      7, 'min_dist':    381, 'max_dist':    425, 'probability': 0.023635, 'to_alt':      2, 'from_alt':      3},
        {'from': 123227, 'from_side':    'l', 'to': 123231, 'to_side':    'r', 'mean_dist':    549, 'mapq':  60060, 'bcount':     13, 'min_dist':    488, 'max_dist':    605, 'probability': 0.135136, 'to_alt':      1, 'from_alt':      2},
        {'from': 123227, 'from_side':    'l', 'to': 123237, 'to_side':    'r', 'mean_dist':    542, 'mapq':  60060, 'bcount':      8, 'min_dist':    523, 'max_dist':    560, 'probability': 0.033108, 'to_alt':      1, 'from_alt':      2},
        {'from': 123227, 'from_side':    'r', 'to': 123230, 'to_side':    'l', 'mean_dist':    196, 'mapq':  60060, 'bcount':      5, 'min_dist':    185, 'max_dist':    202, 'probability': 0.007368, 'to_alt':      1, 'from_alt':      2},
        {'from': 123227, 'from_side':    'r', 'to': 123236, 'to_side':    'l', 'mean_dist':    224, 'mapq':  60060, 'bcount':     12, 'min_dist':    191, 'max_dist':    374, 'probability': 0.064232, 'to_alt':      1, 'from_alt':      2},
        {'from': 123228, 'from_side':    'l', 'to': 123224, 'to_side':    'r', 'mean_dist':     43, 'mapq':  60060, 'bcount':     10, 'min_dist':     32, 'max_dist':     89, 'probability': 0.037322, 'to_alt':      2, 'from_alt':      1},
        {'from': 123228, 'from_side':    'r', 'to':  51925, 'to_side':    'l', 'mean_dist':     55, 'mapq':  60060, 'bcount':      7, 'min_dist':     44, 'max_dist':     84, 'probability': 0.014765, 'to_alt':      2, 'from_alt':      1},
        {'from': 123229, 'from_side':    'l', 'to': 123225, 'to_side':    'r', 'mean_dist':    -42, 'mapq':  60060, 'bcount':      7, 'min_dist':    -49, 'max_dist':    -36, 'probability': 0.014765, 'to_alt':      2, 'from_alt':      2},
        {'from': 123229, 'from_side':    'l', 'to': 123230, 'to_side':    'r', 'mean_dist':    590, 'mapq':  60060, 'bcount':      3, 'min_dist':    526, 'max_dist':    628, 'probability': 0.005063, 'to_alt':      2, 'from_alt':      2},
        {'from': 123229, 'from_side':    'r', 'to': 123224, 'to_side':    'l', 'mean_dist':    179, 'mapq':  60060, 'bcount':      9, 'min_dist':    162, 'max_dist':    207, 'probability': 0.027818, 'to_alt':      2, 'from_alt':      1},
        {'from': 123230, 'from_side':    'l', 'to': 123227, 'to_side':    'r', 'mean_dist':    196, 'mapq':  60060, 'bcount':      5, 'min_dist':    185, 'max_dist':    202, 'probability': 0.007368, 'to_alt':      2, 'from_alt':      1},
        {'from': 123230, 'from_side':    'r', 'to': 123225, 'to_side':    'l', 'mean_dist':    -40, 'mapq':  60060, 'bcount':      8, 'min_dist':    -49, 'max_dist':    -20, 'probability': 0.020422, 'to_alt':      2, 'from_alt':      2},
        {'from': 123230, 'from_side':    'r', 'to': 123229, 'to_side':    'l', 'mean_dist':    590, 'mapq':  60060, 'bcount':      3, 'min_dist':    526, 'max_dist':    628, 'probability': 0.005063, 'to_alt':      2, 'from_alt':      2},
        {'from': 123231, 'from_side':    'l', 'to': 123225, 'to_side':    'r', 'mean_dist':    -42, 'mapq':  60060, 'bcount':      7, 'min_dist':    -47, 'max_dist':    -37, 'probability': 0.014765, 'to_alt':      2, 'from_alt':      2},
        {'from': 123231, 'from_side':    'l', 'to': 123238, 'to_side':    'r', 'mean_dist':    564, 'mapq':  60060, 'bcount':      2, 'min_dist':    552, 'max_dist':    575, 'probability': 0.003280, 'to_alt':      2, 'from_alt':      2},
        {'from': 123231, 'from_side':    'r', 'to': 123227, 'to_side':    'l', 'mean_dist':    549, 'mapq':  60060, 'bcount':     13, 'min_dist':    488, 'max_dist':    605, 'probability': 0.135136, 'to_alt':      2, 'from_alt':      1},
        {'from': 123236, 'from_side':    'l', 'to': 123227, 'to_side':    'r', 'mean_dist':    224, 'mapq':  60060, 'bcount':     12, 'min_dist':    191, 'max_dist':    374, 'probability': 0.064232, 'to_alt':      2, 'from_alt':      1},
        {'from': 123236, 'from_side':    'r', 'to': 123226, 'to_side':    'l', 'mean_dist':      2, 'mapq':  60060, 'bcount':      9, 'min_dist':      0, 'max_dist':      4, 'probability': 0.027818, 'to_alt':      2, 'from_alt':      1},
        {'from': 123237, 'from_side':    'l', 'to': 123226, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':      6, 'min_dist':     -3, 'max_dist':      9, 'probability': 0.010512, 'to_alt':      3, 'from_alt':      1},
        {'from': 123237, 'from_side':    'r', 'to': 123227, 'to_side':    'l', 'mean_dist':    542, 'mapq':  60060, 'bcount':      8, 'min_dist':    523, 'max_dist':    560, 'probability': 0.033108, 'to_alt':      2, 'from_alt':      1},
        {'from': 123238, 'from_side':    'l', 'to': 123226, 'to_side':    'r', 'mean_dist':      3, 'mapq':  60060, 'bcount':      8, 'min_dist':      1, 'max_dist':      9, 'probability': 0.020422, 'to_alt':      3, 'from_alt':      2},
        {'from': 123238, 'from_side':    'l', 'to': 123226, 'to_side':    'r', 'mean_dist':    400, 'mapq':  60060, 'bcount':      7, 'min_dist':    381, 'max_dist':    425, 'probability': 0.023635, 'to_alt':      3, 'from_alt':      2},
        {'from': 123238, 'from_side':    'r', 'to': 123225, 'to_side':    'l', 'mean_dist':    -42, 'mapq':  60060, 'bcount':      9, 'min_dist':    -48, 'max_dist':    -32, 'probability': 0.027818, 'to_alt':      2, 'from_alt':      2},
        {'from': 123238, 'from_side':    'r', 'to': 123231, 'to_side':    'l', 'mean_dist':    564, 'mapq':  60060, 'bcount':      2, 'min_dist':    552, 'max_dist':    575, 'probability': 0.003280, 'to_alt':      2, 'from_alt':      2}
        ]) )
    result_paths.append( pd.DataFrame({
        'pid':      [    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600],
        'pos':      [      0,      1,      2,      3,      4,      5,      6,      7,      8,      9,     10,     11,     12,     13,     14,     15,     16,     17,     18,     19],
        'phase0':   [    601,    601,    601,    601,    601,    601,    601,    601,    601,    601,    601,    601,    601,    601,    601,    601,    601,    601,    601,    601],
        'scaf0':    [   9064, 123226, 123238, 123225, 123231, 123227, 123236, 123226, 123237, 123227, 123230, 123225, 123229, 123224,  51925,  41269, 123224, 123228,  51925,  67414],
        'strand0':  [    '-',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '-',    '+',    '+',    '+',    '+'],
        'dist0':    [      0,     -5,    400,    -42,    -42,    549,    224,      2,      0,    542,    196,    -40,    -42,    179,    965,      4,   -519,     43,     55,     87],
        'phase1':   [   -602,   -602,    602,    602,    602,   -602,   -602,   -602,   -602,   -602,   -602,    602,    602,   -602,   -602,   -602,   -602,   -602,   -602,   -602],
        'scaf1':    [     -1,     -1, 123238,     -1, 123231,     -1,     -1,     -1,     -1,     -1,     -1,     -1, 123229,     -1,     -1,     -1,     -1,     -1,     -1,     -1],
        'strand1':  [     '',     '',    '+',     '',    '+',     '',     '',     '',     '',     '',     '',     '',    '+',     '',     '',     '',     '',     '',     '',     ''],
        'dist1':    [      0,      0,      3,      0,    564,      0,      0,      0,      0,      0,      0,      0,    590,      0,      0,      0,      0,      0,      0,      0]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    600,    600,    600],
        'pos':      [      0,      1,      2],
        'scaf0':    [ 123238, 123225, 123231],
        'strand0':  [    '+',    '+',    '+'],
        'dist0':    [      0,    -42,    -42]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    601,    601,    601,    601,    601,    601,    601],
        'pos':      [      0,      1,      2,      3,      4,      5,      6],
        'scaf0':    [ 123231, 123227, 123236, 123226, 123237, 123227, 123230],
        'strand0':  [    '+',    '+',    '+',    '+',    '+',    '+',    '+'],
        'dist0':    [      0,    549,    224,      2,      0,    542,    196]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    602,    602,    602],
        'pos':      [      0,      1,      2],
        'scaf0':    [ 123230, 123225, 123229],
        'strand0':  [    '+',    '+',    '+'],
        'dist0':    [      0,    -40,    -42]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    603,    603,    603,    603,    603,    603,    603,    603],
        'pos':      [      0,      1,      2,      3,      4,      5,      6,      7],
        'scaf0':    [ 123229, 123224,  51925,  41269, 123224, 123228,  51925,  67414],
        'strand0':  [    '+',    '+',    '+',    '-',    '+',    '+',    '+',    '+'],
        'dist0':    [      0,    179,    965,      4,   -519,     43,     55,     87]
        }) )
    result_loops.append( pd.DataFrame({
        'pid':      [    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600,    600],
        'pos':      [      0,      1,      2,      3,      4,      5,      6,      7,      8,      9,     10,     11,     12],
        'scaf0':    [   9064, 123226, 123238, 123225, 123231, 123227, 123236, 123226, 123237, 123227, 123230, 123225, 123229],
        'strand0':  [    '-',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '+',    '+'],
        'dist0':    [      0,     -5,      3,    -42,    -42,    549,    224,      2,      0,    542,    196,    -40,    -42]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    600,    600],
        'pos':      [      0,      1],
        'scaf0':    [ 123226, 123238],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,    400]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    601,    601],
        'pos':      [      0,      1],
        'scaf0':    [ 123238, 123231],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,    564]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    602,    602],
        'pos':      [      0,      1],
        'scaf0':    [ 123230, 123229],
        'strand0':  [    '+',    '+'],
        'dist0':    [      0,    590]
        }) )
#
    # Test 7
    scaffolds.append( pd.DataFrame({'case':7, 'scaffold':[17428, 48692, 123617]}) )
    scaffold_graph.append( pd.DataFrame([
        {'from':  17428, 'from_side':    'l', 'length':      2, 'scaf1':  17428, 'strand1':    '+', 'dist1':  28194},
        {'from':  17428, 'from_side':    'r', 'length':      2, 'scaf1':  48692, 'strand1':    '-', 'dist1':   1916},
        {'from':  17428, 'from_side':    'r', 'length':      3, 'scaf1': 123617, 'strand1':    '-', 'dist1':    -44, 'scaf2':  48692, 'strand2':    '-', 'dist2':    -38},
        {'from':  48692, 'from_side':    'r', 'length':      2, 'scaf1':  17428, 'strand1':    '-', 'dist1':   1916},
        {'from':  48692, 'from_side':    'r', 'length':      3, 'scaf1': 123617, 'strand1':    '+', 'dist1':    -38, 'scaf2':  17428, 'strand2':    '-', 'dist2':    -44},
        {'from': 123617, 'from_side':    'l', 'length':      2, 'scaf1':  48692, 'strand1':    '-', 'dist1':    -38},
        {'from': 123617, 'from_side':    'r', 'length':      2, 'scaf1':  17428, 'strand1':    '-', 'dist1':    -44}
        ]) )
    scaf_bridges.append( pd.DataFrame([
        {'from':  17428, 'from_side':    'l', 'to':  17428, 'to_side':    'l', 'mean_dist':  28194, 'mapq':  60060, 'bcount':      2, 'min_dist':  28194, 'max_dist':  28194, 'probability': 0.500000, 'to_alt':      1, 'from_alt':      1},
        {'from':  17428, 'from_side':    'r', 'to':  48692, 'to_side':    'r', 'mean_dist':   1916, 'mapq':  60060, 'bcount':      1, 'min_dist':   1916, 'max_dist':   1916, 'probability': 0.003098, 'to_alt':      2, 'from_alt':      2},
        {'from':  17428, 'from_side':    'r', 'to': 123617, 'to_side':    'r', 'mean_dist':    -44, 'mapq':  60060, 'bcount':      6, 'min_dist':    -50, 'max_dist':    -28, 'probability': 0.010512, 'to_alt':      1, 'from_alt':      2},
        {'from':  48692, 'from_side':    'r', 'to':  17428, 'to_side':    'r', 'mean_dist':   1916, 'mapq':  60060, 'bcount':      1, 'min_dist':   1916, 'max_dist':   1916, 'probability': 0.003098, 'to_alt':      2, 'from_alt':      2},
        {'from':  48692, 'from_side':    'r', 'to': 123617, 'to_side':    'l', 'mean_dist':    -38, 'mapq':  60060, 'bcount':      5, 'min_dist':    -48, 'max_dist':    -17, 'probability': 0.007368, 'to_alt':      1, 'from_alt':      2},
        {'from': 123617, 'from_side':    'l', 'to':  48692, 'to_side':    'r', 'mean_dist':    -38, 'mapq':  60060, 'bcount':      5, 'min_dist':    -48, 'max_dist':    -17, 'probability': 0.007368, 'to_alt':      2, 'from_alt':      1},
        {'from': 123617, 'from_side':    'r', 'to':  17428, 'to_side':    'r', 'mean_dist':    -44, 'mapq':  60060, 'bcount':      6, 'min_dist':    -50, 'max_dist':    -28, 'probability': 0.010512, 'to_alt':      2, 'from_alt':      1}
        ]) )
    result_paths.append( pd.DataFrame({
        'pid':      [    700,    700,    700,    700],
        'pos':      [      0,      1,      2,      3],
        'phase0':   [    701,    701,    701,    701],
        'scaf0':    [  48692, 123617,  17428,  17428],
        'strand0':  [    '+',    '+',    '-',    '+'],
        'dist0':    [      0,    -38,    -44,  28194],
        'phase1':   [   -702,    702,    702,   -704],
        'scaf1':    [     -1,     -1,  17428,     -1],
        'strand1':  [     '',     '',    '-',     ''],
        'dist1':    [      0,      0,   1916,      0]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    700,    700,    700],
        'pos':      [      0,      1,      2],
        'scaf0':    [  48692, 123617,  17428],
        'strand0':  [    '+',    '+',    '-'],
        'dist0':    [      0,    -38,    -44]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    700,    700],
        'pos':      [      0,      1],
        'scaf0':    [  48692,  17428],
        'strand0':  [    '+',    '-'],
        'dist0':    [      0,   1916]
        }) )
    result_untraversed.append( pd.DataFrame({
        'pid':      [    701,    701],
        'pos':      [      0,      1],
        'scaf0':    [  17428,  17428],
        'strand0':  [    '-',    '+'],
        'dist0':    [      0,  28194]
        }) )
#
    # Test 8
    scaffolds.append( pd.DataFrame({'case':8, 'scaffold':[197, 198, 199, 39830]}) )
    scaffold_graph.append( pd.DataFrame([
        {'from':    197, 'from_side':    'r', 'length':      4, 'scaf1':    198, 'strand1':    '+', 'dist1':      0, 'scaf2':    199, 'strand2':    '+', 'dist2':   -616, 'scaf3':    198, 'strand3':    '+', 'dist3':      3},
        {'from':    198, 'from_side':    'l', 'length':      2, 'scaf1':    197, 'strand1':    '-', 'dist1':      0},
        {'from':    198, 'from_side':    'l', 'length':      4, 'scaf1':    199, 'strand1':    '-', 'dist1':      3, 'scaf2':    198, 'strand2':    '-', 'dist2':   -616, 'scaf3':    197, 'strand3':    '-', 'dist3':      0},
        {'from':    198, 'from_side':    'r', 'length':      3, 'scaf1':    199, 'strand1':    '+', 'dist1':   -616, 'scaf2':    198, 'strand2':    '+', 'dist2':      3},
        {'from':    198, 'from_side':    'r', 'length':      2, 'scaf1':  39830, 'strand1':    '-', 'dist1':      0},
        {'from':    199, 'from_side':    'l', 'length':      3, 'scaf1':    198, 'strand1':    '-', 'dist1':   -616, 'scaf2':    197, 'strand2':    '-', 'dist2':      0},
        {'from':    199, 'from_side':    'r', 'length':      3, 'scaf1':    198, 'strand1':    '+', 'dist1':      3, 'scaf2':  39830, 'strand2':    '-', 'dist2':      0},
        {'from':  39830, 'from_side':    'r', 'length':      3, 'scaf1':    198, 'strand1':    '-', 'dist1':      0, 'scaf2':    199, 'strand2':    '-', 'dist2':      3}
        ]) )
    scaf_bridges.append( pd.DataFrame([
        {'from':    197, 'from_side':    'r', 'to':    198, 'to_side':    'l', 'mean_dist':      0, 'mapq':  60060, 'bcount':     15, 'min_dist':      0, 'max_dist':      0, 'probability': 0.129976, 'to_alt':      2, 'from_alt':      1},
        {'from':    198, 'from_side':    'l', 'to':    197, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     15, 'min_dist':      0, 'max_dist':      0, 'probability': 0.129976, 'to_alt':      1, 'from_alt':      2},
        {'from':    198, 'from_side':    'l', 'to':    199, 'to_side':    'r', 'mean_dist':      3, 'mapq':  60060, 'bcount':     13, 'min_dist':     -1, 'max_dist':      5, 'probability': 0.082422, 'to_alt':      1, 'from_alt':      2},
        {'from':    198, 'from_side':    'r', 'to':    199, 'to_side':    'l', 'mean_dist':   -616, 'mapq':  60060, 'bcount':     12, 'min_dist':   -713, 'max_dist':   -568, 'probability': 0.064232, 'to_alt':      1, 'from_alt':      2},
        {'from':    198, 'from_side':    'r', 'to':  39830, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     13, 'min_dist':      0, 'max_dist':      0, 'probability': 0.082422, 'to_alt':      1, 'from_alt':      2},
        {'from':    199, 'from_side':    'l', 'to':    198, 'to_side':    'r', 'mean_dist':   -616, 'mapq':  60060, 'bcount':     12, 'min_dist':   -713, 'max_dist':   -568, 'probability': 0.064232, 'to_alt':      2, 'from_alt':      1},
        {'from':    199, 'from_side':    'r', 'to':    198, 'to_side':    'l', 'mean_dist':      3, 'mapq':  60060, 'bcount':     13, 'min_dist':     -1, 'max_dist':      5, 'probability': 0.082422, 'to_alt':      2, 'from_alt':      1},
        {'from':  39830, 'from_side':    'r', 'to':    198, 'to_side':    'r', 'mean_dist':      0, 'mapq':  60060, 'bcount':     13, 'min_dist':      0, 'max_dist':      0, 'probability': 0.082422, 'to_alt':      2, 'from_alt':      1}
        ]) )
    result_paths.append( pd.DataFrame({
        'pid':      [    800,    800,    800,    800,    800],
        'pos':      [      0,      1,      2,      3,      4],
        'phase0':   [    801,    801,    801,    801,    801],
        'scaf0':    [    197,    198,    199,    198,  39830],
        'strand0':  [    '+',    '+',    '+',    '+',    '-'],
        'dist0':    [      0,      0,   -616,      3,      0],
        'phase1':   [   -802,   -802,   -802,   -802,   -802],
        'scaf1':    [     -1,     -1,     -1,     -1,     -1],
        'strand1':  [     '',     '',     '',     '',     ''],
        'dist1':    [      0,      0,      0,      0,      0]
        }) )
    result_unique_paths.append( pd.DataFrame({
        'pid':      [    800,    800,    800,    800,    800],
        'pos':      [      0,      1,      2,      3,      4],
        'scaf0':    [    197,    198,    199,    198,  39830],
        'strand0':  [    '+',    '+',    '+',    '+',    '-'],
        'dist0':    [      0,      0,   -616,      3,      0]
        }) )
    result_loops.append( pd.DataFrame({
        'pid':      [    800,    800,    800,    800,    800],
        'pos':      [      0,      1,      2,      3,      4],
        'scaf0':    [    197,    198,    199,    198,  39830],
        'strand0':  [    '+',    '+',    '+',    '+',    '-'],
        'dist0':    [      0,      0,   -616,      3,      0]
        }) )
#
    # Combine tests
    scaffolds = pd.concat(scaffolds, ignore_index=True)
    scaffolds.index = scaffolds['scaffold'].values
    scaffolds['left'] = scaffolds['scaffold']
    scaffolds['right'] = scaffolds['scaffold']
    scaffolds[['lside','rside','lextendible','rextendible','circular','size']] = ['l','r',True,True,False,1]
    scaffold_graph = pd.concat(scaffold_graph, ignore_index=True)
    scaf_bridges = pd.concat(scaf_bridges, ignore_index=True)
    org_scaf_conns = pd.concat(org_scaf_conns, ignore_index=True)
    result_paths = pd.concat(result_paths, ignore_index=True)
    if len(result_unique_paths):
        result_unique_paths = pd.concat(result_unique_paths, ignore_index=True)
    if len(result_loops):
        result_loops = pd.concat(result_loops, ignore_index=True)
    if len(result_inversions):
        result_inversions = pd.concat(result_inversions, ignore_index=True)
    if len(result_untraversed):
        result_untraversed = pd.concat(result_untraversed, ignore_index=True)
#
    # Consistency tests
    if len(scaffolds.drop_duplicates()) != len(scaffolds):
        check = scaffolds.groupby(['scaffold']).size()
        check = check[check > 1].reset_index()['scaffold'].values
        print("Warning: Double usage of scaffolds in test: {check}")
    CheckConsistencyOfScaffoldGraph(scaffold_graph)
#
    # Run function
    graph_ext = FindValidExtensionsInScaffoldGraph(scaffold_graph)
    unique_paths, handled_origins = FollowUniquePathsThroughGraph(graph_ext)
    loop_paths, handled_scaf_conns = AddPathThroughLoops([], scaffold_graph, graph_ext, scaf_bridges, org_scaf_conns, ploidy, max_loop_units)
    inversion_paths, handled_scaf_conns = AddPathThroughInvertedRepeats([], handled_scaf_conns, scaffold_graph, scaf_bridges, ploidy)
    untraversed_paths = AddUntraversedConnectedPaths([], graph_ext, handled_origins, handled_scaf_conns)
    scaffold_paths = TraverseScaffoldGraph(scaffolds.drop(columns=['case']), scaffold_graph, graph_ext, scaf_bridges, org_scaf_conns, ploidy, max_loop_units)
#
    # Compare and report failed tests
    failed = False
    org_ploidy = ploidy
    for t in np.unique(scaffolds['case']):
        for test_func in ["FollowUniquePathsThroughGraph","AddPathThroughLoops","AddPathThroughInvertedRepeats","AddUntraversedConnectedPaths","TraverseScaffoldGraph"]:
            if test_func == "FollowUniquePathsThroughGraph":
                res_paths = result_unique_paths
                scaf_paths = unique_paths
            elif test_func == "AddPathThroughLoops":
                res_paths = result_loops
                scaf_paths = loop_paths
            elif test_func == "AddPathThroughInvertedRepeats":
                res_paths = result_inversions
                scaf_paths = inversion_paths
            elif test_func == "AddUntraversedConnectedPaths":
                res_paths = result_untraversed
                scaf_paths = untraversed_paths
            elif test_func == "TraverseScaffoldGraph":
                res_paths = result_paths
                scaf_paths = scaffold_paths
#
            if len(res_paths) == 0:
                obtained_paths = scaf_paths
            else:
                if test_func == "TraverseScaffoldGraph":
                    ploidy = org_ploidy
                else:
                    if 'phase0' not in res_paths.columns:
                        res_paths.insert(2, 'phase0', res_paths['pid'].values+1)
                        scaf_paths.insert(2, 'phase0', scaf_paths['pid'].values+1)
                    ploidy = 1
#
                correct_paths = res_paths[np.isin(res_paths['pid'], np.unique(res_paths.loc[np.isin(res_paths['scaf0'], scaffolds.loc[scaffolds['case'] == t, 'scaffold'].values),'pid'].values))].copy()
                reversed_paths = correct_paths.copy()
                correct_paths['reverse'] = False
                reversed_paths['reverse'] = True
                reversed_paths = ReverseScaffolds(reversed_paths, reversed_paths['reverse'], ploidy)
                tmp_paths = pd.concat([correct_paths, reversed_paths], ignore_index=True)
                correct_haps = []
                for h in range(ploidy):
                    tmp_paths.loc[tmp_paths[f'phase{h}'] < 0, [f'scaf{h}',f'strand{h}',f'dist{h}']] = tmp_paths.loc[tmp_paths[f'phase{h}'] < 0, ['scaf0','strand0','dist0']].values
                    correct_haps.append( tmp_paths.loc[tmp_paths[f'scaf{h}'] >= 0, ['pid','reverse',f'scaf{h}',f'strand{h}',f'dist{h}']].rename(columns={'pid':'cpid',f'scaf{h}':'scaf',f'strand{h}':'strand',f'dist{h}':'dist'}) )
                    correct_haps[-1]['hap'] = h
                correct_haps = pd.concat(correct_haps, ignore_index=True)
                correct_haps['pos'] = correct_haps.groupby(['cpid','hap','reverse'], sort=False).cumcount()
                obtained_paths = scaf_paths[np.isin(scaf_paths['pid'], np.unique(scaf_paths.loc[np.isin(scaf_paths['scaf0'], scaffolds.loc[scaffolds['case'] == t, 'scaffold'].values),'pid'].values))].copy()
                tmp_paths = obtained_paths.copy()
                obtained_haps = []
                for h in range(ploidy):
                    tmp_paths.loc[tmp_paths[f'phase{h}'] < 0, [f'scaf{h}',f'strand{h}',f'dist{h}']] = tmp_paths.loc[tmp_paths[f'phase{h}'] < 0, ['scaf0','strand0','dist0']].values
                    obtained_haps.append( tmp_paths.loc[tmp_paths[f'scaf{h}'] >= 0, ['pid',f'scaf{h}',f'strand{h}',f'dist{h}']].rename(columns={'pid':'opid',f'scaf{h}':'scaf',f'strand{h}':'strand',f'dist{h}':'dist'}) )
                    obtained_haps[-1]['hap'] = h
                obtained_haps = pd.concat(obtained_haps, ignore_index=True)
                obtained_haps['pos'] = obtained_haps.groupby(['opid','hap'], sort=False).cumcount()
                comp = correct_haps.merge(obtained_haps, on=['hap','pos','scaf','strand','dist'], how='inner')
                comp = comp.groupby(['cpid','opid','hap','reverse']).size().reset_index(name='bcount').groupby(['cpid','opid','hap'])[['bcount']].max().reset_index()
                comp = correct_haps[correct_haps['reverse']].groupby(['cpid','hap']).size().reset_index(name='ccount').merge(comp, on=['cpid','hap'], how='inner')
                comp = comp[comp['ccount'] == comp['bcount']].merge( obtained_haps.groupby(['opid','hap']).size().reset_index(name='ocount'), on=['opid','hap'], how='left')
                if test_func == "FollowUniquePathsThroughGraph":
                    comp = comp[comp['ccount'] == comp['ocount']].copy() # In FollowUniquePathsThroughGraph we do produce every path in both directions, so do accept both here
                else:
                    comp = comp[comp['ccount'] == comp['ocount']].groupby(['cpid','hap']).first().reset_index()
                comp = comp.groupby(['cpid','opid']).size().reset_index(name='nhaps')
                comp = comp[comp['nhaps'] == ploidy].copy()
                correct_paths = correct_paths[np.isin(correct_paths['pid'], comp['cpid']) == False].drop(columns=['reverse'])
                obtained_paths = obtained_paths[np.isin(obtained_paths['pid'], comp['opid']) == False].copy()
#
            if len(correct_paths) | len(obtained_paths):
                print(f"TestTraverseScaffoldGraph: Test case {t} failed for {test_func}.")
                if len(correct_paths):
                    print("Unmatched correct paths:")
                    print(correct_paths)
                if len(obtained_paths):
                    print("Unmatched obtained paths:")
                    print(obtained_paths)
                failed = True
#
    return failed

def GaplessTest():
    failed_tests = 0
    failed_tests += TestFindBreakPoints()
    failed_tests += TestDuplicationConflictResolution()
    failed_tests += TestFilterInvalidConnections()
    failed_tests += TestTraverseScaffoldGraph()
    
    if failed_tests == 0:
        print("All tests succeeded.")
    else:
        print(failed_tests, "tests failed.")

def Version():
    print("Program: gapless")
    print("Version: 0.1")
    print("Contact: Stephan Schmeing <stephan.schmeing@uzh.ch>")

def Usage(module=""):
    if "" == module:
        Version()
        print()
        print("Usage:  gapless.py <module> [options]")
        print("Modules:")
        print("split         Splits scaffolds into contigs")
        print("scaffold      Scaffolds contigs and assigns reads to gaps")
        print("extend        Extend scaffold ends")
        print("finish        Create new fasta assembly file")
        print("visualize     Visualizes regions to manually inspect breaks or joins")
        print("test          Short test")
    elif "split" == module:
        print("Usage: gapless.py split [OPTIONS] {assembly}.fa")
        print("Splits scaffolds into contigs.")
        print("  -h, --help                Display this help and exit")
        print("  -n, --minN [int]          Minimum number of N's to split at that position (1)")
        print("  -o, --output FILE.fa      File to which the split sequences should be written to ({assembly}_split.fa)")
    elif "scaffold" == module:
        print("Usage: gapless.py scaffold [OPTIONS] {assembly}.fa {mapping}.paf {repeat}.paf")
        print("Scaffolds contigs and assigns reads to gaps.")
        print("  -h, --help                Display this help and exit")
        print("  -p, --prefix FILE         Prefix for output files ({assembly})")
        print("  -s, --stats FILE.pdf      Output file for plots with statistics regarding input parameters (deactivated)")
        print("      --minLenBreak INT     Minimum length for a read to diverge from a contig to consider a contig break (600)")
        print("      --minMapLength INT    Minimum length of individual mappings of reads (400)")
        print("      --minMapQ INT         Minimum mapping quality of reads (20)")
    elif "extend" == module:
        print("Usage: gapless.py extend -p {prefix} {all_vs_all}.paf")
        print("Extend scaffold ends with reads reaching over the ends.")
        print("  -h, --help                Display this help and exit")
        print("  -p, --prefix FILE         Prefix for output files of scaffolding step (mandatory)")
        print("      --minLenBreak INT     Minimum length for two reads to diverge to consider them incompatible for this contig (1000)")
    elif "finish" == module:
        print("Usage: gapless.py finish [OPTIONS] -s {scaffolds}.csv {assembly}.fa {reads}.fq")
        print("Creates previously defined scaffolds. Only providing necessary reads increases speed and substantially reduces memory requirements.")
        print("  -h, --help                Display this help and exit")
        print("  -f, --format FORMAT       Format of {reads}.fq (fasta/fastq) (Default: fastq if not determinable from read ending)")
        print("  -H, --hap INT             Haplotype starting from 0 written to {output} (default: mixed)")
        print("  --hap[1-9] INT            Haplotypes starting from 0 written to {out[1-9]} (default: mixed)")
        print("  -o, --output FILE.fa      Output file for modified assembly ({assembly}_gapless.fa)")
        print("  --out[1-9] FILE.fa        Additional output files for modified assembly (deactivated)")
        print("  -s, --scaffolds FILE.csv  Csv file from previous steps describing the scaffolding (mandatory)")
    elif "visualize" == module:
        print("Usage: gapless.py visualize [OPTIONS] -o {output}.pdf {mapping}.paf {scaffold}:{start}-{end} [{scaffold}:{start}-{end} ...]")
        print("Visualizes specified regions to manually inspect breaks or joins.")
        print("  -h, --help                Display this help and exit")
        print("  -o, --output FILE.pdf     Output file for visualization (mandatory)")
        print("      --keepAllSubreads     Shows all subreads instead of only the best")
        print("      --minLenBreak INT     Minimum length for a read to diverge from a contig to consider a contig break (600)")
        print("      --minMapLength INT    Minimum length of individual mappings of reads (400)")
        print("      --minMapQ INT         Minimum mapping quality of reads (20)")
        
def main(argv):
    if 0 == len(argv):
        Usage()
        sys.exit()
    
    module = argv[0]
    argv.pop(0)
    if "-h" == module or "--help" == module:
        Usage()
        sys.exit()
    if "--version" == module:
        Version()
        sys.exit()
    if "split" == module:
        try:
            optlist, args = getopt.getopt(argv, 'hn:o:', ['help','minN=','output=','version'])
        except getopt.GetoptError:
            print("Unknown option")
            Usage(module)
            sys.exit(1)
    
        o_file = False
        min_n = False
        for opt, par in optlist:
            if opt in ("-h", "--help"):
                Usage(module)
                sys.exit()
            elif opt in ("-n", "--minN"):
                try:
                    min_n = int(par)
                except ValueError:
                    print("-n,--minN option only accepts integers")
                    sys.exit(1)
            elif opt in ("-o", "--output"):
                o_file = par
            elif opt == "--version":
                Version()
                sys.exit()

        if 1 != len(args):
            print("Wrong number of files. Exactly one file is required.")
            Usage(module)
            sys.exit(1)
    
        GaplessSplit(args[0],o_file,min_n)
    elif "scaffold" == module:
        try:
            optlist, args = getopt.getopt(argv, 'hp:s:', ['help','prefix=','stats=','--minLenBreak=','minMapLength=','minMapQ=','version'])
        except getopt.GetoptError:
            print("Unknown option\n")
            Usage(module)
            sys.exit(1)
            
        prefix = False
        stats = None
        min_length_contig_break = 700
        min_mapping_length = 500
        min_mapq = 20
        for opt, par in optlist:
            if opt in ("-h", "--help"):
                Usage(module)
                sys.exit()
            elif opt in ("-p", "--prefix"):
                prefix = par
            elif opt in ("-s", "--stats"):
                stats = par
                if stats[-4:] != ".pdf":
                    print("stats argument needs to end on .pdf")
                    Usage(module)
                    sys.exit(1)
            elif opt == "--minLenBreak":
                try:
                    min_length_contig_break = int(par)
                except ValueError:
                    print("--minLenBreak option only accepts integers")
                    sys.exit(1)
            elif opt == "--minMapLength":
                try:
                    min_mapping_length = int(par)
                except ValueError:
                    print("--minMapLength option only accepts integers")
                    sys.exit(1)
            elif opt == "--minMapQ":
                try:
                    min_mapq = int(par)
                except ValueError:
                    print("--minMapQ option only accepts integers")
                    sys.exit(1)
            elif opt == "--version":
                Version()
                sys.exit()

                    
        if 3 != len(args):
            print("Wrong number of files. Exactly three files are required.\n")
            Usage(module)
            sys.exit(2)

        GaplessScaffold(args[0], args[1], args[2], min_mapq, min_mapping_length, min_length_contig_break, prefix, stats)
    elif "extend" == module:
        try:
            optlist, args = getopt.getopt(argv, 'hp:', ['help','prefix=','--minLenBreak','version'])
        except getopt.GetoptError:
            print("Unknown option\n")
            Usage(module)
            sys.exit(1)
            
        prefix = False
        min_length_contig_break = 1200
        for opt, par in optlist:
            if opt in ("-h", "--help"):
                Usage(module)
                sys.exit()
            elif opt in ("-p", "--prefix"):
                prefix = par
            elif opt == "--minLenBreak":
                try:
                    min_length_contig_break = int(par)
                except ValueError:
                    print("--minLenBreak option only accepts integers")
                    sys.exit(1)
            elif opt == "--version":
                Version()
                sys.exit()
                    
        if 1 != len(args):
            print("Wrong number of files. Exactly one file is required.\n")
            Usage(module)
            sys.exit(2)
            
        if False == prefix:
            print("prefix argument is mandatory")
            Usage(module)
            sys.exit(1)

        GaplessExtend(args[0], prefix, min_length_contig_break)
    elif "finish" == module:
        num_slots = 10
        try:
            optlist, args = getopt.getopt(argv, 'hf:H:o:s:', ['help','format=','hap=','output=','scaffolds=','version']+[f'hap{i}=' for i in range(1,num_slots)]+[f'out{i}=' for i in range(1,num_slots)])
        except getopt.GetoptError:
            print("Unknown option\n")
            Usage(module)
            sys.exit(1)
        
        read_format = False
        output = [False]*num_slots
        haplotypes = [-1]*num_slots
        scaffolds = False
        for opt, par in optlist:
            if opt in ("-h", "--help"):
                Usage(module)
                sys.exit()
            elif opt in ("-f", "--format"):
                if 'fasta' == par.lower() or 'fa' == par.lower():
                    read_format = 'fasta'
                elif 'fastq' == par.lower() or 'fq' == par.lower():
                    pass # default
                else:
                    print("Unsupported read format: {}.".format(par))
                    Usage(module)
                    sys.exit(1)
            elif opt in ("-H", "--hap"):
                haplotypes[0] = int(par)
            elif opt in [f'--hap{i}' for i in range(1,num_slots)]:
                haplotypes[int(opt[5:])] = int(par)
            elif opt in ("-o", "--output"):
                output[0] = par
            elif opt in [f'--out{i}' for i in range(1,num_slots)]:
                output[int(opt[5:])] = par
            elif opt in ("-s", "--scaffolds"):
                scaffolds = par
            elif opt == "--version":
                Version()
                sys.exit()
        
        if 2 != len(args):
            print("Wrong number of files. Exactly two files are required.")
            Usage(module)
            sys.exit(2)
            
        if False == scaffolds:
            print("scaffolds argument is mandatory")
            Usage(module)
            sys.exit(1)
            
        selected_output = [output[0]]
        selected_haplotypes = [haplotypes[0]]
        for i in range(1,num_slots):
            if output[i] == False:
                if haplotypes[i] != -1:
                    print(f"--hap{i} was specified without --out{i}")
                    Usage(module)
                    sys.exit(1)
            else:
                selected_output.append(output[i])
                selected_haplotypes.append(haplotypes[i])

        GaplessFinish(args[0], args[1], read_format, scaffolds, selected_output, selected_haplotypes)
    elif "visualize" == module:
        try:
            optlist, args = getopt.getopt(argv, 'ho:', ['help','output=','--keepAllSubreads','--minLenBreak=','minMapLength=','minMapQ=','version'])
        except getopt.GetoptError:
            print("Unknown option\n")
            Usage(module)
            sys.exit(1)
            
        output = False
        keep_all_subreads = False
        min_length_contig_break = 700
        min_mapping_length = 500
        min_mapq = 20
        for opt, par in optlist:
            if opt in ("-h", "--help"):
                Usage(module)
                sys.exit()
            elif opt in ("-o", "--output"):
                output = par
            elif opt == "--keepAllSubreads":
                keep_all_subreads = True
            elif opt == "--minLenBreak":
                try:
                    min_length_contig_break = int(par)
                except ValueError:
                    print("--minLenBreak option only accepts integers")
                    sys.exit(1)
            elif opt == "--minMapLength":
                try:
                    min_mapping_length = int(par)
                except ValueError:
                    print("--minMapLength option only accepts integers")
                    sys.exit(1)
            elif opt == "--minMapQ":
                try:
                    min_mapq = int(par)
                except ValueError:
                    print("--minMapQ option only accepts integers")
                    sys.exit(1)
            elif opt == "--version":
                Version()
                sys.exit()
            
        if 2 > len(args):
            print("Wrong number of arguments. The mapping file and at least one region definition are needed.\n")
            Usage(module)
            sys.exit(2)
            
        if False == output:
            print("output argument is mandatory")
            Usage(module)
            sys.exit(1)
            
        GaplessVisualize(args[1:], args[0], output, min_mapq, min_mapping_length, min_length_contig_break, keep_all_subreads)
    elif "test" == module:
        GaplessTest()
    else:
        print("Unknown module: {}.".format(module))
        Usage()
        sys.exit(1)

if __name__ == "__main__":
    main(sys.argv[1:])
